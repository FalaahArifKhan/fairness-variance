{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ded11654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5d85e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T10:55:54.052462Z",
     "start_time": "2024-01-06T10:55:54.038357Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall virny -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf024b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T10:56:09.679156Z",
     "start_time": "2024-01-06T10:56:09.668186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install using an HTTP link\n",
    "# !pip install git+https://github.com/DataResponsibly/Virny.git@feature/prepare_experiments_for_inprocessors\n",
    "\n",
    "# Install using an SSH link\n",
    "# !pip install git+ssh://git@github.com/DataResponsibly/Virny.git@feature/prepare_experiments_for_inprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39463327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T12:55:34.372035Z",
     "start_time": "2024-01-08T12:55:34.034676Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d83962",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T12:55:34.382218Z",
     "start_time": "2024-01-08T12:55:34.372780Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295621db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T12:55:34.875781Z",
     "start_time": "2024-01-08T12:55:34.863823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current location:  /Users/denys_herasymuk/UCU/4course_2term/Bachelor_Thesis/Code/fairness-variance\n"
     ]
    }
   ],
   "source": [
    "cur_folder_name = os.getcwd().split('/')[-1]\n",
    "if cur_folder_name != \"fairness-variance\":\n",
    "    os.chdir(\"../../../..\")\n",
    "\n",
    "print('Current location: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d6dfa",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65442379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T12:55:38.507808Z",
     "start_time": "2024-01-08T12:55:35.139022Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from virny.utils.custom_initializers import create_config_obj\n",
    "from virny.datasets import LawSchoolDataset\n",
    "\n",
    "from configs.constants import TEST_SET_FRACTION, EXPERIMENT_SEEDS\n",
    "\n",
    "from source.experiment_interface import run_exp_iter_with_inprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb488976",
   "metadata": {},
   "source": [
    "## Define Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1f91c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T12:55:38.550130Z",
     "start_time": "2024-01-08T12:55:38.508960Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "EXPERIMENT_NAME = 'ADB_law_school'\n",
    "DB_COLLECTION_NAME = 'one_repair_lvl_many_models'\n",
    "FAIRNESS_INTERVENTION_NAME = 'ADB'\n",
    "FAIR_INTERVENTION_PARAMS_LST = ['debiased_classifier']\n",
    "SAVE_RESULTS_DIR_PATH = os.path.join(ROOT_DIR, 'results', 'diff_fairness_interventions_exp',\n",
    "                                     FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME)\n",
    "\n",
    "config_yaml_path = os.path.join(ROOT_DIR, 'notebooks', 'diff_fairness_interventions_exp',\n",
    "                                FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME, 'law_school_config.yaml')\n",
    "metrics_computation_config = create_config_obj(config_yaml_path=config_yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42b20f",
   "metadata": {},
   "source": [
    "## Define a db writer and custom fields to insert into your database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ed4e58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T12:55:38.592276Z",
     "start_time": "2024-01-08T12:55:38.550311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'fairness_variance'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./configs/secrets.env')\n",
    "os.getenv(\"DB_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2539023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T12:55:38.788647Z",
     "start_time": "2024-01-08T12:55:38.591Z"
    }
   },
   "outputs": [],
   "source": [
    "from source.utils.db_functions import connect_to_mongodb\n",
    "\n",
    "client, collection_obj, db_writer_func = connect_to_mongodb(DB_COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9682b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T12:55:38.830247Z",
     "start_time": "2024-01-08T12:55:38.789667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current session uuid:  0c501743-e9d5-4c52-8d94-4e1e83fcd983\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "custom_table_fields_dct = {\n",
    "    # 'session_uuid': str(uuid.uuid4()),\n",
    "    'session_uuid': '0c501743-e9d5-4c52-8d94-4e1e83fcd983',\n",
    "}\n",
    "print('Current session uuid: ', custom_table_fields_dct['session_uuid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b32200",
   "metadata": {},
   "source": [
    "## Initialize custom objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7470042d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T12:55:39.001349Z",
     "start_time": "2024-01-08T12:55:38.827638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   decile1b  decile3  lsat  ugpa  zfygpa  zgpa fulltime fam_inc male tier  \\\n0      10.0     10.0  44.0   3.5    1.33  1.88      1.0     5.0  0.0  4.0   \n1       5.0      4.0  29.0   3.5   -0.11 -0.57      1.0     4.0  0.0  2.0   \n2       8.0      7.0  37.0   3.4    0.63  0.37      1.0     3.0  1.0  4.0   \n3       8.0      7.0  43.0   3.3    0.67  0.34      1.0     4.0  0.0  4.0   \n4       3.0      2.0  41.0   3.3   -0.67 -1.30      1.0     4.0  0.0  5.0   \n\n    race  \n0  White  \n1  White  \n2  White  \n3  White  \n4  White  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>decile1b</th>\n      <th>decile3</th>\n      <th>lsat</th>\n      <th>ugpa</th>\n      <th>zfygpa</th>\n      <th>zgpa</th>\n      <th>fulltime</th>\n      <th>fam_inc</th>\n      <th>male</th>\n      <th>tier</th>\n      <th>race</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10.0</td>\n      <td>10.0</td>\n      <td>44.0</td>\n      <td>3.5</td>\n      <td>1.33</td>\n      <td>1.88</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>White</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>29.0</td>\n      <td>3.5</td>\n      <td>-0.11</td>\n      <td>-0.57</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>White</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.0</td>\n      <td>7.0</td>\n      <td>37.0</td>\n      <td>3.4</td>\n      <td>0.63</td>\n      <td>0.37</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>White</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8.0</td>\n      <td>7.0</td>\n      <td>43.0</td>\n      <td>3.3</td>\n      <td>0.67</td>\n      <td>0.34</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>White</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>41.0</td>\n      <td>3.3</td>\n      <td>-0.67</td>\n      <td>-1.30</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>White</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = LawSchoolDataset()\n",
    "data_loader.X_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cce54a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T12:55:39.050147Z",
     "start_time": "2024-01-08T12:55:38.998867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(20798, 11)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.X_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c8f07",
   "metadata": {},
   "source": [
    "## Run experiment iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced15cb",
   "metadata": {},
   "source": [
    "### Experiment iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9302f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T12:55:39.088917Z",
     "start_time": "2024-01-08T12:55:39.037655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 1\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26d52fce",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-01-08T13:00:22.581022Z",
     "start_time": "2024-01-08T12:55:39.876602Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:55:39 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 100,\n",
      " 'experiment_iteration': 'Exp_iter_1',\n",
      " 'fair_intervention_params_lst': \"['debiased_classifier']\",\n",
      " 'model_init_seed': 100,\n",
      " 'session_uuid': '0c501743-e9d5-4c52-8d94-4e1e83fcd983'}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52283ec9f011482b95348a2fee31b7e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:55:40 experiment_interface.py INFO    : The dataset is preprocessed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  debiased_classifier\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__fulltime_1.0', 'cat__fulltime_2.0', 'cat__fam_inc_1.0',\n",
      "       'cat__fam_inc_2.0', 'cat__fam_inc_3.0', 'cat__fam_inc_4.0',\n",
      "       'cat__fam_inc_5.0', 'cat__tier_1.0', 'cat__tier_2.0', 'cat__tier_3.0',\n",
      "       'cat__tier_4.0', 'cat__tier_5.0', 'cat__tier_6.0', 'num__decile1b',\n",
      "       'num__decile3', 'num__lsat', 'num__ugpa', 'num__zfygpa', 'num__zgpa',\n",
      "       'male&race_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([ 7102,   593, 18841,  5078, 14172,  8064, 13554, 13401, 17015,\n",
      "            18446,  6938,  3450,  9375, 19994, 16100,  4401,   142, 15143,\n",
      "             2188,  4332],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([ 7102,   593, 18841,  5078, 14172,  8064, 13554, 13401, 17015,\n",
      "            18446,  6938,  3450,  9375, 19994, 16100,  4401,   142, 15143,\n",
      "             2188,  4332],\n",
      "           dtype='int64')\n",
      "Using AdversarialDebiasing postprocessor\n"
     ]
    },
    {
     "data": {
      "text/plain": "Analyze multiple models:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d19ebae36efb4b899f648e99cd8e9f7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Classifiers testing by bootstrap:   0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50e4deec5d1744d09a3109497d92087d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/denys_herasymuk/UCU/4course_2term/Bachelor_Thesis/Code/fairness-variance/faact_venv/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:55:40 deprecation.py     WARNING : From /Users/denys_herasymuk/UCU/4course_2term/Bachelor_Thesis/Code/fairness-variance/faact_venv/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2024-01-08 14:55:40.213801: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.761597; batch adversarial loss: 0.815945\n",
      "epoch 1; iter: 0; batch classifier loss: 0.323079; batch adversarial loss: 0.831027\n",
      "epoch 2; iter: 0; batch classifier loss: 0.242589; batch adversarial loss: 0.694889\n",
      "epoch 3; iter: 0; batch classifier loss: 0.193245; batch adversarial loss: 0.590680\n",
      "epoch 4; iter: 0; batch classifier loss: 0.291019; batch adversarial loss: 0.551831\n",
      "epoch 5; iter: 0; batch classifier loss: 0.163354; batch adversarial loss: 0.447815\n",
      "epoch 6; iter: 0; batch classifier loss: 0.250530; batch adversarial loss: 0.447888\n",
      "epoch 7; iter: 0; batch classifier loss: 0.226480; batch adversarial loss: 0.391707\n",
      "epoch 8; iter: 0; batch classifier loss: 0.255874; batch adversarial loss: 0.349011\n",
      "epoch 9; iter: 0; batch classifier loss: 0.232453; batch adversarial loss: 0.326950\n",
      "epoch 10; iter: 0; batch classifier loss: 0.236332; batch adversarial loss: 0.296181\n",
      "epoch 11; iter: 0; batch classifier loss: 0.263208; batch adversarial loss: 0.267161\n",
      "epoch 12; iter: 0; batch classifier loss: 0.184368; batch adversarial loss: 0.246096\n",
      "epoch 13; iter: 0; batch classifier loss: 0.323518; batch adversarial loss: 0.322369\n",
      "epoch 14; iter: 0; batch classifier loss: 0.189106; batch adversarial loss: 0.240284\n",
      "epoch 15; iter: 0; batch classifier loss: 0.210784; batch adversarial loss: 0.341062\n",
      "epoch 16; iter: 0; batch classifier loss: 0.258605; batch adversarial loss: 0.267052\n",
      "epoch 17; iter: 0; batch classifier loss: 0.181987; batch adversarial loss: 0.217984\n",
      "epoch 18; iter: 0; batch classifier loss: 0.200896; batch adversarial loss: 0.286493\n",
      "epoch 19; iter: 0; batch classifier loss: 0.284793; batch adversarial loss: 0.219162\n",
      "epoch 20; iter: 0; batch classifier loss: 0.218902; batch adversarial loss: 0.334611\n",
      "epoch 21; iter: 0; batch classifier loss: 0.144743; batch adversarial loss: 0.289515\n",
      "epoch 22; iter: 0; batch classifier loss: 0.219113; batch adversarial loss: 0.246272\n",
      "epoch 23; iter: 0; batch classifier loss: 0.252571; batch adversarial loss: 0.216878\n",
      "epoch 24; iter: 0; batch classifier loss: 0.186365; batch adversarial loss: 0.205182\n",
      "epoch 25; iter: 0; batch classifier loss: 0.208296; batch adversarial loss: 0.321963\n",
      "epoch 26; iter: 0; batch classifier loss: 0.223619; batch adversarial loss: 0.288387\n",
      "epoch 27; iter: 0; batch classifier loss: 0.210935; batch adversarial loss: 0.132026\n",
      "epoch 28; iter: 0; batch classifier loss: 0.278617; batch adversarial loss: 0.316739\n",
      "epoch 29; iter: 0; batch classifier loss: 0.269260; batch adversarial loss: 0.162707\n",
      "epoch 30; iter: 0; batch classifier loss: 0.312650; batch adversarial loss: 0.229674\n",
      "epoch 31; iter: 0; batch classifier loss: 0.196248; batch adversarial loss: 0.201686\n",
      "epoch 32; iter: 0; batch classifier loss: 0.174098; batch adversarial loss: 0.307132\n",
      "epoch 33; iter: 0; batch classifier loss: 0.178585; batch adversarial loss: 0.242095\n",
      "epoch 34; iter: 0; batch classifier loss: 0.222271; batch adversarial loss: 0.238510\n",
      "epoch 35; iter: 0; batch classifier loss: 0.204651; batch adversarial loss: 0.327091\n",
      "epoch 36; iter: 0; batch classifier loss: 0.217397; batch adversarial loss: 0.381032\n",
      "epoch 37; iter: 0; batch classifier loss: 0.264956; batch adversarial loss: 0.296101\n",
      "epoch 38; iter: 0; batch classifier loss: 0.247712; batch adversarial loss: 0.278068\n",
      "epoch 39; iter: 0; batch classifier loss: 0.283782; batch adversarial loss: 0.199759\n",
      "epoch 40; iter: 0; batch classifier loss: 0.248687; batch adversarial loss: 0.221477\n",
      "epoch 41; iter: 0; batch classifier loss: 0.172739; batch adversarial loss: 0.258904\n",
      "epoch 42; iter: 0; batch classifier loss: 0.253750; batch adversarial loss: 0.210734\n",
      "epoch 43; iter: 0; batch classifier loss: 0.336995; batch adversarial loss: 0.230219\n",
      "epoch 44; iter: 0; batch classifier loss: 0.281394; batch adversarial loss: 0.269832\n",
      "epoch 45; iter: 0; batch classifier loss: 0.174405; batch adversarial loss: 0.233505\n",
      "epoch 46; iter: 0; batch classifier loss: 0.203759; batch adversarial loss: 0.196526\n",
      "epoch 47; iter: 0; batch classifier loss: 0.214237; batch adversarial loss: 0.214070\n",
      "epoch 48; iter: 0; batch classifier loss: 0.227404; batch adversarial loss: 0.323472\n",
      "epoch 49; iter: 0; batch classifier loss: 0.170932; batch adversarial loss: 0.275759\n",
      "epoch 50; iter: 0; batch classifier loss: 0.150144; batch adversarial loss: 0.291949\n",
      "epoch 51; iter: 0; batch classifier loss: 0.258888; batch adversarial loss: 0.405409\n",
      "epoch 52; iter: 0; batch classifier loss: 0.298633; batch adversarial loss: 0.325521\n",
      "epoch 53; iter: 0; batch classifier loss: 0.223998; batch adversarial loss: 0.246865\n",
      "epoch 54; iter: 0; batch classifier loss: 0.215587; batch adversarial loss: 0.227266\n",
      "epoch 55; iter: 0; batch classifier loss: 0.198596; batch adversarial loss: 0.257108\n",
      "epoch 56; iter: 0; batch classifier loss: 0.235724; batch adversarial loss: 0.212275\n",
      "epoch 57; iter: 0; batch classifier loss: 0.205746; batch adversarial loss: 0.293663\n",
      "epoch 58; iter: 0; batch classifier loss: 0.209207; batch adversarial loss: 0.212207\n",
      "epoch 59; iter: 0; batch classifier loss: 0.166349; batch adversarial loss: 0.217884\n",
      "epoch 60; iter: 0; batch classifier loss: 0.163800; batch adversarial loss: 0.299858\n",
      "epoch 61; iter: 0; batch classifier loss: 0.297925; batch adversarial loss: 0.362259\n",
      "epoch 62; iter: 0; batch classifier loss: 0.316060; batch adversarial loss: 0.197126\n",
      "epoch 63; iter: 0; batch classifier loss: 0.196580; batch adversarial loss: 0.215650\n",
      "epoch 64; iter: 0; batch classifier loss: 0.209626; batch adversarial loss: 0.154688\n",
      "epoch 65; iter: 0; batch classifier loss: 0.203802; batch adversarial loss: 0.325943\n",
      "epoch 66; iter: 0; batch classifier loss: 0.146119; batch adversarial loss: 0.379604\n",
      "epoch 67; iter: 0; batch classifier loss: 0.286014; batch adversarial loss: 0.272945\n",
      "epoch 68; iter: 0; batch classifier loss: 0.182409; batch adversarial loss: 0.272204\n",
      "epoch 69; iter: 0; batch classifier loss: 0.308349; batch adversarial loss: 0.258722\n",
      "epoch 70; iter: 0; batch classifier loss: 0.234780; batch adversarial loss: 0.275681\n",
      "epoch 71; iter: 0; batch classifier loss: 0.251700; batch adversarial loss: 0.184271\n",
      "epoch 72; iter: 0; batch classifier loss: 0.192529; batch adversarial loss: 0.223861\n",
      "epoch 73; iter: 0; batch classifier loss: 0.197020; batch adversarial loss: 0.306995\n",
      "epoch 74; iter: 0; batch classifier loss: 0.222266; batch adversarial loss: 0.248391\n",
      "epoch 75; iter: 0; batch classifier loss: 0.234354; batch adversarial loss: 0.177596\n",
      "epoch 76; iter: 0; batch classifier loss: 0.217634; batch adversarial loss: 0.276130\n",
      "epoch 77; iter: 0; batch classifier loss: 0.167085; batch adversarial loss: 0.218397\n",
      "epoch 78; iter: 0; batch classifier loss: 0.216165; batch adversarial loss: 0.244003\n",
      "epoch 79; iter: 0; batch classifier loss: 0.130808; batch adversarial loss: 0.225495\n",
      "epoch 80; iter: 0; batch classifier loss: 0.215866; batch adversarial loss: 0.336443\n",
      "epoch 81; iter: 0; batch classifier loss: 0.255343; batch adversarial loss: 0.258868\n",
      "epoch 82; iter: 0; batch classifier loss: 0.272020; batch adversarial loss: 0.270308\n",
      "epoch 83; iter: 0; batch classifier loss: 0.222573; batch adversarial loss: 0.183768\n",
      "epoch 84; iter: 0; batch classifier loss: 0.116928; batch adversarial loss: 0.239931\n",
      "epoch 85; iter: 0; batch classifier loss: 0.199600; batch adversarial loss: 0.251714\n",
      "epoch 86; iter: 0; batch classifier loss: 0.269857; batch adversarial loss: 0.244607\n",
      "epoch 87; iter: 0; batch classifier loss: 0.300285; batch adversarial loss: 0.180509\n",
      "epoch 88; iter: 0; batch classifier loss: 0.196782; batch adversarial loss: 0.222996\n",
      "epoch 89; iter: 0; batch classifier loss: 0.205945; batch adversarial loss: 0.386465\n",
      "epoch 90; iter: 0; batch classifier loss: 0.224686; batch adversarial loss: 0.366966\n",
      "epoch 91; iter: 0; batch classifier loss: 0.214865; batch adversarial loss: 0.299259\n",
      "epoch 92; iter: 0; batch classifier loss: 0.239537; batch adversarial loss: 0.232178\n",
      "epoch 93; iter: 0; batch classifier loss: 0.144869; batch adversarial loss: 0.289774\n",
      "epoch 94; iter: 0; batch classifier loss: 0.270794; batch adversarial loss: 0.321981\n",
      "epoch 95; iter: 0; batch classifier loss: 0.266094; batch adversarial loss: 0.218105\n",
      "epoch 96; iter: 0; batch classifier loss: 0.240023; batch adversarial loss: 0.334258\n",
      "epoch 97; iter: 0; batch classifier loss: 0.197765; batch adversarial loss: 0.214244\n",
      "epoch 98; iter: 0; batch classifier loss: 0.275102; batch adversarial loss: 0.300798\n",
      "epoch 99; iter: 0; batch classifier loss: 0.208935; batch adversarial loss: 0.342497\n",
      "epoch 100; iter: 0; batch classifier loss: 0.255215; batch adversarial loss: 0.227681\n",
      "epoch 101; iter: 0; batch classifier loss: 0.128676; batch adversarial loss: 0.218838\n",
      "epoch 102; iter: 0; batch classifier loss: 0.200663; batch adversarial loss: 0.286521\n",
      "epoch 103; iter: 0; batch classifier loss: 0.204718; batch adversarial loss: 0.303552\n",
      "epoch 104; iter: 0; batch classifier loss: 0.174994; batch adversarial loss: 0.346696\n",
      "epoch 105; iter: 0; batch classifier loss: 0.298595; batch adversarial loss: 0.270922\n",
      "epoch 106; iter: 0; batch classifier loss: 0.216885; batch adversarial loss: 0.243785\n",
      "epoch 107; iter: 0; batch classifier loss: 0.148916; batch adversarial loss: 0.242748\n",
      "epoch 108; iter: 0; batch classifier loss: 0.177884; batch adversarial loss: 0.282571\n",
      "epoch 109; iter: 0; batch classifier loss: 0.195436; batch adversarial loss: 0.268404\n",
      "epoch 110; iter: 0; batch classifier loss: 0.242890; batch adversarial loss: 0.234866\n",
      "epoch 111; iter: 0; batch classifier loss: 0.178931; batch adversarial loss: 0.199434\n",
      "epoch 112; iter: 0; batch classifier loss: 0.200389; batch adversarial loss: 0.196346\n",
      "epoch 113; iter: 0; batch classifier loss: 0.199736; batch adversarial loss: 0.235942\n",
      "epoch 114; iter: 0; batch classifier loss: 0.228745; batch adversarial loss: 0.269383\n",
      "epoch 115; iter: 0; batch classifier loss: 0.179192; batch adversarial loss: 0.267116\n",
      "epoch 116; iter: 0; batch classifier loss: 0.306114; batch adversarial loss: 0.202621\n",
      "epoch 117; iter: 0; batch classifier loss: 0.145860; batch adversarial loss: 0.259712\n",
      "epoch 118; iter: 0; batch classifier loss: 0.215663; batch adversarial loss: 0.274527\n",
      "epoch 119; iter: 0; batch classifier loss: 0.198041; batch adversarial loss: 0.234360\n",
      "epoch 120; iter: 0; batch classifier loss: 0.151721; batch adversarial loss: 0.285997\n",
      "epoch 121; iter: 0; batch classifier loss: 0.233568; batch adversarial loss: 0.180379\n",
      "epoch 122; iter: 0; batch classifier loss: 0.204894; batch adversarial loss: 0.289824\n",
      "epoch 123; iter: 0; batch classifier loss: 0.169584; batch adversarial loss: 0.182898\n",
      "epoch 124; iter: 0; batch classifier loss: 0.244807; batch adversarial loss: 0.372166\n",
      "epoch 125; iter: 0; batch classifier loss: 0.230746; batch adversarial loss: 0.365027\n",
      "epoch 126; iter: 0; batch classifier loss: 0.298612; batch adversarial loss: 0.324902\n",
      "epoch 127; iter: 0; batch classifier loss: 0.177346; batch adversarial loss: 0.317651\n",
      "epoch 128; iter: 0; batch classifier loss: 0.211656; batch adversarial loss: 0.266093\n",
      "epoch 129; iter: 0; batch classifier loss: 0.192015; batch adversarial loss: 0.186213\n",
      "epoch 130; iter: 0; batch classifier loss: 0.193153; batch adversarial loss: 0.217326\n",
      "epoch 131; iter: 0; batch classifier loss: 0.250936; batch adversarial loss: 0.258436\n",
      "epoch 132; iter: 0; batch classifier loss: 0.244892; batch adversarial loss: 0.310022\n",
      "epoch 133; iter: 0; batch classifier loss: 0.218497; batch adversarial loss: 0.235138\n",
      "epoch 134; iter: 0; batch classifier loss: 0.243510; batch adversarial loss: 0.254775\n",
      "epoch 135; iter: 0; batch classifier loss: 0.181856; batch adversarial loss: 0.272771\n",
      "epoch 136; iter: 0; batch classifier loss: 0.113920; batch adversarial loss: 0.307511\n",
      "epoch 137; iter: 0; batch classifier loss: 0.176151; batch adversarial loss: 0.271680\n",
      "epoch 138; iter: 0; batch classifier loss: 0.209556; batch adversarial loss: 0.318163\n",
      "epoch 139; iter: 0; batch classifier loss: 0.191438; batch adversarial loss: 0.317614\n",
      "epoch 140; iter: 0; batch classifier loss: 0.162887; batch adversarial loss: 0.264103\n",
      "epoch 141; iter: 0; batch classifier loss: 0.220635; batch adversarial loss: 0.258295\n",
      "epoch 142; iter: 0; batch classifier loss: 0.096737; batch adversarial loss: 0.191830\n",
      "epoch 143; iter: 0; batch classifier loss: 0.200377; batch adversarial loss: 0.231882\n",
      "epoch 144; iter: 0; batch classifier loss: 0.300201; batch adversarial loss: 0.188563\n",
      "epoch 145; iter: 0; batch classifier loss: 0.156641; batch adversarial loss: 0.204525\n",
      "epoch 146; iter: 0; batch classifier loss: 0.210201; batch adversarial loss: 0.294952\n",
      "epoch 147; iter: 0; batch classifier loss: 0.277109; batch adversarial loss: 0.412237\n",
      "epoch 148; iter: 0; batch classifier loss: 0.179711; batch adversarial loss: 0.273724\n",
      "epoch 149; iter: 0; batch classifier loss: 0.132318; batch adversarial loss: 0.325162\n",
      "epoch 150; iter: 0; batch classifier loss: 0.158765; batch adversarial loss: 0.209997\n",
      "epoch 151; iter: 0; batch classifier loss: 0.279504; batch adversarial loss: 0.236698\n",
      "epoch 152; iter: 0; batch classifier loss: 0.212654; batch adversarial loss: 0.271049\n",
      "epoch 153; iter: 0; batch classifier loss: 0.196133; batch adversarial loss: 0.317769\n",
      "epoch 154; iter: 0; batch classifier loss: 0.232471; batch adversarial loss: 0.398362\n",
      "epoch 155; iter: 0; batch classifier loss: 0.229092; batch adversarial loss: 0.264390\n",
      "epoch 156; iter: 0; batch classifier loss: 0.154747; batch adversarial loss: 0.323025\n",
      "epoch 157; iter: 0; batch classifier loss: 0.137128; batch adversarial loss: 0.277583\n",
      "epoch 158; iter: 0; batch classifier loss: 0.143993; batch adversarial loss: 0.260559\n",
      "epoch 159; iter: 0; batch classifier loss: 0.181926; batch adversarial loss: 0.290070\n",
      "epoch 160; iter: 0; batch classifier loss: 0.223096; batch adversarial loss: 0.343793\n",
      "epoch 161; iter: 0; batch classifier loss: 0.210357; batch adversarial loss: 0.191590\n",
      "epoch 162; iter: 0; batch classifier loss: 0.185850; batch adversarial loss: 0.254488\n",
      "epoch 163; iter: 0; batch classifier loss: 0.168747; batch adversarial loss: 0.262932\n",
      "epoch 164; iter: 0; batch classifier loss: 0.171729; batch adversarial loss: 0.254033\n",
      "epoch 165; iter: 0; batch classifier loss: 0.158394; batch adversarial loss: 0.268920\n",
      "epoch 166; iter: 0; batch classifier loss: 0.177614; batch adversarial loss: 0.187344\n",
      "epoch 167; iter: 0; batch classifier loss: 0.126098; batch adversarial loss: 0.197136\n",
      "epoch 168; iter: 0; batch classifier loss: 0.187675; batch adversarial loss: 0.231401\n",
      "epoch 169; iter: 0; batch classifier loss: 0.163577; batch adversarial loss: 0.394984\n",
      "epoch 170; iter: 0; batch classifier loss: 0.149338; batch adversarial loss: 0.260768\n",
      "epoch 171; iter: 0; batch classifier loss: 0.213401; batch adversarial loss: 0.270144\n",
      "epoch 172; iter: 0; batch classifier loss: 0.146639; batch adversarial loss: 0.228555\n",
      "epoch 173; iter: 0; batch classifier loss: 0.295382; batch adversarial loss: 0.361004\n",
      "epoch 174; iter: 0; batch classifier loss: 0.144060; batch adversarial loss: 0.294803\n",
      "epoch 175; iter: 0; batch classifier loss: 0.191696; batch adversarial loss: 0.251446\n",
      "epoch 176; iter: 0; batch classifier loss: 0.163886; batch adversarial loss: 0.260320\n",
      "epoch 177; iter: 0; batch classifier loss: 0.195987; batch adversarial loss: 0.177979\n",
      "epoch 178; iter: 0; batch classifier loss: 0.191845; batch adversarial loss: 0.429615\n",
      "epoch 179; iter: 0; batch classifier loss: 0.168142; batch adversarial loss: 0.207788\n",
      "epoch 180; iter: 0; batch classifier loss: 0.306779; batch adversarial loss: 0.196690\n",
      "epoch 181; iter: 0; batch classifier loss: 0.205757; batch adversarial loss: 0.281004\n",
      "epoch 182; iter: 0; batch classifier loss: 0.290913; batch adversarial loss: 0.321731\n",
      "epoch 183; iter: 0; batch classifier loss: 0.253706; batch adversarial loss: 0.292239\n",
      "epoch 184; iter: 0; batch classifier loss: 0.119299; batch adversarial loss: 0.415938\n",
      "epoch 185; iter: 0; batch classifier loss: 0.232564; batch adversarial loss: 0.315014\n",
      "epoch 186; iter: 0; batch classifier loss: 0.276536; batch adversarial loss: 0.277135\n",
      "epoch 187; iter: 0; batch classifier loss: 0.151870; batch adversarial loss: 0.179718\n",
      "epoch 188; iter: 0; batch classifier loss: 0.282456; batch adversarial loss: 0.263897\n",
      "epoch 189; iter: 0; batch classifier loss: 0.241394; batch adversarial loss: 0.267115\n",
      "epoch 190; iter: 0; batch classifier loss: 0.180936; batch adversarial loss: 0.262875\n",
      "epoch 191; iter: 0; batch classifier loss: 0.245427; batch adversarial loss: 0.354614\n",
      "epoch 192; iter: 0; batch classifier loss: 0.194215; batch adversarial loss: 0.225995\n",
      "epoch 193; iter: 0; batch classifier loss: 0.204355; batch adversarial loss: 0.259211\n",
      "epoch 194; iter: 0; batch classifier loss: 0.218639; batch adversarial loss: 0.327277\n",
      "epoch 195; iter: 0; batch classifier loss: 0.210628; batch adversarial loss: 0.223829\n",
      "epoch 196; iter: 0; batch classifier loss: 0.206227; batch adversarial loss: 0.218271\n",
      "epoch 197; iter: 0; batch classifier loss: 0.157947; batch adversarial loss: 0.334418\n",
      "epoch 198; iter: 0; batch classifier loss: 0.308298; batch adversarial loss: 0.389322\n",
      "epoch 199; iter: 0; batch classifier loss: 0.234887; batch adversarial loss: 0.332276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:55:53.768340: W tensorflow/c/c_api.cc:305] Operation '{name:'39b90f08-ae25-11ee-bc15-ae7d8bf09116/39b90f08-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:767 op device:{requested: '', assigned: ''} def:{{{node 39b90f08-ae25-11ee-bc15-ae7d8bf09116/39b90f08-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b90f08-ae25-11ee-bc15-ae7d8bf09116/39b90f08-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b90f08-ae25-11ee-bc15-ae7d8bf09116/39b90f08-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.693674; batch adversarial loss: 0.532959\n",
      "epoch 1; iter: 0; batch classifier loss: 0.314255; batch adversarial loss: 0.416482\n",
      "epoch 2; iter: 0; batch classifier loss: 0.201104; batch adversarial loss: 0.354058\n",
      "epoch 3; iter: 0; batch classifier loss: 0.196339; batch adversarial loss: 0.357758\n",
      "epoch 4; iter: 0; batch classifier loss: 0.221233; batch adversarial loss: 0.307689\n",
      "epoch 5; iter: 0; batch classifier loss: 0.239851; batch adversarial loss: 0.280206\n",
      "epoch 6; iter: 0; batch classifier loss: 0.325623; batch adversarial loss: 0.388515\n",
      "epoch 7; iter: 0; batch classifier loss: 0.197389; batch adversarial loss: 0.237166\n",
      "epoch 8; iter: 0; batch classifier loss: 0.183431; batch adversarial loss: 0.299330\n",
      "epoch 9; iter: 0; batch classifier loss: 0.222760; batch adversarial loss: 0.194992\n",
      "epoch 10; iter: 0; batch classifier loss: 0.278461; batch adversarial loss: 0.243771\n",
      "epoch 11; iter: 0; batch classifier loss: 0.252559; batch adversarial loss: 0.282527\n",
      "epoch 12; iter: 0; batch classifier loss: 0.253431; batch adversarial loss: 0.285247\n",
      "epoch 13; iter: 0; batch classifier loss: 0.285733; batch adversarial loss: 0.277832\n",
      "epoch 14; iter: 0; batch classifier loss: 0.238586; batch adversarial loss: 0.199582\n",
      "epoch 15; iter: 0; batch classifier loss: 0.230450; batch adversarial loss: 0.209794\n",
      "epoch 16; iter: 0; batch classifier loss: 0.346407; batch adversarial loss: 0.371602\n",
      "epoch 17; iter: 0; batch classifier loss: 0.250692; batch adversarial loss: 0.298347\n",
      "epoch 18; iter: 0; batch classifier loss: 0.215327; batch adversarial loss: 0.315582\n",
      "epoch 19; iter: 0; batch classifier loss: 0.231903; batch adversarial loss: 0.303616\n",
      "epoch 20; iter: 0; batch classifier loss: 0.226391; batch adversarial loss: 0.229519\n",
      "epoch 21; iter: 0; batch classifier loss: 0.207642; batch adversarial loss: 0.239240\n",
      "epoch 22; iter: 0; batch classifier loss: 0.202293; batch adversarial loss: 0.276406\n",
      "epoch 23; iter: 0; batch classifier loss: 0.264135; batch adversarial loss: 0.308959\n",
      "epoch 24; iter: 0; batch classifier loss: 0.228776; batch adversarial loss: 0.338100\n",
      "epoch 25; iter: 0; batch classifier loss: 0.239754; batch adversarial loss: 0.194544\n",
      "epoch 26; iter: 0; batch classifier loss: 0.259789; batch adversarial loss: 0.206528\n",
      "epoch 27; iter: 0; batch classifier loss: 0.225879; batch adversarial loss: 0.252424\n",
      "epoch 28; iter: 0; batch classifier loss: 0.206446; batch adversarial loss: 0.162804\n",
      "epoch 29; iter: 0; batch classifier loss: 0.239430; batch adversarial loss: 0.296558\n",
      "epoch 30; iter: 0; batch classifier loss: 0.204635; batch adversarial loss: 0.254519\n",
      "epoch 31; iter: 0; batch classifier loss: 0.294082; batch adversarial loss: 0.312109\n",
      "epoch 32; iter: 0; batch classifier loss: 0.281329; batch adversarial loss: 0.284410\n",
      "epoch 33; iter: 0; batch classifier loss: 0.215777; batch adversarial loss: 0.210533\n",
      "epoch 34; iter: 0; batch classifier loss: 0.197765; batch adversarial loss: 0.221366\n",
      "epoch 35; iter: 0; batch classifier loss: 0.245664; batch adversarial loss: 0.362396\n",
      "epoch 36; iter: 0; batch classifier loss: 0.225735; batch adversarial loss: 0.216764\n",
      "epoch 37; iter: 0; batch classifier loss: 0.286678; batch adversarial loss: 0.284294\n",
      "epoch 38; iter: 0; batch classifier loss: 0.212903; batch adversarial loss: 0.212352\n",
      "epoch 39; iter: 0; batch classifier loss: 0.192458; batch adversarial loss: 0.272274\n",
      "epoch 40; iter: 0; batch classifier loss: 0.268068; batch adversarial loss: 0.293957\n",
      "epoch 41; iter: 0; batch classifier loss: 0.280368; batch adversarial loss: 0.200033\n",
      "epoch 42; iter: 0; batch classifier loss: 0.215449; batch adversarial loss: 0.256774\n",
      "epoch 43; iter: 0; batch classifier loss: 0.243799; batch adversarial loss: 0.287418\n",
      "epoch 44; iter: 0; batch classifier loss: 0.166006; batch adversarial loss: 0.148101\n",
      "epoch 45; iter: 0; batch classifier loss: 0.268295; batch adversarial loss: 0.213725\n",
      "epoch 46; iter: 0; batch classifier loss: 0.234470; batch adversarial loss: 0.244133\n",
      "epoch 47; iter: 0; batch classifier loss: 0.311608; batch adversarial loss: 0.311679\n",
      "epoch 48; iter: 0; batch classifier loss: 0.228430; batch adversarial loss: 0.242401\n",
      "epoch 49; iter: 0; batch classifier loss: 0.224324; batch adversarial loss: 0.317771\n",
      "epoch 50; iter: 0; batch classifier loss: 0.304444; batch adversarial loss: 0.235624\n",
      "epoch 51; iter: 0; batch classifier loss: 0.174741; batch adversarial loss: 0.190045\n",
      "epoch 52; iter: 0; batch classifier loss: 0.178269; batch adversarial loss: 0.179432\n",
      "epoch 53; iter: 0; batch classifier loss: 0.298939; batch adversarial loss: 0.271067\n",
      "epoch 54; iter: 0; batch classifier loss: 0.180093; batch adversarial loss: 0.268824\n",
      "epoch 55; iter: 0; batch classifier loss: 0.196220; batch adversarial loss: 0.175794\n",
      "epoch 56; iter: 0; batch classifier loss: 0.240436; batch adversarial loss: 0.331364\n",
      "epoch 57; iter: 0; batch classifier loss: 0.176133; batch adversarial loss: 0.238273\n",
      "epoch 58; iter: 0; batch classifier loss: 0.196234; batch adversarial loss: 0.144125\n",
      "epoch 59; iter: 0; batch classifier loss: 0.253810; batch adversarial loss: 0.372477\n",
      "epoch 60; iter: 0; batch classifier loss: 0.195865; batch adversarial loss: 0.233809\n",
      "epoch 61; iter: 0; batch classifier loss: 0.233480; batch adversarial loss: 0.249392\n",
      "epoch 62; iter: 0; batch classifier loss: 0.311741; batch adversarial loss: 0.246602\n",
      "epoch 63; iter: 0; batch classifier loss: 0.273273; batch adversarial loss: 0.208946\n",
      "epoch 64; iter: 0; batch classifier loss: 0.174924; batch adversarial loss: 0.189096\n",
      "epoch 65; iter: 0; batch classifier loss: 0.221788; batch adversarial loss: 0.272339\n",
      "epoch 66; iter: 0; batch classifier loss: 0.166165; batch adversarial loss: 0.272755\n",
      "epoch 67; iter: 0; batch classifier loss: 0.215432; batch adversarial loss: 0.182079\n",
      "epoch 68; iter: 0; batch classifier loss: 0.194084; batch adversarial loss: 0.201656\n",
      "epoch 69; iter: 0; batch classifier loss: 0.199264; batch adversarial loss: 0.370101\n",
      "epoch 70; iter: 0; batch classifier loss: 0.199650; batch adversarial loss: 0.174686\n",
      "epoch 71; iter: 0; batch classifier loss: 0.196828; batch adversarial loss: 0.235905\n",
      "epoch 72; iter: 0; batch classifier loss: 0.176196; batch adversarial loss: 0.154935\n",
      "epoch 73; iter: 0; batch classifier loss: 0.202813; batch adversarial loss: 0.277852\n",
      "epoch 74; iter: 0; batch classifier loss: 0.271264; batch adversarial loss: 0.252297\n",
      "epoch 75; iter: 0; batch classifier loss: 0.140714; batch adversarial loss: 0.255426\n",
      "epoch 76; iter: 0; batch classifier loss: 0.136586; batch adversarial loss: 0.156761\n",
      "epoch 77; iter: 0; batch classifier loss: 0.198826; batch adversarial loss: 0.327385\n",
      "epoch 78; iter: 0; batch classifier loss: 0.214604; batch adversarial loss: 0.300107\n",
      "epoch 79; iter: 0; batch classifier loss: 0.214471; batch adversarial loss: 0.282538\n",
      "epoch 80; iter: 0; batch classifier loss: 0.168918; batch adversarial loss: 0.217089\n",
      "epoch 81; iter: 0; batch classifier loss: 0.173721; batch adversarial loss: 0.208181\n",
      "epoch 82; iter: 0; batch classifier loss: 0.190505; batch adversarial loss: 0.266380\n",
      "epoch 83; iter: 0; batch classifier loss: 0.248916; batch adversarial loss: 0.309089\n",
      "epoch 84; iter: 0; batch classifier loss: 0.281739; batch adversarial loss: 0.276016\n",
      "epoch 85; iter: 0; batch classifier loss: 0.293760; batch adversarial loss: 0.262956\n",
      "epoch 86; iter: 0; batch classifier loss: 0.173344; batch adversarial loss: 0.353940\n",
      "epoch 87; iter: 0; batch classifier loss: 0.188754; batch adversarial loss: 0.176500\n",
      "epoch 88; iter: 0; batch classifier loss: 0.124029; batch adversarial loss: 0.301700\n",
      "epoch 89; iter: 0; batch classifier loss: 0.218763; batch adversarial loss: 0.211229\n",
      "epoch 90; iter: 0; batch classifier loss: 0.193532; batch adversarial loss: 0.270212\n",
      "epoch 91; iter: 0; batch classifier loss: 0.176310; batch adversarial loss: 0.263007\n",
      "epoch 92; iter: 0; batch classifier loss: 0.180855; batch adversarial loss: 0.267725\n",
      "epoch 93; iter: 0; batch classifier loss: 0.259550; batch adversarial loss: 0.322598\n",
      "epoch 94; iter: 0; batch classifier loss: 0.227098; batch adversarial loss: 0.275692\n",
      "epoch 95; iter: 0; batch classifier loss: 0.135526; batch adversarial loss: 0.248089\n",
      "epoch 96; iter: 0; batch classifier loss: 0.139190; batch adversarial loss: 0.377920\n",
      "epoch 97; iter: 0; batch classifier loss: 0.213952; batch adversarial loss: 0.237725\n",
      "epoch 98; iter: 0; batch classifier loss: 0.208053; batch adversarial loss: 0.215030\n",
      "epoch 99; iter: 0; batch classifier loss: 0.227353; batch adversarial loss: 0.348947\n",
      "epoch 100; iter: 0; batch classifier loss: 0.217433; batch adversarial loss: 0.293700\n",
      "epoch 101; iter: 0; batch classifier loss: 0.158354; batch adversarial loss: 0.186054\n",
      "epoch 102; iter: 0; batch classifier loss: 0.127957; batch adversarial loss: 0.196873\n",
      "epoch 103; iter: 0; batch classifier loss: 0.193039; batch adversarial loss: 0.283982\n",
      "epoch 104; iter: 0; batch classifier loss: 0.162645; batch adversarial loss: 0.306978\n",
      "epoch 105; iter: 0; batch classifier loss: 0.160419; batch adversarial loss: 0.281953\n",
      "epoch 106; iter: 0; batch classifier loss: 0.216977; batch adversarial loss: 0.340596\n",
      "epoch 107; iter: 0; batch classifier loss: 0.230700; batch adversarial loss: 0.201067\n",
      "epoch 108; iter: 0; batch classifier loss: 0.209659; batch adversarial loss: 0.174771\n",
      "epoch 109; iter: 0; batch classifier loss: 0.213859; batch adversarial loss: 0.223774\n",
      "epoch 110; iter: 0; batch classifier loss: 0.133335; batch adversarial loss: 0.192250\n",
      "epoch 111; iter: 0; batch classifier loss: 0.168928; batch adversarial loss: 0.320329\n",
      "epoch 112; iter: 0; batch classifier loss: 0.184153; batch adversarial loss: 0.306854\n",
      "epoch 113; iter: 0; batch classifier loss: 0.157700; batch adversarial loss: 0.255314\n",
      "epoch 114; iter: 0; batch classifier loss: 0.178475; batch adversarial loss: 0.113877\n",
      "epoch 115; iter: 0; batch classifier loss: 0.257038; batch adversarial loss: 0.282288\n",
      "epoch 116; iter: 0; batch classifier loss: 0.262303; batch adversarial loss: 0.190007\n",
      "epoch 117; iter: 0; batch classifier loss: 0.281022; batch adversarial loss: 0.198561\n",
      "epoch 118; iter: 0; batch classifier loss: 0.242285; batch adversarial loss: 0.234663\n",
      "epoch 119; iter: 0; batch classifier loss: 0.194578; batch adversarial loss: 0.311910\n",
      "epoch 120; iter: 0; batch classifier loss: 0.258986; batch adversarial loss: 0.344300\n",
      "epoch 121; iter: 0; batch classifier loss: 0.229373; batch adversarial loss: 0.326597\n",
      "epoch 122; iter: 0; batch classifier loss: 0.137440; batch adversarial loss: 0.241983\n",
      "epoch 123; iter: 0; batch classifier loss: 0.137854; batch adversarial loss: 0.277914\n",
      "epoch 124; iter: 0; batch classifier loss: 0.235358; batch adversarial loss: 0.230299\n",
      "epoch 125; iter: 0; batch classifier loss: 0.223004; batch adversarial loss: 0.175792\n",
      "epoch 126; iter: 0; batch classifier loss: 0.172092; batch adversarial loss: 0.270762\n",
      "epoch 127; iter: 0; batch classifier loss: 0.227227; batch adversarial loss: 0.221277\n",
      "epoch 128; iter: 0; batch classifier loss: 0.213831; batch adversarial loss: 0.292127\n",
      "epoch 129; iter: 0; batch classifier loss: 0.175374; batch adversarial loss: 0.205899\n",
      "epoch 130; iter: 0; batch classifier loss: 0.168927; batch adversarial loss: 0.264234\n",
      "epoch 131; iter: 0; batch classifier loss: 0.203530; batch adversarial loss: 0.332246\n",
      "epoch 132; iter: 0; batch classifier loss: 0.175640; batch adversarial loss: 0.167699\n",
      "epoch 133; iter: 0; batch classifier loss: 0.151143; batch adversarial loss: 0.293634\n",
      "epoch 134; iter: 0; batch classifier loss: 0.167285; batch adversarial loss: 0.295263\n",
      "epoch 135; iter: 0; batch classifier loss: 0.239692; batch adversarial loss: 0.298206\n",
      "epoch 136; iter: 0; batch classifier loss: 0.179379; batch adversarial loss: 0.180205\n",
      "epoch 137; iter: 0; batch classifier loss: 0.209120; batch adversarial loss: 0.234466\n",
      "epoch 138; iter: 0; batch classifier loss: 0.197207; batch adversarial loss: 0.252289\n",
      "epoch 139; iter: 0; batch classifier loss: 0.216528; batch adversarial loss: 0.303124\n",
      "epoch 140; iter: 0; batch classifier loss: 0.117614; batch adversarial loss: 0.235112\n",
      "epoch 141; iter: 0; batch classifier loss: 0.260462; batch adversarial loss: 0.272124\n",
      "epoch 142; iter: 0; batch classifier loss: 0.192604; batch adversarial loss: 0.285678\n",
      "epoch 143; iter: 0; batch classifier loss: 0.193588; batch adversarial loss: 0.284111\n",
      "epoch 144; iter: 0; batch classifier loss: 0.173221; batch adversarial loss: 0.382724\n",
      "epoch 145; iter: 0; batch classifier loss: 0.178481; batch adversarial loss: 0.428478\n",
      "epoch 146; iter: 0; batch classifier loss: 0.149242; batch adversarial loss: 0.284813\n",
      "epoch 147; iter: 0; batch classifier loss: 0.143099; batch adversarial loss: 0.194000\n",
      "epoch 148; iter: 0; batch classifier loss: 0.165585; batch adversarial loss: 0.353984\n",
      "epoch 149; iter: 0; batch classifier loss: 0.156362; batch adversarial loss: 0.195734\n",
      "epoch 150; iter: 0; batch classifier loss: 0.250934; batch adversarial loss: 0.322131\n",
      "epoch 151; iter: 0; batch classifier loss: 0.177921; batch adversarial loss: 0.221218\n",
      "epoch 152; iter: 0; batch classifier loss: 0.238885; batch adversarial loss: 0.202745\n",
      "epoch 153; iter: 0; batch classifier loss: 0.161289; batch adversarial loss: 0.208544\n",
      "epoch 154; iter: 0; batch classifier loss: 0.196499; batch adversarial loss: 0.173380\n",
      "epoch 155; iter: 0; batch classifier loss: 0.219866; batch adversarial loss: 0.266642\n",
      "epoch 156; iter: 0; batch classifier loss: 0.193882; batch adversarial loss: 0.274444\n",
      "epoch 157; iter: 0; batch classifier loss: 0.249735; batch adversarial loss: 0.207192\n",
      "epoch 158; iter: 0; batch classifier loss: 0.176363; batch adversarial loss: 0.323509\n",
      "epoch 159; iter: 0; batch classifier loss: 0.245252; batch adversarial loss: 0.219752\n",
      "epoch 160; iter: 0; batch classifier loss: 0.201210; batch adversarial loss: 0.286915\n",
      "epoch 161; iter: 0; batch classifier loss: 0.204709; batch adversarial loss: 0.194316\n",
      "epoch 162; iter: 0; batch classifier loss: 0.188309; batch adversarial loss: 0.220472\n",
      "epoch 163; iter: 0; batch classifier loss: 0.176214; batch adversarial loss: 0.150828\n",
      "epoch 164; iter: 0; batch classifier loss: 0.248781; batch adversarial loss: 0.228381\n",
      "epoch 165; iter: 0; batch classifier loss: 0.132982; batch adversarial loss: 0.214787\n",
      "epoch 166; iter: 0; batch classifier loss: 0.187232; batch adversarial loss: 0.249439\n",
      "epoch 167; iter: 0; batch classifier loss: 0.179101; batch adversarial loss: 0.276464\n",
      "epoch 168; iter: 0; batch classifier loss: 0.245308; batch adversarial loss: 0.257752\n",
      "epoch 169; iter: 0; batch classifier loss: 0.167686; batch adversarial loss: 0.260252\n",
      "epoch 170; iter: 0; batch classifier loss: 0.195880; batch adversarial loss: 0.255959\n",
      "epoch 171; iter: 0; batch classifier loss: 0.204333; batch adversarial loss: 0.306171\n",
      "epoch 172; iter: 0; batch classifier loss: 0.226164; batch adversarial loss: 0.252406\n",
      "epoch 173; iter: 0; batch classifier loss: 0.197965; batch adversarial loss: 0.304093\n",
      "epoch 174; iter: 0; batch classifier loss: 0.195891; batch adversarial loss: 0.292619\n",
      "epoch 175; iter: 0; batch classifier loss: 0.263144; batch adversarial loss: 0.278072\n",
      "epoch 176; iter: 0; batch classifier loss: 0.184847; batch adversarial loss: 0.333928\n",
      "epoch 177; iter: 0; batch classifier loss: 0.169366; batch adversarial loss: 0.240857\n",
      "epoch 178; iter: 0; batch classifier loss: 0.216529; batch adversarial loss: 0.274766\n",
      "epoch 179; iter: 0; batch classifier loss: 0.137995; batch adversarial loss: 0.214526\n",
      "epoch 180; iter: 0; batch classifier loss: 0.152896; batch adversarial loss: 0.249979\n",
      "epoch 181; iter: 0; batch classifier loss: 0.184541; batch adversarial loss: 0.281581\n",
      "epoch 182; iter: 0; batch classifier loss: 0.127489; batch adversarial loss: 0.265409\n",
      "epoch 183; iter: 0; batch classifier loss: 0.138669; batch adversarial loss: 0.252958\n",
      "epoch 184; iter: 0; batch classifier loss: 0.107123; batch adversarial loss: 0.196862\n",
      "epoch 185; iter: 0; batch classifier loss: 0.200919; batch adversarial loss: 0.288693\n",
      "epoch 186; iter: 0; batch classifier loss: 0.162045; batch adversarial loss: 0.241277\n",
      "epoch 187; iter: 0; batch classifier loss: 0.205306; batch adversarial loss: 0.268207\n",
      "epoch 188; iter: 0; batch classifier loss: 0.223910; batch adversarial loss: 0.199155\n",
      "epoch 189; iter: 0; batch classifier loss: 0.123242; batch adversarial loss: 0.298756\n",
      "epoch 190; iter: 0; batch classifier loss: 0.229494; batch adversarial loss: 0.266681\n",
      "epoch 191; iter: 0; batch classifier loss: 0.243229; batch adversarial loss: 0.257103\n",
      "epoch 192; iter: 0; batch classifier loss: 0.198779; batch adversarial loss: 0.335797\n",
      "epoch 193; iter: 0; batch classifier loss: 0.163439; batch adversarial loss: 0.291271\n",
      "epoch 194; iter: 0; batch classifier loss: 0.179158; batch adversarial loss: 0.272257\n",
      "epoch 195; iter: 0; batch classifier loss: 0.195960; batch adversarial loss: 0.227657\n",
      "epoch 196; iter: 0; batch classifier loss: 0.225919; batch adversarial loss: 0.296471\n",
      "epoch 197; iter: 0; batch classifier loss: 0.149066; batch adversarial loss: 0.256617\n",
      "epoch 198; iter: 0; batch classifier loss: 0.158058; batch adversarial loss: 0.270722\n",
      "epoch 199; iter: 0; batch classifier loss: 0.192100; batch adversarial loss: 0.294149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:56:07.272045: W tensorflow/c/c_api.cc:305] Operation '{name:'39b90fda-ae25-11ee-bc15-ae7d8bf09116/39b90fda-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:1557 op device:{requested: '', assigned: ''} def:{{{node 39b90fda-ae25-11ee-bc15-ae7d8bf09116/39b90fda-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b90fda-ae25-11ee-bc15-ae7d8bf09116/39b90fda-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b90fda-ae25-11ee-bc15-ae7d8bf09116/39b90fda-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.674593; batch adversarial loss: 0.794205\n",
      "epoch 1; iter: 0; batch classifier loss: 0.330921; batch adversarial loss: 0.709332\n",
      "epoch 2; iter: 0; batch classifier loss: 0.193882; batch adversarial loss: 0.602720\n",
      "epoch 3; iter: 0; batch classifier loss: 0.142573; batch adversarial loss: 0.514248\n",
      "epoch 4; iter: 0; batch classifier loss: 0.196005; batch adversarial loss: 0.463497\n",
      "epoch 5; iter: 0; batch classifier loss: 0.310625; batch adversarial loss: 0.405235\n",
      "epoch 6; iter: 0; batch classifier loss: 0.188361; batch adversarial loss: 0.374358\n",
      "epoch 7; iter: 0; batch classifier loss: 0.307818; batch adversarial loss: 0.339731\n",
      "epoch 8; iter: 0; batch classifier loss: 0.234640; batch adversarial loss: 0.378656\n",
      "epoch 9; iter: 0; batch classifier loss: 0.185496; batch adversarial loss: 0.341901\n",
      "epoch 10; iter: 0; batch classifier loss: 0.229726; batch adversarial loss: 0.309176\n",
      "epoch 11; iter: 0; batch classifier loss: 0.235196; batch adversarial loss: 0.274100\n",
      "epoch 12; iter: 0; batch classifier loss: 0.351532; batch adversarial loss: 0.325255\n",
      "epoch 13; iter: 0; batch classifier loss: 0.221701; batch adversarial loss: 0.229348\n",
      "epoch 14; iter: 0; batch classifier loss: 0.229463; batch adversarial loss: 0.308125\n",
      "epoch 15; iter: 0; batch classifier loss: 0.264800; batch adversarial loss: 0.247080\n",
      "epoch 16; iter: 0; batch classifier loss: 0.186312; batch adversarial loss: 0.287500\n",
      "epoch 17; iter: 0; batch classifier loss: 0.237963; batch adversarial loss: 0.300179\n",
      "epoch 18; iter: 0; batch classifier loss: 0.313656; batch adversarial loss: 0.248052\n",
      "epoch 19; iter: 0; batch classifier loss: 0.185774; batch adversarial loss: 0.203300\n",
      "epoch 20; iter: 0; batch classifier loss: 0.209403; batch adversarial loss: 0.285961\n",
      "epoch 21; iter: 0; batch classifier loss: 0.173380; batch adversarial loss: 0.238469\n",
      "epoch 22; iter: 0; batch classifier loss: 0.240173; batch adversarial loss: 0.375496\n",
      "epoch 23; iter: 0; batch classifier loss: 0.336784; batch adversarial loss: 0.270361\n",
      "epoch 24; iter: 0; batch classifier loss: 0.165453; batch adversarial loss: 0.328913\n",
      "epoch 25; iter: 0; batch classifier loss: 0.246480; batch adversarial loss: 0.268658\n",
      "epoch 26; iter: 0; batch classifier loss: 0.384851; batch adversarial loss: 0.230403\n",
      "epoch 27; iter: 0; batch classifier loss: 0.288433; batch adversarial loss: 0.251899\n",
      "epoch 28; iter: 0; batch classifier loss: 0.253603; batch adversarial loss: 0.328742\n",
      "epoch 29; iter: 0; batch classifier loss: 0.205606; batch adversarial loss: 0.274348\n",
      "epoch 30; iter: 0; batch classifier loss: 0.156850; batch adversarial loss: 0.149715\n",
      "epoch 31; iter: 0; batch classifier loss: 0.184602; batch adversarial loss: 0.230709\n",
      "epoch 32; iter: 0; batch classifier loss: 0.138635; batch adversarial loss: 0.187220\n",
      "epoch 33; iter: 0; batch classifier loss: 0.241668; batch adversarial loss: 0.345241\n",
      "epoch 34; iter: 0; batch classifier loss: 0.220533; batch adversarial loss: 0.246612\n",
      "epoch 35; iter: 0; batch classifier loss: 0.225574; batch adversarial loss: 0.259874\n",
      "epoch 36; iter: 0; batch classifier loss: 0.146624; batch adversarial loss: 0.225370\n",
      "epoch 37; iter: 0; batch classifier loss: 0.230348; batch adversarial loss: 0.248758\n",
      "epoch 38; iter: 0; batch classifier loss: 0.169825; batch adversarial loss: 0.295957\n",
      "epoch 39; iter: 0; batch classifier loss: 0.194725; batch adversarial loss: 0.183726\n",
      "epoch 40; iter: 0; batch classifier loss: 0.197603; batch adversarial loss: 0.314226\n",
      "epoch 41; iter: 0; batch classifier loss: 0.186238; batch adversarial loss: 0.195157\n",
      "epoch 42; iter: 0; batch classifier loss: 0.225432; batch adversarial loss: 0.263913\n",
      "epoch 43; iter: 0; batch classifier loss: 0.159404; batch adversarial loss: 0.201650\n",
      "epoch 44; iter: 0; batch classifier loss: 0.217119; batch adversarial loss: 0.232433\n",
      "epoch 45; iter: 0; batch classifier loss: 0.189801; batch adversarial loss: 0.284902\n",
      "epoch 46; iter: 0; batch classifier loss: 0.263931; batch adversarial loss: 0.311183\n",
      "epoch 47; iter: 0; batch classifier loss: 0.115430; batch adversarial loss: 0.318335\n",
      "epoch 48; iter: 0; batch classifier loss: 0.274796; batch adversarial loss: 0.271309\n",
      "epoch 49; iter: 0; batch classifier loss: 0.192553; batch adversarial loss: 0.242916\n",
      "epoch 50; iter: 0; batch classifier loss: 0.247522; batch adversarial loss: 0.250304\n",
      "epoch 51; iter: 0; batch classifier loss: 0.160214; batch adversarial loss: 0.343547\n",
      "epoch 52; iter: 0; batch classifier loss: 0.281711; batch adversarial loss: 0.282834\n",
      "epoch 53; iter: 0; batch classifier loss: 0.194434; batch adversarial loss: 0.327001\n",
      "epoch 54; iter: 0; batch classifier loss: 0.173317; batch adversarial loss: 0.212854\n",
      "epoch 55; iter: 0; batch classifier loss: 0.258686; batch adversarial loss: 0.265515\n",
      "epoch 56; iter: 0; batch classifier loss: 0.231739; batch adversarial loss: 0.218784\n",
      "epoch 57; iter: 0; batch classifier loss: 0.266956; batch adversarial loss: 0.242741\n",
      "epoch 58; iter: 0; batch classifier loss: 0.175994; batch adversarial loss: 0.325060\n",
      "epoch 59; iter: 0; batch classifier loss: 0.163086; batch adversarial loss: 0.253917\n",
      "epoch 60; iter: 0; batch classifier loss: 0.230909; batch adversarial loss: 0.260240\n",
      "epoch 61; iter: 0; batch classifier loss: 0.206226; batch adversarial loss: 0.203708\n",
      "epoch 62; iter: 0; batch classifier loss: 0.312795; batch adversarial loss: 0.296710\n",
      "epoch 63; iter: 0; batch classifier loss: 0.121635; batch adversarial loss: 0.239765\n",
      "epoch 64; iter: 0; batch classifier loss: 0.239744; batch adversarial loss: 0.290494\n",
      "epoch 65; iter: 0; batch classifier loss: 0.225564; batch adversarial loss: 0.190840\n",
      "epoch 66; iter: 0; batch classifier loss: 0.232183; batch adversarial loss: 0.283548\n",
      "epoch 67; iter: 0; batch classifier loss: 0.222795; batch adversarial loss: 0.326412\n",
      "epoch 68; iter: 0; batch classifier loss: 0.187640; batch adversarial loss: 0.124370\n",
      "epoch 69; iter: 0; batch classifier loss: 0.194479; batch adversarial loss: 0.213731\n",
      "epoch 70; iter: 0; batch classifier loss: 0.187583; batch adversarial loss: 0.127872\n",
      "epoch 71; iter: 0; batch classifier loss: 0.169845; batch adversarial loss: 0.266934\n",
      "epoch 72; iter: 0; batch classifier loss: 0.245935; batch adversarial loss: 0.375645\n",
      "epoch 73; iter: 0; batch classifier loss: 0.217870; batch adversarial loss: 0.267987\n",
      "epoch 74; iter: 0; batch classifier loss: 0.157743; batch adversarial loss: 0.175420\n",
      "epoch 75; iter: 0; batch classifier loss: 0.153595; batch adversarial loss: 0.278777\n",
      "epoch 76; iter: 0; batch classifier loss: 0.207934; batch adversarial loss: 0.268678\n",
      "epoch 77; iter: 0; batch classifier loss: 0.276307; batch adversarial loss: 0.263963\n",
      "epoch 78; iter: 0; batch classifier loss: 0.119971; batch adversarial loss: 0.257860\n",
      "epoch 79; iter: 0; batch classifier loss: 0.231670; batch adversarial loss: 0.270224\n",
      "epoch 80; iter: 0; batch classifier loss: 0.217465; batch adversarial loss: 0.298159\n",
      "epoch 81; iter: 0; batch classifier loss: 0.191561; batch adversarial loss: 0.227643\n",
      "epoch 82; iter: 0; batch classifier loss: 0.217934; batch adversarial loss: 0.230034\n",
      "epoch 83; iter: 0; batch classifier loss: 0.130498; batch adversarial loss: 0.325391\n",
      "epoch 84; iter: 0; batch classifier loss: 0.159064; batch adversarial loss: 0.326677\n",
      "epoch 85; iter: 0; batch classifier loss: 0.134873; batch adversarial loss: 0.156577\n",
      "epoch 86; iter: 0; batch classifier loss: 0.217267; batch adversarial loss: 0.196050\n",
      "epoch 87; iter: 0; batch classifier loss: 0.161416; batch adversarial loss: 0.294533\n",
      "epoch 88; iter: 0; batch classifier loss: 0.266580; batch adversarial loss: 0.210966\n",
      "epoch 89; iter: 0; batch classifier loss: 0.138079; batch adversarial loss: 0.251341\n",
      "epoch 90; iter: 0; batch classifier loss: 0.185917; batch adversarial loss: 0.315498\n",
      "epoch 91; iter: 0; batch classifier loss: 0.297557; batch adversarial loss: 0.318132\n",
      "epoch 92; iter: 0; batch classifier loss: 0.231645; batch adversarial loss: 0.347404\n",
      "epoch 93; iter: 0; batch classifier loss: 0.185352; batch adversarial loss: 0.281176\n",
      "epoch 94; iter: 0; batch classifier loss: 0.176987; batch adversarial loss: 0.285828\n",
      "epoch 95; iter: 0; batch classifier loss: 0.246147; batch adversarial loss: 0.240673\n",
      "epoch 96; iter: 0; batch classifier loss: 0.242242; batch adversarial loss: 0.314828\n",
      "epoch 97; iter: 0; batch classifier loss: 0.211724; batch adversarial loss: 0.224500\n",
      "epoch 98; iter: 0; batch classifier loss: 0.151582; batch adversarial loss: 0.194876\n",
      "epoch 99; iter: 0; batch classifier loss: 0.205893; batch adversarial loss: 0.302451\n",
      "epoch 100; iter: 0; batch classifier loss: 0.202923; batch adversarial loss: 0.365067\n",
      "epoch 101; iter: 0; batch classifier loss: 0.121242; batch adversarial loss: 0.240330\n",
      "epoch 102; iter: 0; batch classifier loss: 0.206448; batch adversarial loss: 0.291804\n",
      "epoch 103; iter: 0; batch classifier loss: 0.208742; batch adversarial loss: 0.289401\n",
      "epoch 104; iter: 0; batch classifier loss: 0.161371; batch adversarial loss: 0.232452\n",
      "epoch 105; iter: 0; batch classifier loss: 0.207364; batch adversarial loss: 0.309631\n",
      "epoch 106; iter: 0; batch classifier loss: 0.161948; batch adversarial loss: 0.262342\n",
      "epoch 107; iter: 0; batch classifier loss: 0.249807; batch adversarial loss: 0.224463\n",
      "epoch 108; iter: 0; batch classifier loss: 0.240370; batch adversarial loss: 0.202054\n",
      "epoch 109; iter: 0; batch classifier loss: 0.171421; batch adversarial loss: 0.241065\n",
      "epoch 110; iter: 0; batch classifier loss: 0.215171; batch adversarial loss: 0.288700\n",
      "epoch 111; iter: 0; batch classifier loss: 0.188032; batch adversarial loss: 0.263070\n",
      "epoch 112; iter: 0; batch classifier loss: 0.163682; batch adversarial loss: 0.269321\n",
      "epoch 113; iter: 0; batch classifier loss: 0.172052; batch adversarial loss: 0.327732\n",
      "epoch 114; iter: 0; batch classifier loss: 0.211177; batch adversarial loss: 0.188573\n",
      "epoch 115; iter: 0; batch classifier loss: 0.187775; batch adversarial loss: 0.207112\n",
      "epoch 116; iter: 0; batch classifier loss: 0.170001; batch adversarial loss: 0.301972\n",
      "epoch 117; iter: 0; batch classifier loss: 0.199541; batch adversarial loss: 0.309574\n",
      "epoch 118; iter: 0; batch classifier loss: 0.209461; batch adversarial loss: 0.260026\n",
      "epoch 119; iter: 0; batch classifier loss: 0.217875; batch adversarial loss: 0.249257\n",
      "epoch 120; iter: 0; batch classifier loss: 0.188176; batch adversarial loss: 0.150277\n",
      "epoch 121; iter: 0; batch classifier loss: 0.227284; batch adversarial loss: 0.223835\n",
      "epoch 122; iter: 0; batch classifier loss: 0.170978; batch adversarial loss: 0.220239\n",
      "epoch 123; iter: 0; batch classifier loss: 0.293943; batch adversarial loss: 0.376744\n",
      "epoch 124; iter: 0; batch classifier loss: 0.157651; batch adversarial loss: 0.341150\n",
      "epoch 125; iter: 0; batch classifier loss: 0.228975; batch adversarial loss: 0.321930\n",
      "epoch 126; iter: 0; batch classifier loss: 0.227260; batch adversarial loss: 0.218494\n",
      "epoch 127; iter: 0; batch classifier loss: 0.208182; batch adversarial loss: 0.124515\n",
      "epoch 128; iter: 0; batch classifier loss: 0.172534; batch adversarial loss: 0.193007\n",
      "epoch 129; iter: 0; batch classifier loss: 0.262247; batch adversarial loss: 0.208320\n",
      "epoch 130; iter: 0; batch classifier loss: 0.230906; batch adversarial loss: 0.215743\n",
      "epoch 131; iter: 0; batch classifier loss: 0.179198; batch adversarial loss: 0.212473\n",
      "epoch 132; iter: 0; batch classifier loss: 0.154684; batch adversarial loss: 0.211135\n",
      "epoch 133; iter: 0; batch classifier loss: 0.181127; batch adversarial loss: 0.321397\n",
      "epoch 134; iter: 0; batch classifier loss: 0.200832; batch adversarial loss: 0.384084\n",
      "epoch 135; iter: 0; batch classifier loss: 0.189965; batch adversarial loss: 0.149296\n",
      "epoch 136; iter: 0; batch classifier loss: 0.204304; batch adversarial loss: 0.312211\n",
      "epoch 137; iter: 0; batch classifier loss: 0.224317; batch adversarial loss: 0.240951\n",
      "epoch 138; iter: 0; batch classifier loss: 0.180793; batch adversarial loss: 0.339037\n",
      "epoch 139; iter: 0; batch classifier loss: 0.257934; batch adversarial loss: 0.289286\n",
      "epoch 140; iter: 0; batch classifier loss: 0.167262; batch adversarial loss: 0.213418\n",
      "epoch 141; iter: 0; batch classifier loss: 0.176040; batch adversarial loss: 0.253727\n",
      "epoch 142; iter: 0; batch classifier loss: 0.221407; batch adversarial loss: 0.242775\n",
      "epoch 143; iter: 0; batch classifier loss: 0.228431; batch adversarial loss: 0.281215\n",
      "epoch 144; iter: 0; batch classifier loss: 0.154261; batch adversarial loss: 0.294222\n",
      "epoch 145; iter: 0; batch classifier loss: 0.232317; batch adversarial loss: 0.225811\n",
      "epoch 146; iter: 0; batch classifier loss: 0.199925; batch adversarial loss: 0.267036\n",
      "epoch 147; iter: 0; batch classifier loss: 0.176989; batch adversarial loss: 0.284127\n",
      "epoch 148; iter: 0; batch classifier loss: 0.163988; batch adversarial loss: 0.215852\n",
      "epoch 149; iter: 0; batch classifier loss: 0.175923; batch adversarial loss: 0.321335\n",
      "epoch 150; iter: 0; batch classifier loss: 0.236792; batch adversarial loss: 0.291003\n",
      "epoch 151; iter: 0; batch classifier loss: 0.229291; batch adversarial loss: 0.319573\n",
      "epoch 152; iter: 0; batch classifier loss: 0.239759; batch adversarial loss: 0.192824\n",
      "epoch 153; iter: 0; batch classifier loss: 0.131287; batch adversarial loss: 0.355872\n",
      "epoch 154; iter: 0; batch classifier loss: 0.157124; batch adversarial loss: 0.290550\n",
      "epoch 155; iter: 0; batch classifier loss: 0.141438; batch adversarial loss: 0.255363\n",
      "epoch 156; iter: 0; batch classifier loss: 0.181112; batch adversarial loss: 0.278250\n",
      "epoch 157; iter: 0; batch classifier loss: 0.192685; batch adversarial loss: 0.267100\n",
      "epoch 158; iter: 0; batch classifier loss: 0.210224; batch adversarial loss: 0.227489\n",
      "epoch 159; iter: 0; batch classifier loss: 0.212661; batch adversarial loss: 0.226343\n",
      "epoch 160; iter: 0; batch classifier loss: 0.223637; batch adversarial loss: 0.269943\n",
      "epoch 161; iter: 0; batch classifier loss: 0.300382; batch adversarial loss: 0.303916\n",
      "epoch 162; iter: 0; batch classifier loss: 0.270864; batch adversarial loss: 0.218807\n",
      "epoch 163; iter: 0; batch classifier loss: 0.239339; batch adversarial loss: 0.309972\n",
      "epoch 164; iter: 0; batch classifier loss: 0.197498; batch adversarial loss: 0.252248\n",
      "epoch 165; iter: 0; batch classifier loss: 0.209804; batch adversarial loss: 0.226974\n",
      "epoch 166; iter: 0; batch classifier loss: 0.267674; batch adversarial loss: 0.224604\n",
      "epoch 167; iter: 0; batch classifier loss: 0.144743; batch adversarial loss: 0.212689\n",
      "epoch 168; iter: 0; batch classifier loss: 0.176939; batch adversarial loss: 0.226543\n",
      "epoch 169; iter: 0; batch classifier loss: 0.141162; batch adversarial loss: 0.288819\n",
      "epoch 170; iter: 0; batch classifier loss: 0.169372; batch adversarial loss: 0.180210\n",
      "epoch 171; iter: 0; batch classifier loss: 0.227275; batch adversarial loss: 0.274735\n",
      "epoch 172; iter: 0; batch classifier loss: 0.275459; batch adversarial loss: 0.272206\n",
      "epoch 173; iter: 0; batch classifier loss: 0.213172; batch adversarial loss: 0.156176\n",
      "epoch 174; iter: 0; batch classifier loss: 0.147259; batch adversarial loss: 0.299326\n",
      "epoch 175; iter: 0; batch classifier loss: 0.189017; batch adversarial loss: 0.314839\n",
      "epoch 176; iter: 0; batch classifier loss: 0.175521; batch adversarial loss: 0.430718\n",
      "epoch 177; iter: 0; batch classifier loss: 0.219830; batch adversarial loss: 0.264255\n",
      "epoch 178; iter: 0; batch classifier loss: 0.192194; batch adversarial loss: 0.223675\n",
      "epoch 179; iter: 0; batch classifier loss: 0.211163; batch adversarial loss: 0.177744\n",
      "epoch 180; iter: 0; batch classifier loss: 0.188584; batch adversarial loss: 0.339916\n",
      "epoch 181; iter: 0; batch classifier loss: 0.220817; batch adversarial loss: 0.235085\n",
      "epoch 182; iter: 0; batch classifier loss: 0.179409; batch adversarial loss: 0.305806\n",
      "epoch 183; iter: 0; batch classifier loss: 0.233625; batch adversarial loss: 0.209736\n",
      "epoch 184; iter: 0; batch classifier loss: 0.286377; batch adversarial loss: 0.293494\n",
      "epoch 185; iter: 0; batch classifier loss: 0.161476; batch adversarial loss: 0.332818\n",
      "epoch 186; iter: 0; batch classifier loss: 0.214278; batch adversarial loss: 0.240954\n",
      "epoch 187; iter: 0; batch classifier loss: 0.216007; batch adversarial loss: 0.214543\n",
      "epoch 188; iter: 0; batch classifier loss: 0.169271; batch adversarial loss: 0.234116\n",
      "epoch 189; iter: 0; batch classifier loss: 0.210429; batch adversarial loss: 0.227009\n",
      "epoch 190; iter: 0; batch classifier loss: 0.152931; batch adversarial loss: 0.244148\n",
      "epoch 191; iter: 0; batch classifier loss: 0.172053; batch adversarial loss: 0.160322\n",
      "epoch 192; iter: 0; batch classifier loss: 0.209054; batch adversarial loss: 0.257124\n",
      "epoch 193; iter: 0; batch classifier loss: 0.140493; batch adversarial loss: 0.212373\n",
      "epoch 194; iter: 0; batch classifier loss: 0.126911; batch adversarial loss: 0.283489\n",
      "epoch 195; iter: 0; batch classifier loss: 0.244522; batch adversarial loss: 0.210805\n",
      "epoch 196; iter: 0; batch classifier loss: 0.247539; batch adversarial loss: 0.306744\n",
      "epoch 197; iter: 0; batch classifier loss: 0.169389; batch adversarial loss: 0.315924\n",
      "epoch 198; iter: 0; batch classifier loss: 0.195568; batch adversarial loss: 0.238536\n",
      "epoch 199; iter: 0; batch classifier loss: 0.236952; batch adversarial loss: 0.249289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:56:20.736036: W tensorflow/c/c_api.cc:305] Operation '{name:'39b91020-ae25-11ee-bc15-ae7d8bf09116/39b91020-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:2347 op device:{requested: '', assigned: ''} def:{{{node 39b91020-ae25-11ee-bc15-ae7d8bf09116/39b91020-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b91020-ae25-11ee-bc15-ae7d8bf09116/39b91020-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b91020-ae25-11ee-bc15-ae7d8bf09116/39b91020-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.706423; batch adversarial loss: 0.529969\n",
      "epoch 1; iter: 0; batch classifier loss: 0.946622; batch adversarial loss: 0.573293\n",
      "epoch 2; iter: 0; batch classifier loss: 1.263609; batch adversarial loss: 0.627032\n",
      "epoch 3; iter: 0; batch classifier loss: 1.612743; batch adversarial loss: 0.600251\n",
      "epoch 4; iter: 0; batch classifier loss: 1.725144; batch adversarial loss: 0.586471\n",
      "epoch 5; iter: 0; batch classifier loss: 1.796891; batch adversarial loss: 0.576146\n",
      "epoch 6; iter: 0; batch classifier loss: 1.718796; batch adversarial loss: 0.544753\n",
      "epoch 7; iter: 0; batch classifier loss: 1.412819; batch adversarial loss: 0.485223\n",
      "epoch 8; iter: 0; batch classifier loss: 1.209722; batch adversarial loss: 0.517748\n",
      "epoch 9; iter: 0; batch classifier loss: 0.964469; batch adversarial loss: 0.403720\n",
      "epoch 10; iter: 0; batch classifier loss: 0.848762; batch adversarial loss: 0.377340\n",
      "epoch 11; iter: 0; batch classifier loss: 0.696498; batch adversarial loss: 0.400868\n",
      "epoch 12; iter: 0; batch classifier loss: 0.706051; batch adversarial loss: 0.312636\n",
      "epoch 13; iter: 0; batch classifier loss: 0.746291; batch adversarial loss: 0.452530\n",
      "epoch 14; iter: 0; batch classifier loss: 0.635143; batch adversarial loss: 0.404368\n",
      "epoch 15; iter: 0; batch classifier loss: 0.312844; batch adversarial loss: 0.382955\n",
      "epoch 16; iter: 0; batch classifier loss: 0.225426; batch adversarial loss: 0.231257\n",
      "epoch 17; iter: 0; batch classifier loss: 0.199621; batch adversarial loss: 0.240420\n",
      "epoch 18; iter: 0; batch classifier loss: 0.342227; batch adversarial loss: 0.383719\n",
      "epoch 19; iter: 0; batch classifier loss: 0.193934; batch adversarial loss: 0.336042\n",
      "epoch 20; iter: 0; batch classifier loss: 0.263786; batch adversarial loss: 0.327012\n",
      "epoch 21; iter: 0; batch classifier loss: 0.222208; batch adversarial loss: 0.224243\n",
      "epoch 22; iter: 0; batch classifier loss: 0.200805; batch adversarial loss: 0.292656\n",
      "epoch 23; iter: 0; batch classifier loss: 0.180385; batch adversarial loss: 0.193689\n",
      "epoch 24; iter: 0; batch classifier loss: 0.202237; batch adversarial loss: 0.230908\n",
      "epoch 25; iter: 0; batch classifier loss: 0.215731; batch adversarial loss: 0.260748\n",
      "epoch 26; iter: 0; batch classifier loss: 0.343241; batch adversarial loss: 0.228551\n",
      "epoch 27; iter: 0; batch classifier loss: 0.212994; batch adversarial loss: 0.199799\n",
      "epoch 28; iter: 0; batch classifier loss: 0.272905; batch adversarial loss: 0.237512\n",
      "epoch 29; iter: 0; batch classifier loss: 0.321517; batch adversarial loss: 0.323063\n",
      "epoch 30; iter: 0; batch classifier loss: 0.181037; batch adversarial loss: 0.282498\n",
      "epoch 31; iter: 0; batch classifier loss: 0.254456; batch adversarial loss: 0.316752\n",
      "epoch 32; iter: 0; batch classifier loss: 0.201607; batch adversarial loss: 0.307600\n",
      "epoch 33; iter: 0; batch classifier loss: 0.318838; batch adversarial loss: 0.333559\n",
      "epoch 34; iter: 0; batch classifier loss: 0.171550; batch adversarial loss: 0.272762\n",
      "epoch 35; iter: 0; batch classifier loss: 0.166877; batch adversarial loss: 0.330373\n",
      "epoch 36; iter: 0; batch classifier loss: 0.231427; batch adversarial loss: 0.187567\n",
      "epoch 37; iter: 0; batch classifier loss: 0.278225; batch adversarial loss: 0.220534\n",
      "epoch 38; iter: 0; batch classifier loss: 0.258004; batch adversarial loss: 0.259843\n",
      "epoch 39; iter: 0; batch classifier loss: 0.240675; batch adversarial loss: 0.217872\n",
      "epoch 40; iter: 0; batch classifier loss: 0.277965; batch adversarial loss: 0.257691\n",
      "epoch 41; iter: 0; batch classifier loss: 0.199560; batch adversarial loss: 0.240606\n",
      "epoch 42; iter: 0; batch classifier loss: 0.289683; batch adversarial loss: 0.242915\n",
      "epoch 43; iter: 0; batch classifier loss: 0.208004; batch adversarial loss: 0.173233\n",
      "epoch 44; iter: 0; batch classifier loss: 0.244800; batch adversarial loss: 0.240201\n",
      "epoch 45; iter: 0; batch classifier loss: 0.284195; batch adversarial loss: 0.357355\n",
      "epoch 46; iter: 0; batch classifier loss: 0.209338; batch adversarial loss: 0.151375\n",
      "epoch 47; iter: 0; batch classifier loss: 0.208910; batch adversarial loss: 0.247848\n",
      "epoch 48; iter: 0; batch classifier loss: 0.272432; batch adversarial loss: 0.225765\n",
      "epoch 49; iter: 0; batch classifier loss: 0.288273; batch adversarial loss: 0.223631\n",
      "epoch 50; iter: 0; batch classifier loss: 0.245375; batch adversarial loss: 0.199548\n",
      "epoch 51; iter: 0; batch classifier loss: 0.179088; batch adversarial loss: 0.208008\n",
      "epoch 52; iter: 0; batch classifier loss: 0.245727; batch adversarial loss: 0.265013\n",
      "epoch 53; iter: 0; batch classifier loss: 0.208519; batch adversarial loss: 0.112834\n",
      "epoch 54; iter: 0; batch classifier loss: 0.209514; batch adversarial loss: 0.285188\n",
      "epoch 55; iter: 0; batch classifier loss: 0.233195; batch adversarial loss: 0.244284\n",
      "epoch 56; iter: 0; batch classifier loss: 0.270122; batch adversarial loss: 0.174067\n",
      "epoch 57; iter: 0; batch classifier loss: 0.230375; batch adversarial loss: 0.244912\n",
      "epoch 58; iter: 0; batch classifier loss: 0.214925; batch adversarial loss: 0.267121\n",
      "epoch 59; iter: 0; batch classifier loss: 0.262882; batch adversarial loss: 0.330577\n",
      "epoch 60; iter: 0; batch classifier loss: 0.296163; batch adversarial loss: 0.232284\n",
      "epoch 61; iter: 0; batch classifier loss: 0.219571; batch adversarial loss: 0.256471\n",
      "epoch 62; iter: 0; batch classifier loss: 0.177386; batch adversarial loss: 0.299312\n",
      "epoch 63; iter: 0; batch classifier loss: 0.182537; batch adversarial loss: 0.194055\n",
      "epoch 64; iter: 0; batch classifier loss: 0.241118; batch adversarial loss: 0.233316\n",
      "epoch 65; iter: 0; batch classifier loss: 0.242968; batch adversarial loss: 0.223813\n",
      "epoch 66; iter: 0; batch classifier loss: 0.232048; batch adversarial loss: 0.264014\n",
      "epoch 67; iter: 0; batch classifier loss: 0.242760; batch adversarial loss: 0.271094\n",
      "epoch 68; iter: 0; batch classifier loss: 0.317444; batch adversarial loss: 0.316166\n",
      "epoch 69; iter: 0; batch classifier loss: 0.194671; batch adversarial loss: 0.365101\n",
      "epoch 70; iter: 0; batch classifier loss: 0.226253; batch adversarial loss: 0.355983\n",
      "epoch 71; iter: 0; batch classifier loss: 0.224892; batch adversarial loss: 0.383907\n",
      "epoch 72; iter: 0; batch classifier loss: 0.269213; batch adversarial loss: 0.275913\n",
      "epoch 73; iter: 0; batch classifier loss: 0.180456; batch adversarial loss: 0.352132\n",
      "epoch 74; iter: 0; batch classifier loss: 0.165603; batch adversarial loss: 0.269096\n",
      "epoch 75; iter: 0; batch classifier loss: 0.170609; batch adversarial loss: 0.294759\n",
      "epoch 76; iter: 0; batch classifier loss: 0.268565; batch adversarial loss: 0.347220\n",
      "epoch 77; iter: 0; batch classifier loss: 0.190790; batch adversarial loss: 0.243661\n",
      "epoch 78; iter: 0; batch classifier loss: 0.302961; batch adversarial loss: 0.242657\n",
      "epoch 79; iter: 0; batch classifier loss: 0.253339; batch adversarial loss: 0.218237\n",
      "epoch 80; iter: 0; batch classifier loss: 0.142443; batch adversarial loss: 0.347908\n",
      "epoch 81; iter: 0; batch classifier loss: 0.299046; batch adversarial loss: 0.293880\n",
      "epoch 82; iter: 0; batch classifier loss: 0.252133; batch adversarial loss: 0.293098\n",
      "epoch 83; iter: 0; batch classifier loss: 0.234679; batch adversarial loss: 0.306442\n",
      "epoch 84; iter: 0; batch classifier loss: 0.235783; batch adversarial loss: 0.257413\n",
      "epoch 85; iter: 0; batch classifier loss: 0.235295; batch adversarial loss: 0.248221\n",
      "epoch 86; iter: 0; batch classifier loss: 0.196587; batch adversarial loss: 0.242624\n",
      "epoch 87; iter: 0; batch classifier loss: 0.202938; batch adversarial loss: 0.254202\n",
      "epoch 88; iter: 0; batch classifier loss: 0.278570; batch adversarial loss: 0.245899\n",
      "epoch 89; iter: 0; batch classifier loss: 0.197222; batch adversarial loss: 0.267097\n",
      "epoch 90; iter: 0; batch classifier loss: 0.315012; batch adversarial loss: 0.391888\n",
      "epoch 91; iter: 0; batch classifier loss: 0.247394; batch adversarial loss: 0.227093\n",
      "epoch 92; iter: 0; batch classifier loss: 0.129838; batch adversarial loss: 0.167361\n",
      "epoch 93; iter: 0; batch classifier loss: 0.249046; batch adversarial loss: 0.260174\n",
      "epoch 94; iter: 0; batch classifier loss: 0.225196; batch adversarial loss: 0.242480\n",
      "epoch 95; iter: 0; batch classifier loss: 0.211728; batch adversarial loss: 0.315005\n",
      "epoch 96; iter: 0; batch classifier loss: 0.218040; batch adversarial loss: 0.253552\n",
      "epoch 97; iter: 0; batch classifier loss: 0.329433; batch adversarial loss: 0.319376\n",
      "epoch 98; iter: 0; batch classifier loss: 0.232646; batch adversarial loss: 0.268626\n",
      "epoch 99; iter: 0; batch classifier loss: 0.192535; batch adversarial loss: 0.281151\n",
      "epoch 100; iter: 0; batch classifier loss: 0.180044; batch adversarial loss: 0.261967\n",
      "epoch 101; iter: 0; batch classifier loss: 0.230847; batch adversarial loss: 0.346823\n",
      "epoch 102; iter: 0; batch classifier loss: 0.129126; batch adversarial loss: 0.343885\n",
      "epoch 103; iter: 0; batch classifier loss: 0.182729; batch adversarial loss: 0.323583\n",
      "epoch 104; iter: 0; batch classifier loss: 0.200353; batch adversarial loss: 0.324437\n",
      "epoch 105; iter: 0; batch classifier loss: 0.193399; batch adversarial loss: 0.264419\n",
      "epoch 106; iter: 0; batch classifier loss: 0.172632; batch adversarial loss: 0.343279\n",
      "epoch 107; iter: 0; batch classifier loss: 0.274460; batch adversarial loss: 0.213911\n",
      "epoch 108; iter: 0; batch classifier loss: 0.182457; batch adversarial loss: 0.263243\n",
      "epoch 109; iter: 0; batch classifier loss: 0.273929; batch adversarial loss: 0.319942\n",
      "epoch 110; iter: 0; batch classifier loss: 0.153832; batch adversarial loss: 0.298698\n",
      "epoch 111; iter: 0; batch classifier loss: 0.206214; batch adversarial loss: 0.262191\n",
      "epoch 112; iter: 0; batch classifier loss: 0.172489; batch adversarial loss: 0.328224\n",
      "epoch 113; iter: 0; batch classifier loss: 0.140443; batch adversarial loss: 0.331998\n",
      "epoch 114; iter: 0; batch classifier loss: 0.189124; batch adversarial loss: 0.218224\n",
      "epoch 115; iter: 0; batch classifier loss: 0.155297; batch adversarial loss: 0.281179\n",
      "epoch 116; iter: 0; batch classifier loss: 0.183952; batch adversarial loss: 0.307975\n",
      "epoch 117; iter: 0; batch classifier loss: 0.301581; batch adversarial loss: 0.324843\n",
      "epoch 118; iter: 0; batch classifier loss: 0.391637; batch adversarial loss: 0.197298\n",
      "epoch 119; iter: 0; batch classifier loss: 0.143408; batch adversarial loss: 0.337936\n",
      "epoch 120; iter: 0; batch classifier loss: 0.145618; batch adversarial loss: 0.271849\n",
      "epoch 121; iter: 0; batch classifier loss: 0.287651; batch adversarial loss: 0.294676\n",
      "epoch 122; iter: 0; batch classifier loss: 0.166968; batch adversarial loss: 0.314252\n",
      "epoch 123; iter: 0; batch classifier loss: 0.234374; batch adversarial loss: 0.258117\n",
      "epoch 124; iter: 0; batch classifier loss: 0.199122; batch adversarial loss: 0.319657\n",
      "epoch 125; iter: 0; batch classifier loss: 0.249223; batch adversarial loss: 0.219832\n",
      "epoch 126; iter: 0; batch classifier loss: 0.244052; batch adversarial loss: 0.300063\n",
      "epoch 127; iter: 0; batch classifier loss: 0.195318; batch adversarial loss: 0.220771\n",
      "epoch 128; iter: 0; batch classifier loss: 0.201481; batch adversarial loss: 0.187779\n",
      "epoch 129; iter: 0; batch classifier loss: 0.182079; batch adversarial loss: 0.226844\n",
      "epoch 130; iter: 0; batch classifier loss: 0.201839; batch adversarial loss: 0.199760\n",
      "epoch 131; iter: 0; batch classifier loss: 0.286472; batch adversarial loss: 0.243520\n",
      "epoch 132; iter: 0; batch classifier loss: 0.189309; batch adversarial loss: 0.412900\n",
      "epoch 133; iter: 0; batch classifier loss: 0.280920; batch adversarial loss: 0.270832\n",
      "epoch 134; iter: 0; batch classifier loss: 0.183235; batch adversarial loss: 0.293039\n",
      "epoch 135; iter: 0; batch classifier loss: 0.252283; batch adversarial loss: 0.371394\n",
      "epoch 136; iter: 0; batch classifier loss: 0.214422; batch adversarial loss: 0.248893\n",
      "epoch 137; iter: 0; batch classifier loss: 0.186740; batch adversarial loss: 0.290646\n",
      "epoch 138; iter: 0; batch classifier loss: 0.309111; batch adversarial loss: 0.215118\n",
      "epoch 139; iter: 0; batch classifier loss: 0.251764; batch adversarial loss: 0.271779\n",
      "epoch 140; iter: 0; batch classifier loss: 0.237507; batch adversarial loss: 0.235655\n",
      "epoch 141; iter: 0; batch classifier loss: 0.163766; batch adversarial loss: 0.184013\n",
      "epoch 142; iter: 0; batch classifier loss: 0.201743; batch adversarial loss: 0.344518\n",
      "epoch 143; iter: 0; batch classifier loss: 0.184771; batch adversarial loss: 0.320042\n",
      "epoch 144; iter: 0; batch classifier loss: 0.246350; batch adversarial loss: 0.420787\n",
      "epoch 145; iter: 0; batch classifier loss: 0.194467; batch adversarial loss: 0.330958\n",
      "epoch 146; iter: 0; batch classifier loss: 0.121000; batch adversarial loss: 0.337142\n",
      "epoch 147; iter: 0; batch classifier loss: 0.106697; batch adversarial loss: 0.229376\n",
      "epoch 148; iter: 0; batch classifier loss: 0.142153; batch adversarial loss: 0.344204\n",
      "epoch 149; iter: 0; batch classifier loss: 0.245032; batch adversarial loss: 0.288113\n",
      "epoch 150; iter: 0; batch classifier loss: 0.211385; batch adversarial loss: 0.382063\n",
      "epoch 151; iter: 0; batch classifier loss: 0.173901; batch adversarial loss: 0.201508\n",
      "epoch 152; iter: 0; batch classifier loss: 0.303406; batch adversarial loss: 0.333301\n",
      "epoch 153; iter: 0; batch classifier loss: 0.271016; batch adversarial loss: 0.389717\n",
      "epoch 154; iter: 0; batch classifier loss: 0.230605; batch adversarial loss: 0.208560\n",
      "epoch 155; iter: 0; batch classifier loss: 0.131692; batch adversarial loss: 0.237068\n",
      "epoch 156; iter: 0; batch classifier loss: 0.274155; batch adversarial loss: 0.195090\n",
      "epoch 157; iter: 0; batch classifier loss: 0.194085; batch adversarial loss: 0.320387\n",
      "epoch 158; iter: 0; batch classifier loss: 0.199432; batch adversarial loss: 0.251626\n",
      "epoch 159; iter: 0; batch classifier loss: 0.208714; batch adversarial loss: 0.366141\n",
      "epoch 160; iter: 0; batch classifier loss: 0.131673; batch adversarial loss: 0.403804\n",
      "epoch 161; iter: 0; batch classifier loss: 0.257206; batch adversarial loss: 0.265876\n",
      "epoch 162; iter: 0; batch classifier loss: 0.210701; batch adversarial loss: 0.283807\n",
      "epoch 163; iter: 0; batch classifier loss: 0.171537; batch adversarial loss: 0.300684\n",
      "epoch 164; iter: 0; batch classifier loss: 0.276001; batch adversarial loss: 0.252636\n",
      "epoch 165; iter: 0; batch classifier loss: 0.157893; batch adversarial loss: 0.176898\n",
      "epoch 166; iter: 0; batch classifier loss: 0.161697; batch adversarial loss: 0.227266\n",
      "epoch 167; iter: 0; batch classifier loss: 0.284969; batch adversarial loss: 0.277500\n",
      "epoch 168; iter: 0; batch classifier loss: 0.226209; batch adversarial loss: 0.254197\n",
      "epoch 169; iter: 0; batch classifier loss: 0.121123; batch adversarial loss: 0.311547\n",
      "epoch 170; iter: 0; batch classifier loss: 0.142551; batch adversarial loss: 0.219139\n",
      "epoch 171; iter: 0; batch classifier loss: 0.186540; batch adversarial loss: 0.308392\n",
      "epoch 172; iter: 0; batch classifier loss: 0.182577; batch adversarial loss: 0.260911\n",
      "epoch 173; iter: 0; batch classifier loss: 0.147948; batch adversarial loss: 0.226368\n",
      "epoch 174; iter: 0; batch classifier loss: 0.247964; batch adversarial loss: 0.240278\n",
      "epoch 175; iter: 0; batch classifier loss: 0.165763; batch adversarial loss: 0.218667\n",
      "epoch 176; iter: 0; batch classifier loss: 0.169266; batch adversarial loss: 0.286396\n",
      "epoch 177; iter: 0; batch classifier loss: 0.234730; batch adversarial loss: 0.224625\n",
      "epoch 178; iter: 0; batch classifier loss: 0.172227; batch adversarial loss: 0.284458\n",
      "epoch 179; iter: 0; batch classifier loss: 0.186896; batch adversarial loss: 0.343443\n",
      "epoch 180; iter: 0; batch classifier loss: 0.119274; batch adversarial loss: 0.306394\n",
      "epoch 181; iter: 0; batch classifier loss: 0.175428; batch adversarial loss: 0.273207\n",
      "epoch 182; iter: 0; batch classifier loss: 0.213857; batch adversarial loss: 0.332137\n",
      "epoch 183; iter: 0; batch classifier loss: 0.206572; batch adversarial loss: 0.255054\n",
      "epoch 184; iter: 0; batch classifier loss: 0.162933; batch adversarial loss: 0.277923\n",
      "epoch 185; iter: 0; batch classifier loss: 0.189179; batch adversarial loss: 0.168260\n",
      "epoch 186; iter: 0; batch classifier loss: 0.192085; batch adversarial loss: 0.394271\n",
      "epoch 187; iter: 0; batch classifier loss: 0.163788; batch adversarial loss: 0.212611\n",
      "epoch 188; iter: 0; batch classifier loss: 0.197336; batch adversarial loss: 0.286113\n",
      "epoch 189; iter: 0; batch classifier loss: 0.179209; batch adversarial loss: 0.358780\n",
      "epoch 190; iter: 0; batch classifier loss: 0.240565; batch adversarial loss: 0.305965\n",
      "epoch 191; iter: 0; batch classifier loss: 0.133778; batch adversarial loss: 0.317072\n",
      "epoch 192; iter: 0; batch classifier loss: 0.225592; batch adversarial loss: 0.246027\n",
      "epoch 193; iter: 0; batch classifier loss: 0.190250; batch adversarial loss: 0.321835\n",
      "epoch 194; iter: 0; batch classifier loss: 0.136612; batch adversarial loss: 0.273624\n",
      "epoch 195; iter: 0; batch classifier loss: 0.208728; batch adversarial loss: 0.252860\n",
      "epoch 196; iter: 0; batch classifier loss: 0.200741; batch adversarial loss: 0.366425\n",
      "epoch 197; iter: 0; batch classifier loss: 0.207974; batch adversarial loss: 0.473680\n",
      "epoch 198; iter: 0; batch classifier loss: 0.249609; batch adversarial loss: 0.198870\n",
      "epoch 199; iter: 0; batch classifier loss: 0.208773; batch adversarial loss: 0.272716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:56:34.164957: W tensorflow/c/c_api.cc:305] Operation '{name:'39b91052-ae25-11ee-bc15-ae7d8bf09116/39b91052-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:3137 op device:{requested: '', assigned: ''} def:{{{node 39b91052-ae25-11ee-bc15-ae7d8bf09116/39b91052-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b91052-ae25-11ee-bc15-ae7d8bf09116/39b91052-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b91052-ae25-11ee-bc15-ae7d8bf09116/39b91052-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.816403; batch adversarial loss: 0.703588\n",
      "epoch 1; iter: 0; batch classifier loss: 0.290841; batch adversarial loss: 0.563727\n",
      "epoch 2; iter: 0; batch classifier loss: 0.280306; batch adversarial loss: 0.536358\n",
      "epoch 3; iter: 0; batch classifier loss: 0.324429; batch adversarial loss: 0.458522\n",
      "epoch 4; iter: 0; batch classifier loss: 0.271110; batch adversarial loss: 0.424810\n",
      "epoch 5; iter: 0; batch classifier loss: 0.232002; batch adversarial loss: 0.388320\n",
      "epoch 6; iter: 0; batch classifier loss: 0.214427; batch adversarial loss: 0.382112\n",
      "epoch 7; iter: 0; batch classifier loss: 0.267164; batch adversarial loss: 0.382068\n",
      "epoch 8; iter: 0; batch classifier loss: 0.313081; batch adversarial loss: 0.322934\n",
      "epoch 9; iter: 0; batch classifier loss: 0.271785; batch adversarial loss: 0.350013\n",
      "epoch 10; iter: 0; batch classifier loss: 0.239150; batch adversarial loss: 0.290993\n",
      "epoch 11; iter: 0; batch classifier loss: 0.244402; batch adversarial loss: 0.357310\n",
      "epoch 12; iter: 0; batch classifier loss: 0.207545; batch adversarial loss: 0.279411\n",
      "epoch 13; iter: 0; batch classifier loss: 0.285800; batch adversarial loss: 0.221280\n",
      "epoch 14; iter: 0; batch classifier loss: 0.183507; batch adversarial loss: 0.344873\n",
      "epoch 15; iter: 0; batch classifier loss: 0.272807; batch adversarial loss: 0.229021\n",
      "epoch 16; iter: 0; batch classifier loss: 0.209064; batch adversarial loss: 0.292949\n",
      "epoch 17; iter: 0; batch classifier loss: 0.277909; batch adversarial loss: 0.229228\n",
      "epoch 18; iter: 0; batch classifier loss: 0.211700; batch adversarial loss: 0.206126\n",
      "epoch 19; iter: 0; batch classifier loss: 0.141992; batch adversarial loss: 0.251030\n",
      "epoch 20; iter: 0; batch classifier loss: 0.256753; batch adversarial loss: 0.269366\n",
      "epoch 21; iter: 0; batch classifier loss: 0.196253; batch adversarial loss: 0.265689\n",
      "epoch 22; iter: 0; batch classifier loss: 0.257443; batch adversarial loss: 0.250464\n",
      "epoch 23; iter: 0; batch classifier loss: 0.213331; batch adversarial loss: 0.269580\n",
      "epoch 24; iter: 0; batch classifier loss: 0.232078; batch adversarial loss: 0.234499\n",
      "epoch 25; iter: 0; batch classifier loss: 0.194027; batch adversarial loss: 0.428834\n",
      "epoch 26; iter: 0; batch classifier loss: 0.221336; batch adversarial loss: 0.243178\n",
      "epoch 27; iter: 0; batch classifier loss: 0.171589; batch adversarial loss: 0.303871\n",
      "epoch 28; iter: 0; batch classifier loss: 0.288209; batch adversarial loss: 0.354854\n",
      "epoch 29; iter: 0; batch classifier loss: 0.165894; batch adversarial loss: 0.372871\n",
      "epoch 30; iter: 0; batch classifier loss: 0.215259; batch adversarial loss: 0.254404\n",
      "epoch 31; iter: 0; batch classifier loss: 0.276510; batch adversarial loss: 0.202067\n",
      "epoch 32; iter: 0; batch classifier loss: 0.263035; batch adversarial loss: 0.226546\n",
      "epoch 33; iter: 0; batch classifier loss: 0.284785; batch adversarial loss: 0.281352\n",
      "epoch 34; iter: 0; batch classifier loss: 0.258629; batch adversarial loss: 0.170554\n",
      "epoch 35; iter: 0; batch classifier loss: 0.223232; batch adversarial loss: 0.260511\n",
      "epoch 36; iter: 0; batch classifier loss: 0.281785; batch adversarial loss: 0.341536\n",
      "epoch 37; iter: 0; batch classifier loss: 0.223265; batch adversarial loss: 0.230348\n",
      "epoch 38; iter: 0; batch classifier loss: 0.232114; batch adversarial loss: 0.234380\n",
      "epoch 39; iter: 0; batch classifier loss: 0.279345; batch adversarial loss: 0.147424\n",
      "epoch 40; iter: 0; batch classifier loss: 0.202167; batch adversarial loss: 0.195742\n",
      "epoch 41; iter: 0; batch classifier loss: 0.232022; batch adversarial loss: 0.133829\n",
      "epoch 42; iter: 0; batch classifier loss: 0.189709; batch adversarial loss: 0.234047\n",
      "epoch 43; iter: 0; batch classifier loss: 0.268125; batch adversarial loss: 0.200338\n",
      "epoch 44; iter: 0; batch classifier loss: 0.134500; batch adversarial loss: 0.269883\n",
      "epoch 45; iter: 0; batch classifier loss: 0.194232; batch adversarial loss: 0.213031\n",
      "epoch 46; iter: 0; batch classifier loss: 0.149129; batch adversarial loss: 0.222310\n",
      "epoch 47; iter: 0; batch classifier loss: 0.183846; batch adversarial loss: 0.322856\n",
      "epoch 48; iter: 0; batch classifier loss: 0.174357; batch adversarial loss: 0.306511\n",
      "epoch 49; iter: 0; batch classifier loss: 0.191146; batch adversarial loss: 0.249536\n",
      "epoch 50; iter: 0; batch classifier loss: 0.166992; batch adversarial loss: 0.272256\n",
      "epoch 51; iter: 0; batch classifier loss: 0.242718; batch adversarial loss: 0.308763\n",
      "epoch 52; iter: 0; batch classifier loss: 0.222200; batch adversarial loss: 0.227210\n",
      "epoch 53; iter: 0; batch classifier loss: 0.215346; batch adversarial loss: 0.255262\n",
      "epoch 54; iter: 0; batch classifier loss: 0.241728; batch adversarial loss: 0.258743\n",
      "epoch 55; iter: 0; batch classifier loss: 0.251203; batch adversarial loss: 0.308007\n",
      "epoch 56; iter: 0; batch classifier loss: 0.202854; batch adversarial loss: 0.280686\n",
      "epoch 57; iter: 0; batch classifier loss: 0.250562; batch adversarial loss: 0.311418\n",
      "epoch 58; iter: 0; batch classifier loss: 0.200253; batch adversarial loss: 0.308265\n",
      "epoch 59; iter: 0; batch classifier loss: 0.189385; batch adversarial loss: 0.217434\n",
      "epoch 60; iter: 0; batch classifier loss: 0.205999; batch adversarial loss: 0.256476\n",
      "epoch 61; iter: 0; batch classifier loss: 0.189936; batch adversarial loss: 0.190165\n",
      "epoch 62; iter: 0; batch classifier loss: 0.232302; batch adversarial loss: 0.214247\n",
      "epoch 63; iter: 0; batch classifier loss: 0.265334; batch adversarial loss: 0.347163\n",
      "epoch 64; iter: 0; batch classifier loss: 0.167682; batch adversarial loss: 0.246634\n",
      "epoch 65; iter: 0; batch classifier loss: 0.150510; batch adversarial loss: 0.238087\n",
      "epoch 66; iter: 0; batch classifier loss: 0.162033; batch adversarial loss: 0.195180\n",
      "epoch 67; iter: 0; batch classifier loss: 0.233813; batch adversarial loss: 0.265178\n",
      "epoch 68; iter: 0; batch classifier loss: 0.212738; batch adversarial loss: 0.444533\n",
      "epoch 69; iter: 0; batch classifier loss: 0.152205; batch adversarial loss: 0.127642\n",
      "epoch 70; iter: 0; batch classifier loss: 0.230161; batch adversarial loss: 0.214964\n",
      "epoch 71; iter: 0; batch classifier loss: 0.202837; batch adversarial loss: 0.228202\n",
      "epoch 72; iter: 0; batch classifier loss: 0.268842; batch adversarial loss: 0.178778\n",
      "epoch 73; iter: 0; batch classifier loss: 0.262433; batch adversarial loss: 0.284043\n",
      "epoch 74; iter: 0; batch classifier loss: 0.152411; batch adversarial loss: 0.256929\n",
      "epoch 75; iter: 0; batch classifier loss: 0.166037; batch adversarial loss: 0.261836\n",
      "epoch 76; iter: 0; batch classifier loss: 0.262374; batch adversarial loss: 0.192331\n",
      "epoch 77; iter: 0; batch classifier loss: 0.142411; batch adversarial loss: 0.318363\n",
      "epoch 78; iter: 0; batch classifier loss: 0.276574; batch adversarial loss: 0.217881\n",
      "epoch 79; iter: 0; batch classifier loss: 0.207118; batch adversarial loss: 0.184262\n",
      "epoch 80; iter: 0; batch classifier loss: 0.258169; batch adversarial loss: 0.299849\n",
      "epoch 81; iter: 0; batch classifier loss: 0.240990; batch adversarial loss: 0.257416\n",
      "epoch 82; iter: 0; batch classifier loss: 0.168525; batch adversarial loss: 0.254692\n",
      "epoch 83; iter: 0; batch classifier loss: 0.171348; batch adversarial loss: 0.187365\n",
      "epoch 84; iter: 0; batch classifier loss: 0.188839; batch adversarial loss: 0.269313\n",
      "epoch 85; iter: 0; batch classifier loss: 0.216425; batch adversarial loss: 0.257767\n",
      "epoch 86; iter: 0; batch classifier loss: 0.204946; batch adversarial loss: 0.283787\n",
      "epoch 87; iter: 0; batch classifier loss: 0.195881; batch adversarial loss: 0.267754\n",
      "epoch 88; iter: 0; batch classifier loss: 0.174034; batch adversarial loss: 0.323696\n",
      "epoch 89; iter: 0; batch classifier loss: 0.232297; batch adversarial loss: 0.238170\n",
      "epoch 90; iter: 0; batch classifier loss: 0.146807; batch adversarial loss: 0.183482\n",
      "epoch 91; iter: 0; batch classifier loss: 0.201400; batch adversarial loss: 0.239902\n",
      "epoch 92; iter: 0; batch classifier loss: 0.250950; batch adversarial loss: 0.202457\n",
      "epoch 93; iter: 0; batch classifier loss: 0.233170; batch adversarial loss: 0.323299\n",
      "epoch 94; iter: 0; batch classifier loss: 0.116492; batch adversarial loss: 0.307483\n",
      "epoch 95; iter: 0; batch classifier loss: 0.219810; batch adversarial loss: 0.211389\n",
      "epoch 96; iter: 0; batch classifier loss: 0.191757; batch adversarial loss: 0.242057\n",
      "epoch 97; iter: 0; batch classifier loss: 0.141116; batch adversarial loss: 0.292217\n",
      "epoch 98; iter: 0; batch classifier loss: 0.235402; batch adversarial loss: 0.219988\n",
      "epoch 99; iter: 0; batch classifier loss: 0.228403; batch adversarial loss: 0.237022\n",
      "epoch 100; iter: 0; batch classifier loss: 0.261347; batch adversarial loss: 0.290347\n",
      "epoch 101; iter: 0; batch classifier loss: 0.198247; batch adversarial loss: 0.433410\n",
      "epoch 102; iter: 0; batch classifier loss: 0.159629; batch adversarial loss: 0.225837\n",
      "epoch 103; iter: 0; batch classifier loss: 0.211348; batch adversarial loss: 0.227658\n",
      "epoch 104; iter: 0; batch classifier loss: 0.118029; batch adversarial loss: 0.249750\n",
      "epoch 105; iter: 0; batch classifier loss: 0.232022; batch adversarial loss: 0.244709\n",
      "epoch 106; iter: 0; batch classifier loss: 0.190201; batch adversarial loss: 0.201527\n",
      "epoch 107; iter: 0; batch classifier loss: 0.220274; batch adversarial loss: 0.237848\n",
      "epoch 108; iter: 0; batch classifier loss: 0.178895; batch adversarial loss: 0.239603\n",
      "epoch 109; iter: 0; batch classifier loss: 0.160759; batch adversarial loss: 0.253703\n",
      "epoch 110; iter: 0; batch classifier loss: 0.188170; batch adversarial loss: 0.370073\n",
      "epoch 111; iter: 0; batch classifier loss: 0.163465; batch adversarial loss: 0.227755\n",
      "epoch 112; iter: 0; batch classifier loss: 0.283607; batch adversarial loss: 0.228064\n",
      "epoch 113; iter: 0; batch classifier loss: 0.216592; batch adversarial loss: 0.316164\n",
      "epoch 114; iter: 0; batch classifier loss: 0.227275; batch adversarial loss: 0.246626\n",
      "epoch 115; iter: 0; batch classifier loss: 0.168990; batch adversarial loss: 0.257995\n",
      "epoch 116; iter: 0; batch classifier loss: 0.221884; batch adversarial loss: 0.244889\n",
      "epoch 117; iter: 0; batch classifier loss: 0.142637; batch adversarial loss: 0.214685\n",
      "epoch 118; iter: 0; batch classifier loss: 0.190357; batch adversarial loss: 0.351431\n",
      "epoch 119; iter: 0; batch classifier loss: 0.296036; batch adversarial loss: 0.205999\n",
      "epoch 120; iter: 0; batch classifier loss: 0.259027; batch adversarial loss: 0.309615\n",
      "epoch 121; iter: 0; batch classifier loss: 0.199875; batch adversarial loss: 0.237864\n",
      "epoch 122; iter: 0; batch classifier loss: 0.303616; batch adversarial loss: 0.326780\n",
      "epoch 123; iter: 0; batch classifier loss: 0.314301; batch adversarial loss: 0.342992\n",
      "epoch 124; iter: 0; batch classifier loss: 0.239485; batch adversarial loss: 0.271547\n",
      "epoch 125; iter: 0; batch classifier loss: 0.245490; batch adversarial loss: 0.253541\n",
      "epoch 126; iter: 0; batch classifier loss: 0.204032; batch adversarial loss: 0.265340\n",
      "epoch 127; iter: 0; batch classifier loss: 0.217857; batch adversarial loss: 0.299013\n",
      "epoch 128; iter: 0; batch classifier loss: 0.247916; batch adversarial loss: 0.228328\n",
      "epoch 129; iter: 0; batch classifier loss: 0.226856; batch adversarial loss: 0.286743\n",
      "epoch 130; iter: 0; batch classifier loss: 0.169914; batch adversarial loss: 0.309233\n",
      "epoch 131; iter: 0; batch classifier loss: 0.264889; batch adversarial loss: 0.229995\n",
      "epoch 132; iter: 0; batch classifier loss: 0.166917; batch adversarial loss: 0.255719\n",
      "epoch 133; iter: 0; batch classifier loss: 0.284600; batch adversarial loss: 0.232927\n",
      "epoch 134; iter: 0; batch classifier loss: 0.137791; batch adversarial loss: 0.204115\n",
      "epoch 135; iter: 0; batch classifier loss: 0.231786; batch adversarial loss: 0.203375\n",
      "epoch 136; iter: 0; batch classifier loss: 0.149862; batch adversarial loss: 0.211295\n",
      "epoch 137; iter: 0; batch classifier loss: 0.186356; batch adversarial loss: 0.315191\n",
      "epoch 138; iter: 0; batch classifier loss: 0.162189; batch adversarial loss: 0.193968\n",
      "epoch 139; iter: 0; batch classifier loss: 0.272516; batch adversarial loss: 0.260846\n",
      "epoch 140; iter: 0; batch classifier loss: 0.227331; batch adversarial loss: 0.297188\n",
      "epoch 141; iter: 0; batch classifier loss: 0.121252; batch adversarial loss: 0.330254\n",
      "epoch 142; iter: 0; batch classifier loss: 0.156570; batch adversarial loss: 0.322489\n",
      "epoch 143; iter: 0; batch classifier loss: 0.142768; batch adversarial loss: 0.230814\n",
      "epoch 144; iter: 0; batch classifier loss: 0.147578; batch adversarial loss: 0.336436\n",
      "epoch 145; iter: 0; batch classifier loss: 0.180488; batch adversarial loss: 0.297107\n",
      "epoch 146; iter: 0; batch classifier loss: 0.183026; batch adversarial loss: 0.299163\n",
      "epoch 147; iter: 0; batch classifier loss: 0.217643; batch adversarial loss: 0.204568\n",
      "epoch 148; iter: 0; batch classifier loss: 0.183457; batch adversarial loss: 0.432429\n",
      "epoch 149; iter: 0; batch classifier loss: 0.287532; batch adversarial loss: 0.218074\n",
      "epoch 150; iter: 0; batch classifier loss: 0.200252; batch adversarial loss: 0.226820\n",
      "epoch 151; iter: 0; batch classifier loss: 0.206025; batch adversarial loss: 0.213188\n",
      "epoch 152; iter: 0; batch classifier loss: 0.139951; batch adversarial loss: 0.344742\n",
      "epoch 153; iter: 0; batch classifier loss: 0.146321; batch adversarial loss: 0.272013\n",
      "epoch 154; iter: 0; batch classifier loss: 0.285759; batch adversarial loss: 0.278201\n",
      "epoch 155; iter: 0; batch classifier loss: 0.206770; batch adversarial loss: 0.258109\n",
      "epoch 156; iter: 0; batch classifier loss: 0.259548; batch adversarial loss: 0.298277\n",
      "epoch 157; iter: 0; batch classifier loss: 0.152047; batch adversarial loss: 0.223142\n",
      "epoch 158; iter: 0; batch classifier loss: 0.161937; batch adversarial loss: 0.267726\n",
      "epoch 159; iter: 0; batch classifier loss: 0.184271; batch adversarial loss: 0.297342\n",
      "epoch 160; iter: 0; batch classifier loss: 0.272063; batch adversarial loss: 0.197784\n",
      "epoch 161; iter: 0; batch classifier loss: 0.244850; batch adversarial loss: 0.306720\n",
      "epoch 162; iter: 0; batch classifier loss: 0.203751; batch adversarial loss: 0.271891\n",
      "epoch 163; iter: 0; batch classifier loss: 0.272221; batch adversarial loss: 0.181685\n",
      "epoch 164; iter: 0; batch classifier loss: 0.242759; batch adversarial loss: 0.246792\n",
      "epoch 165; iter: 0; batch classifier loss: 0.142855; batch adversarial loss: 0.232104\n",
      "epoch 166; iter: 0; batch classifier loss: 0.199964; batch adversarial loss: 0.218206\n",
      "epoch 167; iter: 0; batch classifier loss: 0.286427; batch adversarial loss: 0.255691\n",
      "epoch 168; iter: 0; batch classifier loss: 0.162998; batch adversarial loss: 0.312976\n",
      "epoch 169; iter: 0; batch classifier loss: 0.148785; batch adversarial loss: 0.344976\n",
      "epoch 170; iter: 0; batch classifier loss: 0.199634; batch adversarial loss: 0.282738\n",
      "epoch 171; iter: 0; batch classifier loss: 0.185482; batch adversarial loss: 0.286511\n",
      "epoch 172; iter: 0; batch classifier loss: 0.235715; batch adversarial loss: 0.255986\n",
      "epoch 173; iter: 0; batch classifier loss: 0.253058; batch adversarial loss: 0.220328\n",
      "epoch 174; iter: 0; batch classifier loss: 0.180963; batch adversarial loss: 0.332298\n",
      "epoch 175; iter: 0; batch classifier loss: 0.200846; batch adversarial loss: 0.398218\n",
      "epoch 176; iter: 0; batch classifier loss: 0.195010; batch adversarial loss: 0.222185\n",
      "epoch 177; iter: 0; batch classifier loss: 0.212960; batch adversarial loss: 0.252357\n",
      "epoch 178; iter: 0; batch classifier loss: 0.243533; batch adversarial loss: 0.173239\n",
      "epoch 179; iter: 0; batch classifier loss: 0.171962; batch adversarial loss: 0.276113\n",
      "epoch 180; iter: 0; batch classifier loss: 0.283848; batch adversarial loss: 0.303369\n",
      "epoch 181; iter: 0; batch classifier loss: 0.202179; batch adversarial loss: 0.253106\n",
      "epoch 182; iter: 0; batch classifier loss: 0.243287; batch adversarial loss: 0.287035\n",
      "epoch 183; iter: 0; batch classifier loss: 0.148194; batch adversarial loss: 0.299377\n",
      "epoch 184; iter: 0; batch classifier loss: 0.251085; batch adversarial loss: 0.279135\n",
      "epoch 185; iter: 0; batch classifier loss: 0.213145; batch adversarial loss: 0.286209\n",
      "epoch 186; iter: 0; batch classifier loss: 0.207954; batch adversarial loss: 0.224666\n",
      "epoch 187; iter: 0; batch classifier loss: 0.195584; batch adversarial loss: 0.280079\n",
      "epoch 188; iter: 0; batch classifier loss: 0.278473; batch adversarial loss: 0.250262\n",
      "epoch 189; iter: 0; batch classifier loss: 0.131101; batch adversarial loss: 0.275613\n",
      "epoch 190; iter: 0; batch classifier loss: 0.231958; batch adversarial loss: 0.234040\n",
      "epoch 191; iter: 0; batch classifier loss: 0.193390; batch adversarial loss: 0.240028\n",
      "epoch 192; iter: 0; batch classifier loss: 0.184144; batch adversarial loss: 0.189824\n",
      "epoch 193; iter: 0; batch classifier loss: 0.242993; batch adversarial loss: 0.122883\n",
      "epoch 194; iter: 0; batch classifier loss: 0.202849; batch adversarial loss: 0.271529\n",
      "epoch 195; iter: 0; batch classifier loss: 0.159432; batch adversarial loss: 0.275326\n",
      "epoch 196; iter: 0; batch classifier loss: 0.166691; batch adversarial loss: 0.310723\n",
      "epoch 197; iter: 0; batch classifier loss: 0.174998; batch adversarial loss: 0.295744\n",
      "epoch 198; iter: 0; batch classifier loss: 0.207175; batch adversarial loss: 0.301054\n",
      "epoch 199; iter: 0; batch classifier loss: 0.174699; batch adversarial loss: 0.265170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:56:47.806936: W tensorflow/c/c_api.cc:305] Operation '{name:'39b91084-ae25-11ee-bc15-ae7d8bf09116/39b91084-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:3927 op device:{requested: '', assigned: ''} def:{{{node 39b91084-ae25-11ee-bc15-ae7d8bf09116/39b91084-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b91084-ae25-11ee-bc15-ae7d8bf09116/39b91084-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b91084-ae25-11ee-bc15-ae7d8bf09116/39b91084-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.629478; batch adversarial loss: 0.721559\n",
      "epoch 1; iter: 0; batch classifier loss: 0.230808; batch adversarial loss: 0.638922\n",
      "epoch 2; iter: 0; batch classifier loss: 0.183496; batch adversarial loss: 0.558830\n",
      "epoch 3; iter: 0; batch classifier loss: 0.249721; batch adversarial loss: 0.456061\n",
      "epoch 4; iter: 0; batch classifier loss: 0.246963; batch adversarial loss: 0.409425\n",
      "epoch 5; iter: 0; batch classifier loss: 0.190899; batch adversarial loss: 0.318589\n",
      "epoch 6; iter: 0; batch classifier loss: 0.297664; batch adversarial loss: 0.316906\n",
      "epoch 7; iter: 0; batch classifier loss: 0.182267; batch adversarial loss: 0.352395\n",
      "epoch 8; iter: 0; batch classifier loss: 0.167243; batch adversarial loss: 0.279795\n",
      "epoch 9; iter: 0; batch classifier loss: 0.301817; batch adversarial loss: 0.273158\n",
      "epoch 10; iter: 0; batch classifier loss: 0.354462; batch adversarial loss: 0.368062\n",
      "epoch 11; iter: 0; batch classifier loss: 0.257211; batch adversarial loss: 0.314889\n",
      "epoch 12; iter: 0; batch classifier loss: 0.289462; batch adversarial loss: 0.282366\n",
      "epoch 13; iter: 0; batch classifier loss: 0.202487; batch adversarial loss: 0.249052\n",
      "epoch 14; iter: 0; batch classifier loss: 0.239342; batch adversarial loss: 0.318917\n",
      "epoch 15; iter: 0; batch classifier loss: 0.172810; batch adversarial loss: 0.277826\n",
      "epoch 16; iter: 0; batch classifier loss: 0.180367; batch adversarial loss: 0.203413\n",
      "epoch 17; iter: 0; batch classifier loss: 0.213870; batch adversarial loss: 0.315199\n",
      "epoch 18; iter: 0; batch classifier loss: 0.248512; batch adversarial loss: 0.299985\n",
      "epoch 19; iter: 0; batch classifier loss: 0.219733; batch adversarial loss: 0.290442\n",
      "epoch 20; iter: 0; batch classifier loss: 0.168369; batch adversarial loss: 0.303431\n",
      "epoch 21; iter: 0; batch classifier loss: 0.151843; batch adversarial loss: 0.235417\n",
      "epoch 22; iter: 0; batch classifier loss: 0.264543; batch adversarial loss: 0.315504\n",
      "epoch 23; iter: 0; batch classifier loss: 0.296237; batch adversarial loss: 0.233362\n",
      "epoch 24; iter: 0; batch classifier loss: 0.245101; batch adversarial loss: 0.339844\n",
      "epoch 25; iter: 0; batch classifier loss: 0.199306; batch adversarial loss: 0.234418\n",
      "epoch 26; iter: 0; batch classifier loss: 0.225247; batch adversarial loss: 0.226963\n",
      "epoch 27; iter: 0; batch classifier loss: 0.160014; batch adversarial loss: 0.219850\n",
      "epoch 28; iter: 0; batch classifier loss: 0.229228; batch adversarial loss: 0.198848\n",
      "epoch 29; iter: 0; batch classifier loss: 0.152464; batch adversarial loss: 0.285553\n",
      "epoch 30; iter: 0; batch classifier loss: 0.278878; batch adversarial loss: 0.246546\n",
      "epoch 31; iter: 0; batch classifier loss: 0.193797; batch adversarial loss: 0.344053\n",
      "epoch 32; iter: 0; batch classifier loss: 0.260678; batch adversarial loss: 0.221976\n",
      "epoch 33; iter: 0; batch classifier loss: 0.229917; batch adversarial loss: 0.337855\n",
      "epoch 34; iter: 0; batch classifier loss: 0.253625; batch adversarial loss: 0.277243\n",
      "epoch 35; iter: 0; batch classifier loss: 0.273276; batch adversarial loss: 0.274401\n",
      "epoch 36; iter: 0; batch classifier loss: 0.169401; batch adversarial loss: 0.245208\n",
      "epoch 37; iter: 0; batch classifier loss: 0.254295; batch adversarial loss: 0.214708\n",
      "epoch 38; iter: 0; batch classifier loss: 0.247606; batch adversarial loss: 0.341463\n",
      "epoch 39; iter: 0; batch classifier loss: 0.189686; batch adversarial loss: 0.294997\n",
      "epoch 40; iter: 0; batch classifier loss: 0.169802; batch adversarial loss: 0.198310\n",
      "epoch 41; iter: 0; batch classifier loss: 0.295665; batch adversarial loss: 0.348573\n",
      "epoch 42; iter: 0; batch classifier loss: 0.145072; batch adversarial loss: 0.215516\n",
      "epoch 43; iter: 0; batch classifier loss: 0.307460; batch adversarial loss: 0.317764\n",
      "epoch 44; iter: 0; batch classifier loss: 0.216635; batch adversarial loss: 0.311793\n",
      "epoch 45; iter: 0; batch classifier loss: 0.233230; batch adversarial loss: 0.281361\n",
      "epoch 46; iter: 0; batch classifier loss: 0.196793; batch adversarial loss: 0.230130\n",
      "epoch 47; iter: 0; batch classifier loss: 0.221875; batch adversarial loss: 0.367359\n",
      "epoch 48; iter: 0; batch classifier loss: 0.158591; batch adversarial loss: 0.197420\n",
      "epoch 49; iter: 0; batch classifier loss: 0.217898; batch adversarial loss: 0.279716\n",
      "epoch 50; iter: 0; batch classifier loss: 0.232729; batch adversarial loss: 0.216078\n",
      "epoch 51; iter: 0; batch classifier loss: 0.146132; batch adversarial loss: 0.301542\n",
      "epoch 52; iter: 0; batch classifier loss: 0.322436; batch adversarial loss: 0.235270\n",
      "epoch 53; iter: 0; batch classifier loss: 0.171837; batch adversarial loss: 0.264247\n",
      "epoch 54; iter: 0; batch classifier loss: 0.230430; batch adversarial loss: 0.243883\n",
      "epoch 55; iter: 0; batch classifier loss: 0.241671; batch adversarial loss: 0.246058\n",
      "epoch 56; iter: 0; batch classifier loss: 0.168558; batch adversarial loss: 0.226784\n",
      "epoch 57; iter: 0; batch classifier loss: 0.148424; batch adversarial loss: 0.256409\n",
      "epoch 58; iter: 0; batch classifier loss: 0.254090; batch adversarial loss: 0.278288\n",
      "epoch 59; iter: 0; batch classifier loss: 0.178614; batch adversarial loss: 0.292308\n",
      "epoch 60; iter: 0; batch classifier loss: 0.247547; batch adversarial loss: 0.209724\n",
      "epoch 61; iter: 0; batch classifier loss: 0.140992; batch adversarial loss: 0.354735\n",
      "epoch 62; iter: 0; batch classifier loss: 0.196713; batch adversarial loss: 0.353444\n",
      "epoch 63; iter: 0; batch classifier loss: 0.242186; batch adversarial loss: 0.343844\n",
      "epoch 64; iter: 0; batch classifier loss: 0.321312; batch adversarial loss: 0.269307\n",
      "epoch 65; iter: 0; batch classifier loss: 0.171931; batch adversarial loss: 0.224151\n",
      "epoch 66; iter: 0; batch classifier loss: 0.234628; batch adversarial loss: 0.166249\n",
      "epoch 67; iter: 0; batch classifier loss: 0.242786; batch adversarial loss: 0.317396\n",
      "epoch 68; iter: 0; batch classifier loss: 0.236817; batch adversarial loss: 0.293278\n",
      "epoch 69; iter: 0; batch classifier loss: 0.111684; batch adversarial loss: 0.265412\n",
      "epoch 70; iter: 0; batch classifier loss: 0.243368; batch adversarial loss: 0.247946\n",
      "epoch 71; iter: 0; batch classifier loss: 0.131025; batch adversarial loss: 0.190260\n",
      "epoch 72; iter: 0; batch classifier loss: 0.163835; batch adversarial loss: 0.240399\n",
      "epoch 73; iter: 0; batch classifier loss: 0.212566; batch adversarial loss: 0.289541\n",
      "epoch 74; iter: 0; batch classifier loss: 0.234127; batch adversarial loss: 0.315532\n",
      "epoch 75; iter: 0; batch classifier loss: 0.245063; batch adversarial loss: 0.224878\n",
      "epoch 76; iter: 0; batch classifier loss: 0.179439; batch adversarial loss: 0.264622\n",
      "epoch 77; iter: 0; batch classifier loss: 0.210335; batch adversarial loss: 0.244295\n",
      "epoch 78; iter: 0; batch classifier loss: 0.142164; batch adversarial loss: 0.310414\n",
      "epoch 79; iter: 0; batch classifier loss: 0.182281; batch adversarial loss: 0.327141\n",
      "epoch 80; iter: 0; batch classifier loss: 0.180529; batch adversarial loss: 0.275520\n",
      "epoch 81; iter: 0; batch classifier loss: 0.180886; batch adversarial loss: 0.265233\n",
      "epoch 82; iter: 0; batch classifier loss: 0.246680; batch adversarial loss: 0.289795\n",
      "epoch 83; iter: 0; batch classifier loss: 0.209156; batch adversarial loss: 0.242577\n",
      "epoch 84; iter: 0; batch classifier loss: 0.187052; batch adversarial loss: 0.206914\n",
      "epoch 85; iter: 0; batch classifier loss: 0.192289; batch adversarial loss: 0.266055\n",
      "epoch 86; iter: 0; batch classifier loss: 0.209471; batch adversarial loss: 0.306264\n",
      "epoch 87; iter: 0; batch classifier loss: 0.246189; batch adversarial loss: 0.277354\n",
      "epoch 88; iter: 0; batch classifier loss: 0.222585; batch adversarial loss: 0.341594\n",
      "epoch 89; iter: 0; batch classifier loss: 0.166279; batch adversarial loss: 0.233767\n",
      "epoch 90; iter: 0; batch classifier loss: 0.294357; batch adversarial loss: 0.223483\n",
      "epoch 91; iter: 0; batch classifier loss: 0.128895; batch adversarial loss: 0.242919\n",
      "epoch 92; iter: 0; batch classifier loss: 0.178880; batch adversarial loss: 0.330192\n",
      "epoch 93; iter: 0; batch classifier loss: 0.175956; batch adversarial loss: 0.236027\n",
      "epoch 94; iter: 0; batch classifier loss: 0.241275; batch adversarial loss: 0.241945\n",
      "epoch 95; iter: 0; batch classifier loss: 0.142435; batch adversarial loss: 0.278321\n",
      "epoch 96; iter: 0; batch classifier loss: 0.261490; batch adversarial loss: 0.220337\n",
      "epoch 97; iter: 0; batch classifier loss: 0.103686; batch adversarial loss: 0.245942\n",
      "epoch 98; iter: 0; batch classifier loss: 0.163122; batch adversarial loss: 0.235869\n",
      "epoch 99; iter: 0; batch classifier loss: 0.245678; batch adversarial loss: 0.218941\n",
      "epoch 100; iter: 0; batch classifier loss: 0.209901; batch adversarial loss: 0.277267\n",
      "epoch 101; iter: 0; batch classifier loss: 0.155360; batch adversarial loss: 0.269833\n",
      "epoch 102; iter: 0; batch classifier loss: 0.225527; batch adversarial loss: 0.295849\n",
      "epoch 103; iter: 0; batch classifier loss: 0.241375; batch adversarial loss: 0.258631\n",
      "epoch 104; iter: 0; batch classifier loss: 0.120201; batch adversarial loss: 0.283657\n",
      "epoch 105; iter: 0; batch classifier loss: 0.237956; batch adversarial loss: 0.261986\n",
      "epoch 106; iter: 0; batch classifier loss: 0.226390; batch adversarial loss: 0.252519\n",
      "epoch 107; iter: 0; batch classifier loss: 0.187090; batch adversarial loss: 0.193396\n",
      "epoch 108; iter: 0; batch classifier loss: 0.231695; batch adversarial loss: 0.334662\n",
      "epoch 109; iter: 0; batch classifier loss: 0.164904; batch adversarial loss: 0.304971\n",
      "epoch 110; iter: 0; batch classifier loss: 0.233503; batch adversarial loss: 0.221558\n",
      "epoch 111; iter: 0; batch classifier loss: 0.163631; batch adversarial loss: 0.280701\n",
      "epoch 112; iter: 0; batch classifier loss: 0.159996; batch adversarial loss: 0.220804\n",
      "epoch 113; iter: 0; batch classifier loss: 0.171239; batch adversarial loss: 0.236157\n",
      "epoch 114; iter: 0; batch classifier loss: 0.221750; batch adversarial loss: 0.220704\n",
      "epoch 115; iter: 0; batch classifier loss: 0.258287; batch adversarial loss: 0.245816\n",
      "epoch 116; iter: 0; batch classifier loss: 0.173460; batch adversarial loss: 0.334392\n",
      "epoch 117; iter: 0; batch classifier loss: 0.264004; batch adversarial loss: 0.282356\n",
      "epoch 118; iter: 0; batch classifier loss: 0.178366; batch adversarial loss: 0.276954\n",
      "epoch 119; iter: 0; batch classifier loss: 0.229454; batch adversarial loss: 0.255534\n",
      "epoch 120; iter: 0; batch classifier loss: 0.145762; batch adversarial loss: 0.393229\n",
      "epoch 121; iter: 0; batch classifier loss: 0.266424; batch adversarial loss: 0.265671\n",
      "epoch 122; iter: 0; batch classifier loss: 0.229414; batch adversarial loss: 0.269004\n",
      "epoch 123; iter: 0; batch classifier loss: 0.213027; batch adversarial loss: 0.290435\n",
      "epoch 124; iter: 0; batch classifier loss: 0.234298; batch adversarial loss: 0.268745\n",
      "epoch 125; iter: 0; batch classifier loss: 0.257650; batch adversarial loss: 0.341988\n",
      "epoch 126; iter: 0; batch classifier loss: 0.164424; batch adversarial loss: 0.308285\n",
      "epoch 127; iter: 0; batch classifier loss: 0.165133; batch adversarial loss: 0.284796\n",
      "epoch 128; iter: 0; batch classifier loss: 0.200751; batch adversarial loss: 0.261914\n",
      "epoch 129; iter: 0; batch classifier loss: 0.201661; batch adversarial loss: 0.242755\n",
      "epoch 130; iter: 0; batch classifier loss: 0.220687; batch adversarial loss: 0.227961\n",
      "epoch 131; iter: 0; batch classifier loss: 0.169557; batch adversarial loss: 0.270773\n",
      "epoch 132; iter: 0; batch classifier loss: 0.215057; batch adversarial loss: 0.226287\n",
      "epoch 133; iter: 0; batch classifier loss: 0.143432; batch adversarial loss: 0.352431\n",
      "epoch 134; iter: 0; batch classifier loss: 0.200268; batch adversarial loss: 0.290363\n",
      "epoch 135; iter: 0; batch classifier loss: 0.195798; batch adversarial loss: 0.263141\n",
      "epoch 136; iter: 0; batch classifier loss: 0.237335; batch adversarial loss: 0.221830\n",
      "epoch 137; iter: 0; batch classifier loss: 0.264787; batch adversarial loss: 0.261993\n",
      "epoch 138; iter: 0; batch classifier loss: 0.194267; batch adversarial loss: 0.244792\n",
      "epoch 139; iter: 0; batch classifier loss: 0.219350; batch adversarial loss: 0.253154\n",
      "epoch 140; iter: 0; batch classifier loss: 0.220366; batch adversarial loss: 0.245070\n",
      "epoch 141; iter: 0; batch classifier loss: 0.205509; batch adversarial loss: 0.336641\n",
      "epoch 142; iter: 0; batch classifier loss: 0.222454; batch adversarial loss: 0.259756\n",
      "epoch 143; iter: 0; batch classifier loss: 0.216714; batch adversarial loss: 0.247229\n",
      "epoch 144; iter: 0; batch classifier loss: 0.121332; batch adversarial loss: 0.281530\n",
      "epoch 145; iter: 0; batch classifier loss: 0.166701; batch adversarial loss: 0.422469\n",
      "epoch 146; iter: 0; batch classifier loss: 0.202809; batch adversarial loss: 0.307113\n",
      "epoch 147; iter: 0; batch classifier loss: 0.273445; batch adversarial loss: 0.355157\n",
      "epoch 148; iter: 0; batch classifier loss: 0.265924; batch adversarial loss: 0.245178\n",
      "epoch 149; iter: 0; batch classifier loss: 0.185473; batch adversarial loss: 0.301578\n",
      "epoch 150; iter: 0; batch classifier loss: 0.170674; batch adversarial loss: 0.324958\n",
      "epoch 151; iter: 0; batch classifier loss: 0.284944; batch adversarial loss: 0.299963\n",
      "epoch 152; iter: 0; batch classifier loss: 0.179328; batch adversarial loss: 0.323619\n",
      "epoch 153; iter: 0; batch classifier loss: 0.166933; batch adversarial loss: 0.249137\n",
      "epoch 154; iter: 0; batch classifier loss: 0.133653; batch adversarial loss: 0.330358\n",
      "epoch 155; iter: 0; batch classifier loss: 0.181059; batch adversarial loss: 0.206024\n",
      "epoch 156; iter: 0; batch classifier loss: 0.162950; batch adversarial loss: 0.181520\n",
      "epoch 157; iter: 0; batch classifier loss: 0.185217; batch adversarial loss: 0.187553\n",
      "epoch 158; iter: 0; batch classifier loss: 0.236686; batch adversarial loss: 0.345892\n",
      "epoch 159; iter: 0; batch classifier loss: 0.170036; batch adversarial loss: 0.268077\n",
      "epoch 160; iter: 0; batch classifier loss: 0.146012; batch adversarial loss: 0.266344\n",
      "epoch 161; iter: 0; batch classifier loss: 0.199447; batch adversarial loss: 0.282727\n",
      "epoch 162; iter: 0; batch classifier loss: 0.164272; batch adversarial loss: 0.293155\n",
      "epoch 163; iter: 0; batch classifier loss: 0.204933; batch adversarial loss: 0.281960\n",
      "epoch 164; iter: 0; batch classifier loss: 0.256168; batch adversarial loss: 0.330483\n",
      "epoch 165; iter: 0; batch classifier loss: 0.221590; batch adversarial loss: 0.298616\n",
      "epoch 166; iter: 0; batch classifier loss: 0.227458; batch adversarial loss: 0.139810\n",
      "epoch 167; iter: 0; batch classifier loss: 0.146367; batch adversarial loss: 0.234306\n",
      "epoch 168; iter: 0; batch classifier loss: 0.152937; batch adversarial loss: 0.198230\n",
      "epoch 169; iter: 0; batch classifier loss: 0.173413; batch adversarial loss: 0.302429\n",
      "epoch 170; iter: 0; batch classifier loss: 0.235039; batch adversarial loss: 0.363221\n",
      "epoch 171; iter: 0; batch classifier loss: 0.183823; batch adversarial loss: 0.334406\n",
      "epoch 172; iter: 0; batch classifier loss: 0.194031; batch adversarial loss: 0.326459\n",
      "epoch 173; iter: 0; batch classifier loss: 0.194311; batch adversarial loss: 0.351980\n",
      "epoch 174; iter: 0; batch classifier loss: 0.097331; batch adversarial loss: 0.226130\n",
      "epoch 175; iter: 0; batch classifier loss: 0.253807; batch adversarial loss: 0.250307\n",
      "epoch 176; iter: 0; batch classifier loss: 0.212841; batch adversarial loss: 0.284683\n",
      "epoch 177; iter: 0; batch classifier loss: 0.179168; batch adversarial loss: 0.286036\n",
      "epoch 178; iter: 0; batch classifier loss: 0.160825; batch adversarial loss: 0.282063\n",
      "epoch 179; iter: 0; batch classifier loss: 0.240737; batch adversarial loss: 0.320362\n",
      "epoch 180; iter: 0; batch classifier loss: 0.158145; batch adversarial loss: 0.206718\n",
      "epoch 181; iter: 0; batch classifier loss: 0.227114; batch adversarial loss: 0.310434\n",
      "epoch 182; iter: 0; batch classifier loss: 0.219607; batch adversarial loss: 0.298944\n",
      "epoch 183; iter: 0; batch classifier loss: 0.192107; batch adversarial loss: 0.299547\n",
      "epoch 184; iter: 0; batch classifier loss: 0.222484; batch adversarial loss: 0.309390\n",
      "epoch 185; iter: 0; batch classifier loss: 0.164621; batch adversarial loss: 0.347578\n",
      "epoch 186; iter: 0; batch classifier loss: 0.178196; batch adversarial loss: 0.348650\n",
      "epoch 187; iter: 0; batch classifier loss: 0.165908; batch adversarial loss: 0.271191\n",
      "epoch 188; iter: 0; batch classifier loss: 0.221773; batch adversarial loss: 0.192086\n",
      "epoch 189; iter: 0; batch classifier loss: 0.199170; batch adversarial loss: 0.263522\n",
      "epoch 190; iter: 0; batch classifier loss: 0.270737; batch adversarial loss: 0.233123\n",
      "epoch 191; iter: 0; batch classifier loss: 0.167880; batch adversarial loss: 0.195643\n",
      "epoch 192; iter: 0; batch classifier loss: 0.131025; batch adversarial loss: 0.177398\n",
      "epoch 193; iter: 0; batch classifier loss: 0.182288; batch adversarial loss: 0.306064\n",
      "epoch 194; iter: 0; batch classifier loss: 0.191090; batch adversarial loss: 0.337230\n",
      "epoch 195; iter: 0; batch classifier loss: 0.243521; batch adversarial loss: 0.265801\n",
      "epoch 196; iter: 0; batch classifier loss: 0.190311; batch adversarial loss: 0.279747\n",
      "epoch 197; iter: 0; batch classifier loss: 0.135946; batch adversarial loss: 0.314367\n",
      "epoch 198; iter: 0; batch classifier loss: 0.284307; batch adversarial loss: 0.409785\n",
      "epoch 199; iter: 0; batch classifier loss: 0.235099; batch adversarial loss: 0.269820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:57:01.533584: W tensorflow/c/c_api.cc:305] Operation '{name:'39b910b6-ae25-11ee-bc15-ae7d8bf09116/39b910b6-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:4717 op device:{requested: '', assigned: ''} def:{{{node 39b910b6-ae25-11ee-bc15-ae7d8bf09116/39b910b6-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b910b6-ae25-11ee-bc15-ae7d8bf09116/39b910b6-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b910b6-ae25-11ee-bc15-ae7d8bf09116/39b910b6-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.704084; batch adversarial loss: 0.717862\n",
      "epoch 1; iter: 0; batch classifier loss: 0.165431; batch adversarial loss: 0.619950\n",
      "epoch 2; iter: 0; batch classifier loss: 0.259461; batch adversarial loss: 0.521357\n",
      "epoch 3; iter: 0; batch classifier loss: 0.269800; batch adversarial loss: 0.463419\n",
      "epoch 4; iter: 0; batch classifier loss: 0.302053; batch adversarial loss: 0.460783\n",
      "epoch 5; iter: 0; batch classifier loss: 0.198575; batch adversarial loss: 0.340997\n",
      "epoch 6; iter: 0; batch classifier loss: 0.216051; batch adversarial loss: 0.331480\n",
      "epoch 7; iter: 0; batch classifier loss: 0.211988; batch adversarial loss: 0.356771\n",
      "epoch 8; iter: 0; batch classifier loss: 0.310497; batch adversarial loss: 0.281984\n",
      "epoch 9; iter: 0; batch classifier loss: 0.223987; batch adversarial loss: 0.341089\n",
      "epoch 10; iter: 0; batch classifier loss: 0.323547; batch adversarial loss: 0.275560\n",
      "epoch 11; iter: 0; batch classifier loss: 0.207658; batch adversarial loss: 0.347714\n",
      "epoch 12; iter: 0; batch classifier loss: 0.213196; batch adversarial loss: 0.300940\n",
      "epoch 13; iter: 0; batch classifier loss: 0.303306; batch adversarial loss: 0.271293\n",
      "epoch 14; iter: 0; batch classifier loss: 0.155498; batch adversarial loss: 0.316723\n",
      "epoch 15; iter: 0; batch classifier loss: 0.336175; batch adversarial loss: 0.247408\n",
      "epoch 16; iter: 0; batch classifier loss: 0.202636; batch adversarial loss: 0.205705\n",
      "epoch 17; iter: 0; batch classifier loss: 0.206466; batch adversarial loss: 0.263409\n",
      "epoch 18; iter: 0; batch classifier loss: 0.239056; batch adversarial loss: 0.349708\n",
      "epoch 19; iter: 0; batch classifier loss: 0.267881; batch adversarial loss: 0.332506\n",
      "epoch 20; iter: 0; batch classifier loss: 0.189845; batch adversarial loss: 0.303979\n",
      "epoch 21; iter: 0; batch classifier loss: 0.175634; batch adversarial loss: 0.254815\n",
      "epoch 22; iter: 0; batch classifier loss: 0.162576; batch adversarial loss: 0.195514\n",
      "epoch 23; iter: 0; batch classifier loss: 0.128655; batch adversarial loss: 0.283740\n",
      "epoch 24; iter: 0; batch classifier loss: 0.226583; batch adversarial loss: 0.168671\n",
      "epoch 25; iter: 0; batch classifier loss: 0.216173; batch adversarial loss: 0.349734\n",
      "epoch 26; iter: 0; batch classifier loss: 0.206987; batch adversarial loss: 0.228697\n",
      "epoch 27; iter: 0; batch classifier loss: 0.208348; batch adversarial loss: 0.241257\n",
      "epoch 28; iter: 0; batch classifier loss: 0.223281; batch adversarial loss: 0.322227\n",
      "epoch 29; iter: 0; batch classifier loss: 0.239807; batch adversarial loss: 0.244980\n",
      "epoch 30; iter: 0; batch classifier loss: 0.200596; batch adversarial loss: 0.238667\n",
      "epoch 31; iter: 0; batch classifier loss: 0.210557; batch adversarial loss: 0.235950\n",
      "epoch 32; iter: 0; batch classifier loss: 0.238308; batch adversarial loss: 0.274759\n",
      "epoch 33; iter: 0; batch classifier loss: 0.287511; batch adversarial loss: 0.313461\n",
      "epoch 34; iter: 0; batch classifier loss: 0.219351; batch adversarial loss: 0.221718\n",
      "epoch 35; iter: 0; batch classifier loss: 0.271589; batch adversarial loss: 0.202769\n",
      "epoch 36; iter: 0; batch classifier loss: 0.296175; batch adversarial loss: 0.236921\n",
      "epoch 37; iter: 0; batch classifier loss: 0.218230; batch adversarial loss: 0.177481\n",
      "epoch 38; iter: 0; batch classifier loss: 0.121283; batch adversarial loss: 0.092134\n",
      "epoch 39; iter: 0; batch classifier loss: 0.280817; batch adversarial loss: 0.247803\n",
      "epoch 40; iter: 0; batch classifier loss: 0.152117; batch adversarial loss: 0.319091\n",
      "epoch 41; iter: 0; batch classifier loss: 0.229927; batch adversarial loss: 0.206655\n",
      "epoch 42; iter: 0; batch classifier loss: 0.279078; batch adversarial loss: 0.212666\n",
      "epoch 43; iter: 0; batch classifier loss: 0.264296; batch adversarial loss: 0.292450\n",
      "epoch 44; iter: 0; batch classifier loss: 0.210140; batch adversarial loss: 0.233823\n",
      "epoch 45; iter: 0; batch classifier loss: 0.293718; batch adversarial loss: 0.216301\n",
      "epoch 46; iter: 0; batch classifier loss: 0.251610; batch adversarial loss: 0.385691\n",
      "epoch 47; iter: 0; batch classifier loss: 0.238075; batch adversarial loss: 0.216283\n",
      "epoch 48; iter: 0; batch classifier loss: 0.204303; batch adversarial loss: 0.256186\n",
      "epoch 49; iter: 0; batch classifier loss: 0.269731; batch adversarial loss: 0.230754\n",
      "epoch 50; iter: 0; batch classifier loss: 0.183839; batch adversarial loss: 0.258184\n",
      "epoch 51; iter: 0; batch classifier loss: 0.209451; batch adversarial loss: 0.309995\n",
      "epoch 52; iter: 0; batch classifier loss: 0.231731; batch adversarial loss: 0.377547\n",
      "epoch 53; iter: 0; batch classifier loss: 0.192116; batch adversarial loss: 0.282013\n",
      "epoch 54; iter: 0; batch classifier loss: 0.214856; batch adversarial loss: 0.321740\n",
      "epoch 55; iter: 0; batch classifier loss: 0.271558; batch adversarial loss: 0.243563\n",
      "epoch 56; iter: 0; batch classifier loss: 0.146302; batch adversarial loss: 0.238552\n",
      "epoch 57; iter: 0; batch classifier loss: 0.335780; batch adversarial loss: 0.330656\n",
      "epoch 58; iter: 0; batch classifier loss: 0.203099; batch adversarial loss: 0.280044\n",
      "epoch 59; iter: 0; batch classifier loss: 0.312091; batch adversarial loss: 0.299834\n",
      "epoch 60; iter: 0; batch classifier loss: 0.218354; batch adversarial loss: 0.295988\n",
      "epoch 61; iter: 0; batch classifier loss: 0.306012; batch adversarial loss: 0.356087\n",
      "epoch 62; iter: 0; batch classifier loss: 0.319284; batch adversarial loss: 0.217273\n",
      "epoch 63; iter: 0; batch classifier loss: 0.204969; batch adversarial loss: 0.274720\n",
      "epoch 64; iter: 0; batch classifier loss: 0.168077; batch adversarial loss: 0.277843\n",
      "epoch 65; iter: 0; batch classifier loss: 0.182797; batch adversarial loss: 0.301055\n",
      "epoch 66; iter: 0; batch classifier loss: 0.204338; batch adversarial loss: 0.186192\n",
      "epoch 67; iter: 0; batch classifier loss: 0.320611; batch adversarial loss: 0.315690\n",
      "epoch 68; iter: 0; batch classifier loss: 0.238644; batch adversarial loss: 0.316538\n",
      "epoch 69; iter: 0; batch classifier loss: 0.276461; batch adversarial loss: 0.176043\n",
      "epoch 70; iter: 0; batch classifier loss: 0.280782; batch adversarial loss: 0.275303\n",
      "epoch 71; iter: 0; batch classifier loss: 0.181865; batch adversarial loss: 0.230067\n",
      "epoch 72; iter: 0; batch classifier loss: 0.196222; batch adversarial loss: 0.283808\n",
      "epoch 73; iter: 0; batch classifier loss: 0.227572; batch adversarial loss: 0.303842\n",
      "epoch 74; iter: 0; batch classifier loss: 0.244981; batch adversarial loss: 0.235740\n",
      "epoch 75; iter: 0; batch classifier loss: 0.294666; batch adversarial loss: 0.274269\n",
      "epoch 76; iter: 0; batch classifier loss: 0.153589; batch adversarial loss: 0.289570\n",
      "epoch 77; iter: 0; batch classifier loss: 0.235860; batch adversarial loss: 0.319400\n",
      "epoch 78; iter: 0; batch classifier loss: 0.164771; batch adversarial loss: 0.248769\n",
      "epoch 79; iter: 0; batch classifier loss: 0.188895; batch adversarial loss: 0.315390\n",
      "epoch 80; iter: 0; batch classifier loss: 0.185266; batch adversarial loss: 0.199729\n",
      "epoch 81; iter: 0; batch classifier loss: 0.160659; batch adversarial loss: 0.328145\n",
      "epoch 82; iter: 0; batch classifier loss: 0.245912; batch adversarial loss: 0.255116\n",
      "epoch 83; iter: 0; batch classifier loss: 0.299548; batch adversarial loss: 0.239582\n",
      "epoch 84; iter: 0; batch classifier loss: 0.171026; batch adversarial loss: 0.287606\n",
      "epoch 85; iter: 0; batch classifier loss: 0.144776; batch adversarial loss: 0.244066\n",
      "epoch 86; iter: 0; batch classifier loss: 0.239294; batch adversarial loss: 0.267708\n",
      "epoch 87; iter: 0; batch classifier loss: 0.251290; batch adversarial loss: 0.318139\n",
      "epoch 88; iter: 0; batch classifier loss: 0.153815; batch adversarial loss: 0.287419\n",
      "epoch 89; iter: 0; batch classifier loss: 0.221597; batch adversarial loss: 0.307909\n",
      "epoch 90; iter: 0; batch classifier loss: 0.103568; batch adversarial loss: 0.165024\n",
      "epoch 91; iter: 0; batch classifier loss: 0.246060; batch adversarial loss: 0.358140\n",
      "epoch 92; iter: 0; batch classifier loss: 0.317881; batch adversarial loss: 0.360237\n",
      "epoch 93; iter: 0; batch classifier loss: 0.164574; batch adversarial loss: 0.226645\n",
      "epoch 94; iter: 0; batch classifier loss: 0.190795; batch adversarial loss: 0.283983\n",
      "epoch 95; iter: 0; batch classifier loss: 0.262769; batch adversarial loss: 0.297574\n",
      "epoch 96; iter: 0; batch classifier loss: 0.202646; batch adversarial loss: 0.221376\n",
      "epoch 97; iter: 0; batch classifier loss: 0.250889; batch adversarial loss: 0.287736\n",
      "epoch 98; iter: 0; batch classifier loss: 0.194208; batch adversarial loss: 0.342570\n",
      "epoch 99; iter: 0; batch classifier loss: 0.257743; batch adversarial loss: 0.258742\n",
      "epoch 100; iter: 0; batch classifier loss: 0.158160; batch adversarial loss: 0.177378\n",
      "epoch 101; iter: 0; batch classifier loss: 0.224227; batch adversarial loss: 0.181491\n",
      "epoch 102; iter: 0; batch classifier loss: 0.198550; batch adversarial loss: 0.299915\n",
      "epoch 103; iter: 0; batch classifier loss: 0.194330; batch adversarial loss: 0.250842\n",
      "epoch 104; iter: 0; batch classifier loss: 0.193779; batch adversarial loss: 0.145489\n",
      "epoch 105; iter: 0; batch classifier loss: 0.211230; batch adversarial loss: 0.136556\n",
      "epoch 106; iter: 0; batch classifier loss: 0.166409; batch adversarial loss: 0.240347\n",
      "epoch 107; iter: 0; batch classifier loss: 0.202745; batch adversarial loss: 0.330328\n",
      "epoch 108; iter: 0; batch classifier loss: 0.226324; batch adversarial loss: 0.173446\n",
      "epoch 109; iter: 0; batch classifier loss: 0.249911; batch adversarial loss: 0.311745\n",
      "epoch 110; iter: 0; batch classifier loss: 0.169542; batch adversarial loss: 0.312155\n",
      "epoch 111; iter: 0; batch classifier loss: 0.134986; batch adversarial loss: 0.215443\n",
      "epoch 112; iter: 0; batch classifier loss: 0.158451; batch adversarial loss: 0.234469\n",
      "epoch 113; iter: 0; batch classifier loss: 0.192240; batch adversarial loss: 0.304593\n",
      "epoch 114; iter: 0; batch classifier loss: 0.189405; batch adversarial loss: 0.221508\n",
      "epoch 115; iter: 0; batch classifier loss: 0.258452; batch adversarial loss: 0.244987\n",
      "epoch 116; iter: 0; batch classifier loss: 0.253811; batch adversarial loss: 0.351982\n",
      "epoch 117; iter: 0; batch classifier loss: 0.182186; batch adversarial loss: 0.252530\n",
      "epoch 118; iter: 0; batch classifier loss: 0.196763; batch adversarial loss: 0.206998\n",
      "epoch 119; iter: 0; batch classifier loss: 0.235102; batch adversarial loss: 0.229820\n",
      "epoch 120; iter: 0; batch classifier loss: 0.173416; batch adversarial loss: 0.254823\n",
      "epoch 121; iter: 0; batch classifier loss: 0.188588; batch adversarial loss: 0.240284\n",
      "epoch 122; iter: 0; batch classifier loss: 0.130738; batch adversarial loss: 0.289322\n",
      "epoch 123; iter: 0; batch classifier loss: 0.176000; batch adversarial loss: 0.284947\n",
      "epoch 124; iter: 0; batch classifier loss: 0.162442; batch adversarial loss: 0.228877\n",
      "epoch 125; iter: 0; batch classifier loss: 0.226680; batch adversarial loss: 0.323955\n",
      "epoch 126; iter: 0; batch classifier loss: 0.235233; batch adversarial loss: 0.158099\n",
      "epoch 127; iter: 0; batch classifier loss: 0.324048; batch adversarial loss: 0.302916\n",
      "epoch 128; iter: 0; batch classifier loss: 0.238378; batch adversarial loss: 0.228215\n",
      "epoch 129; iter: 0; batch classifier loss: 0.239028; batch adversarial loss: 0.281715\n",
      "epoch 130; iter: 0; batch classifier loss: 0.209735; batch adversarial loss: 0.312667\n",
      "epoch 131; iter: 0; batch classifier loss: 0.186578; batch adversarial loss: 0.223245\n",
      "epoch 132; iter: 0; batch classifier loss: 0.174444; batch adversarial loss: 0.339245\n",
      "epoch 133; iter: 0; batch classifier loss: 0.191495; batch adversarial loss: 0.425263\n",
      "epoch 134; iter: 0; batch classifier loss: 0.171510; batch adversarial loss: 0.342694\n",
      "epoch 135; iter: 0; batch classifier loss: 0.209508; batch adversarial loss: 0.248418\n",
      "epoch 136; iter: 0; batch classifier loss: 0.175709; batch adversarial loss: 0.376077\n",
      "epoch 137; iter: 0; batch classifier loss: 0.143455; batch adversarial loss: 0.314124\n",
      "epoch 138; iter: 0; batch classifier loss: 0.262976; batch adversarial loss: 0.178381\n",
      "epoch 139; iter: 0; batch classifier loss: 0.183901; batch adversarial loss: 0.254004\n",
      "epoch 140; iter: 0; batch classifier loss: 0.220243; batch adversarial loss: 0.228037\n",
      "epoch 141; iter: 0; batch classifier loss: 0.204082; batch adversarial loss: 0.315227\n",
      "epoch 142; iter: 0; batch classifier loss: 0.179617; batch adversarial loss: 0.313909\n",
      "epoch 143; iter: 0; batch classifier loss: 0.177740; batch adversarial loss: 0.184623\n",
      "epoch 144; iter: 0; batch classifier loss: 0.307590; batch adversarial loss: 0.340160\n",
      "epoch 145; iter: 0; batch classifier loss: 0.273887; batch adversarial loss: 0.337074\n",
      "epoch 146; iter: 0; batch classifier loss: 0.257344; batch adversarial loss: 0.378874\n",
      "epoch 147; iter: 0; batch classifier loss: 0.146742; batch adversarial loss: 0.243102\n",
      "epoch 148; iter: 0; batch classifier loss: 0.243817; batch adversarial loss: 0.204295\n",
      "epoch 149; iter: 0; batch classifier loss: 0.172983; batch adversarial loss: 0.241173\n",
      "epoch 150; iter: 0; batch classifier loss: 0.244336; batch adversarial loss: 0.250234\n",
      "epoch 151; iter: 0; batch classifier loss: 0.179032; batch adversarial loss: 0.257280\n",
      "epoch 152; iter: 0; batch classifier loss: 0.214816; batch adversarial loss: 0.138720\n",
      "epoch 153; iter: 0; batch classifier loss: 0.146491; batch adversarial loss: 0.305945\n",
      "epoch 154; iter: 0; batch classifier loss: 0.245832; batch adversarial loss: 0.181160\n",
      "epoch 155; iter: 0; batch classifier loss: 0.226224; batch adversarial loss: 0.199259\n",
      "epoch 156; iter: 0; batch classifier loss: 0.132360; batch adversarial loss: 0.193654\n",
      "epoch 157; iter: 0; batch classifier loss: 0.126316; batch adversarial loss: 0.134265\n",
      "epoch 158; iter: 0; batch classifier loss: 0.222581; batch adversarial loss: 0.168426\n",
      "epoch 159; iter: 0; batch classifier loss: 0.159352; batch adversarial loss: 0.224599\n",
      "epoch 160; iter: 0; batch classifier loss: 0.201181; batch adversarial loss: 0.266438\n",
      "epoch 161; iter: 0; batch classifier loss: 0.197106; batch adversarial loss: 0.263728\n",
      "epoch 162; iter: 0; batch classifier loss: 0.316502; batch adversarial loss: 0.308848\n",
      "epoch 163; iter: 0; batch classifier loss: 0.268279; batch adversarial loss: 0.244384\n",
      "epoch 164; iter: 0; batch classifier loss: 0.223068; batch adversarial loss: 0.242867\n",
      "epoch 165; iter: 0; batch classifier loss: 0.182865; batch adversarial loss: 0.275952\n",
      "epoch 166; iter: 0; batch classifier loss: 0.238707; batch adversarial loss: 0.354032\n",
      "epoch 167; iter: 0; batch classifier loss: 0.188685; batch adversarial loss: 0.370916\n",
      "epoch 168; iter: 0; batch classifier loss: 0.178084; batch adversarial loss: 0.289892\n",
      "epoch 169; iter: 0; batch classifier loss: 0.170920; batch adversarial loss: 0.335624\n",
      "epoch 170; iter: 0; batch classifier loss: 0.273919; batch adversarial loss: 0.281844\n",
      "epoch 171; iter: 0; batch classifier loss: 0.109554; batch adversarial loss: 0.174801\n",
      "epoch 172; iter: 0; batch classifier loss: 0.315471; batch adversarial loss: 0.282515\n",
      "epoch 173; iter: 0; batch classifier loss: 0.177462; batch adversarial loss: 0.191252\n",
      "epoch 174; iter: 0; batch classifier loss: 0.205830; batch adversarial loss: 0.285018\n",
      "epoch 175; iter: 0; batch classifier loss: 0.204898; batch adversarial loss: 0.254212\n",
      "epoch 176; iter: 0; batch classifier loss: 0.295869; batch adversarial loss: 0.232730\n",
      "epoch 177; iter: 0; batch classifier loss: 0.292998; batch adversarial loss: 0.277999\n",
      "epoch 178; iter: 0; batch classifier loss: 0.196046; batch adversarial loss: 0.170497\n",
      "epoch 179; iter: 0; batch classifier loss: 0.185903; batch adversarial loss: 0.323336\n",
      "epoch 180; iter: 0; batch classifier loss: 0.184273; batch adversarial loss: 0.226938\n",
      "epoch 181; iter: 0; batch classifier loss: 0.217911; batch adversarial loss: 0.212065\n",
      "epoch 182; iter: 0; batch classifier loss: 0.256325; batch adversarial loss: 0.332504\n",
      "epoch 183; iter: 0; batch classifier loss: 0.225213; batch adversarial loss: 0.219635\n",
      "epoch 184; iter: 0; batch classifier loss: 0.292126; batch adversarial loss: 0.287252\n",
      "epoch 185; iter: 0; batch classifier loss: 0.166208; batch adversarial loss: 0.143629\n",
      "epoch 186; iter: 0; batch classifier loss: 0.199314; batch adversarial loss: 0.254155\n",
      "epoch 187; iter: 0; batch classifier loss: 0.213385; batch adversarial loss: 0.342966\n",
      "epoch 188; iter: 0; batch classifier loss: 0.211039; batch adversarial loss: 0.283895\n",
      "epoch 189; iter: 0; batch classifier loss: 0.196619; batch adversarial loss: 0.297259\n",
      "epoch 190; iter: 0; batch classifier loss: 0.195496; batch adversarial loss: 0.327564\n",
      "epoch 191; iter: 0; batch classifier loss: 0.283269; batch adversarial loss: 0.278485\n",
      "epoch 192; iter: 0; batch classifier loss: 0.193481; batch adversarial loss: 0.295742\n",
      "epoch 193; iter: 0; batch classifier loss: 0.235291; batch adversarial loss: 0.429721\n",
      "epoch 194; iter: 0; batch classifier loss: 0.179172; batch adversarial loss: 0.241734\n",
      "epoch 195; iter: 0; batch classifier loss: 0.225532; batch adversarial loss: 0.306368\n",
      "epoch 196; iter: 0; batch classifier loss: 0.228448; batch adversarial loss: 0.244500\n",
      "epoch 197; iter: 0; batch classifier loss: 0.233179; batch adversarial loss: 0.299363\n",
      "epoch 198; iter: 0; batch classifier loss: 0.181147; batch adversarial loss: 0.344168\n",
      "epoch 199; iter: 0; batch classifier loss: 0.237676; batch adversarial loss: 0.370900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:57:15.384538: W tensorflow/c/c_api.cc:305] Operation '{name:'39b910de-ae25-11ee-bc15-ae7d8bf09116/39b910de-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:5507 op device:{requested: '', assigned: ''} def:{{{node 39b910de-ae25-11ee-bc15-ae7d8bf09116/39b910de-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b910de-ae25-11ee-bc15-ae7d8bf09116/39b910de-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b910de-ae25-11ee-bc15-ae7d8bf09116/39b910de-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.634672; batch adversarial loss: 0.429489\n",
      "epoch 1; iter: 0; batch classifier loss: 0.863272; batch adversarial loss: 0.522394\n",
      "epoch 2; iter: 0; batch classifier loss: 1.482677; batch adversarial loss: 0.620856\n",
      "epoch 3; iter: 0; batch classifier loss: 1.466949; batch adversarial loss: 0.660109\n",
      "epoch 4; iter: 0; batch classifier loss: 1.501732; batch adversarial loss: 0.547516\n",
      "epoch 5; iter: 0; batch classifier loss: 1.465360; batch adversarial loss: 0.501882\n",
      "epoch 6; iter: 0; batch classifier loss: 1.365956; batch adversarial loss: 0.455924\n",
      "epoch 7; iter: 0; batch classifier loss: 1.158477; batch adversarial loss: 0.458163\n",
      "epoch 8; iter: 0; batch classifier loss: 0.935137; batch adversarial loss: 0.384411\n",
      "epoch 9; iter: 0; batch classifier loss: 1.036031; batch adversarial loss: 0.377428\n",
      "epoch 10; iter: 0; batch classifier loss: 0.919592; batch adversarial loss: 0.360368\n",
      "epoch 11; iter: 0; batch classifier loss: 0.672372; batch adversarial loss: 0.395563\n",
      "epoch 12; iter: 0; batch classifier loss: 0.322955; batch adversarial loss: 0.428252\n",
      "epoch 13; iter: 0; batch classifier loss: 0.341296; batch adversarial loss: 0.268852\n",
      "epoch 14; iter: 0; batch classifier loss: 0.311370; batch adversarial loss: 0.256557\n",
      "epoch 15; iter: 0; batch classifier loss: 0.300238; batch adversarial loss: 0.309186\n",
      "epoch 16; iter: 0; batch classifier loss: 0.256803; batch adversarial loss: 0.140207\n",
      "epoch 17; iter: 0; batch classifier loss: 0.197675; batch adversarial loss: 0.213721\n",
      "epoch 18; iter: 0; batch classifier loss: 0.268618; batch adversarial loss: 0.275662\n",
      "epoch 19; iter: 0; batch classifier loss: 0.246859; batch adversarial loss: 0.263196\n",
      "epoch 20; iter: 0; batch classifier loss: 0.316332; batch adversarial loss: 0.274523\n",
      "epoch 21; iter: 0; batch classifier loss: 0.260603; batch adversarial loss: 0.304350\n",
      "epoch 22; iter: 0; batch classifier loss: 0.158955; batch adversarial loss: 0.169366\n",
      "epoch 23; iter: 0; batch classifier loss: 0.243877; batch adversarial loss: 0.184736\n",
      "epoch 24; iter: 0; batch classifier loss: 0.165298; batch adversarial loss: 0.272797\n",
      "epoch 25; iter: 0; batch classifier loss: 0.236278; batch adversarial loss: 0.188389\n",
      "epoch 26; iter: 0; batch classifier loss: 0.153641; batch adversarial loss: 0.332313\n",
      "epoch 27; iter: 0; batch classifier loss: 0.161433; batch adversarial loss: 0.219876\n",
      "epoch 28; iter: 0; batch classifier loss: 0.245416; batch adversarial loss: 0.279096\n",
      "epoch 29; iter: 0; batch classifier loss: 0.361936; batch adversarial loss: 0.243535\n",
      "epoch 30; iter: 0; batch classifier loss: 0.225800; batch adversarial loss: 0.304340\n",
      "epoch 31; iter: 0; batch classifier loss: 0.256421; batch adversarial loss: 0.189710\n",
      "epoch 32; iter: 0; batch classifier loss: 0.249327; batch adversarial loss: 0.290131\n",
      "epoch 33; iter: 0; batch classifier loss: 0.222998; batch adversarial loss: 0.272107\n",
      "epoch 34; iter: 0; batch classifier loss: 0.257959; batch adversarial loss: 0.221939\n",
      "epoch 35; iter: 0; batch classifier loss: 0.292123; batch adversarial loss: 0.298616\n",
      "epoch 36; iter: 0; batch classifier loss: 0.213300; batch adversarial loss: 0.343726\n",
      "epoch 37; iter: 0; batch classifier loss: 0.197897; batch adversarial loss: 0.171806\n",
      "epoch 38; iter: 0; batch classifier loss: 0.219743; batch adversarial loss: 0.205507\n",
      "epoch 39; iter: 0; batch classifier loss: 0.285543; batch adversarial loss: 0.292445\n",
      "epoch 40; iter: 0; batch classifier loss: 0.314279; batch adversarial loss: 0.232439\n",
      "epoch 41; iter: 0; batch classifier loss: 0.242915; batch adversarial loss: 0.267766\n",
      "epoch 42; iter: 0; batch classifier loss: 0.245524; batch adversarial loss: 0.156511\n",
      "epoch 43; iter: 0; batch classifier loss: 0.276499; batch adversarial loss: 0.208683\n",
      "epoch 44; iter: 0; batch classifier loss: 0.154566; batch adversarial loss: 0.246689\n",
      "epoch 45; iter: 0; batch classifier loss: 0.210537; batch adversarial loss: 0.362110\n",
      "epoch 46; iter: 0; batch classifier loss: 0.223340; batch adversarial loss: 0.199548\n",
      "epoch 47; iter: 0; batch classifier loss: 0.221263; batch adversarial loss: 0.494518\n",
      "epoch 48; iter: 0; batch classifier loss: 0.129143; batch adversarial loss: 0.253128\n",
      "epoch 49; iter: 0; batch classifier loss: 0.140105; batch adversarial loss: 0.348210\n",
      "epoch 50; iter: 0; batch classifier loss: 0.288495; batch adversarial loss: 0.240684\n",
      "epoch 51; iter: 0; batch classifier loss: 0.220661; batch adversarial loss: 0.178037\n",
      "epoch 52; iter: 0; batch classifier loss: 0.280666; batch adversarial loss: 0.356128\n",
      "epoch 53; iter: 0; batch classifier loss: 0.163996; batch adversarial loss: 0.263631\n",
      "epoch 54; iter: 0; batch classifier loss: 0.299224; batch adversarial loss: 0.276645\n",
      "epoch 55; iter: 0; batch classifier loss: 0.225445; batch adversarial loss: 0.355907\n",
      "epoch 56; iter: 0; batch classifier loss: 0.185399; batch adversarial loss: 0.197617\n",
      "epoch 57; iter: 0; batch classifier loss: 0.155963; batch adversarial loss: 0.175658\n",
      "epoch 58; iter: 0; batch classifier loss: 0.188188; batch adversarial loss: 0.177416\n",
      "epoch 59; iter: 0; batch classifier loss: 0.265078; batch adversarial loss: 0.190816\n",
      "epoch 60; iter: 0; batch classifier loss: 0.191211; batch adversarial loss: 0.170850\n",
      "epoch 61; iter: 0; batch classifier loss: 0.299814; batch adversarial loss: 0.267528\n",
      "epoch 62; iter: 0; batch classifier loss: 0.253894; batch adversarial loss: 0.231657\n",
      "epoch 63; iter: 0; batch classifier loss: 0.222534; batch adversarial loss: 0.324060\n",
      "epoch 64; iter: 0; batch classifier loss: 0.187492; batch adversarial loss: 0.289592\n",
      "epoch 65; iter: 0; batch classifier loss: 0.220590; batch adversarial loss: 0.259868\n",
      "epoch 66; iter: 0; batch classifier loss: 0.284697; batch adversarial loss: 0.245974\n",
      "epoch 67; iter: 0; batch classifier loss: 0.205449; batch adversarial loss: 0.235263\n",
      "epoch 68; iter: 0; batch classifier loss: 0.246520; batch adversarial loss: 0.339289\n",
      "epoch 69; iter: 0; batch classifier loss: 0.319842; batch adversarial loss: 0.269767\n",
      "epoch 70; iter: 0; batch classifier loss: 0.242543; batch adversarial loss: 0.337953\n",
      "epoch 71; iter: 0; batch classifier loss: 0.182079; batch adversarial loss: 0.261472\n",
      "epoch 72; iter: 0; batch classifier loss: 0.186681; batch adversarial loss: 0.256763\n",
      "epoch 73; iter: 0; batch classifier loss: 0.183836; batch adversarial loss: 0.287697\n",
      "epoch 74; iter: 0; batch classifier loss: 0.259020; batch adversarial loss: 0.205164\n",
      "epoch 75; iter: 0; batch classifier loss: 0.251337; batch adversarial loss: 0.339589\n",
      "epoch 76; iter: 0; batch classifier loss: 0.201393; batch adversarial loss: 0.231900\n",
      "epoch 77; iter: 0; batch classifier loss: 0.220103; batch adversarial loss: 0.343127\n",
      "epoch 78; iter: 0; batch classifier loss: 0.191964; batch adversarial loss: 0.276264\n",
      "epoch 79; iter: 0; batch classifier loss: 0.150587; batch adversarial loss: 0.356558\n",
      "epoch 80; iter: 0; batch classifier loss: 0.290192; batch adversarial loss: 0.276400\n",
      "epoch 81; iter: 0; batch classifier loss: 0.245567; batch adversarial loss: 0.245309\n",
      "epoch 82; iter: 0; batch classifier loss: 0.282303; batch adversarial loss: 0.186730\n",
      "epoch 83; iter: 0; batch classifier loss: 0.207793; batch adversarial loss: 0.357644\n",
      "epoch 84; iter: 0; batch classifier loss: 0.330426; batch adversarial loss: 0.190984\n",
      "epoch 85; iter: 0; batch classifier loss: 0.233027; batch adversarial loss: 0.203074\n",
      "epoch 86; iter: 0; batch classifier loss: 0.199822; batch adversarial loss: 0.238620\n",
      "epoch 87; iter: 0; batch classifier loss: 0.141075; batch adversarial loss: 0.220888\n",
      "epoch 88; iter: 0; batch classifier loss: 0.303079; batch adversarial loss: 0.279805\n",
      "epoch 89; iter: 0; batch classifier loss: 0.229257; batch adversarial loss: 0.278220\n",
      "epoch 90; iter: 0; batch classifier loss: 0.256781; batch adversarial loss: 0.268028\n",
      "epoch 91; iter: 0; batch classifier loss: 0.131781; batch adversarial loss: 0.276160\n",
      "epoch 92; iter: 0; batch classifier loss: 0.163133; batch adversarial loss: 0.281459\n",
      "epoch 93; iter: 0; batch classifier loss: 0.282263; batch adversarial loss: 0.239554\n",
      "epoch 94; iter: 0; batch classifier loss: 0.195214; batch adversarial loss: 0.211419\n",
      "epoch 95; iter: 0; batch classifier loss: 0.295599; batch adversarial loss: 0.248750\n",
      "epoch 96; iter: 0; batch classifier loss: 0.217963; batch adversarial loss: 0.327892\n",
      "epoch 97; iter: 0; batch classifier loss: 0.265522; batch adversarial loss: 0.191835\n",
      "epoch 98; iter: 0; batch classifier loss: 0.273753; batch adversarial loss: 0.188891\n",
      "epoch 99; iter: 0; batch classifier loss: 0.206570; batch adversarial loss: 0.265211\n",
      "epoch 100; iter: 0; batch classifier loss: 0.148838; batch adversarial loss: 0.246684\n",
      "epoch 101; iter: 0; batch classifier loss: 0.105250; batch adversarial loss: 0.206937\n",
      "epoch 102; iter: 0; batch classifier loss: 0.227251; batch adversarial loss: 0.122687\n",
      "epoch 103; iter: 0; batch classifier loss: 0.212683; batch adversarial loss: 0.223415\n",
      "epoch 104; iter: 0; batch classifier loss: 0.220363; batch adversarial loss: 0.244444\n",
      "epoch 105; iter: 0; batch classifier loss: 0.177338; batch adversarial loss: 0.204448\n",
      "epoch 106; iter: 0; batch classifier loss: 0.168082; batch adversarial loss: 0.274353\n",
      "epoch 107; iter: 0; batch classifier loss: 0.194041; batch adversarial loss: 0.246576\n",
      "epoch 108; iter: 0; batch classifier loss: 0.180928; batch adversarial loss: 0.295813\n",
      "epoch 109; iter: 0; batch classifier loss: 0.185670; batch adversarial loss: 0.224782\n",
      "epoch 110; iter: 0; batch classifier loss: 0.210600; batch adversarial loss: 0.208430\n",
      "epoch 111; iter: 0; batch classifier loss: 0.166739; batch adversarial loss: 0.236478\n",
      "epoch 112; iter: 0; batch classifier loss: 0.261578; batch adversarial loss: 0.294553\n",
      "epoch 113; iter: 0; batch classifier loss: 0.129299; batch adversarial loss: 0.209583\n",
      "epoch 114; iter: 0; batch classifier loss: 0.160699; batch adversarial loss: 0.243713\n",
      "epoch 115; iter: 0; batch classifier loss: 0.206435; batch adversarial loss: 0.255596\n",
      "epoch 116; iter: 0; batch classifier loss: 0.270120; batch adversarial loss: 0.175959\n",
      "epoch 117; iter: 0; batch classifier loss: 0.226569; batch adversarial loss: 0.250751\n",
      "epoch 118; iter: 0; batch classifier loss: 0.221858; batch adversarial loss: 0.284476\n",
      "epoch 119; iter: 0; batch classifier loss: 0.169406; batch adversarial loss: 0.262143\n",
      "epoch 120; iter: 0; batch classifier loss: 0.178999; batch adversarial loss: 0.216035\n",
      "epoch 121; iter: 0; batch classifier loss: 0.285705; batch adversarial loss: 0.167920\n",
      "epoch 122; iter: 0; batch classifier loss: 0.234614; batch adversarial loss: 0.283551\n",
      "epoch 123; iter: 0; batch classifier loss: 0.227728; batch adversarial loss: 0.225275\n",
      "epoch 124; iter: 0; batch classifier loss: 0.119951; batch adversarial loss: 0.220541\n",
      "epoch 125; iter: 0; batch classifier loss: 0.218216; batch adversarial loss: 0.274840\n",
      "epoch 126; iter: 0; batch classifier loss: 0.234808; batch adversarial loss: 0.205898\n",
      "epoch 127; iter: 0; batch classifier loss: 0.160035; batch adversarial loss: 0.222986\n",
      "epoch 128; iter: 0; batch classifier loss: 0.161889; batch adversarial loss: 0.212368\n",
      "epoch 129; iter: 0; batch classifier loss: 0.253099; batch adversarial loss: 0.181125\n",
      "epoch 130; iter: 0; batch classifier loss: 0.215615; batch adversarial loss: 0.338817\n",
      "epoch 131; iter: 0; batch classifier loss: 0.170269; batch adversarial loss: 0.262441\n",
      "epoch 132; iter: 0; batch classifier loss: 0.229541; batch adversarial loss: 0.151124\n",
      "epoch 133; iter: 0; batch classifier loss: 0.184611; batch adversarial loss: 0.315370\n",
      "epoch 134; iter: 0; batch classifier loss: 0.266296; batch adversarial loss: 0.306172\n",
      "epoch 135; iter: 0; batch classifier loss: 0.171894; batch adversarial loss: 0.202282\n",
      "epoch 136; iter: 0; batch classifier loss: 0.202769; batch adversarial loss: 0.198860\n",
      "epoch 137; iter: 0; batch classifier loss: 0.199077; batch adversarial loss: 0.277356\n",
      "epoch 138; iter: 0; batch classifier loss: 0.129799; batch adversarial loss: 0.289986\n",
      "epoch 139; iter: 0; batch classifier loss: 0.267901; batch adversarial loss: 0.264245\n",
      "epoch 140; iter: 0; batch classifier loss: 0.222465; batch adversarial loss: 0.271997\n",
      "epoch 141; iter: 0; batch classifier loss: 0.171488; batch adversarial loss: 0.200094\n",
      "epoch 142; iter: 0; batch classifier loss: 0.270525; batch adversarial loss: 0.275262\n",
      "epoch 143; iter: 0; batch classifier loss: 0.146786; batch adversarial loss: 0.190584\n",
      "epoch 144; iter: 0; batch classifier loss: 0.230932; batch adversarial loss: 0.432904\n",
      "epoch 145; iter: 0; batch classifier loss: 0.122070; batch adversarial loss: 0.236604\n",
      "epoch 146; iter: 0; batch classifier loss: 0.193742; batch adversarial loss: 0.301469\n",
      "epoch 147; iter: 0; batch classifier loss: 0.154558; batch adversarial loss: 0.223557\n",
      "epoch 148; iter: 0; batch classifier loss: 0.195484; batch adversarial loss: 0.269704\n",
      "epoch 149; iter: 0; batch classifier loss: 0.269080; batch adversarial loss: 0.338870\n",
      "epoch 150; iter: 0; batch classifier loss: 0.174547; batch adversarial loss: 0.215908\n",
      "epoch 151; iter: 0; batch classifier loss: 0.168915; batch adversarial loss: 0.233510\n",
      "epoch 152; iter: 0; batch classifier loss: 0.213618; batch adversarial loss: 0.309079\n",
      "epoch 153; iter: 0; batch classifier loss: 0.189458; batch adversarial loss: 0.273669\n",
      "epoch 154; iter: 0; batch classifier loss: 0.248353; batch adversarial loss: 0.175144\n",
      "epoch 155; iter: 0; batch classifier loss: 0.144206; batch adversarial loss: 0.254416\n",
      "epoch 156; iter: 0; batch classifier loss: 0.206961; batch adversarial loss: 0.222693\n",
      "epoch 157; iter: 0; batch classifier loss: 0.181451; batch adversarial loss: 0.274048\n",
      "epoch 158; iter: 0; batch classifier loss: 0.177207; batch adversarial loss: 0.233668\n",
      "epoch 159; iter: 0; batch classifier loss: 0.132364; batch adversarial loss: 0.293997\n",
      "epoch 160; iter: 0; batch classifier loss: 0.267848; batch adversarial loss: 0.335431\n",
      "epoch 161; iter: 0; batch classifier loss: 0.183315; batch adversarial loss: 0.264562\n",
      "epoch 162; iter: 0; batch classifier loss: 0.172516; batch adversarial loss: 0.218560\n",
      "epoch 163; iter: 0; batch classifier loss: 0.253004; batch adversarial loss: 0.233519\n",
      "epoch 164; iter: 0; batch classifier loss: 0.224009; batch adversarial loss: 0.278361\n",
      "epoch 165; iter: 0; batch classifier loss: 0.203399; batch adversarial loss: 0.354158\n",
      "epoch 166; iter: 0; batch classifier loss: 0.261974; batch adversarial loss: 0.175478\n",
      "epoch 167; iter: 0; batch classifier loss: 0.192085; batch adversarial loss: 0.268080\n",
      "epoch 168; iter: 0; batch classifier loss: 0.182798; batch adversarial loss: 0.337077\n",
      "epoch 169; iter: 0; batch classifier loss: 0.171938; batch adversarial loss: 0.225966\n",
      "epoch 170; iter: 0; batch classifier loss: 0.141375; batch adversarial loss: 0.163598\n",
      "epoch 171; iter: 0; batch classifier loss: 0.229599; batch adversarial loss: 0.181666\n",
      "epoch 172; iter: 0; batch classifier loss: 0.180595; batch adversarial loss: 0.219855\n",
      "epoch 173; iter: 0; batch classifier loss: 0.261034; batch adversarial loss: 0.224680\n",
      "epoch 174; iter: 0; batch classifier loss: 0.190595; batch adversarial loss: 0.156417\n",
      "epoch 175; iter: 0; batch classifier loss: 0.159615; batch adversarial loss: 0.289676\n",
      "epoch 176; iter: 0; batch classifier loss: 0.177917; batch adversarial loss: 0.178717\n",
      "epoch 177; iter: 0; batch classifier loss: 0.192167; batch adversarial loss: 0.217000\n",
      "epoch 178; iter: 0; batch classifier loss: 0.149373; batch adversarial loss: 0.202872\n",
      "epoch 179; iter: 0; batch classifier loss: 0.239364; batch adversarial loss: 0.263560\n",
      "epoch 180; iter: 0; batch classifier loss: 0.235769; batch adversarial loss: 0.262666\n",
      "epoch 181; iter: 0; batch classifier loss: 0.259186; batch adversarial loss: 0.372219\n",
      "epoch 182; iter: 0; batch classifier loss: 0.214479; batch adversarial loss: 0.303655\n",
      "epoch 183; iter: 0; batch classifier loss: 0.246034; batch adversarial loss: 0.252873\n",
      "epoch 184; iter: 0; batch classifier loss: 0.182256; batch adversarial loss: 0.203145\n",
      "epoch 185; iter: 0; batch classifier loss: 0.121907; batch adversarial loss: 0.160736\n",
      "epoch 186; iter: 0; batch classifier loss: 0.233063; batch adversarial loss: 0.181771\n",
      "epoch 187; iter: 0; batch classifier loss: 0.232531; batch adversarial loss: 0.318813\n",
      "epoch 188; iter: 0; batch classifier loss: 0.207590; batch adversarial loss: 0.336336\n",
      "epoch 189; iter: 0; batch classifier loss: 0.149704; batch adversarial loss: 0.282424\n",
      "epoch 190; iter: 0; batch classifier loss: 0.184358; batch adversarial loss: 0.296192\n",
      "epoch 191; iter: 0; batch classifier loss: 0.155301; batch adversarial loss: 0.224920\n",
      "epoch 192; iter: 0; batch classifier loss: 0.237968; batch adversarial loss: 0.219015\n",
      "epoch 193; iter: 0; batch classifier loss: 0.164478; batch adversarial loss: 0.251986\n",
      "epoch 194; iter: 0; batch classifier loss: 0.281763; batch adversarial loss: 0.259077\n",
      "epoch 195; iter: 0; batch classifier loss: 0.174721; batch adversarial loss: 0.289048\n",
      "epoch 196; iter: 0; batch classifier loss: 0.176039; batch adversarial loss: 0.256675\n",
      "epoch 197; iter: 0; batch classifier loss: 0.209655; batch adversarial loss: 0.353119\n",
      "epoch 198; iter: 0; batch classifier loss: 0.207687; batch adversarial loss: 0.264952\n",
      "epoch 199; iter: 0; batch classifier loss: 0.246739; batch adversarial loss: 0.215156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:57:29.238812: W tensorflow/c/c_api.cc:305] Operation '{name:'39b91110-ae25-11ee-bc15-ae7d8bf09116/39b91110-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:6297 op device:{requested: '', assigned: ''} def:{{{node 39b91110-ae25-11ee-bc15-ae7d8bf09116/39b91110-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b91110-ae25-11ee-bc15-ae7d8bf09116/39b91110-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b91110-ae25-11ee-bc15-ae7d8bf09116/39b91110-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.721501; batch adversarial loss: 0.818443\n",
      "epoch 1; iter: 0; batch classifier loss: 0.274283; batch adversarial loss: 0.814552\n",
      "epoch 2; iter: 0; batch classifier loss: 0.359490; batch adversarial loss: 0.695378\n",
      "epoch 3; iter: 0; batch classifier loss: 0.219220; batch adversarial loss: 0.611061\n",
      "epoch 4; iter: 0; batch classifier loss: 0.231240; batch adversarial loss: 0.524580\n",
      "epoch 5; iter: 0; batch classifier loss: 0.171895; batch adversarial loss: 0.459528\n",
      "epoch 6; iter: 0; batch classifier loss: 0.268611; batch adversarial loss: 0.424365\n",
      "epoch 7; iter: 0; batch classifier loss: 0.200282; batch adversarial loss: 0.376297\n",
      "epoch 8; iter: 0; batch classifier loss: 0.189589; batch adversarial loss: 0.356526\n",
      "epoch 9; iter: 0; batch classifier loss: 0.228421; batch adversarial loss: 0.368269\n",
      "epoch 10; iter: 0; batch classifier loss: 0.244375; batch adversarial loss: 0.355887\n",
      "epoch 11; iter: 0; batch classifier loss: 0.218943; batch adversarial loss: 0.331211\n",
      "epoch 12; iter: 0; batch classifier loss: 0.148168; batch adversarial loss: 0.278373\n",
      "epoch 13; iter: 0; batch classifier loss: 0.209642; batch adversarial loss: 0.246604\n",
      "epoch 14; iter: 0; batch classifier loss: 0.224973; batch adversarial loss: 0.307416\n",
      "epoch 15; iter: 0; batch classifier loss: 0.274097; batch adversarial loss: 0.291960\n",
      "epoch 16; iter: 0; batch classifier loss: 0.167044; batch adversarial loss: 0.303214\n",
      "epoch 17; iter: 0; batch classifier loss: 0.197149; batch adversarial loss: 0.285980\n",
      "epoch 18; iter: 0; batch classifier loss: 0.260531; batch adversarial loss: 0.244626\n",
      "epoch 19; iter: 0; batch classifier loss: 0.199958; batch adversarial loss: 0.242625\n",
      "epoch 20; iter: 0; batch classifier loss: 0.244025; batch adversarial loss: 0.388253\n",
      "epoch 21; iter: 0; batch classifier loss: 0.208603; batch adversarial loss: 0.221314\n",
      "epoch 22; iter: 0; batch classifier loss: 0.181737; batch adversarial loss: 0.272725\n",
      "epoch 23; iter: 0; batch classifier loss: 0.284584; batch adversarial loss: 0.229897\n",
      "epoch 24; iter: 0; batch classifier loss: 0.230188; batch adversarial loss: 0.255157\n",
      "epoch 25; iter: 0; batch classifier loss: 0.277290; batch adversarial loss: 0.222847\n",
      "epoch 26; iter: 0; batch classifier loss: 0.176720; batch adversarial loss: 0.257918\n",
      "epoch 27; iter: 0; batch classifier loss: 0.246955; batch adversarial loss: 0.243424\n",
      "epoch 28; iter: 0; batch classifier loss: 0.187326; batch adversarial loss: 0.315558\n",
      "epoch 29; iter: 0; batch classifier loss: 0.249705; batch adversarial loss: 0.303850\n",
      "epoch 30; iter: 0; batch classifier loss: 0.185646; batch adversarial loss: 0.265684\n",
      "epoch 31; iter: 0; batch classifier loss: 0.184270; batch adversarial loss: 0.274803\n",
      "epoch 32; iter: 0; batch classifier loss: 0.183269; batch adversarial loss: 0.332563\n",
      "epoch 33; iter: 0; batch classifier loss: 0.241879; batch adversarial loss: 0.286626\n",
      "epoch 34; iter: 0; batch classifier loss: 0.204680; batch adversarial loss: 0.254530\n",
      "epoch 35; iter: 0; batch classifier loss: 0.185000; batch adversarial loss: 0.249531\n",
      "epoch 36; iter: 0; batch classifier loss: 0.213042; batch adversarial loss: 0.220909\n",
      "epoch 37; iter: 0; batch classifier loss: 0.209044; batch adversarial loss: 0.251188\n",
      "epoch 38; iter: 0; batch classifier loss: 0.247928; batch adversarial loss: 0.272219\n",
      "epoch 39; iter: 0; batch classifier loss: 0.205738; batch adversarial loss: 0.280581\n",
      "epoch 40; iter: 0; batch classifier loss: 0.228045; batch adversarial loss: 0.243073\n",
      "epoch 41; iter: 0; batch classifier loss: 0.248149; batch adversarial loss: 0.299238\n",
      "epoch 42; iter: 0; batch classifier loss: 0.212939; batch adversarial loss: 0.162216\n",
      "epoch 43; iter: 0; batch classifier loss: 0.226371; batch adversarial loss: 0.161016\n",
      "epoch 44; iter: 0; batch classifier loss: 0.204080; batch adversarial loss: 0.211711\n",
      "epoch 45; iter: 0; batch classifier loss: 0.296832; batch adversarial loss: 0.232196\n",
      "epoch 46; iter: 0; batch classifier loss: 0.170226; batch adversarial loss: 0.322160\n",
      "epoch 47; iter: 0; batch classifier loss: 0.213123; batch adversarial loss: 0.234235\n",
      "epoch 48; iter: 0; batch classifier loss: 0.252120; batch adversarial loss: 0.240825\n",
      "epoch 49; iter: 0; batch classifier loss: 0.246211; batch adversarial loss: 0.201558\n",
      "epoch 50; iter: 0; batch classifier loss: 0.189649; batch adversarial loss: 0.329760\n",
      "epoch 51; iter: 0; batch classifier loss: 0.180965; batch adversarial loss: 0.300407\n",
      "epoch 52; iter: 0; batch classifier loss: 0.204841; batch adversarial loss: 0.333452\n",
      "epoch 53; iter: 0; batch classifier loss: 0.221011; batch adversarial loss: 0.229040\n",
      "epoch 54; iter: 0; batch classifier loss: 0.186969; batch adversarial loss: 0.263665\n",
      "epoch 55; iter: 0; batch classifier loss: 0.204107; batch adversarial loss: 0.160371\n",
      "epoch 56; iter: 0; batch classifier loss: 0.187523; batch adversarial loss: 0.244118\n",
      "epoch 57; iter: 0; batch classifier loss: 0.152007; batch adversarial loss: 0.244489\n",
      "epoch 58; iter: 0; batch classifier loss: 0.207409; batch adversarial loss: 0.255417\n",
      "epoch 59; iter: 0; batch classifier loss: 0.157660; batch adversarial loss: 0.231331\n",
      "epoch 60; iter: 0; batch classifier loss: 0.287379; batch adversarial loss: 0.278780\n",
      "epoch 61; iter: 0; batch classifier loss: 0.197542; batch adversarial loss: 0.335815\n",
      "epoch 62; iter: 0; batch classifier loss: 0.174460; batch adversarial loss: 0.225326\n",
      "epoch 63; iter: 0; batch classifier loss: 0.182075; batch adversarial loss: 0.264183\n",
      "epoch 64; iter: 0; batch classifier loss: 0.223027; batch adversarial loss: 0.280226\n",
      "epoch 65; iter: 0; batch classifier loss: 0.194683; batch adversarial loss: 0.234699\n",
      "epoch 66; iter: 0; batch classifier loss: 0.241759; batch adversarial loss: 0.217949\n",
      "epoch 67; iter: 0; batch classifier loss: 0.239954; batch adversarial loss: 0.313014\n",
      "epoch 68; iter: 0; batch classifier loss: 0.218548; batch adversarial loss: 0.268535\n",
      "epoch 69; iter: 0; batch classifier loss: 0.180063; batch adversarial loss: 0.302466\n",
      "epoch 70; iter: 0; batch classifier loss: 0.213374; batch adversarial loss: 0.351291\n",
      "epoch 71; iter: 0; batch classifier loss: 0.258362; batch adversarial loss: 0.324449\n",
      "epoch 72; iter: 0; batch classifier loss: 0.169931; batch adversarial loss: 0.269551\n",
      "epoch 73; iter: 0; batch classifier loss: 0.243430; batch adversarial loss: 0.299524\n",
      "epoch 74; iter: 0; batch classifier loss: 0.218181; batch adversarial loss: 0.300310\n",
      "epoch 75; iter: 0; batch classifier loss: 0.231629; batch adversarial loss: 0.262658\n",
      "epoch 76; iter: 0; batch classifier loss: 0.164955; batch adversarial loss: 0.371471\n",
      "epoch 77; iter: 0; batch classifier loss: 0.187911; batch adversarial loss: 0.271055\n",
      "epoch 78; iter: 0; batch classifier loss: 0.223116; batch adversarial loss: 0.230626\n",
      "epoch 79; iter: 0; batch classifier loss: 0.206958; batch adversarial loss: 0.231488\n",
      "epoch 80; iter: 0; batch classifier loss: 0.245158; batch adversarial loss: 0.238069\n",
      "epoch 81; iter: 0; batch classifier loss: 0.218394; batch adversarial loss: 0.274010\n",
      "epoch 82; iter: 0; batch classifier loss: 0.221144; batch adversarial loss: 0.260921\n",
      "epoch 83; iter: 0; batch classifier loss: 0.180541; batch adversarial loss: 0.261517\n",
      "epoch 84; iter: 0; batch classifier loss: 0.201271; batch adversarial loss: 0.313655\n",
      "epoch 85; iter: 0; batch classifier loss: 0.221371; batch adversarial loss: 0.330493\n",
      "epoch 86; iter: 0; batch classifier loss: 0.148857; batch adversarial loss: 0.259873\n",
      "epoch 87; iter: 0; batch classifier loss: 0.174631; batch adversarial loss: 0.213522\n",
      "epoch 88; iter: 0; batch classifier loss: 0.279307; batch adversarial loss: 0.297766\n",
      "epoch 89; iter: 0; batch classifier loss: 0.197471; batch adversarial loss: 0.300049\n",
      "epoch 90; iter: 0; batch classifier loss: 0.264986; batch adversarial loss: 0.243604\n",
      "epoch 91; iter: 0; batch classifier loss: 0.227441; batch adversarial loss: 0.305245\n",
      "epoch 92; iter: 0; batch classifier loss: 0.268645; batch adversarial loss: 0.252915\n",
      "epoch 93; iter: 0; batch classifier loss: 0.150947; batch adversarial loss: 0.314214\n",
      "epoch 94; iter: 0; batch classifier loss: 0.154195; batch adversarial loss: 0.190220\n",
      "epoch 95; iter: 0; batch classifier loss: 0.147701; batch adversarial loss: 0.282416\n",
      "epoch 96; iter: 0; batch classifier loss: 0.153856; batch adversarial loss: 0.239566\n",
      "epoch 97; iter: 0; batch classifier loss: 0.287010; batch adversarial loss: 0.337027\n",
      "epoch 98; iter: 0; batch classifier loss: 0.226254; batch adversarial loss: 0.238602\n",
      "epoch 99; iter: 0; batch classifier loss: 0.216590; batch adversarial loss: 0.269944\n",
      "epoch 100; iter: 0; batch classifier loss: 0.179464; batch adversarial loss: 0.287057\n",
      "epoch 101; iter: 0; batch classifier loss: 0.275809; batch adversarial loss: 0.216540\n",
      "epoch 102; iter: 0; batch classifier loss: 0.205686; batch adversarial loss: 0.192913\n",
      "epoch 103; iter: 0; batch classifier loss: 0.252293; batch adversarial loss: 0.290652\n",
      "epoch 104; iter: 0; batch classifier loss: 0.177447; batch adversarial loss: 0.250046\n",
      "epoch 105; iter: 0; batch classifier loss: 0.209639; batch adversarial loss: 0.237057\n",
      "epoch 106; iter: 0; batch classifier loss: 0.177776; batch adversarial loss: 0.267559\n",
      "epoch 107; iter: 0; batch classifier loss: 0.117356; batch adversarial loss: 0.245848\n",
      "epoch 108; iter: 0; batch classifier loss: 0.195511; batch adversarial loss: 0.341729\n",
      "epoch 109; iter: 0; batch classifier loss: 0.216142; batch adversarial loss: 0.391566\n",
      "epoch 110; iter: 0; batch classifier loss: 0.178646; batch adversarial loss: 0.284470\n",
      "epoch 111; iter: 0; batch classifier loss: 0.160695; batch adversarial loss: 0.243951\n",
      "epoch 112; iter: 0; batch classifier loss: 0.245970; batch adversarial loss: 0.240881\n",
      "epoch 113; iter: 0; batch classifier loss: 0.198273; batch adversarial loss: 0.273591\n",
      "epoch 114; iter: 0; batch classifier loss: 0.188400; batch adversarial loss: 0.283761\n",
      "epoch 115; iter: 0; batch classifier loss: 0.243554; batch adversarial loss: 0.460030\n",
      "epoch 116; iter: 0; batch classifier loss: 0.203523; batch adversarial loss: 0.280956\n",
      "epoch 117; iter: 0; batch classifier loss: 0.215428; batch adversarial loss: 0.244995\n",
      "epoch 118; iter: 0; batch classifier loss: 0.146355; batch adversarial loss: 0.203488\n",
      "epoch 119; iter: 0; batch classifier loss: 0.223616; batch adversarial loss: 0.352880\n",
      "epoch 120; iter: 0; batch classifier loss: 0.242080; batch adversarial loss: 0.358776\n",
      "epoch 121; iter: 0; batch classifier loss: 0.208324; batch adversarial loss: 0.223096\n",
      "epoch 122; iter: 0; batch classifier loss: 0.170981; batch adversarial loss: 0.297191\n",
      "epoch 123; iter: 0; batch classifier loss: 0.284853; batch adversarial loss: 0.273971\n",
      "epoch 124; iter: 0; batch classifier loss: 0.195927; batch adversarial loss: 0.195149\n",
      "epoch 125; iter: 0; batch classifier loss: 0.320569; batch adversarial loss: 0.461393\n",
      "epoch 126; iter: 0; batch classifier loss: 0.174333; batch adversarial loss: 0.301644\n",
      "epoch 127; iter: 0; batch classifier loss: 0.153988; batch adversarial loss: 0.244510\n",
      "epoch 128; iter: 0; batch classifier loss: 0.223229; batch adversarial loss: 0.275576\n",
      "epoch 129; iter: 0; batch classifier loss: 0.218010; batch adversarial loss: 0.427793\n",
      "epoch 130; iter: 0; batch classifier loss: 0.221088; batch adversarial loss: 0.281687\n",
      "epoch 131; iter: 0; batch classifier loss: 0.183331; batch adversarial loss: 0.394989\n",
      "epoch 132; iter: 0; batch classifier loss: 0.196395; batch adversarial loss: 0.292956\n",
      "epoch 133; iter: 0; batch classifier loss: 0.250127; batch adversarial loss: 0.221983\n",
      "epoch 134; iter: 0; batch classifier loss: 0.252178; batch adversarial loss: 0.230484\n",
      "epoch 135; iter: 0; batch classifier loss: 0.168716; batch adversarial loss: 0.248780\n",
      "epoch 136; iter: 0; batch classifier loss: 0.270441; batch adversarial loss: 0.315739\n",
      "epoch 137; iter: 0; batch classifier loss: 0.183787; batch adversarial loss: 0.184814\n",
      "epoch 138; iter: 0; batch classifier loss: 0.213915; batch adversarial loss: 0.216460\n",
      "epoch 139; iter: 0; batch classifier loss: 0.194969; batch adversarial loss: 0.230939\n",
      "epoch 140; iter: 0; batch classifier loss: 0.199201; batch adversarial loss: 0.389159\n",
      "epoch 141; iter: 0; batch classifier loss: 0.194196; batch adversarial loss: 0.195247\n",
      "epoch 142; iter: 0; batch classifier loss: 0.139195; batch adversarial loss: 0.237412\n",
      "epoch 143; iter: 0; batch classifier loss: 0.206380; batch adversarial loss: 0.283266\n",
      "epoch 144; iter: 0; batch classifier loss: 0.151402; batch adversarial loss: 0.191763\n",
      "epoch 145; iter: 0; batch classifier loss: 0.197818; batch adversarial loss: 0.276440\n",
      "epoch 146; iter: 0; batch classifier loss: 0.125609; batch adversarial loss: 0.342357\n",
      "epoch 147; iter: 0; batch classifier loss: 0.190087; batch adversarial loss: 0.234911\n",
      "epoch 148; iter: 0; batch classifier loss: 0.206086; batch adversarial loss: 0.265591\n",
      "epoch 149; iter: 0; batch classifier loss: 0.202255; batch adversarial loss: 0.231480\n",
      "epoch 150; iter: 0; batch classifier loss: 0.135311; batch adversarial loss: 0.252221\n",
      "epoch 151; iter: 0; batch classifier loss: 0.245321; batch adversarial loss: 0.231560\n",
      "epoch 152; iter: 0; batch classifier loss: 0.182012; batch adversarial loss: 0.260602\n",
      "epoch 153; iter: 0; batch classifier loss: 0.238027; batch adversarial loss: 0.335698\n",
      "epoch 154; iter: 0; batch classifier loss: 0.240260; batch adversarial loss: 0.295302\n",
      "epoch 155; iter: 0; batch classifier loss: 0.245348; batch adversarial loss: 0.203682\n",
      "epoch 156; iter: 0; batch classifier loss: 0.173256; batch adversarial loss: 0.236536\n",
      "epoch 157; iter: 0; batch classifier loss: 0.110896; batch adversarial loss: 0.288486\n",
      "epoch 158; iter: 0; batch classifier loss: 0.221132; batch adversarial loss: 0.271360\n",
      "epoch 159; iter: 0; batch classifier loss: 0.208505; batch adversarial loss: 0.304763\n",
      "epoch 160; iter: 0; batch classifier loss: 0.211467; batch adversarial loss: 0.378184\n",
      "epoch 161; iter: 0; batch classifier loss: 0.186020; batch adversarial loss: 0.255711\n",
      "epoch 162; iter: 0; batch classifier loss: 0.144564; batch adversarial loss: 0.201971\n",
      "epoch 163; iter: 0; batch classifier loss: 0.198649; batch adversarial loss: 0.344690\n",
      "epoch 164; iter: 0; batch classifier loss: 0.206988; batch adversarial loss: 0.219947\n",
      "epoch 165; iter: 0; batch classifier loss: 0.232377; batch adversarial loss: 0.287133\n",
      "epoch 166; iter: 0; batch classifier loss: 0.140070; batch adversarial loss: 0.306586\n",
      "epoch 167; iter: 0; batch classifier loss: 0.272889; batch adversarial loss: 0.280374\n",
      "epoch 168; iter: 0; batch classifier loss: 0.200350; batch adversarial loss: 0.279154\n",
      "epoch 169; iter: 0; batch classifier loss: 0.165092; batch adversarial loss: 0.205331\n",
      "epoch 170; iter: 0; batch classifier loss: 0.182798; batch adversarial loss: 0.208016\n",
      "epoch 171; iter: 0; batch classifier loss: 0.235547; batch adversarial loss: 0.309171\n",
      "epoch 172; iter: 0; batch classifier loss: 0.136448; batch adversarial loss: 0.367013\n",
      "epoch 173; iter: 0; batch classifier loss: 0.161347; batch adversarial loss: 0.262758\n",
      "epoch 174; iter: 0; batch classifier loss: 0.222140; batch adversarial loss: 0.290159\n",
      "epoch 175; iter: 0; batch classifier loss: 0.234481; batch adversarial loss: 0.284890\n",
      "epoch 176; iter: 0; batch classifier loss: 0.146613; batch adversarial loss: 0.222282\n",
      "epoch 177; iter: 0; batch classifier loss: 0.179659; batch adversarial loss: 0.325912\n",
      "epoch 178; iter: 0; batch classifier loss: 0.250144; batch adversarial loss: 0.233251\n",
      "epoch 179; iter: 0; batch classifier loss: 0.163839; batch adversarial loss: 0.219226\n",
      "epoch 180; iter: 0; batch classifier loss: 0.263599; batch adversarial loss: 0.310229\n",
      "epoch 181; iter: 0; batch classifier loss: 0.206078; batch adversarial loss: 0.264972\n",
      "epoch 182; iter: 0; batch classifier loss: 0.174763; batch adversarial loss: 0.200557\n",
      "epoch 183; iter: 0; batch classifier loss: 0.244948; batch adversarial loss: 0.217286\n",
      "epoch 184; iter: 0; batch classifier loss: 0.220683; batch adversarial loss: 0.217405\n",
      "epoch 185; iter: 0; batch classifier loss: 0.153891; batch adversarial loss: 0.262989\n",
      "epoch 186; iter: 0; batch classifier loss: 0.265016; batch adversarial loss: 0.318017\n",
      "epoch 187; iter: 0; batch classifier loss: 0.187292; batch adversarial loss: 0.406889\n",
      "epoch 188; iter: 0; batch classifier loss: 0.245562; batch adversarial loss: 0.346319\n",
      "epoch 189; iter: 0; batch classifier loss: 0.275098; batch adversarial loss: 0.328795\n",
      "epoch 190; iter: 0; batch classifier loss: 0.286992; batch adversarial loss: 0.259467\n",
      "epoch 191; iter: 0; batch classifier loss: 0.258971; batch adversarial loss: 0.411849\n",
      "epoch 192; iter: 0; batch classifier loss: 0.182425; batch adversarial loss: 0.194116\n",
      "epoch 193; iter: 0; batch classifier loss: 0.187450; batch adversarial loss: 0.247463\n",
      "epoch 194; iter: 0; batch classifier loss: 0.177814; batch adversarial loss: 0.261556\n",
      "epoch 195; iter: 0; batch classifier loss: 0.220732; batch adversarial loss: 0.192784\n",
      "epoch 196; iter: 0; batch classifier loss: 0.218797; batch adversarial loss: 0.275123\n",
      "epoch 197; iter: 0; batch classifier loss: 0.183303; batch adversarial loss: 0.239834\n",
      "epoch 198; iter: 0; batch classifier loss: 0.213599; batch adversarial loss: 0.322134\n",
      "epoch 199; iter: 0; batch classifier loss: 0.179816; batch adversarial loss: 0.314442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:57:43.260880: W tensorflow/c/c_api.cc:305] Operation '{name:'39b91138-ae25-11ee-bc15-ae7d8bf09116/39b91138-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:7087 op device:{requested: '', assigned: ''} def:{{{node 39b91138-ae25-11ee-bc15-ae7d8bf09116/39b91138-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b91138-ae25-11ee-bc15-ae7d8bf09116/39b91138-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b91138-ae25-11ee-bc15-ae7d8bf09116/39b91138-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.657810; batch adversarial loss: 0.406192\n",
      "epoch 1; iter: 0; batch classifier loss: 0.621928; batch adversarial loss: 0.411689\n",
      "epoch 2; iter: 0; batch classifier loss: 1.073497; batch adversarial loss: 0.610888\n",
      "epoch 3; iter: 0; batch classifier loss: 1.554241; batch adversarial loss: 0.656552\n",
      "epoch 4; iter: 0; batch classifier loss: 1.816630; batch adversarial loss: 0.613507\n",
      "epoch 5; iter: 0; batch classifier loss: 1.892131; batch adversarial loss: 0.574449\n",
      "epoch 6; iter: 0; batch classifier loss: 1.906832; batch adversarial loss: 0.596440\n",
      "epoch 7; iter: 0; batch classifier loss: 1.609569; batch adversarial loss: 0.453991\n",
      "epoch 8; iter: 0; batch classifier loss: 1.154886; batch adversarial loss: 0.388878\n",
      "epoch 9; iter: 0; batch classifier loss: 1.011148; batch adversarial loss: 0.350754\n",
      "epoch 10; iter: 0; batch classifier loss: 0.749192; batch adversarial loss: 0.345487\n",
      "epoch 11; iter: 0; batch classifier loss: 0.524775; batch adversarial loss: 0.293857\n",
      "epoch 12; iter: 0; batch classifier loss: 0.343927; batch adversarial loss: 0.245985\n",
      "epoch 13; iter: 0; batch classifier loss: 0.194006; batch adversarial loss: 0.255939\n",
      "epoch 14; iter: 0; batch classifier loss: 0.274277; batch adversarial loss: 0.314902\n",
      "epoch 15; iter: 0; batch classifier loss: 0.160284; batch adversarial loss: 0.212605\n",
      "epoch 16; iter: 0; batch classifier loss: 0.242632; batch adversarial loss: 0.280040\n",
      "epoch 17; iter: 0; batch classifier loss: 0.189896; batch adversarial loss: 0.369990\n",
      "epoch 18; iter: 0; batch classifier loss: 0.289416; batch adversarial loss: 0.335524\n",
      "epoch 19; iter: 0; batch classifier loss: 0.234968; batch adversarial loss: 0.231369\n",
      "epoch 20; iter: 0; batch classifier loss: 0.216653; batch adversarial loss: 0.132226\n",
      "epoch 21; iter: 0; batch classifier loss: 0.233927; batch adversarial loss: 0.240268\n",
      "epoch 22; iter: 0; batch classifier loss: 0.205004; batch adversarial loss: 0.273671\n",
      "epoch 23; iter: 0; batch classifier loss: 0.172712; batch adversarial loss: 0.244234\n",
      "epoch 24; iter: 0; batch classifier loss: 0.255857; batch adversarial loss: 0.371656\n",
      "epoch 25; iter: 0; batch classifier loss: 0.275486; batch adversarial loss: 0.209832\n",
      "epoch 26; iter: 0; batch classifier loss: 0.239041; batch adversarial loss: 0.264640\n",
      "epoch 27; iter: 0; batch classifier loss: 0.167660; batch adversarial loss: 0.234450\n",
      "epoch 28; iter: 0; batch classifier loss: 0.258510; batch adversarial loss: 0.257467\n",
      "epoch 29; iter: 0; batch classifier loss: 0.151533; batch adversarial loss: 0.160223\n",
      "epoch 30; iter: 0; batch classifier loss: 0.161939; batch adversarial loss: 0.241154\n",
      "epoch 31; iter: 0; batch classifier loss: 0.194022; batch adversarial loss: 0.172075\n",
      "epoch 32; iter: 0; batch classifier loss: 0.267696; batch adversarial loss: 0.259200\n",
      "epoch 33; iter: 0; batch classifier loss: 0.230826; batch adversarial loss: 0.345690\n",
      "epoch 34; iter: 0; batch classifier loss: 0.153394; batch adversarial loss: 0.257566\n",
      "epoch 35; iter: 0; batch classifier loss: 0.120859; batch adversarial loss: 0.167902\n",
      "epoch 36; iter: 0; batch classifier loss: 0.257814; batch adversarial loss: 0.178476\n",
      "epoch 37; iter: 0; batch classifier loss: 0.247751; batch adversarial loss: 0.274572\n",
      "epoch 38; iter: 0; batch classifier loss: 0.183185; batch adversarial loss: 0.320617\n",
      "epoch 39; iter: 0; batch classifier loss: 0.195184; batch adversarial loss: 0.244439\n",
      "epoch 40; iter: 0; batch classifier loss: 0.372413; batch adversarial loss: 0.274741\n",
      "epoch 41; iter: 0; batch classifier loss: 0.194762; batch adversarial loss: 0.326742\n",
      "epoch 42; iter: 0; batch classifier loss: 0.252029; batch adversarial loss: 0.311291\n",
      "epoch 43; iter: 0; batch classifier loss: 0.237983; batch adversarial loss: 0.217895\n",
      "epoch 44; iter: 0; batch classifier loss: 0.281352; batch adversarial loss: 0.358360\n",
      "epoch 45; iter: 0; batch classifier loss: 0.247412; batch adversarial loss: 0.233583\n",
      "epoch 46; iter: 0; batch classifier loss: 0.224900; batch adversarial loss: 0.275752\n",
      "epoch 47; iter: 0; batch classifier loss: 0.198141; batch adversarial loss: 0.194414\n",
      "epoch 48; iter: 0; batch classifier loss: 0.173377; batch adversarial loss: 0.259732\n",
      "epoch 49; iter: 0; batch classifier loss: 0.205063; batch adversarial loss: 0.198984\n",
      "epoch 50; iter: 0; batch classifier loss: 0.211083; batch adversarial loss: 0.268797\n",
      "epoch 51; iter: 0; batch classifier loss: 0.248401; batch adversarial loss: 0.246319\n",
      "epoch 52; iter: 0; batch classifier loss: 0.288472; batch adversarial loss: 0.263763\n",
      "epoch 53; iter: 0; batch classifier loss: 0.264929; batch adversarial loss: 0.258709\n",
      "epoch 54; iter: 0; batch classifier loss: 0.218918; batch adversarial loss: 0.291239\n",
      "epoch 55; iter: 0; batch classifier loss: 0.211422; batch adversarial loss: 0.349773\n",
      "epoch 56; iter: 0; batch classifier loss: 0.203607; batch adversarial loss: 0.248888\n",
      "epoch 57; iter: 0; batch classifier loss: 0.231121; batch adversarial loss: 0.222230\n",
      "epoch 58; iter: 0; batch classifier loss: 0.304455; batch adversarial loss: 0.281316\n",
      "epoch 59; iter: 0; batch classifier loss: 0.210111; batch adversarial loss: 0.274026\n",
      "epoch 60; iter: 0; batch classifier loss: 0.231186; batch adversarial loss: 0.226277\n",
      "epoch 61; iter: 0; batch classifier loss: 0.245020; batch adversarial loss: 0.277732\n",
      "epoch 62; iter: 0; batch classifier loss: 0.177422; batch adversarial loss: 0.362861\n",
      "epoch 63; iter: 0; batch classifier loss: 0.180683; batch adversarial loss: 0.246155\n",
      "epoch 64; iter: 0; batch classifier loss: 0.148783; batch adversarial loss: 0.379896\n",
      "epoch 65; iter: 0; batch classifier loss: 0.214944; batch adversarial loss: 0.267694\n",
      "epoch 66; iter: 0; batch classifier loss: 0.200187; batch adversarial loss: 0.295064\n",
      "epoch 67; iter: 0; batch classifier loss: 0.225839; batch adversarial loss: 0.193103\n",
      "epoch 68; iter: 0; batch classifier loss: 0.175820; batch adversarial loss: 0.281679\n",
      "epoch 69; iter: 0; batch classifier loss: 0.182935; batch adversarial loss: 0.178745\n",
      "epoch 70; iter: 0; batch classifier loss: 0.183625; batch adversarial loss: 0.249749\n",
      "epoch 71; iter: 0; batch classifier loss: 0.201126; batch adversarial loss: 0.207611\n",
      "epoch 72; iter: 0; batch classifier loss: 0.206728; batch adversarial loss: 0.268231\n",
      "epoch 73; iter: 0; batch classifier loss: 0.249301; batch adversarial loss: 0.179503\n",
      "epoch 74; iter: 0; batch classifier loss: 0.207725; batch adversarial loss: 0.350614\n",
      "epoch 75; iter: 0; batch classifier loss: 0.182312; batch adversarial loss: 0.173499\n",
      "epoch 76; iter: 0; batch classifier loss: 0.236346; batch adversarial loss: 0.259143\n",
      "epoch 77; iter: 0; batch classifier loss: 0.185438; batch adversarial loss: 0.241336\n",
      "epoch 78; iter: 0; batch classifier loss: 0.191729; batch adversarial loss: 0.243073\n",
      "epoch 79; iter: 0; batch classifier loss: 0.251855; batch adversarial loss: 0.142876\n",
      "epoch 80; iter: 0; batch classifier loss: 0.146775; batch adversarial loss: 0.271137\n",
      "epoch 81; iter: 0; batch classifier loss: 0.321456; batch adversarial loss: 0.170224\n",
      "epoch 82; iter: 0; batch classifier loss: 0.200750; batch adversarial loss: 0.364045\n",
      "epoch 83; iter: 0; batch classifier loss: 0.170778; batch adversarial loss: 0.205383\n",
      "epoch 84; iter: 0; batch classifier loss: 0.106914; batch adversarial loss: 0.252352\n",
      "epoch 85; iter: 0; batch classifier loss: 0.146436; batch adversarial loss: 0.262728\n",
      "epoch 86; iter: 0; batch classifier loss: 0.231807; batch adversarial loss: 0.321566\n",
      "epoch 87; iter: 0; batch classifier loss: 0.220190; batch adversarial loss: 0.177492\n",
      "epoch 88; iter: 0; batch classifier loss: 0.223294; batch adversarial loss: 0.237956\n",
      "epoch 89; iter: 0; batch classifier loss: 0.173372; batch adversarial loss: 0.258251\n",
      "epoch 90; iter: 0; batch classifier loss: 0.235733; batch adversarial loss: 0.222983\n",
      "epoch 91; iter: 0; batch classifier loss: 0.169927; batch adversarial loss: 0.267896\n",
      "epoch 92; iter: 0; batch classifier loss: 0.186144; batch adversarial loss: 0.269444\n",
      "epoch 93; iter: 0; batch classifier loss: 0.203779; batch adversarial loss: 0.342116\n",
      "epoch 94; iter: 0; batch classifier loss: 0.269588; batch adversarial loss: 0.270455\n",
      "epoch 95; iter: 0; batch classifier loss: 0.167480; batch adversarial loss: 0.278025\n",
      "epoch 96; iter: 0; batch classifier loss: 0.195569; batch adversarial loss: 0.211862\n",
      "epoch 97; iter: 0; batch classifier loss: 0.151979; batch adversarial loss: 0.318822\n",
      "epoch 98; iter: 0; batch classifier loss: 0.199534; batch adversarial loss: 0.352747\n",
      "epoch 99; iter: 0; batch classifier loss: 0.129444; batch adversarial loss: 0.267521\n",
      "epoch 100; iter: 0; batch classifier loss: 0.181538; batch adversarial loss: 0.229455\n",
      "epoch 101; iter: 0; batch classifier loss: 0.223641; batch adversarial loss: 0.342512\n",
      "epoch 102; iter: 0; batch classifier loss: 0.178563; batch adversarial loss: 0.326521\n",
      "epoch 103; iter: 0; batch classifier loss: 0.214203; batch adversarial loss: 0.217877\n",
      "epoch 104; iter: 0; batch classifier loss: 0.206961; batch adversarial loss: 0.196603\n",
      "epoch 105; iter: 0; batch classifier loss: 0.200452; batch adversarial loss: 0.223608\n",
      "epoch 106; iter: 0; batch classifier loss: 0.197385; batch adversarial loss: 0.186906\n",
      "epoch 107; iter: 0; batch classifier loss: 0.199264; batch adversarial loss: 0.253596\n",
      "epoch 108; iter: 0; batch classifier loss: 0.165345; batch adversarial loss: 0.232164\n",
      "epoch 109; iter: 0; batch classifier loss: 0.169919; batch adversarial loss: 0.168879\n",
      "epoch 110; iter: 0; batch classifier loss: 0.230950; batch adversarial loss: 0.235173\n",
      "epoch 111; iter: 0; batch classifier loss: 0.245519; batch adversarial loss: 0.218639\n",
      "epoch 112; iter: 0; batch classifier loss: 0.192489; batch adversarial loss: 0.188191\n",
      "epoch 113; iter: 0; batch classifier loss: 0.158960; batch adversarial loss: 0.216932\n",
      "epoch 114; iter: 0; batch classifier loss: 0.218157; batch adversarial loss: 0.263828\n",
      "epoch 115; iter: 0; batch classifier loss: 0.233150; batch adversarial loss: 0.242566\n",
      "epoch 116; iter: 0; batch classifier loss: 0.212330; batch adversarial loss: 0.309169\n",
      "epoch 117; iter: 0; batch classifier loss: 0.169758; batch adversarial loss: 0.277830\n",
      "epoch 118; iter: 0; batch classifier loss: 0.156276; batch adversarial loss: 0.403781\n",
      "epoch 119; iter: 0; batch classifier loss: 0.143584; batch adversarial loss: 0.291395\n",
      "epoch 120; iter: 0; batch classifier loss: 0.146423; batch adversarial loss: 0.175879\n",
      "epoch 121; iter: 0; batch classifier loss: 0.169205; batch adversarial loss: 0.225718\n",
      "epoch 122; iter: 0; batch classifier loss: 0.121550; batch adversarial loss: 0.241772\n",
      "epoch 123; iter: 0; batch classifier loss: 0.159698; batch adversarial loss: 0.247947\n",
      "epoch 124; iter: 0; batch classifier loss: 0.193321; batch adversarial loss: 0.183148\n",
      "epoch 125; iter: 0; batch classifier loss: 0.174353; batch adversarial loss: 0.253804\n",
      "epoch 126; iter: 0; batch classifier loss: 0.148990; batch adversarial loss: 0.178128\n",
      "epoch 127; iter: 0; batch classifier loss: 0.167911; batch adversarial loss: 0.265503\n",
      "epoch 128; iter: 0; batch classifier loss: 0.202699; batch adversarial loss: 0.311173\n",
      "epoch 129; iter: 0; batch classifier loss: 0.143601; batch adversarial loss: 0.263048\n",
      "epoch 130; iter: 0; batch classifier loss: 0.185644; batch adversarial loss: 0.241276\n",
      "epoch 131; iter: 0; batch classifier loss: 0.129313; batch adversarial loss: 0.272235\n",
      "epoch 132; iter: 0; batch classifier loss: 0.143278; batch adversarial loss: 0.299106\n",
      "epoch 133; iter: 0; batch classifier loss: 0.211748; batch adversarial loss: 0.205385\n",
      "epoch 134; iter: 0; batch classifier loss: 0.210511; batch adversarial loss: 0.417427\n",
      "epoch 135; iter: 0; batch classifier loss: 0.158702; batch adversarial loss: 0.200800\n",
      "epoch 136; iter: 0; batch classifier loss: 0.220669; batch adversarial loss: 0.308815\n",
      "epoch 137; iter: 0; batch classifier loss: 0.138404; batch adversarial loss: 0.268036\n",
      "epoch 138; iter: 0; batch classifier loss: 0.222141; batch adversarial loss: 0.145376\n",
      "epoch 139; iter: 0; batch classifier loss: 0.180433; batch adversarial loss: 0.128973\n",
      "epoch 140; iter: 0; batch classifier loss: 0.168909; batch adversarial loss: 0.249024\n",
      "epoch 141; iter: 0; batch classifier loss: 0.163105; batch adversarial loss: 0.214218\n",
      "epoch 142; iter: 0; batch classifier loss: 0.175986; batch adversarial loss: 0.254930\n",
      "epoch 143; iter: 0; batch classifier loss: 0.243481; batch adversarial loss: 0.345410\n",
      "epoch 144; iter: 0; batch classifier loss: 0.141610; batch adversarial loss: 0.219200\n",
      "epoch 145; iter: 0; batch classifier loss: 0.224876; batch adversarial loss: 0.268599\n",
      "epoch 146; iter: 0; batch classifier loss: 0.205630; batch adversarial loss: 0.267782\n",
      "epoch 147; iter: 0; batch classifier loss: 0.194955; batch adversarial loss: 0.273556\n",
      "epoch 148; iter: 0; batch classifier loss: 0.162533; batch adversarial loss: 0.228215\n",
      "epoch 149; iter: 0; batch classifier loss: 0.147611; batch adversarial loss: 0.330596\n",
      "epoch 150; iter: 0; batch classifier loss: 0.162508; batch adversarial loss: 0.291905\n",
      "epoch 151; iter: 0; batch classifier loss: 0.167305; batch adversarial loss: 0.285041\n",
      "epoch 152; iter: 0; batch classifier loss: 0.256618; batch adversarial loss: 0.355918\n",
      "epoch 153; iter: 0; batch classifier loss: 0.230553; batch adversarial loss: 0.248287\n",
      "epoch 154; iter: 0; batch classifier loss: 0.158919; batch adversarial loss: 0.214738\n",
      "epoch 155; iter: 0; batch classifier loss: 0.208060; batch adversarial loss: 0.283685\n",
      "epoch 156; iter: 0; batch classifier loss: 0.142661; batch adversarial loss: 0.302639\n",
      "epoch 157; iter: 0; batch classifier loss: 0.222140; batch adversarial loss: 0.169418\n",
      "epoch 158; iter: 0; batch classifier loss: 0.142454; batch adversarial loss: 0.205287\n",
      "epoch 159; iter: 0; batch classifier loss: 0.187423; batch adversarial loss: 0.292769\n",
      "epoch 160; iter: 0; batch classifier loss: 0.155870; batch adversarial loss: 0.221644\n",
      "epoch 161; iter: 0; batch classifier loss: 0.174966; batch adversarial loss: 0.245050\n",
      "epoch 162; iter: 0; batch classifier loss: 0.230419; batch adversarial loss: 0.206036\n",
      "epoch 163; iter: 0; batch classifier loss: 0.135122; batch adversarial loss: 0.309799\n",
      "epoch 164; iter: 0; batch classifier loss: 0.222884; batch adversarial loss: 0.197660\n",
      "epoch 165; iter: 0; batch classifier loss: 0.163173; batch adversarial loss: 0.244082\n",
      "epoch 166; iter: 0; batch classifier loss: 0.159663; batch adversarial loss: 0.240347\n",
      "epoch 167; iter: 0; batch classifier loss: 0.167088; batch adversarial loss: 0.402536\n",
      "epoch 168; iter: 0; batch classifier loss: 0.113730; batch adversarial loss: 0.299217\n",
      "epoch 169; iter: 0; batch classifier loss: 0.235315; batch adversarial loss: 0.261103\n",
      "epoch 170; iter: 0; batch classifier loss: 0.200628; batch adversarial loss: 0.241819\n",
      "epoch 171; iter: 0; batch classifier loss: 0.150520; batch adversarial loss: 0.405249\n",
      "epoch 172; iter: 0; batch classifier loss: 0.180828; batch adversarial loss: 0.248503\n",
      "epoch 173; iter: 0; batch classifier loss: 0.200170; batch adversarial loss: 0.329631\n",
      "epoch 174; iter: 0; batch classifier loss: 0.138626; batch adversarial loss: 0.322118\n",
      "epoch 175; iter: 0; batch classifier loss: 0.223534; batch adversarial loss: 0.176641\n",
      "epoch 176; iter: 0; batch classifier loss: 0.162206; batch adversarial loss: 0.322928\n",
      "epoch 177; iter: 0; batch classifier loss: 0.226816; batch adversarial loss: 0.239030\n",
      "epoch 178; iter: 0; batch classifier loss: 0.156054; batch adversarial loss: 0.200277\n",
      "epoch 179; iter: 0; batch classifier loss: 0.186244; batch adversarial loss: 0.219532\n",
      "epoch 180; iter: 0; batch classifier loss: 0.168100; batch adversarial loss: 0.211658\n",
      "epoch 181; iter: 0; batch classifier loss: 0.190962; batch adversarial loss: 0.297948\n",
      "epoch 182; iter: 0; batch classifier loss: 0.155076; batch adversarial loss: 0.265186\n",
      "epoch 183; iter: 0; batch classifier loss: 0.136572; batch adversarial loss: 0.344338\n",
      "epoch 184; iter: 0; batch classifier loss: 0.226304; batch adversarial loss: 0.303797\n",
      "epoch 185; iter: 0; batch classifier loss: 0.252255; batch adversarial loss: 0.362425\n",
      "epoch 186; iter: 0; batch classifier loss: 0.187573; batch adversarial loss: 0.263938\n",
      "epoch 187; iter: 0; batch classifier loss: 0.139292; batch adversarial loss: 0.305437\n",
      "epoch 188; iter: 0; batch classifier loss: 0.158490; batch adversarial loss: 0.356689\n",
      "epoch 189; iter: 0; batch classifier loss: 0.153866; batch adversarial loss: 0.319550\n",
      "epoch 190; iter: 0; batch classifier loss: 0.225074; batch adversarial loss: 0.288838\n",
      "epoch 191; iter: 0; batch classifier loss: 0.249032; batch adversarial loss: 0.297265\n",
      "epoch 192; iter: 0; batch classifier loss: 0.260990; batch adversarial loss: 0.204308\n",
      "epoch 193; iter: 0; batch classifier loss: 0.212473; batch adversarial loss: 0.195228\n",
      "epoch 194; iter: 0; batch classifier loss: 0.194325; batch adversarial loss: 0.320663\n",
      "epoch 195; iter: 0; batch classifier loss: 0.136526; batch adversarial loss: 0.329847\n",
      "epoch 196; iter: 0; batch classifier loss: 0.147505; batch adversarial loss: 0.345856\n",
      "epoch 197; iter: 0; batch classifier loss: 0.135047; batch adversarial loss: 0.276452\n",
      "epoch 198; iter: 0; batch classifier loss: 0.145575; batch adversarial loss: 0.204840\n",
      "epoch 199; iter: 0; batch classifier loss: 0.179303; batch adversarial loss: 0.244430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:57:57.317834: W tensorflow/c/c_api.cc:305] Operation '{name:'39b9116a-ae25-11ee-bc15-ae7d8bf09116/39b9116a-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:7877 op device:{requested: '', assigned: ''} def:{{{node 39b9116a-ae25-11ee-bc15-ae7d8bf09116/39b9116a-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b9116a-ae25-11ee-bc15-ae7d8bf09116/39b9116a-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b9116a-ae25-11ee-bc15-ae7d8bf09116/39b9116a-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.697396; batch adversarial loss: 0.756470\n",
      "epoch 1; iter: 0; batch classifier loss: 0.311421; batch adversarial loss: 0.700916\n",
      "epoch 2; iter: 0; batch classifier loss: 0.223839; batch adversarial loss: 0.612527\n",
      "epoch 3; iter: 0; batch classifier loss: 0.185797; batch adversarial loss: 0.520961\n",
      "epoch 4; iter: 0; batch classifier loss: 0.312431; batch adversarial loss: 0.513908\n",
      "epoch 5; iter: 0; batch classifier loss: 0.230759; batch adversarial loss: 0.390237\n",
      "epoch 6; iter: 0; batch classifier loss: 0.257330; batch adversarial loss: 0.351517\n",
      "epoch 7; iter: 0; batch classifier loss: 0.138336; batch adversarial loss: 0.380084\n",
      "epoch 8; iter: 0; batch classifier loss: 0.152400; batch adversarial loss: 0.328552\n",
      "epoch 9; iter: 0; batch classifier loss: 0.100543; batch adversarial loss: 0.269600\n",
      "epoch 10; iter: 0; batch classifier loss: 0.220809; batch adversarial loss: 0.339406\n",
      "epoch 11; iter: 0; batch classifier loss: 0.212580; batch adversarial loss: 0.321348\n",
      "epoch 12; iter: 0; batch classifier loss: 0.201342; batch adversarial loss: 0.221392\n",
      "epoch 13; iter: 0; batch classifier loss: 0.255980; batch adversarial loss: 0.271901\n",
      "epoch 14; iter: 0; batch classifier loss: 0.195947; batch adversarial loss: 0.360152\n",
      "epoch 15; iter: 0; batch classifier loss: 0.283203; batch adversarial loss: 0.358082\n",
      "epoch 16; iter: 0; batch classifier loss: 0.191066; batch adversarial loss: 0.276314\n",
      "epoch 17; iter: 0; batch classifier loss: 0.289269; batch adversarial loss: 0.326055\n",
      "epoch 18; iter: 0; batch classifier loss: 0.225492; batch adversarial loss: 0.263696\n",
      "epoch 19; iter: 0; batch classifier loss: 0.276804; batch adversarial loss: 0.383175\n",
      "epoch 20; iter: 0; batch classifier loss: 0.199296; batch adversarial loss: 0.221644\n",
      "epoch 21; iter: 0; batch classifier loss: 0.250556; batch adversarial loss: 0.241005\n",
      "epoch 22; iter: 0; batch classifier loss: 0.220424; batch adversarial loss: 0.267209\n",
      "epoch 23; iter: 0; batch classifier loss: 0.234628; batch adversarial loss: 0.281284\n",
      "epoch 24; iter: 0; batch classifier loss: 0.254902; batch adversarial loss: 0.255221\n",
      "epoch 25; iter: 0; batch classifier loss: 0.207262; batch adversarial loss: 0.223789\n",
      "epoch 26; iter: 0; batch classifier loss: 0.273594; batch adversarial loss: 0.304126\n",
      "epoch 27; iter: 0; batch classifier loss: 0.158787; batch adversarial loss: 0.316701\n",
      "epoch 28; iter: 0; batch classifier loss: 0.171884; batch adversarial loss: 0.276194\n",
      "epoch 29; iter: 0; batch classifier loss: 0.273468; batch adversarial loss: 0.354818\n",
      "epoch 30; iter: 0; batch classifier loss: 0.235845; batch adversarial loss: 0.187451\n",
      "epoch 31; iter: 0; batch classifier loss: 0.215631; batch adversarial loss: 0.262191\n",
      "epoch 32; iter: 0; batch classifier loss: 0.229495; batch adversarial loss: 0.262641\n",
      "epoch 33; iter: 0; batch classifier loss: 0.250167; batch adversarial loss: 0.197567\n",
      "epoch 34; iter: 0; batch classifier loss: 0.204251; batch adversarial loss: 0.175208\n",
      "epoch 35; iter: 0; batch classifier loss: 0.196047; batch adversarial loss: 0.198528\n",
      "epoch 36; iter: 0; batch classifier loss: 0.180796; batch adversarial loss: 0.244952\n",
      "epoch 37; iter: 0; batch classifier loss: 0.352769; batch adversarial loss: 0.270778\n",
      "epoch 38; iter: 0; batch classifier loss: 0.247262; batch adversarial loss: 0.353103\n",
      "epoch 39; iter: 0; batch classifier loss: 0.228110; batch adversarial loss: 0.212036\n",
      "epoch 40; iter: 0; batch classifier loss: 0.127067; batch adversarial loss: 0.294997\n",
      "epoch 41; iter: 0; batch classifier loss: 0.182503; batch adversarial loss: 0.250726\n",
      "epoch 42; iter: 0; batch classifier loss: 0.143969; batch adversarial loss: 0.362337\n",
      "epoch 43; iter: 0; batch classifier loss: 0.216543; batch adversarial loss: 0.222317\n",
      "epoch 44; iter: 0; batch classifier loss: 0.198140; batch adversarial loss: 0.274922\n",
      "epoch 45; iter: 0; batch classifier loss: 0.222408; batch adversarial loss: 0.312598\n",
      "epoch 46; iter: 0; batch classifier loss: 0.198566; batch adversarial loss: 0.259438\n",
      "epoch 47; iter: 0; batch classifier loss: 0.166450; batch adversarial loss: 0.268681\n",
      "epoch 48; iter: 0; batch classifier loss: 0.312909; batch adversarial loss: 0.225277\n",
      "epoch 49; iter: 0; batch classifier loss: 0.179233; batch adversarial loss: 0.217578\n",
      "epoch 50; iter: 0; batch classifier loss: 0.199971; batch adversarial loss: 0.232364\n",
      "epoch 51; iter: 0; batch classifier loss: 0.190346; batch adversarial loss: 0.209174\n",
      "epoch 52; iter: 0; batch classifier loss: 0.188270; batch adversarial loss: 0.179709\n",
      "epoch 53; iter: 0; batch classifier loss: 0.241538; batch adversarial loss: 0.213374\n",
      "epoch 54; iter: 0; batch classifier loss: 0.261796; batch adversarial loss: 0.222707\n",
      "epoch 55; iter: 0; batch classifier loss: 0.236511; batch adversarial loss: 0.273044\n",
      "epoch 56; iter: 0; batch classifier loss: 0.213829; batch adversarial loss: 0.307133\n",
      "epoch 57; iter: 0; batch classifier loss: 0.223001; batch adversarial loss: 0.347082\n",
      "epoch 58; iter: 0; batch classifier loss: 0.163888; batch adversarial loss: 0.154398\n",
      "epoch 59; iter: 0; batch classifier loss: 0.255183; batch adversarial loss: 0.270860\n",
      "epoch 60; iter: 0; batch classifier loss: 0.198258; batch adversarial loss: 0.199030\n",
      "epoch 61; iter: 0; batch classifier loss: 0.278002; batch adversarial loss: 0.280261\n",
      "epoch 62; iter: 0; batch classifier loss: 0.184461; batch adversarial loss: 0.310805\n",
      "epoch 63; iter: 0; batch classifier loss: 0.257893; batch adversarial loss: 0.292888\n",
      "epoch 64; iter: 0; batch classifier loss: 0.163314; batch adversarial loss: 0.149032\n",
      "epoch 65; iter: 0; batch classifier loss: 0.226922; batch adversarial loss: 0.215194\n",
      "epoch 66; iter: 0; batch classifier loss: 0.295028; batch adversarial loss: 0.296738\n",
      "epoch 67; iter: 0; batch classifier loss: 0.228127; batch adversarial loss: 0.220366\n",
      "epoch 68; iter: 0; batch classifier loss: 0.187030; batch adversarial loss: 0.293496\n",
      "epoch 69; iter: 0; batch classifier loss: 0.244114; batch adversarial loss: 0.259722\n",
      "epoch 70; iter: 0; batch classifier loss: 0.178506; batch adversarial loss: 0.327433\n",
      "epoch 71; iter: 0; batch classifier loss: 0.177801; batch adversarial loss: 0.207085\n",
      "epoch 72; iter: 0; batch classifier loss: 0.263783; batch adversarial loss: 0.289762\n",
      "epoch 73; iter: 0; batch classifier loss: 0.115005; batch adversarial loss: 0.239007\n",
      "epoch 74; iter: 0; batch classifier loss: 0.217847; batch adversarial loss: 0.219151\n",
      "epoch 75; iter: 0; batch classifier loss: 0.177107; batch adversarial loss: 0.236080\n",
      "epoch 76; iter: 0; batch classifier loss: 0.256753; batch adversarial loss: 0.202585\n",
      "epoch 77; iter: 0; batch classifier loss: 0.235485; batch adversarial loss: 0.222220\n",
      "epoch 78; iter: 0; batch classifier loss: 0.271354; batch adversarial loss: 0.302476\n",
      "epoch 79; iter: 0; batch classifier loss: 0.101567; batch adversarial loss: 0.170550\n",
      "epoch 80; iter: 0; batch classifier loss: 0.245063; batch adversarial loss: 0.208780\n",
      "epoch 81; iter: 0; batch classifier loss: 0.210051; batch adversarial loss: 0.247375\n",
      "epoch 82; iter: 0; batch classifier loss: 0.247759; batch adversarial loss: 0.376335\n",
      "epoch 83; iter: 0; batch classifier loss: 0.252485; batch adversarial loss: 0.252304\n",
      "epoch 84; iter: 0; batch classifier loss: 0.188577; batch adversarial loss: 0.256169\n",
      "epoch 85; iter: 0; batch classifier loss: 0.171584; batch adversarial loss: 0.362124\n",
      "epoch 86; iter: 0; batch classifier loss: 0.210555; batch adversarial loss: 0.102181\n",
      "epoch 87; iter: 0; batch classifier loss: 0.210278; batch adversarial loss: 0.198814\n",
      "epoch 88; iter: 0; batch classifier loss: 0.195168; batch adversarial loss: 0.351979\n",
      "epoch 89; iter: 0; batch classifier loss: 0.209000; batch adversarial loss: 0.233389\n",
      "epoch 90; iter: 0; batch classifier loss: 0.167117; batch adversarial loss: 0.308044\n",
      "epoch 91; iter: 0; batch classifier loss: 0.239324; batch adversarial loss: 0.225252\n",
      "epoch 92; iter: 0; batch classifier loss: 0.186418; batch adversarial loss: 0.305167\n",
      "epoch 93; iter: 0; batch classifier loss: 0.209256; batch adversarial loss: 0.291562\n",
      "epoch 94; iter: 0; batch classifier loss: 0.181214; batch adversarial loss: 0.222354\n",
      "epoch 95; iter: 0; batch classifier loss: 0.207868; batch adversarial loss: 0.285741\n",
      "epoch 96; iter: 0; batch classifier loss: 0.176401; batch adversarial loss: 0.272820\n",
      "epoch 97; iter: 0; batch classifier loss: 0.169765; batch adversarial loss: 0.182882\n",
      "epoch 98; iter: 0; batch classifier loss: 0.210286; batch adversarial loss: 0.276490\n",
      "epoch 99; iter: 0; batch classifier loss: 0.290574; batch adversarial loss: 0.281215\n",
      "epoch 100; iter: 0; batch classifier loss: 0.238618; batch adversarial loss: 0.221662\n",
      "epoch 101; iter: 0; batch classifier loss: 0.204344; batch adversarial loss: 0.273780\n",
      "epoch 102; iter: 0; batch classifier loss: 0.145271; batch adversarial loss: 0.195469\n",
      "epoch 103; iter: 0; batch classifier loss: 0.207254; batch adversarial loss: 0.261936\n",
      "epoch 104; iter: 0; batch classifier loss: 0.159076; batch adversarial loss: 0.171643\n",
      "epoch 105; iter: 0; batch classifier loss: 0.162666; batch adversarial loss: 0.236070\n",
      "epoch 106; iter: 0; batch classifier loss: 0.197970; batch adversarial loss: 0.199539\n",
      "epoch 107; iter: 0; batch classifier loss: 0.296568; batch adversarial loss: 0.182973\n",
      "epoch 108; iter: 0; batch classifier loss: 0.182864; batch adversarial loss: 0.313424\n",
      "epoch 109; iter: 0; batch classifier loss: 0.184328; batch adversarial loss: 0.212319\n",
      "epoch 110; iter: 0; batch classifier loss: 0.160962; batch adversarial loss: 0.356453\n",
      "epoch 111; iter: 0; batch classifier loss: 0.201239; batch adversarial loss: 0.363366\n",
      "epoch 112; iter: 0; batch classifier loss: 0.282489; batch adversarial loss: 0.264438\n",
      "epoch 113; iter: 0; batch classifier loss: 0.209537; batch adversarial loss: 0.321531\n",
      "epoch 114; iter: 0; batch classifier loss: 0.240533; batch adversarial loss: 0.304792\n",
      "epoch 115; iter: 0; batch classifier loss: 0.274308; batch adversarial loss: 0.231114\n",
      "epoch 116; iter: 0; batch classifier loss: 0.191744; batch adversarial loss: 0.166728\n",
      "epoch 117; iter: 0; batch classifier loss: 0.111126; batch adversarial loss: 0.276913\n",
      "epoch 118; iter: 0; batch classifier loss: 0.218192; batch adversarial loss: 0.265840\n",
      "epoch 119; iter: 0; batch classifier loss: 0.149514; batch adversarial loss: 0.384798\n",
      "epoch 120; iter: 0; batch classifier loss: 0.198862; batch adversarial loss: 0.249129\n",
      "epoch 121; iter: 0; batch classifier loss: 0.217446; batch adversarial loss: 0.232754\n",
      "epoch 122; iter: 0; batch classifier loss: 0.209903; batch adversarial loss: 0.376632\n",
      "epoch 123; iter: 0; batch classifier loss: 0.182544; batch adversarial loss: 0.209103\n",
      "epoch 124; iter: 0; batch classifier loss: 0.188889; batch adversarial loss: 0.346562\n",
      "epoch 125; iter: 0; batch classifier loss: 0.196159; batch adversarial loss: 0.294932\n",
      "epoch 126; iter: 0; batch classifier loss: 0.218603; batch adversarial loss: 0.166362\n",
      "epoch 127; iter: 0; batch classifier loss: 0.197025; batch adversarial loss: 0.263326\n",
      "epoch 128; iter: 0; batch classifier loss: 0.130722; batch adversarial loss: 0.252709\n",
      "epoch 129; iter: 0; batch classifier loss: 0.156171; batch adversarial loss: 0.206830\n",
      "epoch 130; iter: 0; batch classifier loss: 0.182020; batch adversarial loss: 0.244579\n",
      "epoch 131; iter: 0; batch classifier loss: 0.226667; batch adversarial loss: 0.245048\n",
      "epoch 132; iter: 0; batch classifier loss: 0.157456; batch adversarial loss: 0.194474\n",
      "epoch 133; iter: 0; batch classifier loss: 0.178382; batch adversarial loss: 0.310738\n",
      "epoch 134; iter: 0; batch classifier loss: 0.294730; batch adversarial loss: 0.276238\n",
      "epoch 135; iter: 0; batch classifier loss: 0.301386; batch adversarial loss: 0.201941\n",
      "epoch 136; iter: 0; batch classifier loss: 0.176834; batch adversarial loss: 0.249585\n",
      "epoch 137; iter: 0; batch classifier loss: 0.248955; batch adversarial loss: 0.330275\n",
      "epoch 138; iter: 0; batch classifier loss: 0.217514; batch adversarial loss: 0.362817\n",
      "epoch 139; iter: 0; batch classifier loss: 0.185006; batch adversarial loss: 0.230782\n",
      "epoch 140; iter: 0; batch classifier loss: 0.188785; batch adversarial loss: 0.245448\n",
      "epoch 141; iter: 0; batch classifier loss: 0.187834; batch adversarial loss: 0.184049\n",
      "epoch 142; iter: 0; batch classifier loss: 0.209318; batch adversarial loss: 0.230088\n",
      "epoch 143; iter: 0; batch classifier loss: 0.190164; batch adversarial loss: 0.212339\n",
      "epoch 144; iter: 0; batch classifier loss: 0.146869; batch adversarial loss: 0.175676\n",
      "epoch 145; iter: 0; batch classifier loss: 0.216896; batch adversarial loss: 0.319033\n",
      "epoch 146; iter: 0; batch classifier loss: 0.196963; batch adversarial loss: 0.322017\n",
      "epoch 147; iter: 0; batch classifier loss: 0.199402; batch adversarial loss: 0.288846\n",
      "epoch 148; iter: 0; batch classifier loss: 0.157718; batch adversarial loss: 0.309875\n",
      "epoch 149; iter: 0; batch classifier loss: 0.148954; batch adversarial loss: 0.212588\n",
      "epoch 150; iter: 0; batch classifier loss: 0.196971; batch adversarial loss: 0.302181\n",
      "epoch 151; iter: 0; batch classifier loss: 0.255022; batch adversarial loss: 0.257960\n",
      "epoch 152; iter: 0; batch classifier loss: 0.145794; batch adversarial loss: 0.253767\n",
      "epoch 153; iter: 0; batch classifier loss: 0.148812; batch adversarial loss: 0.347644\n",
      "epoch 154; iter: 0; batch classifier loss: 0.221315; batch adversarial loss: 0.330119\n",
      "epoch 155; iter: 0; batch classifier loss: 0.238021; batch adversarial loss: 0.251983\n",
      "epoch 156; iter: 0; batch classifier loss: 0.165980; batch adversarial loss: 0.239800\n",
      "epoch 157; iter: 0; batch classifier loss: 0.160968; batch adversarial loss: 0.383258\n",
      "epoch 158; iter: 0; batch classifier loss: 0.239764; batch adversarial loss: 0.254224\n",
      "epoch 159; iter: 0; batch classifier loss: 0.170216; batch adversarial loss: 0.341549\n",
      "epoch 160; iter: 0; batch classifier loss: 0.180832; batch adversarial loss: 0.111507\n",
      "epoch 161; iter: 0; batch classifier loss: 0.114245; batch adversarial loss: 0.335512\n",
      "epoch 162; iter: 0; batch classifier loss: 0.130940; batch adversarial loss: 0.261630\n",
      "epoch 163; iter: 0; batch classifier loss: 0.219380; batch adversarial loss: 0.170380\n",
      "epoch 164; iter: 0; batch classifier loss: 0.145905; batch adversarial loss: 0.215739\n",
      "epoch 165; iter: 0; batch classifier loss: 0.287266; batch adversarial loss: 0.257880\n",
      "epoch 166; iter: 0; batch classifier loss: 0.132302; batch adversarial loss: 0.226115\n",
      "epoch 167; iter: 0; batch classifier loss: 0.199852; batch adversarial loss: 0.349950\n",
      "epoch 168; iter: 0; batch classifier loss: 0.216337; batch adversarial loss: 0.346226\n",
      "epoch 169; iter: 0; batch classifier loss: 0.247285; batch adversarial loss: 0.278390\n",
      "epoch 170; iter: 0; batch classifier loss: 0.266120; batch adversarial loss: 0.190362\n",
      "epoch 171; iter: 0; batch classifier loss: 0.257330; batch adversarial loss: 0.252479\n",
      "epoch 172; iter: 0; batch classifier loss: 0.261478; batch adversarial loss: 0.226853\n",
      "epoch 173; iter: 0; batch classifier loss: 0.168746; batch adversarial loss: 0.362016\n",
      "epoch 174; iter: 0; batch classifier loss: 0.171754; batch adversarial loss: 0.206302\n",
      "epoch 175; iter: 0; batch classifier loss: 0.175351; batch adversarial loss: 0.252663\n",
      "epoch 176; iter: 0; batch classifier loss: 0.173730; batch adversarial loss: 0.267667\n",
      "epoch 177; iter: 0; batch classifier loss: 0.206280; batch adversarial loss: 0.345156\n",
      "epoch 178; iter: 0; batch classifier loss: 0.208001; batch adversarial loss: 0.151298\n",
      "epoch 179; iter: 0; batch classifier loss: 0.293975; batch adversarial loss: 0.304096\n",
      "epoch 180; iter: 0; batch classifier loss: 0.254926; batch adversarial loss: 0.179852\n",
      "epoch 181; iter: 0; batch classifier loss: 0.184166; batch adversarial loss: 0.321076\n",
      "epoch 182; iter: 0; batch classifier loss: 0.177972; batch adversarial loss: 0.324475\n",
      "epoch 183; iter: 0; batch classifier loss: 0.195024; batch adversarial loss: 0.257670\n",
      "epoch 184; iter: 0; batch classifier loss: 0.146068; batch adversarial loss: 0.340375\n",
      "epoch 185; iter: 0; batch classifier loss: 0.222171; batch adversarial loss: 0.218661\n",
      "epoch 186; iter: 0; batch classifier loss: 0.134673; batch adversarial loss: 0.292997\n",
      "epoch 187; iter: 0; batch classifier loss: 0.165936; batch adversarial loss: 0.192345\n",
      "epoch 188; iter: 0; batch classifier loss: 0.228721; batch adversarial loss: 0.294815\n",
      "epoch 189; iter: 0; batch classifier loss: 0.170491; batch adversarial loss: 0.264373\n",
      "epoch 190; iter: 0; batch classifier loss: 0.195940; batch adversarial loss: 0.242980\n",
      "epoch 191; iter: 0; batch classifier loss: 0.195440; batch adversarial loss: 0.255107\n",
      "epoch 192; iter: 0; batch classifier loss: 0.261942; batch adversarial loss: 0.241749\n",
      "epoch 193; iter: 0; batch classifier loss: 0.220221; batch adversarial loss: 0.227566\n",
      "epoch 194; iter: 0; batch classifier loss: 0.237406; batch adversarial loss: 0.208085\n",
      "epoch 195; iter: 0; batch classifier loss: 0.124985; batch adversarial loss: 0.314060\n",
      "epoch 196; iter: 0; batch classifier loss: 0.257724; batch adversarial loss: 0.305354\n",
      "epoch 197; iter: 0; batch classifier loss: 0.222736; batch adversarial loss: 0.214580\n",
      "epoch 198; iter: 0; batch classifier loss: 0.303503; batch adversarial loss: 0.208363\n",
      "epoch 199; iter: 0; batch classifier loss: 0.175562; batch adversarial loss: 0.242450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:58:10.772325: W tensorflow/c/c_api.cc:305] Operation '{name:'39b91192-ae25-11ee-bc15-ae7d8bf09116/39b91192-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:8667 op device:{requested: '', assigned: ''} def:{{{node 39b91192-ae25-11ee-bc15-ae7d8bf09116/39b91192-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b91192-ae25-11ee-bc15-ae7d8bf09116/39b91192-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b91192-ae25-11ee-bc15-ae7d8bf09116/39b91192-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.814129; batch adversarial loss: 0.447693\n",
      "epoch 1; iter: 0; batch classifier loss: 0.772484; batch adversarial loss: 0.447326\n",
      "epoch 2; iter: 0; batch classifier loss: 1.319097; batch adversarial loss: 0.603935\n",
      "epoch 3; iter: 0; batch classifier loss: 1.608601; batch adversarial loss: 0.628673\n",
      "epoch 4; iter: 0; batch classifier loss: 1.649779; batch adversarial loss: 0.699646\n",
      "epoch 5; iter: 0; batch classifier loss: 1.878915; batch adversarial loss: 0.603967\n",
      "epoch 6; iter: 0; batch classifier loss: 1.983897; batch adversarial loss: 0.546448\n",
      "epoch 7; iter: 0; batch classifier loss: 2.152931; batch adversarial loss: 0.451718\n",
      "epoch 8; iter: 0; batch classifier loss: 2.098136; batch adversarial loss: 0.464523\n",
      "epoch 9; iter: 0; batch classifier loss: 1.997627; batch adversarial loss: 0.459779\n",
      "epoch 10; iter: 0; batch classifier loss: 1.971260; batch adversarial loss: 0.412248\n",
      "epoch 11; iter: 0; batch classifier loss: 1.990083; batch adversarial loss: 0.380400\n",
      "epoch 12; iter: 0; batch classifier loss: 1.540475; batch adversarial loss: 0.350744\n",
      "epoch 13; iter: 0; batch classifier loss: 0.924603; batch adversarial loss: 0.293513\n",
      "epoch 14; iter: 0; batch classifier loss: 0.512431; batch adversarial loss: 0.302100\n",
      "epoch 15; iter: 0; batch classifier loss: 0.292872; batch adversarial loss: 0.288314\n",
      "epoch 16; iter: 0; batch classifier loss: 0.239820; batch adversarial loss: 0.264908\n",
      "epoch 17; iter: 0; batch classifier loss: 0.322772; batch adversarial loss: 0.270040\n",
      "epoch 18; iter: 0; batch classifier loss: 0.295911; batch adversarial loss: 0.295482\n",
      "epoch 19; iter: 0; batch classifier loss: 0.228741; batch adversarial loss: 0.242722\n",
      "epoch 20; iter: 0; batch classifier loss: 0.218531; batch adversarial loss: 0.286446\n",
      "epoch 21; iter: 0; batch classifier loss: 0.245911; batch adversarial loss: 0.207288\n",
      "epoch 22; iter: 0; batch classifier loss: 0.307916; batch adversarial loss: 0.271529\n",
      "epoch 23; iter: 0; batch classifier loss: 0.255921; batch adversarial loss: 0.258617\n",
      "epoch 24; iter: 0; batch classifier loss: 0.185991; batch adversarial loss: 0.351742\n",
      "epoch 25; iter: 0; batch classifier loss: 0.183732; batch adversarial loss: 0.258809\n",
      "epoch 26; iter: 0; batch classifier loss: 0.245964; batch adversarial loss: 0.277942\n",
      "epoch 27; iter: 0; batch classifier loss: 0.228274; batch adversarial loss: 0.248557\n",
      "epoch 28; iter: 0; batch classifier loss: 0.222744; batch adversarial loss: 0.156509\n",
      "epoch 29; iter: 0; batch classifier loss: 0.299900; batch adversarial loss: 0.243446\n",
      "epoch 30; iter: 0; batch classifier loss: 0.138586; batch adversarial loss: 0.257998\n",
      "epoch 31; iter: 0; batch classifier loss: 0.226711; batch adversarial loss: 0.221967\n",
      "epoch 32; iter: 0; batch classifier loss: 0.256400; batch adversarial loss: 0.209186\n",
      "epoch 33; iter: 0; batch classifier loss: 0.298895; batch adversarial loss: 0.334091\n",
      "epoch 34; iter: 0; batch classifier loss: 0.209446; batch adversarial loss: 0.248353\n",
      "epoch 35; iter: 0; batch classifier loss: 0.273086; batch adversarial loss: 0.140420\n",
      "epoch 36; iter: 0; batch classifier loss: 0.215507; batch adversarial loss: 0.232678\n",
      "epoch 37; iter: 0; batch classifier loss: 0.190860; batch adversarial loss: 0.201013\n",
      "epoch 38; iter: 0; batch classifier loss: 0.208520; batch adversarial loss: 0.182205\n",
      "epoch 39; iter: 0; batch classifier loss: 0.242860; batch adversarial loss: 0.229642\n",
      "epoch 40; iter: 0; batch classifier loss: 0.235434; batch adversarial loss: 0.237762\n",
      "epoch 41; iter: 0; batch classifier loss: 0.233234; batch adversarial loss: 0.248875\n",
      "epoch 42; iter: 0; batch classifier loss: 0.197463; batch adversarial loss: 0.211671\n",
      "epoch 43; iter: 0; batch classifier loss: 0.145702; batch adversarial loss: 0.303471\n",
      "epoch 44; iter: 0; batch classifier loss: 0.212602; batch adversarial loss: 0.225072\n",
      "epoch 45; iter: 0; batch classifier loss: 0.231433; batch adversarial loss: 0.363827\n",
      "epoch 46; iter: 0; batch classifier loss: 0.225765; batch adversarial loss: 0.201999\n",
      "epoch 47; iter: 0; batch classifier loss: 0.204163; batch adversarial loss: 0.331750\n",
      "epoch 48; iter: 0; batch classifier loss: 0.164182; batch adversarial loss: 0.338807\n",
      "epoch 49; iter: 0; batch classifier loss: 0.239594; batch adversarial loss: 0.287619\n",
      "epoch 50; iter: 0; batch classifier loss: 0.179295; batch adversarial loss: 0.316351\n",
      "epoch 51; iter: 0; batch classifier loss: 0.202163; batch adversarial loss: 0.229090\n",
      "epoch 52; iter: 0; batch classifier loss: 0.198334; batch adversarial loss: 0.219185\n",
      "epoch 53; iter: 0; batch classifier loss: 0.167309; batch adversarial loss: 0.241828\n",
      "epoch 54; iter: 0; batch classifier loss: 0.148228; batch adversarial loss: 0.196936\n",
      "epoch 55; iter: 0; batch classifier loss: 0.194555; batch adversarial loss: 0.338070\n",
      "epoch 56; iter: 0; batch classifier loss: 0.213498; batch adversarial loss: 0.255541\n",
      "epoch 57; iter: 0; batch classifier loss: 0.165047; batch adversarial loss: 0.176230\n",
      "epoch 58; iter: 0; batch classifier loss: 0.177706; batch adversarial loss: 0.184242\n",
      "epoch 59; iter: 0; batch classifier loss: 0.192721; batch adversarial loss: 0.167233\n",
      "epoch 60; iter: 0; batch classifier loss: 0.193451; batch adversarial loss: 0.228480\n",
      "epoch 61; iter: 0; batch classifier loss: 0.249012; batch adversarial loss: 0.297394\n",
      "epoch 62; iter: 0; batch classifier loss: 0.176517; batch adversarial loss: 0.186310\n",
      "epoch 63; iter: 0; batch classifier loss: 0.182235; batch adversarial loss: 0.249843\n",
      "epoch 64; iter: 0; batch classifier loss: 0.229807; batch adversarial loss: 0.249776\n",
      "epoch 65; iter: 0; batch classifier loss: 0.246190; batch adversarial loss: 0.367590\n",
      "epoch 66; iter: 0; batch classifier loss: 0.120203; batch adversarial loss: 0.192814\n",
      "epoch 67; iter: 0; batch classifier loss: 0.200067; batch adversarial loss: 0.320785\n",
      "epoch 68; iter: 0; batch classifier loss: 0.227853; batch adversarial loss: 0.181163\n",
      "epoch 69; iter: 0; batch classifier loss: 0.194740; batch adversarial loss: 0.328890\n",
      "epoch 70; iter: 0; batch classifier loss: 0.236710; batch adversarial loss: 0.191061\n",
      "epoch 71; iter: 0; batch classifier loss: 0.156317; batch adversarial loss: 0.180594\n",
      "epoch 72; iter: 0; batch classifier loss: 0.185721; batch adversarial loss: 0.297902\n",
      "epoch 73; iter: 0; batch classifier loss: 0.194484; batch adversarial loss: 0.285743\n",
      "epoch 74; iter: 0; batch classifier loss: 0.203102; batch adversarial loss: 0.384391\n",
      "epoch 75; iter: 0; batch classifier loss: 0.235504; batch adversarial loss: 0.270581\n",
      "epoch 76; iter: 0; batch classifier loss: 0.229584; batch adversarial loss: 0.331894\n",
      "epoch 77; iter: 0; batch classifier loss: 0.136136; batch adversarial loss: 0.242995\n",
      "epoch 78; iter: 0; batch classifier loss: 0.146746; batch adversarial loss: 0.278053\n",
      "epoch 79; iter: 0; batch classifier loss: 0.288195; batch adversarial loss: 0.241006\n",
      "epoch 80; iter: 0; batch classifier loss: 0.238638; batch adversarial loss: 0.254873\n",
      "epoch 81; iter: 0; batch classifier loss: 0.170763; batch adversarial loss: 0.218207\n",
      "epoch 82; iter: 0; batch classifier loss: 0.139415; batch adversarial loss: 0.252234\n",
      "epoch 83; iter: 0; batch classifier loss: 0.164932; batch adversarial loss: 0.217822\n",
      "epoch 84; iter: 0; batch classifier loss: 0.209721; batch adversarial loss: 0.322924\n",
      "epoch 85; iter: 0; batch classifier loss: 0.188687; batch adversarial loss: 0.271412\n",
      "epoch 86; iter: 0; batch classifier loss: 0.209864; batch adversarial loss: 0.278820\n",
      "epoch 87; iter: 0; batch classifier loss: 0.248767; batch adversarial loss: 0.249083\n",
      "epoch 88; iter: 0; batch classifier loss: 0.193801; batch adversarial loss: 0.253939\n",
      "epoch 89; iter: 0; batch classifier loss: 0.204570; batch adversarial loss: 0.247845\n",
      "epoch 90; iter: 0; batch classifier loss: 0.217103; batch adversarial loss: 0.366205\n",
      "epoch 91; iter: 0; batch classifier loss: 0.175452; batch adversarial loss: 0.451349\n",
      "epoch 92; iter: 0; batch classifier loss: 0.248163; batch adversarial loss: 0.285339\n",
      "epoch 93; iter: 0; batch classifier loss: 0.241954; batch adversarial loss: 0.280276\n",
      "epoch 94; iter: 0; batch classifier loss: 0.184823; batch adversarial loss: 0.292773\n",
      "epoch 95; iter: 0; batch classifier loss: 0.235991; batch adversarial loss: 0.171132\n",
      "epoch 96; iter: 0; batch classifier loss: 0.263000; batch adversarial loss: 0.199432\n",
      "epoch 97; iter: 0; batch classifier loss: 0.272384; batch adversarial loss: 0.273538\n",
      "epoch 98; iter: 0; batch classifier loss: 0.233016; batch adversarial loss: 0.247131\n",
      "epoch 99; iter: 0; batch classifier loss: 0.225566; batch adversarial loss: 0.228372\n",
      "epoch 100; iter: 0; batch classifier loss: 0.184554; batch adversarial loss: 0.200557\n",
      "epoch 101; iter: 0; batch classifier loss: 0.242438; batch adversarial loss: 0.240729\n",
      "epoch 102; iter: 0; batch classifier loss: 0.201604; batch adversarial loss: 0.283732\n",
      "epoch 103; iter: 0; batch classifier loss: 0.218151; batch adversarial loss: 0.331805\n",
      "epoch 104; iter: 0; batch classifier loss: 0.193240; batch adversarial loss: 0.288651\n",
      "epoch 105; iter: 0; batch classifier loss: 0.214296; batch adversarial loss: 0.236982\n",
      "epoch 106; iter: 0; batch classifier loss: 0.206190; batch adversarial loss: 0.321557\n",
      "epoch 107; iter: 0; batch classifier loss: 0.151235; batch adversarial loss: 0.214354\n",
      "epoch 108; iter: 0; batch classifier loss: 0.148305; batch adversarial loss: 0.239211\n",
      "epoch 109; iter: 0; batch classifier loss: 0.221641; batch adversarial loss: 0.207874\n",
      "epoch 110; iter: 0; batch classifier loss: 0.233477; batch adversarial loss: 0.212206\n",
      "epoch 111; iter: 0; batch classifier loss: 0.231597; batch adversarial loss: 0.157879\n",
      "epoch 112; iter: 0; batch classifier loss: 0.182295; batch adversarial loss: 0.266575\n",
      "epoch 113; iter: 0; batch classifier loss: 0.157154; batch adversarial loss: 0.248130\n",
      "epoch 114; iter: 0; batch classifier loss: 0.208885; batch adversarial loss: 0.284544\n",
      "epoch 115; iter: 0; batch classifier loss: 0.174134; batch adversarial loss: 0.305588\n",
      "epoch 116; iter: 0; batch classifier loss: 0.145716; batch adversarial loss: 0.355778\n",
      "epoch 117; iter: 0; batch classifier loss: 0.164409; batch adversarial loss: 0.150203\n",
      "epoch 118; iter: 0; batch classifier loss: 0.197062; batch adversarial loss: 0.325582\n",
      "epoch 119; iter: 0; batch classifier loss: 0.216187; batch adversarial loss: 0.189821\n",
      "epoch 120; iter: 0; batch classifier loss: 0.134657; batch adversarial loss: 0.311005\n",
      "epoch 121; iter: 0; batch classifier loss: 0.256797; batch adversarial loss: 0.250846\n",
      "epoch 122; iter: 0; batch classifier loss: 0.197990; batch adversarial loss: 0.252572\n",
      "epoch 123; iter: 0; batch classifier loss: 0.159278; batch adversarial loss: 0.295589\n",
      "epoch 124; iter: 0; batch classifier loss: 0.261791; batch adversarial loss: 0.247972\n",
      "epoch 125; iter: 0; batch classifier loss: 0.209781; batch adversarial loss: 0.289829\n",
      "epoch 126; iter: 0; batch classifier loss: 0.239019; batch adversarial loss: 0.227778\n",
      "epoch 127; iter: 0; batch classifier loss: 0.208181; batch adversarial loss: 0.191791\n",
      "epoch 128; iter: 0; batch classifier loss: 0.232207; batch adversarial loss: 0.271819\n",
      "epoch 129; iter: 0; batch classifier loss: 0.181399; batch adversarial loss: 0.250441\n",
      "epoch 130; iter: 0; batch classifier loss: 0.276814; batch adversarial loss: 0.220620\n",
      "epoch 131; iter: 0; batch classifier loss: 0.196911; batch adversarial loss: 0.249989\n",
      "epoch 132; iter: 0; batch classifier loss: 0.228425; batch adversarial loss: 0.293933\n",
      "epoch 133; iter: 0; batch classifier loss: 0.316099; batch adversarial loss: 0.308746\n",
      "epoch 134; iter: 0; batch classifier loss: 0.233752; batch adversarial loss: 0.239189\n",
      "epoch 135; iter: 0; batch classifier loss: 0.196509; batch adversarial loss: 0.280112\n",
      "epoch 136; iter: 0; batch classifier loss: 0.187423; batch adversarial loss: 0.255266\n",
      "epoch 137; iter: 0; batch classifier loss: 0.188497; batch adversarial loss: 0.309616\n",
      "epoch 138; iter: 0; batch classifier loss: 0.144400; batch adversarial loss: 0.220368\n",
      "epoch 139; iter: 0; batch classifier loss: 0.178054; batch adversarial loss: 0.248771\n",
      "epoch 140; iter: 0; batch classifier loss: 0.169704; batch adversarial loss: 0.194124\n",
      "epoch 141; iter: 0; batch classifier loss: 0.172397; batch adversarial loss: 0.184009\n",
      "epoch 142; iter: 0; batch classifier loss: 0.150145; batch adversarial loss: 0.167357\n",
      "epoch 143; iter: 0; batch classifier loss: 0.196300; batch adversarial loss: 0.248640\n",
      "epoch 144; iter: 0; batch classifier loss: 0.157011; batch adversarial loss: 0.248667\n",
      "epoch 145; iter: 0; batch classifier loss: 0.249265; batch adversarial loss: 0.331565\n",
      "epoch 146; iter: 0; batch classifier loss: 0.189260; batch adversarial loss: 0.225800\n",
      "epoch 147; iter: 0; batch classifier loss: 0.191488; batch adversarial loss: 0.178225\n",
      "epoch 148; iter: 0; batch classifier loss: 0.155784; batch adversarial loss: 0.207802\n",
      "epoch 149; iter: 0; batch classifier loss: 0.181484; batch adversarial loss: 0.276835\n",
      "epoch 150; iter: 0; batch classifier loss: 0.167241; batch adversarial loss: 0.286045\n",
      "epoch 151; iter: 0; batch classifier loss: 0.187931; batch adversarial loss: 0.266175\n",
      "epoch 152; iter: 0; batch classifier loss: 0.193624; batch adversarial loss: 0.275857\n",
      "epoch 153; iter: 0; batch classifier loss: 0.144099; batch adversarial loss: 0.262140\n",
      "epoch 154; iter: 0; batch classifier loss: 0.173165; batch adversarial loss: 0.329701\n",
      "epoch 155; iter: 0; batch classifier loss: 0.198912; batch adversarial loss: 0.278163\n",
      "epoch 156; iter: 0; batch classifier loss: 0.179662; batch adversarial loss: 0.232341\n",
      "epoch 157; iter: 0; batch classifier loss: 0.204497; batch adversarial loss: 0.309653\n",
      "epoch 158; iter: 0; batch classifier loss: 0.250344; batch adversarial loss: 0.246637\n",
      "epoch 159; iter: 0; batch classifier loss: 0.125452; batch adversarial loss: 0.279669\n",
      "epoch 160; iter: 0; batch classifier loss: 0.152441; batch adversarial loss: 0.204904\n",
      "epoch 161; iter: 0; batch classifier loss: 0.271315; batch adversarial loss: 0.262664\n",
      "epoch 162; iter: 0; batch classifier loss: 0.146293; batch adversarial loss: 0.293915\n",
      "epoch 163; iter: 0; batch classifier loss: 0.195963; batch adversarial loss: 0.258466\n",
      "epoch 164; iter: 0; batch classifier loss: 0.198689; batch adversarial loss: 0.301334\n",
      "epoch 165; iter: 0; batch classifier loss: 0.223535; batch adversarial loss: 0.324230\n",
      "epoch 166; iter: 0; batch classifier loss: 0.235572; batch adversarial loss: 0.220913\n",
      "epoch 167; iter: 0; batch classifier loss: 0.315565; batch adversarial loss: 0.331497\n",
      "epoch 168; iter: 0; batch classifier loss: 0.216688; batch adversarial loss: 0.315309\n",
      "epoch 169; iter: 0; batch classifier loss: 0.170012; batch adversarial loss: 0.188671\n",
      "epoch 170; iter: 0; batch classifier loss: 0.157426; batch adversarial loss: 0.208815\n",
      "epoch 171; iter: 0; batch classifier loss: 0.203639; batch adversarial loss: 0.345231\n",
      "epoch 172; iter: 0; batch classifier loss: 0.244595; batch adversarial loss: 0.298217\n",
      "epoch 173; iter: 0; batch classifier loss: 0.188204; batch adversarial loss: 0.264605\n",
      "epoch 174; iter: 0; batch classifier loss: 0.217075; batch adversarial loss: 0.286910\n",
      "epoch 175; iter: 0; batch classifier loss: 0.147622; batch adversarial loss: 0.276566\n",
      "epoch 176; iter: 0; batch classifier loss: 0.162129; batch adversarial loss: 0.259973\n",
      "epoch 177; iter: 0; batch classifier loss: 0.237041; batch adversarial loss: 0.291919\n",
      "epoch 178; iter: 0; batch classifier loss: 0.246099; batch adversarial loss: 0.328197\n",
      "epoch 179; iter: 0; batch classifier loss: 0.180369; batch adversarial loss: 0.219980\n",
      "epoch 180; iter: 0; batch classifier loss: 0.183583; batch adversarial loss: 0.221314\n",
      "epoch 181; iter: 0; batch classifier loss: 0.193600; batch adversarial loss: 0.257413\n",
      "epoch 182; iter: 0; batch classifier loss: 0.210666; batch adversarial loss: 0.263324\n",
      "epoch 183; iter: 0; batch classifier loss: 0.202380; batch adversarial loss: 0.276803\n",
      "epoch 184; iter: 0; batch classifier loss: 0.265626; batch adversarial loss: 0.305204\n",
      "epoch 185; iter: 0; batch classifier loss: 0.147787; batch adversarial loss: 0.333098\n",
      "epoch 186; iter: 0; batch classifier loss: 0.204455; batch adversarial loss: 0.351888\n",
      "epoch 187; iter: 0; batch classifier loss: 0.224579; batch adversarial loss: 0.183722\n",
      "epoch 188; iter: 0; batch classifier loss: 0.225932; batch adversarial loss: 0.295783\n",
      "epoch 189; iter: 0; batch classifier loss: 0.211116; batch adversarial loss: 0.266472\n",
      "epoch 190; iter: 0; batch classifier loss: 0.155223; batch adversarial loss: 0.243545\n",
      "epoch 191; iter: 0; batch classifier loss: 0.140187; batch adversarial loss: 0.292079\n",
      "epoch 192; iter: 0; batch classifier loss: 0.212457; batch adversarial loss: 0.326329\n",
      "epoch 193; iter: 0; batch classifier loss: 0.089677; batch adversarial loss: 0.165702\n",
      "epoch 194; iter: 0; batch classifier loss: 0.168309; batch adversarial loss: 0.269809\n",
      "epoch 195; iter: 0; batch classifier loss: 0.099984; batch adversarial loss: 0.234674\n",
      "epoch 196; iter: 0; batch classifier loss: 0.160726; batch adversarial loss: 0.238740\n",
      "epoch 197; iter: 0; batch classifier loss: 0.189702; batch adversarial loss: 0.253338\n",
      "epoch 198; iter: 0; batch classifier loss: 0.213994; batch adversarial loss: 0.311891\n",
      "epoch 199; iter: 0; batch classifier loss: 0.240498; batch adversarial loss: 0.344758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:58:24.255564: W tensorflow/c/c_api.cc:305] Operation '{name:'39b911ba-ae25-11ee-bc15-ae7d8bf09116/39b911ba-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:9457 op device:{requested: '', assigned: ''} def:{{{node 39b911ba-ae25-11ee-bc15-ae7d8bf09116/39b911ba-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b911ba-ae25-11ee-bc15-ae7d8bf09116/39b911ba-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b911ba-ae25-11ee-bc15-ae7d8bf09116/39b911ba-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.684464; batch adversarial loss: 0.716778\n",
      "epoch 1; iter: 0; batch classifier loss: 0.275768; batch adversarial loss: 0.600629\n",
      "epoch 2; iter: 0; batch classifier loss: 0.251927; batch adversarial loss: 0.508407\n",
      "epoch 3; iter: 0; batch classifier loss: 0.275358; batch adversarial loss: 0.475878\n",
      "epoch 4; iter: 0; batch classifier loss: 0.159552; batch adversarial loss: 0.485979\n",
      "epoch 5; iter: 0; batch classifier loss: 0.160639; batch adversarial loss: 0.431710\n",
      "epoch 6; iter: 0; batch classifier loss: 0.196434; batch adversarial loss: 0.377651\n",
      "epoch 7; iter: 0; batch classifier loss: 0.267958; batch adversarial loss: 0.310015\n",
      "epoch 8; iter: 0; batch classifier loss: 0.245217; batch adversarial loss: 0.308620\n",
      "epoch 9; iter: 0; batch classifier loss: 0.235567; batch adversarial loss: 0.337892\n",
      "epoch 10; iter: 0; batch classifier loss: 0.206504; batch adversarial loss: 0.394531\n",
      "epoch 11; iter: 0; batch classifier loss: 0.205415; batch adversarial loss: 0.254961\n",
      "epoch 12; iter: 0; batch classifier loss: 0.279634; batch adversarial loss: 0.311794\n",
      "epoch 13; iter: 0; batch classifier loss: 0.177305; batch adversarial loss: 0.287247\n",
      "epoch 14; iter: 0; batch classifier loss: 0.286481; batch adversarial loss: 0.391509\n",
      "epoch 15; iter: 0; batch classifier loss: 0.245828; batch adversarial loss: 0.320494\n",
      "epoch 16; iter: 0; batch classifier loss: 0.192816; batch adversarial loss: 0.234438\n",
      "epoch 17; iter: 0; batch classifier loss: 0.308376; batch adversarial loss: 0.395177\n",
      "epoch 18; iter: 0; batch classifier loss: 0.216370; batch adversarial loss: 0.302991\n",
      "epoch 19; iter: 0; batch classifier loss: 0.194409; batch adversarial loss: 0.292979\n",
      "epoch 20; iter: 0; batch classifier loss: 0.212416; batch adversarial loss: 0.207952\n",
      "epoch 21; iter: 0; batch classifier loss: 0.179060; batch adversarial loss: 0.300835\n",
      "epoch 22; iter: 0; batch classifier loss: 0.261200; batch adversarial loss: 0.336064\n",
      "epoch 23; iter: 0; batch classifier loss: 0.434864; batch adversarial loss: 0.292062\n",
      "epoch 24; iter: 0; batch classifier loss: 0.364085; batch adversarial loss: 0.315477\n",
      "epoch 25; iter: 0; batch classifier loss: 0.240537; batch adversarial loss: 0.282225\n",
      "epoch 26; iter: 0; batch classifier loss: 0.328064; batch adversarial loss: 0.235951\n",
      "epoch 27; iter: 0; batch classifier loss: 0.244457; batch adversarial loss: 0.283412\n",
      "epoch 28; iter: 0; batch classifier loss: 0.194342; batch adversarial loss: 0.246869\n",
      "epoch 29; iter: 0; batch classifier loss: 0.259753; batch adversarial loss: 0.325850\n",
      "epoch 30; iter: 0; batch classifier loss: 0.269851; batch adversarial loss: 0.324022\n",
      "epoch 31; iter: 0; batch classifier loss: 0.279674; batch adversarial loss: 0.307760\n",
      "epoch 32; iter: 0; batch classifier loss: 0.244278; batch adversarial loss: 0.295813\n",
      "epoch 33; iter: 0; batch classifier loss: 0.216353; batch adversarial loss: 0.252256\n",
      "epoch 34; iter: 0; batch classifier loss: 0.236079; batch adversarial loss: 0.274604\n",
      "epoch 35; iter: 0; batch classifier loss: 0.187597; batch adversarial loss: 0.200167\n",
      "epoch 36; iter: 0; batch classifier loss: 0.303792; batch adversarial loss: 0.299325\n",
      "epoch 37; iter: 0; batch classifier loss: 0.132650; batch adversarial loss: 0.283197\n",
      "epoch 38; iter: 0; batch classifier loss: 0.198094; batch adversarial loss: 0.306359\n",
      "epoch 39; iter: 0; batch classifier loss: 0.163891; batch adversarial loss: 0.162238\n",
      "epoch 40; iter: 0; batch classifier loss: 0.227767; batch adversarial loss: 0.285154\n",
      "epoch 41; iter: 0; batch classifier loss: 0.236751; batch adversarial loss: 0.265826\n",
      "epoch 42; iter: 0; batch classifier loss: 0.241916; batch adversarial loss: 0.312107\n",
      "epoch 43; iter: 0; batch classifier loss: 0.226287; batch adversarial loss: 0.320124\n",
      "epoch 44; iter: 0; batch classifier loss: 0.197653; batch adversarial loss: 0.253740\n",
      "epoch 45; iter: 0; batch classifier loss: 0.193926; batch adversarial loss: 0.235337\n",
      "epoch 46; iter: 0; batch classifier loss: 0.183189; batch adversarial loss: 0.268952\n",
      "epoch 47; iter: 0; batch classifier loss: 0.234862; batch adversarial loss: 0.194346\n",
      "epoch 48; iter: 0; batch classifier loss: 0.260235; batch adversarial loss: 0.302561\n",
      "epoch 49; iter: 0; batch classifier loss: 0.169934; batch adversarial loss: 0.161021\n",
      "epoch 50; iter: 0; batch classifier loss: 0.322833; batch adversarial loss: 0.260252\n",
      "epoch 51; iter: 0; batch classifier loss: 0.235376; batch adversarial loss: 0.223238\n",
      "epoch 52; iter: 0; batch classifier loss: 0.246440; batch adversarial loss: 0.255222\n",
      "epoch 53; iter: 0; batch classifier loss: 0.168225; batch adversarial loss: 0.222919\n",
      "epoch 54; iter: 0; batch classifier loss: 0.217800; batch adversarial loss: 0.250560\n",
      "epoch 55; iter: 0; batch classifier loss: 0.248951; batch adversarial loss: 0.192816\n",
      "epoch 56; iter: 0; batch classifier loss: 0.190915; batch adversarial loss: 0.296834\n",
      "epoch 57; iter: 0; batch classifier loss: 0.256814; batch adversarial loss: 0.252924\n",
      "epoch 58; iter: 0; batch classifier loss: 0.155924; batch adversarial loss: 0.331972\n",
      "epoch 59; iter: 0; batch classifier loss: 0.147288; batch adversarial loss: 0.269806\n",
      "epoch 60; iter: 0; batch classifier loss: 0.226813; batch adversarial loss: 0.245314\n",
      "epoch 61; iter: 0; batch classifier loss: 0.176969; batch adversarial loss: 0.251049\n",
      "epoch 62; iter: 0; batch classifier loss: 0.230227; batch adversarial loss: 0.233579\n",
      "epoch 63; iter: 0; batch classifier loss: 0.153630; batch adversarial loss: 0.217992\n",
      "epoch 64; iter: 0; batch classifier loss: 0.215552; batch adversarial loss: 0.232268\n",
      "epoch 65; iter: 0; batch classifier loss: 0.168094; batch adversarial loss: 0.380555\n",
      "epoch 66; iter: 0; batch classifier loss: 0.189065; batch adversarial loss: 0.329512\n",
      "epoch 67; iter: 0; batch classifier loss: 0.196401; batch adversarial loss: 0.298271\n",
      "epoch 68; iter: 0; batch classifier loss: 0.172052; batch adversarial loss: 0.189135\n",
      "epoch 69; iter: 0; batch classifier loss: 0.256800; batch adversarial loss: 0.271335\n",
      "epoch 70; iter: 0; batch classifier loss: 0.214277; batch adversarial loss: 0.281698\n",
      "epoch 71; iter: 0; batch classifier loss: 0.244695; batch adversarial loss: 0.236723\n",
      "epoch 72; iter: 0; batch classifier loss: 0.311173; batch adversarial loss: 0.197554\n",
      "epoch 73; iter: 0; batch classifier loss: 0.218299; batch adversarial loss: 0.242625\n",
      "epoch 74; iter: 0; batch classifier loss: 0.146155; batch adversarial loss: 0.199853\n",
      "epoch 75; iter: 0; batch classifier loss: 0.294675; batch adversarial loss: 0.215063\n",
      "epoch 76; iter: 0; batch classifier loss: 0.221555; batch adversarial loss: 0.222079\n",
      "epoch 77; iter: 0; batch classifier loss: 0.188899; batch adversarial loss: 0.254324\n",
      "epoch 78; iter: 0; batch classifier loss: 0.246757; batch adversarial loss: 0.290447\n",
      "epoch 79; iter: 0; batch classifier loss: 0.194191; batch adversarial loss: 0.282347\n",
      "epoch 80; iter: 0; batch classifier loss: 0.277780; batch adversarial loss: 0.375055\n",
      "epoch 81; iter: 0; batch classifier loss: 0.180647; batch adversarial loss: 0.176936\n",
      "epoch 82; iter: 0; batch classifier loss: 0.147860; batch adversarial loss: 0.205353\n",
      "epoch 83; iter: 0; batch classifier loss: 0.311744; batch adversarial loss: 0.311687\n",
      "epoch 84; iter: 0; batch classifier loss: 0.236651; batch adversarial loss: 0.275093\n",
      "epoch 85; iter: 0; batch classifier loss: 0.138423; batch adversarial loss: 0.229256\n",
      "epoch 86; iter: 0; batch classifier loss: 0.190399; batch adversarial loss: 0.340372\n",
      "epoch 87; iter: 0; batch classifier loss: 0.147939; batch adversarial loss: 0.329376\n",
      "epoch 88; iter: 0; batch classifier loss: 0.248325; batch adversarial loss: 0.336106\n",
      "epoch 89; iter: 0; batch classifier loss: 0.140290; batch adversarial loss: 0.418134\n",
      "epoch 90; iter: 0; batch classifier loss: 0.200246; batch adversarial loss: 0.349571\n",
      "epoch 91; iter: 0; batch classifier loss: 0.231669; batch adversarial loss: 0.271461\n",
      "epoch 92; iter: 0; batch classifier loss: 0.271812; batch adversarial loss: 0.236076\n",
      "epoch 93; iter: 0; batch classifier loss: 0.176447; batch adversarial loss: 0.305401\n",
      "epoch 94; iter: 0; batch classifier loss: 0.200329; batch adversarial loss: 0.346111\n",
      "epoch 95; iter: 0; batch classifier loss: 0.221515; batch adversarial loss: 0.248354\n",
      "epoch 96; iter: 0; batch classifier loss: 0.202882; batch adversarial loss: 0.299077\n",
      "epoch 97; iter: 0; batch classifier loss: 0.298745; batch adversarial loss: 0.292148\n",
      "epoch 98; iter: 0; batch classifier loss: 0.312347; batch adversarial loss: 0.223937\n",
      "epoch 99; iter: 0; batch classifier loss: 0.215497; batch adversarial loss: 0.168326\n",
      "epoch 100; iter: 0; batch classifier loss: 0.283626; batch adversarial loss: 0.249096\n",
      "epoch 101; iter: 0; batch classifier loss: 0.193989; batch adversarial loss: 0.221607\n",
      "epoch 102; iter: 0; batch classifier loss: 0.156380; batch adversarial loss: 0.178662\n",
      "epoch 103; iter: 0; batch classifier loss: 0.262956; batch adversarial loss: 0.216484\n",
      "epoch 104; iter: 0; batch classifier loss: 0.209189; batch adversarial loss: 0.301717\n",
      "epoch 105; iter: 0; batch classifier loss: 0.136341; batch adversarial loss: 0.336950\n",
      "epoch 106; iter: 0; batch classifier loss: 0.265703; batch adversarial loss: 0.189510\n",
      "epoch 107; iter: 0; batch classifier loss: 0.185595; batch adversarial loss: 0.220644\n",
      "epoch 108; iter: 0; batch classifier loss: 0.198212; batch adversarial loss: 0.288073\n",
      "epoch 109; iter: 0; batch classifier loss: 0.146343; batch adversarial loss: 0.285791\n",
      "epoch 110; iter: 0; batch classifier loss: 0.250241; batch adversarial loss: 0.233315\n",
      "epoch 111; iter: 0; batch classifier loss: 0.188295; batch adversarial loss: 0.248212\n",
      "epoch 112; iter: 0; batch classifier loss: 0.219876; batch adversarial loss: 0.384302\n",
      "epoch 113; iter: 0; batch classifier loss: 0.251571; batch adversarial loss: 0.186727\n",
      "epoch 114; iter: 0; batch classifier loss: 0.220997; batch adversarial loss: 0.176590\n",
      "epoch 115; iter: 0; batch classifier loss: 0.217236; batch adversarial loss: 0.284838\n",
      "epoch 116; iter: 0; batch classifier loss: 0.252789; batch adversarial loss: 0.238884\n",
      "epoch 117; iter: 0; batch classifier loss: 0.194493; batch adversarial loss: 0.220881\n",
      "epoch 118; iter: 0; batch classifier loss: 0.188056; batch adversarial loss: 0.189943\n",
      "epoch 119; iter: 0; batch classifier loss: 0.213091; batch adversarial loss: 0.261348\n",
      "epoch 120; iter: 0; batch classifier loss: 0.196890; batch adversarial loss: 0.211249\n",
      "epoch 121; iter: 0; batch classifier loss: 0.222397; batch adversarial loss: 0.270114\n",
      "epoch 122; iter: 0; batch classifier loss: 0.220814; batch adversarial loss: 0.306345\n",
      "epoch 123; iter: 0; batch classifier loss: 0.186575; batch adversarial loss: 0.240583\n",
      "epoch 124; iter: 0; batch classifier loss: 0.162705; batch adversarial loss: 0.286304\n",
      "epoch 125; iter: 0; batch classifier loss: 0.200013; batch adversarial loss: 0.302301\n",
      "epoch 126; iter: 0; batch classifier loss: 0.189818; batch adversarial loss: 0.147079\n",
      "epoch 127; iter: 0; batch classifier loss: 0.174243; batch adversarial loss: 0.332504\n",
      "epoch 128; iter: 0; batch classifier loss: 0.225211; batch adversarial loss: 0.184031\n",
      "epoch 129; iter: 0; batch classifier loss: 0.164679; batch adversarial loss: 0.206342\n",
      "epoch 130; iter: 0; batch classifier loss: 0.175080; batch adversarial loss: 0.280800\n",
      "epoch 131; iter: 0; batch classifier loss: 0.275924; batch adversarial loss: 0.301567\n",
      "epoch 132; iter: 0; batch classifier loss: 0.236404; batch adversarial loss: 0.326336\n",
      "epoch 133; iter: 0; batch classifier loss: 0.263727; batch adversarial loss: 0.366714\n",
      "epoch 134; iter: 0; batch classifier loss: 0.198111; batch adversarial loss: 0.280684\n",
      "epoch 135; iter: 0; batch classifier loss: 0.176976; batch adversarial loss: 0.299025\n",
      "epoch 136; iter: 0; batch classifier loss: 0.116941; batch adversarial loss: 0.210536\n",
      "epoch 137; iter: 0; batch classifier loss: 0.201306; batch adversarial loss: 0.233675\n",
      "epoch 138; iter: 0; batch classifier loss: 0.177717; batch adversarial loss: 0.223233\n",
      "epoch 139; iter: 0; batch classifier loss: 0.191408; batch adversarial loss: 0.286001\n",
      "epoch 140; iter: 0; batch classifier loss: 0.280651; batch adversarial loss: 0.240967\n",
      "epoch 141; iter: 0; batch classifier loss: 0.160427; batch adversarial loss: 0.281122\n",
      "epoch 142; iter: 0; batch classifier loss: 0.219639; batch adversarial loss: 0.353045\n",
      "epoch 143; iter: 0; batch classifier loss: 0.170333; batch adversarial loss: 0.250139\n",
      "epoch 144; iter: 0; batch classifier loss: 0.273605; batch adversarial loss: 0.325286\n",
      "epoch 145; iter: 0; batch classifier loss: 0.157456; batch adversarial loss: 0.250846\n",
      "epoch 146; iter: 0; batch classifier loss: 0.135124; batch adversarial loss: 0.256027\n",
      "epoch 147; iter: 0; batch classifier loss: 0.264364; batch adversarial loss: 0.366753\n",
      "epoch 148; iter: 0; batch classifier loss: 0.297875; batch adversarial loss: 0.313333\n",
      "epoch 149; iter: 0; batch classifier loss: 0.314745; batch adversarial loss: 0.194576\n",
      "epoch 150; iter: 0; batch classifier loss: 0.240355; batch adversarial loss: 0.339631\n",
      "epoch 151; iter: 0; batch classifier loss: 0.286267; batch adversarial loss: 0.162892\n",
      "epoch 152; iter: 0; batch classifier loss: 0.150387; batch adversarial loss: 0.361558\n",
      "epoch 153; iter: 0; batch classifier loss: 0.196970; batch adversarial loss: 0.233417\n",
      "epoch 154; iter: 0; batch classifier loss: 0.262226; batch adversarial loss: 0.305429\n",
      "epoch 155; iter: 0; batch classifier loss: 0.187278; batch adversarial loss: 0.368356\n",
      "epoch 156; iter: 0; batch classifier loss: 0.215803; batch adversarial loss: 0.402852\n",
      "epoch 157; iter: 0; batch classifier loss: 0.287380; batch adversarial loss: 0.236390\n",
      "epoch 158; iter: 0; batch classifier loss: 0.214933; batch adversarial loss: 0.258822\n",
      "epoch 159; iter: 0; batch classifier loss: 0.190709; batch adversarial loss: 0.245148\n",
      "epoch 160; iter: 0; batch classifier loss: 0.182242; batch adversarial loss: 0.106197\n",
      "epoch 161; iter: 0; batch classifier loss: 0.173746; batch adversarial loss: 0.208248\n",
      "epoch 162; iter: 0; batch classifier loss: 0.172363; batch adversarial loss: 0.264563\n",
      "epoch 163; iter: 0; batch classifier loss: 0.234214; batch adversarial loss: 0.326973\n",
      "epoch 164; iter: 0; batch classifier loss: 0.155960; batch adversarial loss: 0.273932\n",
      "epoch 165; iter: 0; batch classifier loss: 0.154927; batch adversarial loss: 0.204764\n",
      "epoch 166; iter: 0; batch classifier loss: 0.135584; batch adversarial loss: 0.245194\n",
      "epoch 167; iter: 0; batch classifier loss: 0.197290; batch adversarial loss: 0.305126\n",
      "epoch 168; iter: 0; batch classifier loss: 0.195829; batch adversarial loss: 0.215004\n",
      "epoch 169; iter: 0; batch classifier loss: 0.144589; batch adversarial loss: 0.289031\n",
      "epoch 170; iter: 0; batch classifier loss: 0.193817; batch adversarial loss: 0.206246\n",
      "epoch 171; iter: 0; batch classifier loss: 0.185946; batch adversarial loss: 0.189423\n",
      "epoch 172; iter: 0; batch classifier loss: 0.185085; batch adversarial loss: 0.249943\n",
      "epoch 173; iter: 0; batch classifier loss: 0.144416; batch adversarial loss: 0.194319\n",
      "epoch 174; iter: 0; batch classifier loss: 0.263764; batch adversarial loss: 0.269947\n",
      "epoch 175; iter: 0; batch classifier loss: 0.134669; batch adversarial loss: 0.254182\n",
      "epoch 176; iter: 0; batch classifier loss: 0.284505; batch adversarial loss: 0.337679\n",
      "epoch 177; iter: 0; batch classifier loss: 0.231460; batch adversarial loss: 0.313034\n",
      "epoch 178; iter: 0; batch classifier loss: 0.182707; batch adversarial loss: 0.294175\n",
      "epoch 179; iter: 0; batch classifier loss: 0.181160; batch adversarial loss: 0.208274\n",
      "epoch 180; iter: 0; batch classifier loss: 0.128385; batch adversarial loss: 0.211461\n",
      "epoch 181; iter: 0; batch classifier loss: 0.259051; batch adversarial loss: 0.235808\n",
      "epoch 182; iter: 0; batch classifier loss: 0.192473; batch adversarial loss: 0.199494\n",
      "epoch 183; iter: 0; batch classifier loss: 0.204422; batch adversarial loss: 0.277393\n",
      "epoch 184; iter: 0; batch classifier loss: 0.224022; batch adversarial loss: 0.312767\n",
      "epoch 185; iter: 0; batch classifier loss: 0.243952; batch adversarial loss: 0.249900\n",
      "epoch 186; iter: 0; batch classifier loss: 0.238819; batch adversarial loss: 0.224534\n",
      "epoch 187; iter: 0; batch classifier loss: 0.201954; batch adversarial loss: 0.249996\n",
      "epoch 188; iter: 0; batch classifier loss: 0.173263; batch adversarial loss: 0.285491\n",
      "epoch 189; iter: 0; batch classifier loss: 0.140323; batch adversarial loss: 0.212788\n",
      "epoch 190; iter: 0; batch classifier loss: 0.190679; batch adversarial loss: 0.274375\n",
      "epoch 191; iter: 0; batch classifier loss: 0.217817; batch adversarial loss: 0.238684\n",
      "epoch 192; iter: 0; batch classifier loss: 0.181919; batch adversarial loss: 0.270927\n",
      "epoch 193; iter: 0; batch classifier loss: 0.282690; batch adversarial loss: 0.365628\n",
      "epoch 194; iter: 0; batch classifier loss: 0.244977; batch adversarial loss: 0.261930\n",
      "epoch 195; iter: 0; batch classifier loss: 0.229468; batch adversarial loss: 0.209676\n",
      "epoch 196; iter: 0; batch classifier loss: 0.174886; batch adversarial loss: 0.208075\n",
      "epoch 197; iter: 0; batch classifier loss: 0.135907; batch adversarial loss: 0.221990\n",
      "epoch 198; iter: 0; batch classifier loss: 0.176506; batch adversarial loss: 0.190588\n",
      "epoch 199; iter: 0; batch classifier loss: 0.139124; batch adversarial loss: 0.366943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:58:37.836939: W tensorflow/c/c_api.cc:305] Operation '{name:'39b911ec-ae25-11ee-bc15-ae7d8bf09116/39b911ec-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:10247 op device:{requested: '', assigned: ''} def:{{{node 39b911ec-ae25-11ee-bc15-ae7d8bf09116/39b911ec-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b911ec-ae25-11ee-bc15-ae7d8bf09116/39b911ec-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b911ec-ae25-11ee-bc15-ae7d8bf09116/39b911ec-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.704424; batch adversarial loss: 0.484644\n",
      "epoch 1; iter: 0; batch classifier loss: 0.551676; batch adversarial loss: 0.485105\n",
      "epoch 2; iter: 0; batch classifier loss: 0.630739; batch adversarial loss: 0.423399\n",
      "epoch 3; iter: 0; batch classifier loss: 1.358180; batch adversarial loss: 0.572489\n",
      "epoch 4; iter: 0; batch classifier loss: 1.782248; batch adversarial loss: 0.522820\n",
      "epoch 5; iter: 0; batch classifier loss: 1.844219; batch adversarial loss: 0.570902\n",
      "epoch 6; iter: 0; batch classifier loss: 2.073714; batch adversarial loss: 0.520229\n",
      "epoch 7; iter: 0; batch classifier loss: 2.109586; batch adversarial loss: 0.486087\n",
      "epoch 8; iter: 0; batch classifier loss: 2.197402; batch adversarial loss: 0.475181\n",
      "epoch 9; iter: 0; batch classifier loss: 2.131800; batch adversarial loss: 0.431091\n",
      "epoch 10; iter: 0; batch classifier loss: 1.355068; batch adversarial loss: 0.387681\n",
      "epoch 11; iter: 0; batch classifier loss: 0.997655; batch adversarial loss: 0.361196\n",
      "epoch 12; iter: 0; batch classifier loss: 0.743120; batch adversarial loss: 0.391841\n",
      "epoch 13; iter: 0; batch classifier loss: 0.403875; batch adversarial loss: 0.331730\n",
      "epoch 14; iter: 0; batch classifier loss: 0.325896; batch adversarial loss: 0.282207\n",
      "epoch 15; iter: 0; batch classifier loss: 0.242084; batch adversarial loss: 0.212112\n",
      "epoch 16; iter: 0; batch classifier loss: 0.310938; batch adversarial loss: 0.247673\n",
      "epoch 17; iter: 0; batch classifier loss: 0.163506; batch adversarial loss: 0.285440\n",
      "epoch 18; iter: 0; batch classifier loss: 0.277726; batch adversarial loss: 0.280475\n",
      "epoch 19; iter: 0; batch classifier loss: 0.186329; batch adversarial loss: 0.270855\n",
      "epoch 20; iter: 0; batch classifier loss: 0.239308; batch adversarial loss: 0.276929\n",
      "epoch 21; iter: 0; batch classifier loss: 0.273060; batch adversarial loss: 0.320381\n",
      "epoch 22; iter: 0; batch classifier loss: 0.181047; batch adversarial loss: 0.324338\n",
      "epoch 23; iter: 0; batch classifier loss: 0.220314; batch adversarial loss: 0.212614\n",
      "epoch 24; iter: 0; batch classifier loss: 0.331374; batch adversarial loss: 0.398989\n",
      "epoch 25; iter: 0; batch classifier loss: 0.202544; batch adversarial loss: 0.194562\n",
      "epoch 26; iter: 0; batch classifier loss: 0.207707; batch adversarial loss: 0.221005\n",
      "epoch 27; iter: 0; batch classifier loss: 0.175524; batch adversarial loss: 0.264479\n",
      "epoch 28; iter: 0; batch classifier loss: 0.304127; batch adversarial loss: 0.289936\n",
      "epoch 29; iter: 0; batch classifier loss: 0.241967; batch adversarial loss: 0.247354\n",
      "epoch 30; iter: 0; batch classifier loss: 0.248731; batch adversarial loss: 0.291391\n",
      "epoch 31; iter: 0; batch classifier loss: 0.197835; batch adversarial loss: 0.226287\n",
      "epoch 32; iter: 0; batch classifier loss: 0.202495; batch adversarial loss: 0.163600\n",
      "epoch 33; iter: 0; batch classifier loss: 0.258644; batch adversarial loss: 0.255839\n",
      "epoch 34; iter: 0; batch classifier loss: 0.242063; batch adversarial loss: 0.374055\n",
      "epoch 35; iter: 0; batch classifier loss: 0.257642; batch adversarial loss: 0.361071\n",
      "epoch 36; iter: 0; batch classifier loss: 0.236401; batch adversarial loss: 0.240492\n",
      "epoch 37; iter: 0; batch classifier loss: 0.238347; batch adversarial loss: 0.305484\n",
      "epoch 38; iter: 0; batch classifier loss: 0.131737; batch adversarial loss: 0.205205\n",
      "epoch 39; iter: 0; batch classifier loss: 0.278729; batch adversarial loss: 0.359558\n",
      "epoch 40; iter: 0; batch classifier loss: 0.278490; batch adversarial loss: 0.225114\n",
      "epoch 41; iter: 0; batch classifier loss: 0.171683; batch adversarial loss: 0.229128\n",
      "epoch 42; iter: 0; batch classifier loss: 0.190353; batch adversarial loss: 0.310044\n",
      "epoch 43; iter: 0; batch classifier loss: 0.222391; batch adversarial loss: 0.238137\n",
      "epoch 44; iter: 0; batch classifier loss: 0.257205; batch adversarial loss: 0.203178\n",
      "epoch 45; iter: 0; batch classifier loss: 0.201024; batch adversarial loss: 0.292905\n",
      "epoch 46; iter: 0; batch classifier loss: 0.234428; batch adversarial loss: 0.322513\n",
      "epoch 47; iter: 0; batch classifier loss: 0.199045; batch adversarial loss: 0.257989\n",
      "epoch 48; iter: 0; batch classifier loss: 0.202218; batch adversarial loss: 0.365308\n",
      "epoch 49; iter: 0; batch classifier loss: 0.175449; batch adversarial loss: 0.201889\n",
      "epoch 50; iter: 0; batch classifier loss: 0.222635; batch adversarial loss: 0.226344\n",
      "epoch 51; iter: 0; batch classifier loss: 0.214920; batch adversarial loss: 0.300758\n",
      "epoch 52; iter: 0; batch classifier loss: 0.234784; batch adversarial loss: 0.280629\n",
      "epoch 53; iter: 0; batch classifier loss: 0.209486; batch adversarial loss: 0.335786\n",
      "epoch 54; iter: 0; batch classifier loss: 0.253569; batch adversarial loss: 0.189994\n",
      "epoch 55; iter: 0; batch classifier loss: 0.176175; batch adversarial loss: 0.174992\n",
      "epoch 56; iter: 0; batch classifier loss: 0.187131; batch adversarial loss: 0.337230\n",
      "epoch 57; iter: 0; batch classifier loss: 0.380272; batch adversarial loss: 0.216301\n",
      "epoch 58; iter: 0; batch classifier loss: 0.263818; batch adversarial loss: 0.277759\n",
      "epoch 59; iter: 0; batch classifier loss: 0.232954; batch adversarial loss: 0.474399\n",
      "epoch 60; iter: 0; batch classifier loss: 0.193915; batch adversarial loss: 0.220322\n",
      "epoch 61; iter: 0; batch classifier loss: 0.187434; batch adversarial loss: 0.279872\n",
      "epoch 62; iter: 0; batch classifier loss: 0.257841; batch adversarial loss: 0.272253\n",
      "epoch 63; iter: 0; batch classifier loss: 0.225016; batch adversarial loss: 0.219151\n",
      "epoch 64; iter: 0; batch classifier loss: 0.215770; batch adversarial loss: 0.221859\n",
      "epoch 65; iter: 0; batch classifier loss: 0.267241; batch adversarial loss: 0.259278\n",
      "epoch 66; iter: 0; batch classifier loss: 0.228658; batch adversarial loss: 0.267788\n",
      "epoch 67; iter: 0; batch classifier loss: 0.208988; batch adversarial loss: 0.324556\n",
      "epoch 68; iter: 0; batch classifier loss: 0.177070; batch adversarial loss: 0.227985\n",
      "epoch 69; iter: 0; batch classifier loss: 0.201364; batch adversarial loss: 0.256974\n",
      "epoch 70; iter: 0; batch classifier loss: 0.293229; batch adversarial loss: 0.315293\n",
      "epoch 71; iter: 0; batch classifier loss: 0.241944; batch adversarial loss: 0.259863\n",
      "epoch 72; iter: 0; batch classifier loss: 0.254961; batch adversarial loss: 0.268468\n",
      "epoch 73; iter: 0; batch classifier loss: 0.214595; batch adversarial loss: 0.210387\n",
      "epoch 74; iter: 0; batch classifier loss: 0.214793; batch adversarial loss: 0.262357\n",
      "epoch 75; iter: 0; batch classifier loss: 0.238614; batch adversarial loss: 0.283660\n",
      "epoch 76; iter: 0; batch classifier loss: 0.142892; batch adversarial loss: 0.251935\n",
      "epoch 77; iter: 0; batch classifier loss: 0.286899; batch adversarial loss: 0.275443\n",
      "epoch 78; iter: 0; batch classifier loss: 0.275887; batch adversarial loss: 0.325900\n",
      "epoch 79; iter: 0; batch classifier loss: 0.229157; batch adversarial loss: 0.272463\n",
      "epoch 80; iter: 0; batch classifier loss: 0.213569; batch adversarial loss: 0.222530\n",
      "epoch 81; iter: 0; batch classifier loss: 0.276202; batch adversarial loss: 0.186144\n",
      "epoch 82; iter: 0; batch classifier loss: 0.192938; batch adversarial loss: 0.263257\n",
      "epoch 83; iter: 0; batch classifier loss: 0.230888; batch adversarial loss: 0.257647\n",
      "epoch 84; iter: 0; batch classifier loss: 0.236903; batch adversarial loss: 0.207257\n",
      "epoch 85; iter: 0; batch classifier loss: 0.189906; batch adversarial loss: 0.267864\n",
      "epoch 86; iter: 0; batch classifier loss: 0.187593; batch adversarial loss: 0.212474\n",
      "epoch 87; iter: 0; batch classifier loss: 0.204959; batch adversarial loss: 0.206034\n",
      "epoch 88; iter: 0; batch classifier loss: 0.143989; batch adversarial loss: 0.161577\n",
      "epoch 89; iter: 0; batch classifier loss: 0.231508; batch adversarial loss: 0.234092\n",
      "epoch 90; iter: 0; batch classifier loss: 0.208780; batch adversarial loss: 0.235469\n",
      "epoch 91; iter: 0; batch classifier loss: 0.216557; batch adversarial loss: 0.278512\n",
      "epoch 92; iter: 0; batch classifier loss: 0.165645; batch adversarial loss: 0.279784\n",
      "epoch 93; iter: 0; batch classifier loss: 0.257009; batch adversarial loss: 0.244270\n",
      "epoch 94; iter: 0; batch classifier loss: 0.202194; batch adversarial loss: 0.197194\n",
      "epoch 95; iter: 0; batch classifier loss: 0.185730; batch adversarial loss: 0.191727\n",
      "epoch 96; iter: 0; batch classifier loss: 0.272130; batch adversarial loss: 0.269676\n",
      "epoch 97; iter: 0; batch classifier loss: 0.199683; batch adversarial loss: 0.221681\n",
      "epoch 98; iter: 0; batch classifier loss: 0.205888; batch adversarial loss: 0.170594\n",
      "epoch 99; iter: 0; batch classifier loss: 0.196426; batch adversarial loss: 0.253049\n",
      "epoch 100; iter: 0; batch classifier loss: 0.268106; batch adversarial loss: 0.226224\n",
      "epoch 101; iter: 0; batch classifier loss: 0.170845; batch adversarial loss: 0.318401\n",
      "epoch 102; iter: 0; batch classifier loss: 0.221355; batch adversarial loss: 0.338113\n",
      "epoch 103; iter: 0; batch classifier loss: 0.225780; batch adversarial loss: 0.221856\n",
      "epoch 104; iter: 0; batch classifier loss: 0.203322; batch adversarial loss: 0.229742\n",
      "epoch 105; iter: 0; batch classifier loss: 0.140170; batch adversarial loss: 0.302022\n",
      "epoch 106; iter: 0; batch classifier loss: 0.179642; batch adversarial loss: 0.278702\n",
      "epoch 107; iter: 0; batch classifier loss: 0.191240; batch adversarial loss: 0.202462\n",
      "epoch 108; iter: 0; batch classifier loss: 0.128630; batch adversarial loss: 0.256333\n",
      "epoch 109; iter: 0; batch classifier loss: 0.165915; batch adversarial loss: 0.208578\n",
      "epoch 110; iter: 0; batch classifier loss: 0.242417; batch adversarial loss: 0.297181\n",
      "epoch 111; iter: 0; batch classifier loss: 0.168834; batch adversarial loss: 0.214625\n",
      "epoch 112; iter: 0; batch classifier loss: 0.197005; batch adversarial loss: 0.212722\n",
      "epoch 113; iter: 0; batch classifier loss: 0.140429; batch adversarial loss: 0.171804\n",
      "epoch 114; iter: 0; batch classifier loss: 0.178251; batch adversarial loss: 0.260484\n",
      "epoch 115; iter: 0; batch classifier loss: 0.222221; batch adversarial loss: 0.190932\n",
      "epoch 116; iter: 0; batch classifier loss: 0.171685; batch adversarial loss: 0.228166\n",
      "epoch 117; iter: 0; batch classifier loss: 0.214896; batch adversarial loss: 0.180085\n",
      "epoch 118; iter: 0; batch classifier loss: 0.190959; batch adversarial loss: 0.367803\n",
      "epoch 119; iter: 0; batch classifier loss: 0.222041; batch adversarial loss: 0.348501\n",
      "epoch 120; iter: 0; batch classifier loss: 0.244550; batch adversarial loss: 0.266927\n",
      "epoch 121; iter: 0; batch classifier loss: 0.209637; batch adversarial loss: 0.194017\n",
      "epoch 122; iter: 0; batch classifier loss: 0.181906; batch adversarial loss: 0.312253\n",
      "epoch 123; iter: 0; batch classifier loss: 0.115786; batch adversarial loss: 0.283800\n",
      "epoch 124; iter: 0; batch classifier loss: 0.274356; batch adversarial loss: 0.244771\n",
      "epoch 125; iter: 0; batch classifier loss: 0.235827; batch adversarial loss: 0.166212\n",
      "epoch 126; iter: 0; batch classifier loss: 0.264663; batch adversarial loss: 0.284903\n",
      "epoch 127; iter: 0; batch classifier loss: 0.226691; batch adversarial loss: 0.203887\n",
      "epoch 128; iter: 0; batch classifier loss: 0.218219; batch adversarial loss: 0.249733\n",
      "epoch 129; iter: 0; batch classifier loss: 0.189069; batch adversarial loss: 0.290672\n",
      "epoch 130; iter: 0; batch classifier loss: 0.190438; batch adversarial loss: 0.225118\n",
      "epoch 131; iter: 0; batch classifier loss: 0.158638; batch adversarial loss: 0.265781\n",
      "epoch 132; iter: 0; batch classifier loss: 0.203412; batch adversarial loss: 0.286022\n",
      "epoch 133; iter: 0; batch classifier loss: 0.162710; batch adversarial loss: 0.317422\n",
      "epoch 134; iter: 0; batch classifier loss: 0.297604; batch adversarial loss: 0.298526\n",
      "epoch 135; iter: 0; batch classifier loss: 0.236702; batch adversarial loss: 0.326163\n",
      "epoch 136; iter: 0; batch classifier loss: 0.194735; batch adversarial loss: 0.396337\n",
      "epoch 137; iter: 0; batch classifier loss: 0.224569; batch adversarial loss: 0.244877\n",
      "epoch 138; iter: 0; batch classifier loss: 0.172351; batch adversarial loss: 0.291977\n",
      "epoch 139; iter: 0; batch classifier loss: 0.229735; batch adversarial loss: 0.179798\n",
      "epoch 140; iter: 0; batch classifier loss: 0.238983; batch adversarial loss: 0.234316\n",
      "epoch 141; iter: 0; batch classifier loss: 0.268431; batch adversarial loss: 0.364562\n",
      "epoch 142; iter: 0; batch classifier loss: 0.184745; batch adversarial loss: 0.294682\n",
      "epoch 143; iter: 0; batch classifier loss: 0.221102; batch adversarial loss: 0.293230\n",
      "epoch 144; iter: 0; batch classifier loss: 0.324146; batch adversarial loss: 0.269013\n",
      "epoch 145; iter: 0; batch classifier loss: 0.184804; batch adversarial loss: 0.278473\n",
      "epoch 146; iter: 0; batch classifier loss: 0.156943; batch adversarial loss: 0.171914\n",
      "epoch 147; iter: 0; batch classifier loss: 0.160446; batch adversarial loss: 0.292504\n",
      "epoch 148; iter: 0; batch classifier loss: 0.135594; batch adversarial loss: 0.244982\n",
      "epoch 149; iter: 0; batch classifier loss: 0.208393; batch adversarial loss: 0.284512\n",
      "epoch 150; iter: 0; batch classifier loss: 0.193222; batch adversarial loss: 0.205299\n",
      "epoch 151; iter: 0; batch classifier loss: 0.208078; batch adversarial loss: 0.239492\n",
      "epoch 152; iter: 0; batch classifier loss: 0.238869; batch adversarial loss: 0.272692\n",
      "epoch 153; iter: 0; batch classifier loss: 0.222825; batch adversarial loss: 0.280452\n",
      "epoch 154; iter: 0; batch classifier loss: 0.208902; batch adversarial loss: 0.249687\n",
      "epoch 155; iter: 0; batch classifier loss: 0.216046; batch adversarial loss: 0.223836\n",
      "epoch 156; iter: 0; batch classifier loss: 0.150181; batch adversarial loss: 0.194002\n",
      "epoch 157; iter: 0; batch classifier loss: 0.192309; batch adversarial loss: 0.234869\n",
      "epoch 158; iter: 0; batch classifier loss: 0.252531; batch adversarial loss: 0.340096\n",
      "epoch 159; iter: 0; batch classifier loss: 0.244965; batch adversarial loss: 0.220916\n",
      "epoch 160; iter: 0; batch classifier loss: 0.192413; batch adversarial loss: 0.304393\n",
      "epoch 161; iter: 0; batch classifier loss: 0.175651; batch adversarial loss: 0.251391\n",
      "epoch 162; iter: 0; batch classifier loss: 0.164957; batch adversarial loss: 0.220613\n",
      "epoch 163; iter: 0; batch classifier loss: 0.201433; batch adversarial loss: 0.275879\n",
      "epoch 164; iter: 0; batch classifier loss: 0.192930; batch adversarial loss: 0.263584\n",
      "epoch 165; iter: 0; batch classifier loss: 0.302631; batch adversarial loss: 0.404260\n",
      "epoch 166; iter: 0; batch classifier loss: 0.186177; batch adversarial loss: 0.238224\n",
      "epoch 167; iter: 0; batch classifier loss: 0.206776; batch adversarial loss: 0.220303\n",
      "epoch 168; iter: 0; batch classifier loss: 0.177102; batch adversarial loss: 0.205503\n",
      "epoch 169; iter: 0; batch classifier loss: 0.129279; batch adversarial loss: 0.220856\n",
      "epoch 170; iter: 0; batch classifier loss: 0.187638; batch adversarial loss: 0.336407\n",
      "epoch 171; iter: 0; batch classifier loss: 0.230685; batch adversarial loss: 0.284106\n",
      "epoch 172; iter: 0; batch classifier loss: 0.277744; batch adversarial loss: 0.361206\n",
      "epoch 173; iter: 0; batch classifier loss: 0.182407; batch adversarial loss: 0.373166\n",
      "epoch 174; iter: 0; batch classifier loss: 0.171118; batch adversarial loss: 0.298655\n",
      "epoch 175; iter: 0; batch classifier loss: 0.187556; batch adversarial loss: 0.327313\n",
      "epoch 176; iter: 0; batch classifier loss: 0.186981; batch adversarial loss: 0.283194\n",
      "epoch 177; iter: 0; batch classifier loss: 0.200648; batch adversarial loss: 0.255605\n",
      "epoch 178; iter: 0; batch classifier loss: 0.245349; batch adversarial loss: 0.296430\n",
      "epoch 179; iter: 0; batch classifier loss: 0.176469; batch adversarial loss: 0.192070\n",
      "epoch 180; iter: 0; batch classifier loss: 0.145669; batch adversarial loss: 0.206287\n",
      "epoch 181; iter: 0; batch classifier loss: 0.220142; batch adversarial loss: 0.307398\n",
      "epoch 182; iter: 0; batch classifier loss: 0.179904; batch adversarial loss: 0.360608\n",
      "epoch 183; iter: 0; batch classifier loss: 0.212442; batch adversarial loss: 0.216673\n",
      "epoch 184; iter: 0; batch classifier loss: 0.196163; batch adversarial loss: 0.216360\n",
      "epoch 185; iter: 0; batch classifier loss: 0.230568; batch adversarial loss: 0.293938\n",
      "epoch 186; iter: 0; batch classifier loss: 0.206317; batch adversarial loss: 0.301747\n",
      "epoch 187; iter: 0; batch classifier loss: 0.223952; batch adversarial loss: 0.252307\n",
      "epoch 188; iter: 0; batch classifier loss: 0.222475; batch adversarial loss: 0.222503\n",
      "epoch 189; iter: 0; batch classifier loss: 0.200899; batch adversarial loss: 0.402530\n",
      "epoch 190; iter: 0; batch classifier loss: 0.205518; batch adversarial loss: 0.295855\n",
      "epoch 191; iter: 0; batch classifier loss: 0.193258; batch adversarial loss: 0.246778\n",
      "epoch 192; iter: 0; batch classifier loss: 0.144931; batch adversarial loss: 0.382858\n",
      "epoch 193; iter: 0; batch classifier loss: 0.158972; batch adversarial loss: 0.351248\n",
      "epoch 194; iter: 0; batch classifier loss: 0.253658; batch adversarial loss: 0.167605\n",
      "epoch 195; iter: 0; batch classifier loss: 0.218452; batch adversarial loss: 0.220044\n",
      "epoch 196; iter: 0; batch classifier loss: 0.164277; batch adversarial loss: 0.306511\n",
      "epoch 197; iter: 0; batch classifier loss: 0.144234; batch adversarial loss: 0.184806\n",
      "epoch 198; iter: 0; batch classifier loss: 0.221286; batch adversarial loss: 0.315070\n",
      "epoch 199; iter: 0; batch classifier loss: 0.249383; batch adversarial loss: 0.256651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:58:51.595829: W tensorflow/c/c_api.cc:305] Operation '{name:'39b91214-ae25-11ee-bc15-ae7d8bf09116/39b91214-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:11037 op device:{requested: '', assigned: ''} def:{{{node 39b91214-ae25-11ee-bc15-ae7d8bf09116/39b91214-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b91214-ae25-11ee-bc15-ae7d8bf09116/39b91214-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b91214-ae25-11ee-bc15-ae7d8bf09116/39b91214-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.798951; batch adversarial loss: 0.784461\n",
      "epoch 1; iter: 0; batch classifier loss: 0.353633; batch adversarial loss: 0.767648\n",
      "epoch 2; iter: 0; batch classifier loss: 0.336708; batch adversarial loss: 0.661252\n",
      "epoch 3; iter: 0; batch classifier loss: 0.226539; batch adversarial loss: 0.559640\n",
      "epoch 4; iter: 0; batch classifier loss: 0.223393; batch adversarial loss: 0.489568\n",
      "epoch 5; iter: 0; batch classifier loss: 0.208948; batch adversarial loss: 0.418628\n",
      "epoch 6; iter: 0; batch classifier loss: 0.366644; batch adversarial loss: 0.427649\n",
      "epoch 7; iter: 0; batch classifier loss: 0.193598; batch adversarial loss: 0.326844\n",
      "epoch 8; iter: 0; batch classifier loss: 0.247602; batch adversarial loss: 0.376021\n",
      "epoch 9; iter: 0; batch classifier loss: 0.220579; batch adversarial loss: 0.339719\n",
      "epoch 10; iter: 0; batch classifier loss: 0.166215; batch adversarial loss: 0.293696\n",
      "epoch 11; iter: 0; batch classifier loss: 0.187508; batch adversarial loss: 0.348802\n",
      "epoch 12; iter: 0; batch classifier loss: 0.200328; batch adversarial loss: 0.256689\n",
      "epoch 13; iter: 0; batch classifier loss: 0.243665; batch adversarial loss: 0.201794\n",
      "epoch 14; iter: 0; batch classifier loss: 0.158414; batch adversarial loss: 0.353347\n",
      "epoch 15; iter: 0; batch classifier loss: 0.278653; batch adversarial loss: 0.270505\n",
      "epoch 16; iter: 0; batch classifier loss: 0.301257; batch adversarial loss: 0.254097\n",
      "epoch 17; iter: 0; batch classifier loss: 0.337379; batch adversarial loss: 0.251510\n",
      "epoch 18; iter: 0; batch classifier loss: 0.266156; batch adversarial loss: 0.287384\n",
      "epoch 19; iter: 0; batch classifier loss: 0.229682; batch adversarial loss: 0.227305\n",
      "epoch 20; iter: 0; batch classifier loss: 0.154326; batch adversarial loss: 0.300510\n",
      "epoch 21; iter: 0; batch classifier loss: 0.178507; batch adversarial loss: 0.284050\n",
      "epoch 22; iter: 0; batch classifier loss: 0.299484; batch adversarial loss: 0.267364\n",
      "epoch 23; iter: 0; batch classifier loss: 0.246722; batch adversarial loss: 0.249255\n",
      "epoch 24; iter: 0; batch classifier loss: 0.180360; batch adversarial loss: 0.251141\n",
      "epoch 25; iter: 0; batch classifier loss: 0.277588; batch adversarial loss: 0.194199\n",
      "epoch 26; iter: 0; batch classifier loss: 0.230542; batch adversarial loss: 0.306693\n",
      "epoch 27; iter: 0; batch classifier loss: 0.185433; batch adversarial loss: 0.342713\n",
      "epoch 28; iter: 0; batch classifier loss: 0.205016; batch adversarial loss: 0.148312\n",
      "epoch 29; iter: 0; batch classifier loss: 0.176350; batch adversarial loss: 0.216555\n",
      "epoch 30; iter: 0; batch classifier loss: 0.214252; batch adversarial loss: 0.271180\n",
      "epoch 31; iter: 0; batch classifier loss: 0.342238; batch adversarial loss: 0.282081\n",
      "epoch 32; iter: 0; batch classifier loss: 0.204226; batch adversarial loss: 0.367980\n",
      "epoch 33; iter: 0; batch classifier loss: 0.194553; batch adversarial loss: 0.293263\n",
      "epoch 34; iter: 0; batch classifier loss: 0.135788; batch adversarial loss: 0.235763\n",
      "epoch 35; iter: 0; batch classifier loss: 0.226944; batch adversarial loss: 0.198446\n",
      "epoch 36; iter: 0; batch classifier loss: 0.254038; batch adversarial loss: 0.188296\n",
      "epoch 37; iter: 0; batch classifier loss: 0.180910; batch adversarial loss: 0.242084\n",
      "epoch 38; iter: 0; batch classifier loss: 0.216925; batch adversarial loss: 0.343627\n",
      "epoch 39; iter: 0; batch classifier loss: 0.279085; batch adversarial loss: 0.307780\n",
      "epoch 40; iter: 0; batch classifier loss: 0.230276; batch adversarial loss: 0.378871\n",
      "epoch 41; iter: 0; batch classifier loss: 0.221257; batch adversarial loss: 0.215950\n",
      "epoch 42; iter: 0; batch classifier loss: 0.281605; batch adversarial loss: 0.261258\n",
      "epoch 43; iter: 0; batch classifier loss: 0.213819; batch adversarial loss: 0.242100\n",
      "epoch 44; iter: 0; batch classifier loss: 0.275843; batch adversarial loss: 0.241532\n",
      "epoch 45; iter: 0; batch classifier loss: 0.216166; batch adversarial loss: 0.313067\n",
      "epoch 46; iter: 0; batch classifier loss: 0.304084; batch adversarial loss: 0.244723\n",
      "epoch 47; iter: 0; batch classifier loss: 0.279022; batch adversarial loss: 0.170651\n",
      "epoch 48; iter: 0; batch classifier loss: 0.245503; batch adversarial loss: 0.265193\n",
      "epoch 49; iter: 0; batch classifier loss: 0.218430; batch adversarial loss: 0.305535\n",
      "epoch 50; iter: 0; batch classifier loss: 0.271358; batch adversarial loss: 0.294288\n",
      "epoch 51; iter: 0; batch classifier loss: 0.180079; batch adversarial loss: 0.231267\n",
      "epoch 52; iter: 0; batch classifier loss: 0.205085; batch adversarial loss: 0.316417\n",
      "epoch 53; iter: 0; batch classifier loss: 0.213991; batch adversarial loss: 0.180430\n",
      "epoch 54; iter: 0; batch classifier loss: 0.248277; batch adversarial loss: 0.254638\n",
      "epoch 55; iter: 0; batch classifier loss: 0.243054; batch adversarial loss: 0.195838\n",
      "epoch 56; iter: 0; batch classifier loss: 0.251365; batch adversarial loss: 0.288464\n",
      "epoch 57; iter: 0; batch classifier loss: 0.219428; batch adversarial loss: 0.171060\n",
      "epoch 58; iter: 0; batch classifier loss: 0.235066; batch adversarial loss: 0.305663\n",
      "epoch 59; iter: 0; batch classifier loss: 0.230579; batch adversarial loss: 0.402056\n",
      "epoch 60; iter: 0; batch classifier loss: 0.190189; batch adversarial loss: 0.195883\n",
      "epoch 61; iter: 0; batch classifier loss: 0.167456; batch adversarial loss: 0.200435\n",
      "epoch 62; iter: 0; batch classifier loss: 0.248989; batch adversarial loss: 0.273779\n",
      "epoch 63; iter: 0; batch classifier loss: 0.174649; batch adversarial loss: 0.247344\n",
      "epoch 64; iter: 0; batch classifier loss: 0.207646; batch adversarial loss: 0.265486\n",
      "epoch 65; iter: 0; batch classifier loss: 0.144842; batch adversarial loss: 0.241837\n",
      "epoch 66; iter: 0; batch classifier loss: 0.199812; batch adversarial loss: 0.124829\n",
      "epoch 67; iter: 0; batch classifier loss: 0.207724; batch adversarial loss: 0.253623\n",
      "epoch 68; iter: 0; batch classifier loss: 0.227233; batch adversarial loss: 0.264712\n",
      "epoch 69; iter: 0; batch classifier loss: 0.139800; batch adversarial loss: 0.148538\n",
      "epoch 70; iter: 0; batch classifier loss: 0.252168; batch adversarial loss: 0.243379\n",
      "epoch 71; iter: 0; batch classifier loss: 0.253408; batch adversarial loss: 0.255998\n",
      "epoch 72; iter: 0; batch classifier loss: 0.252019; batch adversarial loss: 0.324348\n",
      "epoch 73; iter: 0; batch classifier loss: 0.133325; batch adversarial loss: 0.329055\n",
      "epoch 74; iter: 0; batch classifier loss: 0.138632; batch adversarial loss: 0.265982\n",
      "epoch 75; iter: 0; batch classifier loss: 0.339729; batch adversarial loss: 0.274451\n",
      "epoch 76; iter: 0; batch classifier loss: 0.241009; batch adversarial loss: 0.234508\n",
      "epoch 77; iter: 0; batch classifier loss: 0.197073; batch adversarial loss: 0.220227\n",
      "epoch 78; iter: 0; batch classifier loss: 0.216844; batch adversarial loss: 0.304097\n",
      "epoch 79; iter: 0; batch classifier loss: 0.223340; batch adversarial loss: 0.277408\n",
      "epoch 80; iter: 0; batch classifier loss: 0.262763; batch adversarial loss: 0.257935\n",
      "epoch 81; iter: 0; batch classifier loss: 0.273861; batch adversarial loss: 0.196765\n",
      "epoch 82; iter: 0; batch classifier loss: 0.183030; batch adversarial loss: 0.301758\n",
      "epoch 83; iter: 0; batch classifier loss: 0.216397; batch adversarial loss: 0.277427\n",
      "epoch 84; iter: 0; batch classifier loss: 0.249887; batch adversarial loss: 0.269738\n",
      "epoch 85; iter: 0; batch classifier loss: 0.172207; batch adversarial loss: 0.298962\n",
      "epoch 86; iter: 0; batch classifier loss: 0.228342; batch adversarial loss: 0.375832\n",
      "epoch 87; iter: 0; batch classifier loss: 0.285986; batch adversarial loss: 0.240622\n",
      "epoch 88; iter: 0; batch classifier loss: 0.169198; batch adversarial loss: 0.167290\n",
      "epoch 89; iter: 0; batch classifier loss: 0.285858; batch adversarial loss: 0.200060\n",
      "epoch 90; iter: 0; batch classifier loss: 0.188989; batch adversarial loss: 0.332943\n",
      "epoch 91; iter: 0; batch classifier loss: 0.133781; batch adversarial loss: 0.273303\n",
      "epoch 92; iter: 0; batch classifier loss: 0.191754; batch adversarial loss: 0.244086\n",
      "epoch 93; iter: 0; batch classifier loss: 0.167641; batch adversarial loss: 0.338828\n",
      "epoch 94; iter: 0; batch classifier loss: 0.222846; batch adversarial loss: 0.272611\n",
      "epoch 95; iter: 0; batch classifier loss: 0.199187; batch adversarial loss: 0.209283\n",
      "epoch 96; iter: 0; batch classifier loss: 0.211729; batch adversarial loss: 0.327946\n",
      "epoch 97; iter: 0; batch classifier loss: 0.238673; batch adversarial loss: 0.231284\n",
      "epoch 98; iter: 0; batch classifier loss: 0.240859; batch adversarial loss: 0.218390\n",
      "epoch 99; iter: 0; batch classifier loss: 0.208946; batch adversarial loss: 0.290106\n",
      "epoch 100; iter: 0; batch classifier loss: 0.160441; batch adversarial loss: 0.141023\n",
      "epoch 101; iter: 0; batch classifier loss: 0.243691; batch adversarial loss: 0.203349\n",
      "epoch 102; iter: 0; batch classifier loss: 0.242913; batch adversarial loss: 0.211301\n",
      "epoch 103; iter: 0; batch classifier loss: 0.153826; batch adversarial loss: 0.275197\n",
      "epoch 104; iter: 0; batch classifier loss: 0.251000; batch adversarial loss: 0.221556\n",
      "epoch 105; iter: 0; batch classifier loss: 0.176707; batch adversarial loss: 0.162732\n",
      "epoch 106; iter: 0; batch classifier loss: 0.225141; batch adversarial loss: 0.257762\n",
      "epoch 107; iter: 0; batch classifier loss: 0.221854; batch adversarial loss: 0.207496\n",
      "epoch 108; iter: 0; batch classifier loss: 0.256092; batch adversarial loss: 0.291726\n",
      "epoch 109; iter: 0; batch classifier loss: 0.280532; batch adversarial loss: 0.280200\n",
      "epoch 110; iter: 0; batch classifier loss: 0.202546; batch adversarial loss: 0.333035\n",
      "epoch 111; iter: 0; batch classifier loss: 0.147390; batch adversarial loss: 0.293669\n",
      "epoch 112; iter: 0; batch classifier loss: 0.186194; batch adversarial loss: 0.265475\n",
      "epoch 113; iter: 0; batch classifier loss: 0.278563; batch adversarial loss: 0.213945\n",
      "epoch 114; iter: 0; batch classifier loss: 0.247389; batch adversarial loss: 0.216607\n",
      "epoch 115; iter: 0; batch classifier loss: 0.262196; batch adversarial loss: 0.216794\n",
      "epoch 116; iter: 0; batch classifier loss: 0.196758; batch adversarial loss: 0.295786\n",
      "epoch 117; iter: 0; batch classifier loss: 0.299616; batch adversarial loss: 0.300724\n",
      "epoch 118; iter: 0; batch classifier loss: 0.145781; batch adversarial loss: 0.253603\n",
      "epoch 119; iter: 0; batch classifier loss: 0.184239; batch adversarial loss: 0.262953\n",
      "epoch 120; iter: 0; batch classifier loss: 0.194544; batch adversarial loss: 0.358196\n",
      "epoch 121; iter: 0; batch classifier loss: 0.155546; batch adversarial loss: 0.250729\n",
      "epoch 122; iter: 0; batch classifier loss: 0.212968; batch adversarial loss: 0.171819\n",
      "epoch 123; iter: 0; batch classifier loss: 0.208764; batch adversarial loss: 0.268967\n",
      "epoch 124; iter: 0; batch classifier loss: 0.214711; batch adversarial loss: 0.294297\n",
      "epoch 125; iter: 0; batch classifier loss: 0.237503; batch adversarial loss: 0.343784\n",
      "epoch 126; iter: 0; batch classifier loss: 0.199400; batch adversarial loss: 0.214301\n",
      "epoch 127; iter: 0; batch classifier loss: 0.168160; batch adversarial loss: 0.252494\n",
      "epoch 128; iter: 0; batch classifier loss: 0.184828; batch adversarial loss: 0.251393\n",
      "epoch 129; iter: 0; batch classifier loss: 0.255424; batch adversarial loss: 0.342999\n",
      "epoch 130; iter: 0; batch classifier loss: 0.177095; batch adversarial loss: 0.364152\n",
      "epoch 131; iter: 0; batch classifier loss: 0.220777; batch adversarial loss: 0.369667\n",
      "epoch 132; iter: 0; batch classifier loss: 0.179815; batch adversarial loss: 0.213134\n",
      "epoch 133; iter: 0; batch classifier loss: 0.218652; batch adversarial loss: 0.292784\n",
      "epoch 134; iter: 0; batch classifier loss: 0.165592; batch adversarial loss: 0.286924\n",
      "epoch 135; iter: 0; batch classifier loss: 0.190328; batch adversarial loss: 0.247441\n",
      "epoch 136; iter: 0; batch classifier loss: 0.221107; batch adversarial loss: 0.109073\n",
      "epoch 137; iter: 0; batch classifier loss: 0.135069; batch adversarial loss: 0.264527\n",
      "epoch 138; iter: 0; batch classifier loss: 0.225693; batch adversarial loss: 0.192420\n",
      "epoch 139; iter: 0; batch classifier loss: 0.188364; batch adversarial loss: 0.259742\n",
      "epoch 140; iter: 0; batch classifier loss: 0.172051; batch adversarial loss: 0.279173\n",
      "epoch 141; iter: 0; batch classifier loss: 0.221258; batch adversarial loss: 0.273234\n",
      "epoch 142; iter: 0; batch classifier loss: 0.179813; batch adversarial loss: 0.238382\n",
      "epoch 143; iter: 0; batch classifier loss: 0.177133; batch adversarial loss: 0.374873\n",
      "epoch 144; iter: 0; batch classifier loss: 0.215618; batch adversarial loss: 0.179802\n",
      "epoch 145; iter: 0; batch classifier loss: 0.231796; batch adversarial loss: 0.261666\n",
      "epoch 146; iter: 0; batch classifier loss: 0.206651; batch adversarial loss: 0.315879\n",
      "epoch 147; iter: 0; batch classifier loss: 0.215765; batch adversarial loss: 0.260787\n",
      "epoch 148; iter: 0; batch classifier loss: 0.144581; batch adversarial loss: 0.253423\n",
      "epoch 149; iter: 0; batch classifier loss: 0.272334; batch adversarial loss: 0.272126\n",
      "epoch 150; iter: 0; batch classifier loss: 0.107706; batch adversarial loss: 0.336471\n",
      "epoch 151; iter: 0; batch classifier loss: 0.170676; batch adversarial loss: 0.297428\n",
      "epoch 152; iter: 0; batch classifier loss: 0.152436; batch adversarial loss: 0.228484\n",
      "epoch 153; iter: 0; batch classifier loss: 0.162322; batch adversarial loss: 0.346312\n",
      "epoch 154; iter: 0; batch classifier loss: 0.208673; batch adversarial loss: 0.269773\n",
      "epoch 155; iter: 0; batch classifier loss: 0.237799; batch adversarial loss: 0.190478\n",
      "epoch 156; iter: 0; batch classifier loss: 0.199142; batch adversarial loss: 0.219424\n",
      "epoch 157; iter: 0; batch classifier loss: 0.265628; batch adversarial loss: 0.291094\n",
      "epoch 158; iter: 0; batch classifier loss: 0.176236; batch adversarial loss: 0.161666\n",
      "epoch 159; iter: 0; batch classifier loss: 0.284956; batch adversarial loss: 0.256008\n",
      "epoch 160; iter: 0; batch classifier loss: 0.252368; batch adversarial loss: 0.263565\n",
      "epoch 161; iter: 0; batch classifier loss: 0.272185; batch adversarial loss: 0.261143\n",
      "epoch 162; iter: 0; batch classifier loss: 0.187129; batch adversarial loss: 0.267361\n",
      "epoch 163; iter: 0; batch classifier loss: 0.232478; batch adversarial loss: 0.346807\n",
      "epoch 164; iter: 0; batch classifier loss: 0.218149; batch adversarial loss: 0.276965\n",
      "epoch 165; iter: 0; batch classifier loss: 0.188820; batch adversarial loss: 0.246362\n",
      "epoch 166; iter: 0; batch classifier loss: 0.190482; batch adversarial loss: 0.271866\n",
      "epoch 167; iter: 0; batch classifier loss: 0.310480; batch adversarial loss: 0.284011\n",
      "epoch 168; iter: 0; batch classifier loss: 0.168995; batch adversarial loss: 0.348986\n",
      "epoch 169; iter: 0; batch classifier loss: 0.241134; batch adversarial loss: 0.319773\n",
      "epoch 170; iter: 0; batch classifier loss: 0.259694; batch adversarial loss: 0.319360\n",
      "epoch 171; iter: 0; batch classifier loss: 0.142518; batch adversarial loss: 0.389568\n",
      "epoch 172; iter: 0; batch classifier loss: 0.183335; batch adversarial loss: 0.285102\n",
      "epoch 173; iter: 0; batch classifier loss: 0.163069; batch adversarial loss: 0.275089\n",
      "epoch 174; iter: 0; batch classifier loss: 0.201071; batch adversarial loss: 0.305221\n",
      "epoch 175; iter: 0; batch classifier loss: 0.199397; batch adversarial loss: 0.157040\n",
      "epoch 176; iter: 0; batch classifier loss: 0.197066; batch adversarial loss: 0.226246\n",
      "epoch 177; iter: 0; batch classifier loss: 0.171583; batch adversarial loss: 0.344264\n",
      "epoch 178; iter: 0; batch classifier loss: 0.150108; batch adversarial loss: 0.331596\n",
      "epoch 179; iter: 0; batch classifier loss: 0.194964; batch adversarial loss: 0.281188\n",
      "epoch 180; iter: 0; batch classifier loss: 0.180161; batch adversarial loss: 0.274615\n",
      "epoch 181; iter: 0; batch classifier loss: 0.198400; batch adversarial loss: 0.136975\n",
      "epoch 182; iter: 0; batch classifier loss: 0.244188; batch adversarial loss: 0.250889\n",
      "epoch 183; iter: 0; batch classifier loss: 0.309711; batch adversarial loss: 0.291440\n",
      "epoch 184; iter: 0; batch classifier loss: 0.202383; batch adversarial loss: 0.219575\n",
      "epoch 185; iter: 0; batch classifier loss: 0.199382; batch adversarial loss: 0.316763\n",
      "epoch 186; iter: 0; batch classifier loss: 0.208691; batch adversarial loss: 0.180144\n",
      "epoch 187; iter: 0; batch classifier loss: 0.203711; batch adversarial loss: 0.297929\n",
      "epoch 188; iter: 0; batch classifier loss: 0.163245; batch adversarial loss: 0.236361\n",
      "epoch 189; iter: 0; batch classifier loss: 0.216454; batch adversarial loss: 0.314382\n",
      "epoch 190; iter: 0; batch classifier loss: 0.155379; batch adversarial loss: 0.218346\n",
      "epoch 191; iter: 0; batch classifier loss: 0.149235; batch adversarial loss: 0.252925\n",
      "epoch 192; iter: 0; batch classifier loss: 0.220610; batch adversarial loss: 0.269543\n",
      "epoch 193; iter: 0; batch classifier loss: 0.155027; batch adversarial loss: 0.189412\n",
      "epoch 194; iter: 0; batch classifier loss: 0.126253; batch adversarial loss: 0.324905\n",
      "epoch 195; iter: 0; batch classifier loss: 0.146527; batch adversarial loss: 0.335188\n",
      "epoch 196; iter: 0; batch classifier loss: 0.229794; batch adversarial loss: 0.322588\n",
      "epoch 197; iter: 0; batch classifier loss: 0.247542; batch adversarial loss: 0.267868\n",
      "epoch 198; iter: 0; batch classifier loss: 0.194140; batch adversarial loss: 0.227621\n",
      "epoch 199; iter: 0; batch classifier loss: 0.152736; batch adversarial loss: 0.246678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:59:05.225591: W tensorflow/c/c_api.cc:305] Operation '{name:'39b9125a-ae25-11ee-bc15-ae7d8bf09116/39b9125a-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:11827 op device:{requested: '', assigned: ''} def:{{{node 39b9125a-ae25-11ee-bc15-ae7d8bf09116/39b9125a-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b9125a-ae25-11ee-bc15-ae7d8bf09116/39b9125a-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b9125a-ae25-11ee-bc15-ae7d8bf09116/39b9125a-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.711027; batch adversarial loss: 0.728764\n",
      "epoch 1; iter: 0; batch classifier loss: 0.269605; batch adversarial loss: 0.644881\n",
      "epoch 2; iter: 0; batch classifier loss: 0.312588; batch adversarial loss: 0.541650\n",
      "epoch 3; iter: 0; batch classifier loss: 0.217766; batch adversarial loss: 0.477111\n",
      "epoch 4; iter: 0; batch classifier loss: 0.288279; batch adversarial loss: 0.427456\n",
      "epoch 5; iter: 0; batch classifier loss: 0.265393; batch adversarial loss: 0.383366\n",
      "epoch 6; iter: 0; batch classifier loss: 0.216849; batch adversarial loss: 0.382668\n",
      "epoch 7; iter: 0; batch classifier loss: 0.242986; batch adversarial loss: 0.296161\n",
      "epoch 8; iter: 0; batch classifier loss: 0.270380; batch adversarial loss: 0.331032\n",
      "epoch 9; iter: 0; batch classifier loss: 0.228743; batch adversarial loss: 0.349242\n",
      "epoch 10; iter: 0; batch classifier loss: 0.213109; batch adversarial loss: 0.311086\n",
      "epoch 11; iter: 0; batch classifier loss: 0.150350; batch adversarial loss: 0.203582\n",
      "epoch 12; iter: 0; batch classifier loss: 0.245396; batch adversarial loss: 0.255844\n",
      "epoch 13; iter: 0; batch classifier loss: 0.235960; batch adversarial loss: 0.235654\n",
      "epoch 14; iter: 0; batch classifier loss: 0.200899; batch adversarial loss: 0.281336\n",
      "epoch 15; iter: 0; batch classifier loss: 0.177833; batch adversarial loss: 0.340846\n",
      "epoch 16; iter: 0; batch classifier loss: 0.227331; batch adversarial loss: 0.359380\n",
      "epoch 17; iter: 0; batch classifier loss: 0.223764; batch adversarial loss: 0.257565\n",
      "epoch 18; iter: 0; batch classifier loss: 0.229917; batch adversarial loss: 0.193465\n",
      "epoch 19; iter: 0; batch classifier loss: 0.253502; batch adversarial loss: 0.322755\n",
      "epoch 20; iter: 0; batch classifier loss: 0.198904; batch adversarial loss: 0.273722\n",
      "epoch 21; iter: 0; batch classifier loss: 0.223492; batch adversarial loss: 0.325662\n",
      "epoch 22; iter: 0; batch classifier loss: 0.213722; batch adversarial loss: 0.258838\n",
      "epoch 23; iter: 0; batch classifier loss: 0.322007; batch adversarial loss: 0.304679\n",
      "epoch 24; iter: 0; batch classifier loss: 0.234328; batch adversarial loss: 0.197440\n",
      "epoch 25; iter: 0; batch classifier loss: 0.286604; batch adversarial loss: 0.294561\n",
      "epoch 26; iter: 0; batch classifier loss: 0.127823; batch adversarial loss: 0.205292\n",
      "epoch 27; iter: 0; batch classifier loss: 0.292839; batch adversarial loss: 0.317797\n",
      "epoch 28; iter: 0; batch classifier loss: 0.308434; batch adversarial loss: 0.274870\n",
      "epoch 29; iter: 0; batch classifier loss: 0.205240; batch adversarial loss: 0.170159\n",
      "epoch 30; iter: 0; batch classifier loss: 0.187056; batch adversarial loss: 0.304036\n",
      "epoch 31; iter: 0; batch classifier loss: 0.167872; batch adversarial loss: 0.205979\n",
      "epoch 32; iter: 0; batch classifier loss: 0.194972; batch adversarial loss: 0.233810\n",
      "epoch 33; iter: 0; batch classifier loss: 0.165720; batch adversarial loss: 0.165716\n",
      "epoch 34; iter: 0; batch classifier loss: 0.240117; batch adversarial loss: 0.398262\n",
      "epoch 35; iter: 0; batch classifier loss: 0.202386; batch adversarial loss: 0.243703\n",
      "epoch 36; iter: 0; batch classifier loss: 0.209965; batch adversarial loss: 0.357193\n",
      "epoch 37; iter: 0; batch classifier loss: 0.203070; batch adversarial loss: 0.249592\n",
      "epoch 38; iter: 0; batch classifier loss: 0.217704; batch adversarial loss: 0.151026\n",
      "epoch 39; iter: 0; batch classifier loss: 0.227451; batch adversarial loss: 0.273893\n",
      "epoch 40; iter: 0; batch classifier loss: 0.212921; batch adversarial loss: 0.255016\n",
      "epoch 41; iter: 0; batch classifier loss: 0.174676; batch adversarial loss: 0.131747\n",
      "epoch 42; iter: 0; batch classifier loss: 0.294437; batch adversarial loss: 0.264293\n",
      "epoch 43; iter: 0; batch classifier loss: 0.226385; batch adversarial loss: 0.246135\n",
      "epoch 44; iter: 0; batch classifier loss: 0.224730; batch adversarial loss: 0.168672\n",
      "epoch 45; iter: 0; batch classifier loss: 0.143099; batch adversarial loss: 0.277805\n",
      "epoch 46; iter: 0; batch classifier loss: 0.238265; batch adversarial loss: 0.285393\n",
      "epoch 47; iter: 0; batch classifier loss: 0.177647; batch adversarial loss: 0.268400\n",
      "epoch 48; iter: 0; batch classifier loss: 0.177091; batch adversarial loss: 0.294941\n",
      "epoch 49; iter: 0; batch classifier loss: 0.190328; batch adversarial loss: 0.191772\n",
      "epoch 50; iter: 0; batch classifier loss: 0.228441; batch adversarial loss: 0.295038\n",
      "epoch 51; iter: 0; batch classifier loss: 0.225572; batch adversarial loss: 0.316232\n",
      "epoch 52; iter: 0; batch classifier loss: 0.197579; batch adversarial loss: 0.367895\n",
      "epoch 53; iter: 0; batch classifier loss: 0.259553; batch adversarial loss: 0.300099\n",
      "epoch 54; iter: 0; batch classifier loss: 0.183201; batch adversarial loss: 0.195394\n",
      "epoch 55; iter: 0; batch classifier loss: 0.218133; batch adversarial loss: 0.199373\n",
      "epoch 56; iter: 0; batch classifier loss: 0.186045; batch adversarial loss: 0.312286\n",
      "epoch 57; iter: 0; batch classifier loss: 0.170969; batch adversarial loss: 0.277984\n",
      "epoch 58; iter: 0; batch classifier loss: 0.244092; batch adversarial loss: 0.215805\n",
      "epoch 59; iter: 0; batch classifier loss: 0.196727; batch adversarial loss: 0.238392\n",
      "epoch 60; iter: 0; batch classifier loss: 0.179043; batch adversarial loss: 0.246472\n",
      "epoch 61; iter: 0; batch classifier loss: 0.357491; batch adversarial loss: 0.283738\n",
      "epoch 62; iter: 0; batch classifier loss: 0.190918; batch adversarial loss: 0.178969\n",
      "epoch 63; iter: 0; batch classifier loss: 0.314744; batch adversarial loss: 0.227102\n",
      "epoch 64; iter: 0; batch classifier loss: 0.174263; batch adversarial loss: 0.264948\n",
      "epoch 65; iter: 0; batch classifier loss: 0.230630; batch adversarial loss: 0.191028\n",
      "epoch 66; iter: 0; batch classifier loss: 0.223365; batch adversarial loss: 0.226133\n",
      "epoch 67; iter: 0; batch classifier loss: 0.217716; batch adversarial loss: 0.164709\n",
      "epoch 68; iter: 0; batch classifier loss: 0.282805; batch adversarial loss: 0.233018\n",
      "epoch 69; iter: 0; batch classifier loss: 0.184365; batch adversarial loss: 0.274499\n",
      "epoch 70; iter: 0; batch classifier loss: 0.144916; batch adversarial loss: 0.254097\n",
      "epoch 71; iter: 0; batch classifier loss: 0.220159; batch adversarial loss: 0.280085\n",
      "epoch 72; iter: 0; batch classifier loss: 0.239267; batch adversarial loss: 0.273773\n",
      "epoch 73; iter: 0; batch classifier loss: 0.177048; batch adversarial loss: 0.346514\n",
      "epoch 74; iter: 0; batch classifier loss: 0.194519; batch adversarial loss: 0.190767\n",
      "epoch 75; iter: 0; batch classifier loss: 0.140428; batch adversarial loss: 0.323885\n",
      "epoch 76; iter: 0; batch classifier loss: 0.223969; batch adversarial loss: 0.225941\n",
      "epoch 77; iter: 0; batch classifier loss: 0.184705; batch adversarial loss: 0.172777\n",
      "epoch 78; iter: 0; batch classifier loss: 0.230808; batch adversarial loss: 0.238055\n",
      "epoch 79; iter: 0; batch classifier loss: 0.254987; batch adversarial loss: 0.250290\n",
      "epoch 80; iter: 0; batch classifier loss: 0.222469; batch adversarial loss: 0.371342\n",
      "epoch 81; iter: 0; batch classifier loss: 0.215136; batch adversarial loss: 0.177215\n",
      "epoch 82; iter: 0; batch classifier loss: 0.251665; batch adversarial loss: 0.210542\n",
      "epoch 83; iter: 0; batch classifier loss: 0.173561; batch adversarial loss: 0.308956\n",
      "epoch 84; iter: 0; batch classifier loss: 0.173591; batch adversarial loss: 0.262263\n",
      "epoch 85; iter: 0; batch classifier loss: 0.211150; batch adversarial loss: 0.203382\n",
      "epoch 86; iter: 0; batch classifier loss: 0.319146; batch adversarial loss: 0.252025\n",
      "epoch 87; iter: 0; batch classifier loss: 0.216112; batch adversarial loss: 0.345568\n",
      "epoch 88; iter: 0; batch classifier loss: 0.184695; batch adversarial loss: 0.316811\n",
      "epoch 89; iter: 0; batch classifier loss: 0.239991; batch adversarial loss: 0.319316\n",
      "epoch 90; iter: 0; batch classifier loss: 0.229830; batch adversarial loss: 0.324062\n",
      "epoch 91; iter: 0; batch classifier loss: 0.207557; batch adversarial loss: 0.217078\n",
      "epoch 92; iter: 0; batch classifier loss: 0.169497; batch adversarial loss: 0.323079\n",
      "epoch 93; iter: 0; batch classifier loss: 0.229816; batch adversarial loss: 0.178278\n",
      "epoch 94; iter: 0; batch classifier loss: 0.217415; batch adversarial loss: 0.274937\n",
      "epoch 95; iter: 0; batch classifier loss: 0.125736; batch adversarial loss: 0.314744\n",
      "epoch 96; iter: 0; batch classifier loss: 0.164011; batch adversarial loss: 0.207474\n",
      "epoch 97; iter: 0; batch classifier loss: 0.179538; batch adversarial loss: 0.155679\n",
      "epoch 98; iter: 0; batch classifier loss: 0.171572; batch adversarial loss: 0.208751\n",
      "epoch 99; iter: 0; batch classifier loss: 0.326641; batch adversarial loss: 0.237648\n",
      "epoch 100; iter: 0; batch classifier loss: 0.151032; batch adversarial loss: 0.195998\n",
      "epoch 101; iter: 0; batch classifier loss: 0.218871; batch adversarial loss: 0.297191\n",
      "epoch 102; iter: 0; batch classifier loss: 0.192781; batch adversarial loss: 0.208916\n",
      "epoch 103; iter: 0; batch classifier loss: 0.220423; batch adversarial loss: 0.204661\n",
      "epoch 104; iter: 0; batch classifier loss: 0.184344; batch adversarial loss: 0.269531\n",
      "epoch 105; iter: 0; batch classifier loss: 0.235993; batch adversarial loss: 0.197139\n",
      "epoch 106; iter: 0; batch classifier loss: 0.242589; batch adversarial loss: 0.329423\n",
      "epoch 107; iter: 0; batch classifier loss: 0.145971; batch adversarial loss: 0.273838\n",
      "epoch 108; iter: 0; batch classifier loss: 0.204667; batch adversarial loss: 0.292109\n",
      "epoch 109; iter: 0; batch classifier loss: 0.185800; batch adversarial loss: 0.242011\n",
      "epoch 110; iter: 0; batch classifier loss: 0.217423; batch adversarial loss: 0.287258\n",
      "epoch 111; iter: 0; batch classifier loss: 0.133755; batch adversarial loss: 0.398102\n",
      "epoch 112; iter: 0; batch classifier loss: 0.205366; batch adversarial loss: 0.294917\n",
      "epoch 113; iter: 0; batch classifier loss: 0.126897; batch adversarial loss: 0.292761\n",
      "epoch 114; iter: 0; batch classifier loss: 0.245484; batch adversarial loss: 0.239923\n",
      "epoch 115; iter: 0; batch classifier loss: 0.227048; batch adversarial loss: 0.270273\n",
      "epoch 116; iter: 0; batch classifier loss: 0.130638; batch adversarial loss: 0.261500\n",
      "epoch 117; iter: 0; batch classifier loss: 0.188205; batch adversarial loss: 0.193549\n",
      "epoch 118; iter: 0; batch classifier loss: 0.211349; batch adversarial loss: 0.342903\n",
      "epoch 119; iter: 0; batch classifier loss: 0.156209; batch adversarial loss: 0.147918\n",
      "epoch 120; iter: 0; batch classifier loss: 0.228024; batch adversarial loss: 0.204845\n",
      "epoch 121; iter: 0; batch classifier loss: 0.168807; batch adversarial loss: 0.204599\n",
      "epoch 122; iter: 0; batch classifier loss: 0.148799; batch adversarial loss: 0.294942\n",
      "epoch 123; iter: 0; batch classifier loss: 0.178038; batch adversarial loss: 0.242962\n",
      "epoch 124; iter: 0; batch classifier loss: 0.244200; batch adversarial loss: 0.327106\n",
      "epoch 125; iter: 0; batch classifier loss: 0.229614; batch adversarial loss: 0.345580\n",
      "epoch 126; iter: 0; batch classifier loss: 0.191010; batch adversarial loss: 0.240907\n",
      "epoch 127; iter: 0; batch classifier loss: 0.188564; batch adversarial loss: 0.205476\n",
      "epoch 128; iter: 0; batch classifier loss: 0.241793; batch adversarial loss: 0.273399\n",
      "epoch 129; iter: 0; batch classifier loss: 0.178737; batch adversarial loss: 0.243857\n",
      "epoch 130; iter: 0; batch classifier loss: 0.171042; batch adversarial loss: 0.388641\n",
      "epoch 131; iter: 0; batch classifier loss: 0.250047; batch adversarial loss: 0.316580\n",
      "epoch 132; iter: 0; batch classifier loss: 0.245767; batch adversarial loss: 0.206813\n",
      "epoch 133; iter: 0; batch classifier loss: 0.237219; batch adversarial loss: 0.283220\n",
      "epoch 134; iter: 0; batch classifier loss: 0.306705; batch adversarial loss: 0.414902\n",
      "epoch 135; iter: 0; batch classifier loss: 0.204899; batch adversarial loss: 0.225764\n",
      "epoch 136; iter: 0; batch classifier loss: 0.121225; batch adversarial loss: 0.157039\n",
      "epoch 137; iter: 0; batch classifier loss: 0.219878; batch adversarial loss: 0.338126\n",
      "epoch 138; iter: 0; batch classifier loss: 0.180447; batch adversarial loss: 0.216306\n",
      "epoch 139; iter: 0; batch classifier loss: 0.227303; batch adversarial loss: 0.347362\n",
      "epoch 140; iter: 0; batch classifier loss: 0.178814; batch adversarial loss: 0.197801\n",
      "epoch 141; iter: 0; batch classifier loss: 0.181118; batch adversarial loss: 0.429110\n",
      "epoch 142; iter: 0; batch classifier loss: 0.221860; batch adversarial loss: 0.191982\n",
      "epoch 143; iter: 0; batch classifier loss: 0.193312; batch adversarial loss: 0.259356\n",
      "epoch 144; iter: 0; batch classifier loss: 0.206851; batch adversarial loss: 0.281233\n",
      "epoch 145; iter: 0; batch classifier loss: 0.212707; batch adversarial loss: 0.304500\n",
      "epoch 146; iter: 0; batch classifier loss: 0.262282; batch adversarial loss: 0.242346\n",
      "epoch 147; iter: 0; batch classifier loss: 0.196213; batch adversarial loss: 0.226753\n",
      "epoch 148; iter: 0; batch classifier loss: 0.221310; batch adversarial loss: 0.260818\n",
      "epoch 149; iter: 0; batch classifier loss: 0.250177; batch adversarial loss: 0.387100\n",
      "epoch 150; iter: 0; batch classifier loss: 0.190003; batch adversarial loss: 0.243122\n",
      "epoch 151; iter: 0; batch classifier loss: 0.165955; batch adversarial loss: 0.286011\n",
      "epoch 152; iter: 0; batch classifier loss: 0.264084; batch adversarial loss: 0.259634\n",
      "epoch 153; iter: 0; batch classifier loss: 0.253186; batch adversarial loss: 0.280380\n",
      "epoch 154; iter: 0; batch classifier loss: 0.135245; batch adversarial loss: 0.199219\n",
      "epoch 155; iter: 0; batch classifier loss: 0.284538; batch adversarial loss: 0.195759\n",
      "epoch 156; iter: 0; batch classifier loss: 0.139440; batch adversarial loss: 0.284571\n",
      "epoch 157; iter: 0; batch classifier loss: 0.237908; batch adversarial loss: 0.273656\n",
      "epoch 158; iter: 0; batch classifier loss: 0.222714; batch adversarial loss: 0.270515\n",
      "epoch 159; iter: 0; batch classifier loss: 0.288231; batch adversarial loss: 0.297119\n",
      "epoch 160; iter: 0; batch classifier loss: 0.349421; batch adversarial loss: 0.250037\n",
      "epoch 161; iter: 0; batch classifier loss: 0.177687; batch adversarial loss: 0.291115\n",
      "epoch 162; iter: 0; batch classifier loss: 0.262029; batch adversarial loss: 0.236320\n",
      "epoch 163; iter: 0; batch classifier loss: 0.233886; batch adversarial loss: 0.185724\n",
      "epoch 164; iter: 0; batch classifier loss: 0.223939; batch adversarial loss: 0.352682\n",
      "epoch 165; iter: 0; batch classifier loss: 0.234762; batch adversarial loss: 0.179161\n",
      "epoch 166; iter: 0; batch classifier loss: 0.276554; batch adversarial loss: 0.226572\n",
      "epoch 167; iter: 0; batch classifier loss: 0.224884; batch adversarial loss: 0.230561\n",
      "epoch 168; iter: 0; batch classifier loss: 0.172392; batch adversarial loss: 0.193624\n",
      "epoch 169; iter: 0; batch classifier loss: 0.187443; batch adversarial loss: 0.219086\n",
      "epoch 170; iter: 0; batch classifier loss: 0.215369; batch adversarial loss: 0.283182\n",
      "epoch 171; iter: 0; batch classifier loss: 0.190240; batch adversarial loss: 0.248916\n",
      "epoch 172; iter: 0; batch classifier loss: 0.183369; batch adversarial loss: 0.279358\n",
      "epoch 173; iter: 0; batch classifier loss: 0.178382; batch adversarial loss: 0.349489\n",
      "epoch 174; iter: 0; batch classifier loss: 0.244450; batch adversarial loss: 0.254194\n",
      "epoch 175; iter: 0; batch classifier loss: 0.202093; batch adversarial loss: 0.207936\n",
      "epoch 176; iter: 0; batch classifier loss: 0.235455; batch adversarial loss: 0.297456\n",
      "epoch 177; iter: 0; batch classifier loss: 0.206752; batch adversarial loss: 0.311147\n",
      "epoch 178; iter: 0; batch classifier loss: 0.198856; batch adversarial loss: 0.261940\n",
      "epoch 179; iter: 0; batch classifier loss: 0.140060; batch adversarial loss: 0.244418\n",
      "epoch 180; iter: 0; batch classifier loss: 0.230402; batch adversarial loss: 0.324949\n",
      "epoch 181; iter: 0; batch classifier loss: 0.216375; batch adversarial loss: 0.380306\n",
      "epoch 182; iter: 0; batch classifier loss: 0.151415; batch adversarial loss: 0.245349\n",
      "epoch 183; iter: 0; batch classifier loss: 0.195531; batch adversarial loss: 0.339403\n",
      "epoch 184; iter: 0; batch classifier loss: 0.129104; batch adversarial loss: 0.240922\n",
      "epoch 185; iter: 0; batch classifier loss: 0.199810; batch adversarial loss: 0.189187\n",
      "epoch 186; iter: 0; batch classifier loss: 0.162309; batch adversarial loss: 0.189397\n",
      "epoch 187; iter: 0; batch classifier loss: 0.226902; batch adversarial loss: 0.217065\n",
      "epoch 188; iter: 0; batch classifier loss: 0.244910; batch adversarial loss: 0.164759\n",
      "epoch 189; iter: 0; batch classifier loss: 0.183293; batch adversarial loss: 0.259874\n",
      "epoch 190; iter: 0; batch classifier loss: 0.173008; batch adversarial loss: 0.309880\n",
      "epoch 191; iter: 0; batch classifier loss: 0.190551; batch adversarial loss: 0.389648\n",
      "epoch 192; iter: 0; batch classifier loss: 0.289992; batch adversarial loss: 0.350121\n",
      "epoch 193; iter: 0; batch classifier loss: 0.121518; batch adversarial loss: 0.178563\n",
      "epoch 194; iter: 0; batch classifier loss: 0.317820; batch adversarial loss: 0.358504\n",
      "epoch 195; iter: 0; batch classifier loss: 0.189626; batch adversarial loss: 0.226615\n",
      "epoch 196; iter: 0; batch classifier loss: 0.235734; batch adversarial loss: 0.256599\n",
      "epoch 197; iter: 0; batch classifier loss: 0.271009; batch adversarial loss: 0.209501\n",
      "epoch 198; iter: 0; batch classifier loss: 0.178333; batch adversarial loss: 0.212520\n",
      "epoch 199; iter: 0; batch classifier loss: 0.272148; batch adversarial loss: 0.218332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:59:19.039886: W tensorflow/c/c_api.cc:305] Operation '{name:'39b91570-ae25-11ee-bc15-ae7d8bf09116/39b91570-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:12617 op device:{requested: '', assigned: ''} def:{{{node 39b91570-ae25-11ee-bc15-ae7d8bf09116/39b91570-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b91570-ae25-11ee-bc15-ae7d8bf09116/39b91570-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b91570-ae25-11ee-bc15-ae7d8bf09116/39b91570-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.672137; batch adversarial loss: 0.466113\n",
      "epoch 1; iter: 0; batch classifier loss: 1.032186; batch adversarial loss: 0.595947\n",
      "epoch 2; iter: 0; batch classifier loss: 1.452412; batch adversarial loss: 0.609406\n",
      "epoch 3; iter: 0; batch classifier loss: 1.510380; batch adversarial loss: 0.680394\n",
      "epoch 4; iter: 0; batch classifier loss: 1.637902; batch adversarial loss: 0.586047\n",
      "epoch 5; iter: 0; batch classifier loss: 1.415891; batch adversarial loss: 0.590120\n",
      "epoch 6; iter: 0; batch classifier loss: 1.295867; batch adversarial loss: 0.480276\n",
      "epoch 7; iter: 0; batch classifier loss: 1.102087; batch adversarial loss: 0.447985\n",
      "epoch 8; iter: 0; batch classifier loss: 1.076893; batch adversarial loss: 0.444749\n",
      "epoch 9; iter: 0; batch classifier loss: 0.881019; batch adversarial loss: 0.405106\n",
      "epoch 10; iter: 0; batch classifier loss: 0.738495; batch adversarial loss: 0.461425\n",
      "epoch 11; iter: 0; batch classifier loss: 0.616375; batch adversarial loss: 0.288691\n",
      "epoch 12; iter: 0; batch classifier loss: 0.290522; batch adversarial loss: 0.257181\n",
      "epoch 13; iter: 0; batch classifier loss: 0.223420; batch adversarial loss: 0.270368\n",
      "epoch 14; iter: 0; batch classifier loss: 0.163598; batch adversarial loss: 0.288665\n",
      "epoch 15; iter: 0; batch classifier loss: 0.192894; batch adversarial loss: 0.257880\n",
      "epoch 16; iter: 0; batch classifier loss: 0.262394; batch adversarial loss: 0.335426\n",
      "epoch 17; iter: 0; batch classifier loss: 0.150946; batch adversarial loss: 0.246406\n",
      "epoch 18; iter: 0; batch classifier loss: 0.275704; batch adversarial loss: 0.298081\n",
      "epoch 19; iter: 0; batch classifier loss: 0.196208; batch adversarial loss: 0.381592\n",
      "epoch 20; iter: 0; batch classifier loss: 0.271470; batch adversarial loss: 0.259223\n",
      "epoch 21; iter: 0; batch classifier loss: 0.258359; batch adversarial loss: 0.281030\n",
      "epoch 22; iter: 0; batch classifier loss: 0.250850; batch adversarial loss: 0.285660\n",
      "epoch 23; iter: 0; batch classifier loss: 0.247332; batch adversarial loss: 0.179088\n",
      "epoch 24; iter: 0; batch classifier loss: 0.213821; batch adversarial loss: 0.282860\n",
      "epoch 25; iter: 0; batch classifier loss: 0.304663; batch adversarial loss: 0.220506\n",
      "epoch 26; iter: 0; batch classifier loss: 0.287576; batch adversarial loss: 0.306399\n",
      "epoch 27; iter: 0; batch classifier loss: 0.214336; batch adversarial loss: 0.242717\n",
      "epoch 28; iter: 0; batch classifier loss: 0.174385; batch adversarial loss: 0.245359\n",
      "epoch 29; iter: 0; batch classifier loss: 0.258897; batch adversarial loss: 0.199229\n",
      "epoch 30; iter: 0; batch classifier loss: 0.309256; batch adversarial loss: 0.340035\n",
      "epoch 31; iter: 0; batch classifier loss: 0.300449; batch adversarial loss: 0.246521\n",
      "epoch 32; iter: 0; batch classifier loss: 0.230323; batch adversarial loss: 0.188965\n",
      "epoch 33; iter: 0; batch classifier loss: 0.256312; batch adversarial loss: 0.253020\n",
      "epoch 34; iter: 0; batch classifier loss: 0.235296; batch adversarial loss: 0.250874\n",
      "epoch 35; iter: 0; batch classifier loss: 0.232061; batch adversarial loss: 0.199678\n",
      "epoch 36; iter: 0; batch classifier loss: 0.232925; batch adversarial loss: 0.240195\n",
      "epoch 37; iter: 0; batch classifier loss: 0.262919; batch adversarial loss: 0.245411\n",
      "epoch 38; iter: 0; batch classifier loss: 0.164553; batch adversarial loss: 0.242965\n",
      "epoch 39; iter: 0; batch classifier loss: 0.249221; batch adversarial loss: 0.261421\n",
      "epoch 40; iter: 0; batch classifier loss: 0.223366; batch adversarial loss: 0.349606\n",
      "epoch 41; iter: 0; batch classifier loss: 0.231930; batch adversarial loss: 0.203454\n",
      "epoch 42; iter: 0; batch classifier loss: 0.213302; batch adversarial loss: 0.231388\n",
      "epoch 43; iter: 0; batch classifier loss: 0.273217; batch adversarial loss: 0.229804\n",
      "epoch 44; iter: 0; batch classifier loss: 0.217314; batch adversarial loss: 0.301255\n",
      "epoch 45; iter: 0; batch classifier loss: 0.240332; batch adversarial loss: 0.260441\n",
      "epoch 46; iter: 0; batch classifier loss: 0.259626; batch adversarial loss: 0.217850\n",
      "epoch 47; iter: 0; batch classifier loss: 0.223915; batch adversarial loss: 0.191729\n",
      "epoch 48; iter: 0; batch classifier loss: 0.215012; batch adversarial loss: 0.250269\n",
      "epoch 49; iter: 0; batch classifier loss: 0.196481; batch adversarial loss: 0.205724\n",
      "epoch 50; iter: 0; batch classifier loss: 0.193651; batch adversarial loss: 0.232828\n",
      "epoch 51; iter: 0; batch classifier loss: 0.175111; batch adversarial loss: 0.188342\n",
      "epoch 52; iter: 0; batch classifier loss: 0.253491; batch adversarial loss: 0.270368\n",
      "epoch 53; iter: 0; batch classifier loss: 0.269223; batch adversarial loss: 0.263839\n",
      "epoch 54; iter: 0; batch classifier loss: 0.260043; batch adversarial loss: 0.337307\n",
      "epoch 55; iter: 0; batch classifier loss: 0.225413; batch adversarial loss: 0.228547\n",
      "epoch 56; iter: 0; batch classifier loss: 0.170826; batch adversarial loss: 0.150133\n",
      "epoch 57; iter: 0; batch classifier loss: 0.251153; batch adversarial loss: 0.355880\n",
      "epoch 58; iter: 0; batch classifier loss: 0.238864; batch adversarial loss: 0.222950\n",
      "epoch 59; iter: 0; batch classifier loss: 0.253367; batch adversarial loss: 0.239624\n",
      "epoch 60; iter: 0; batch classifier loss: 0.261730; batch adversarial loss: 0.206102\n",
      "epoch 61; iter: 0; batch classifier loss: 0.237932; batch adversarial loss: 0.273093\n",
      "epoch 62; iter: 0; batch classifier loss: 0.210296; batch adversarial loss: 0.361121\n",
      "epoch 63; iter: 0; batch classifier loss: 0.194574; batch adversarial loss: 0.295803\n",
      "epoch 64; iter: 0; batch classifier loss: 0.182759; batch adversarial loss: 0.154892\n",
      "epoch 65; iter: 0; batch classifier loss: 0.154631; batch adversarial loss: 0.244241\n",
      "epoch 66; iter: 0; batch classifier loss: 0.292748; batch adversarial loss: 0.395027\n",
      "epoch 67; iter: 0; batch classifier loss: 0.230606; batch adversarial loss: 0.194705\n",
      "epoch 68; iter: 0; batch classifier loss: 0.291495; batch adversarial loss: 0.238972\n",
      "epoch 69; iter: 0; batch classifier loss: 0.151005; batch adversarial loss: 0.311465\n",
      "epoch 70; iter: 0; batch classifier loss: 0.174623; batch adversarial loss: 0.181193\n",
      "epoch 71; iter: 0; batch classifier loss: 0.197085; batch adversarial loss: 0.287043\n",
      "epoch 72; iter: 0; batch classifier loss: 0.256827; batch adversarial loss: 0.311550\n",
      "epoch 73; iter: 0; batch classifier loss: 0.202213; batch adversarial loss: 0.271336\n",
      "epoch 74; iter: 0; batch classifier loss: 0.229531; batch adversarial loss: 0.228375\n",
      "epoch 75; iter: 0; batch classifier loss: 0.190103; batch adversarial loss: 0.167066\n",
      "epoch 76; iter: 0; batch classifier loss: 0.137601; batch adversarial loss: 0.281196\n",
      "epoch 77; iter: 0; batch classifier loss: 0.261706; batch adversarial loss: 0.285506\n",
      "epoch 78; iter: 0; batch classifier loss: 0.188132; batch adversarial loss: 0.330622\n",
      "epoch 79; iter: 0; batch classifier loss: 0.234552; batch adversarial loss: 0.260995\n",
      "epoch 80; iter: 0; batch classifier loss: 0.221500; batch adversarial loss: 0.263099\n",
      "epoch 81; iter: 0; batch classifier loss: 0.187364; batch adversarial loss: 0.246240\n",
      "epoch 82; iter: 0; batch classifier loss: 0.186833; batch adversarial loss: 0.315445\n",
      "epoch 83; iter: 0; batch classifier loss: 0.206568; batch adversarial loss: 0.229335\n",
      "epoch 84; iter: 0; batch classifier loss: 0.147575; batch adversarial loss: 0.263344\n",
      "epoch 85; iter: 0; batch classifier loss: 0.176884; batch adversarial loss: 0.214647\n",
      "epoch 86; iter: 0; batch classifier loss: 0.267130; batch adversarial loss: 0.324488\n",
      "epoch 87; iter: 0; batch classifier loss: 0.215205; batch adversarial loss: 0.307522\n",
      "epoch 88; iter: 0; batch classifier loss: 0.227030; batch adversarial loss: 0.261464\n",
      "epoch 89; iter: 0; batch classifier loss: 0.219012; batch adversarial loss: 0.311076\n",
      "epoch 90; iter: 0; batch classifier loss: 0.241362; batch adversarial loss: 0.258317\n",
      "epoch 91; iter: 0; batch classifier loss: 0.185719; batch adversarial loss: 0.255783\n",
      "epoch 92; iter: 0; batch classifier loss: 0.232145; batch adversarial loss: 0.212964\n",
      "epoch 93; iter: 0; batch classifier loss: 0.221472; batch adversarial loss: 0.227719\n",
      "epoch 94; iter: 0; batch classifier loss: 0.164219; batch adversarial loss: 0.227469\n",
      "epoch 95; iter: 0; batch classifier loss: 0.225520; batch adversarial loss: 0.318177\n",
      "epoch 96; iter: 0; batch classifier loss: 0.225674; batch adversarial loss: 0.259723\n",
      "epoch 97; iter: 0; batch classifier loss: 0.218429; batch adversarial loss: 0.278257\n",
      "epoch 98; iter: 0; batch classifier loss: 0.224994; batch adversarial loss: 0.466594\n",
      "epoch 99; iter: 0; batch classifier loss: 0.197775; batch adversarial loss: 0.245179\n",
      "epoch 100; iter: 0; batch classifier loss: 0.156299; batch adversarial loss: 0.193814\n",
      "epoch 101; iter: 0; batch classifier loss: 0.222116; batch adversarial loss: 0.384741\n",
      "epoch 102; iter: 0; batch classifier loss: 0.136834; batch adversarial loss: 0.179066\n",
      "epoch 103; iter: 0; batch classifier loss: 0.214831; batch adversarial loss: 0.265756\n",
      "epoch 104; iter: 0; batch classifier loss: 0.288157; batch adversarial loss: 0.260023\n",
      "epoch 105; iter: 0; batch classifier loss: 0.187925; batch adversarial loss: 0.250271\n",
      "epoch 106; iter: 0; batch classifier loss: 0.170492; batch adversarial loss: 0.245310\n",
      "epoch 107; iter: 0; batch classifier loss: 0.199909; batch adversarial loss: 0.270336\n",
      "epoch 108; iter: 0; batch classifier loss: 0.252374; batch adversarial loss: 0.295316\n",
      "epoch 109; iter: 0; batch classifier loss: 0.194048; batch adversarial loss: 0.247465\n",
      "epoch 110; iter: 0; batch classifier loss: 0.204085; batch adversarial loss: 0.288880\n",
      "epoch 111; iter: 0; batch classifier loss: 0.258417; batch adversarial loss: 0.210758\n",
      "epoch 112; iter: 0; batch classifier loss: 0.348579; batch adversarial loss: 0.235419\n",
      "epoch 113; iter: 0; batch classifier loss: 0.268028; batch adversarial loss: 0.231910\n",
      "epoch 114; iter: 0; batch classifier loss: 0.255174; batch adversarial loss: 0.302677\n",
      "epoch 115; iter: 0; batch classifier loss: 0.165113; batch adversarial loss: 0.258325\n",
      "epoch 116; iter: 0; batch classifier loss: 0.201601; batch adversarial loss: 0.270044\n",
      "epoch 117; iter: 0; batch classifier loss: 0.179722; batch adversarial loss: 0.301383\n",
      "epoch 118; iter: 0; batch classifier loss: 0.233901; batch adversarial loss: 0.316119\n",
      "epoch 119; iter: 0; batch classifier loss: 0.263899; batch adversarial loss: 0.325289\n",
      "epoch 120; iter: 0; batch classifier loss: 0.174658; batch adversarial loss: 0.320130\n",
      "epoch 121; iter: 0; batch classifier loss: 0.163270; batch adversarial loss: 0.238956\n",
      "epoch 122; iter: 0; batch classifier loss: 0.254471; batch adversarial loss: 0.220956\n",
      "epoch 123; iter: 0; batch classifier loss: 0.231675; batch adversarial loss: 0.255235\n",
      "epoch 124; iter: 0; batch classifier loss: 0.234524; batch adversarial loss: 0.220746\n",
      "epoch 125; iter: 0; batch classifier loss: 0.219965; batch adversarial loss: 0.180481\n",
      "epoch 126; iter: 0; batch classifier loss: 0.157123; batch adversarial loss: 0.291375\n",
      "epoch 127; iter: 0; batch classifier loss: 0.148619; batch adversarial loss: 0.151232\n",
      "epoch 128; iter: 0; batch classifier loss: 0.244218; batch adversarial loss: 0.262107\n",
      "epoch 129; iter: 0; batch classifier loss: 0.166054; batch adversarial loss: 0.177215\n",
      "epoch 130; iter: 0; batch classifier loss: 0.185197; batch adversarial loss: 0.352849\n",
      "epoch 131; iter: 0; batch classifier loss: 0.174248; batch adversarial loss: 0.222248\n",
      "epoch 132; iter: 0; batch classifier loss: 0.182408; batch adversarial loss: 0.167503\n",
      "epoch 133; iter: 0; batch classifier loss: 0.220825; batch adversarial loss: 0.245290\n",
      "epoch 134; iter: 0; batch classifier loss: 0.129119; batch adversarial loss: 0.333409\n",
      "epoch 135; iter: 0; batch classifier loss: 0.229770; batch adversarial loss: 0.335426\n",
      "epoch 136; iter: 0; batch classifier loss: 0.190362; batch adversarial loss: 0.203415\n",
      "epoch 137; iter: 0; batch classifier loss: 0.209067; batch adversarial loss: 0.234495\n",
      "epoch 138; iter: 0; batch classifier loss: 0.191133; batch adversarial loss: 0.252347\n",
      "epoch 139; iter: 0; batch classifier loss: 0.180961; batch adversarial loss: 0.284831\n",
      "epoch 140; iter: 0; batch classifier loss: 0.220529; batch adversarial loss: 0.268964\n",
      "epoch 141; iter: 0; batch classifier loss: 0.190707; batch adversarial loss: 0.216393\n",
      "epoch 142; iter: 0; batch classifier loss: 0.179951; batch adversarial loss: 0.324991\n",
      "epoch 143; iter: 0; batch classifier loss: 0.253845; batch adversarial loss: 0.291831\n",
      "epoch 144; iter: 0; batch classifier loss: 0.227160; batch adversarial loss: 0.347833\n",
      "epoch 145; iter: 0; batch classifier loss: 0.276899; batch adversarial loss: 0.245429\n",
      "epoch 146; iter: 0; batch classifier loss: 0.195626; batch adversarial loss: 0.165764\n",
      "epoch 147; iter: 0; batch classifier loss: 0.225222; batch adversarial loss: 0.299784\n",
      "epoch 148; iter: 0; batch classifier loss: 0.214137; batch adversarial loss: 0.285441\n",
      "epoch 149; iter: 0; batch classifier loss: 0.178257; batch adversarial loss: 0.229284\n",
      "epoch 150; iter: 0; batch classifier loss: 0.144729; batch adversarial loss: 0.284755\n",
      "epoch 151; iter: 0; batch classifier loss: 0.155537; batch adversarial loss: 0.346805\n",
      "epoch 152; iter: 0; batch classifier loss: 0.198164; batch adversarial loss: 0.223242\n",
      "epoch 153; iter: 0; batch classifier loss: 0.198917; batch adversarial loss: 0.229081\n",
      "epoch 154; iter: 0; batch classifier loss: 0.221367; batch adversarial loss: 0.364398\n",
      "epoch 155; iter: 0; batch classifier loss: 0.137734; batch adversarial loss: 0.211683\n",
      "epoch 156; iter: 0; batch classifier loss: 0.145721; batch adversarial loss: 0.246839\n",
      "epoch 157; iter: 0; batch classifier loss: 0.247203; batch adversarial loss: 0.263491\n",
      "epoch 158; iter: 0; batch classifier loss: 0.270679; batch adversarial loss: 0.359881\n",
      "epoch 159; iter: 0; batch classifier loss: 0.183331; batch adversarial loss: 0.266219\n",
      "epoch 160; iter: 0; batch classifier loss: 0.166437; batch adversarial loss: 0.215152\n",
      "epoch 161; iter: 0; batch classifier loss: 0.273804; batch adversarial loss: 0.285330\n",
      "epoch 162; iter: 0; batch classifier loss: 0.176263; batch adversarial loss: 0.292648\n",
      "epoch 163; iter: 0; batch classifier loss: 0.150971; batch adversarial loss: 0.251799\n",
      "epoch 164; iter: 0; batch classifier loss: 0.206661; batch adversarial loss: 0.275688\n",
      "epoch 165; iter: 0; batch classifier loss: 0.194681; batch adversarial loss: 0.228988\n",
      "epoch 166; iter: 0; batch classifier loss: 0.165615; batch adversarial loss: 0.325433\n",
      "epoch 167; iter: 0; batch classifier loss: 0.186192; batch adversarial loss: 0.384026\n",
      "epoch 168; iter: 0; batch classifier loss: 0.275230; batch adversarial loss: 0.257305\n",
      "epoch 169; iter: 0; batch classifier loss: 0.228100; batch adversarial loss: 0.253779\n",
      "epoch 170; iter: 0; batch classifier loss: 0.192627; batch adversarial loss: 0.265097\n",
      "epoch 171; iter: 0; batch classifier loss: 0.209733; batch adversarial loss: 0.360329\n",
      "epoch 172; iter: 0; batch classifier loss: 0.154400; batch adversarial loss: 0.181745\n",
      "epoch 173; iter: 0; batch classifier loss: 0.196355; batch adversarial loss: 0.304366\n",
      "epoch 174; iter: 0; batch classifier loss: 0.196323; batch adversarial loss: 0.387751\n",
      "epoch 175; iter: 0; batch classifier loss: 0.198505; batch adversarial loss: 0.213187\n",
      "epoch 176; iter: 0; batch classifier loss: 0.188713; batch adversarial loss: 0.216064\n",
      "epoch 177; iter: 0; batch classifier loss: 0.229535; batch adversarial loss: 0.328423\n",
      "epoch 178; iter: 0; batch classifier loss: 0.176936; batch adversarial loss: 0.203941\n",
      "epoch 179; iter: 0; batch classifier loss: 0.209597; batch adversarial loss: 0.229281\n",
      "epoch 180; iter: 0; batch classifier loss: 0.184280; batch adversarial loss: 0.232501\n",
      "epoch 181; iter: 0; batch classifier loss: 0.174478; batch adversarial loss: 0.205594\n",
      "epoch 182; iter: 0; batch classifier loss: 0.191251; batch adversarial loss: 0.295264\n",
      "epoch 183; iter: 0; batch classifier loss: 0.194673; batch adversarial loss: 0.245196\n",
      "epoch 184; iter: 0; batch classifier loss: 0.218449; batch adversarial loss: 0.342240\n",
      "epoch 185; iter: 0; batch classifier loss: 0.126222; batch adversarial loss: 0.191082\n",
      "epoch 186; iter: 0; batch classifier loss: 0.137713; batch adversarial loss: 0.419600\n",
      "epoch 187; iter: 0; batch classifier loss: 0.188299; batch adversarial loss: 0.356576\n",
      "epoch 188; iter: 0; batch classifier loss: 0.190417; batch adversarial loss: 0.209036\n",
      "epoch 189; iter: 0; batch classifier loss: 0.215308; batch adversarial loss: 0.317843\n",
      "epoch 190; iter: 0; batch classifier loss: 0.191426; batch adversarial loss: 0.223662\n",
      "epoch 191; iter: 0; batch classifier loss: 0.198834; batch adversarial loss: 0.317110\n",
      "epoch 192; iter: 0; batch classifier loss: 0.114036; batch adversarial loss: 0.235599\n",
      "epoch 193; iter: 0; batch classifier loss: 0.292128; batch adversarial loss: 0.266449\n",
      "epoch 194; iter: 0; batch classifier loss: 0.160532; batch adversarial loss: 0.193329\n",
      "epoch 195; iter: 0; batch classifier loss: 0.134210; batch adversarial loss: 0.345773\n",
      "epoch 196; iter: 0; batch classifier loss: 0.215001; batch adversarial loss: 0.264928\n",
      "epoch 197; iter: 0; batch classifier loss: 0.325789; batch adversarial loss: 0.325553\n",
      "epoch 198; iter: 0; batch classifier loss: 0.198864; batch adversarial loss: 0.389982\n",
      "epoch 199; iter: 0; batch classifier loss: 0.212558; batch adversarial loss: 0.300731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:59:32.926989: W tensorflow/c/c_api.cc:305] Operation '{name:'39b915ac-ae25-11ee-bc15-ae7d8bf09116/39b915ac-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:13407 op device:{requested: '', assigned: ''} def:{{{node 39b915ac-ae25-11ee-bc15-ae7d8bf09116/39b915ac-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b915ac-ae25-11ee-bc15-ae7d8bf09116/39b915ac-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b915ac-ae25-11ee-bc15-ae7d8bf09116/39b915ac-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.730018; batch adversarial loss: 0.833528\n",
      "epoch 1; iter: 0; batch classifier loss: 0.308060; batch adversarial loss: 0.880607\n",
      "epoch 2; iter: 0; batch classifier loss: 0.237093; batch adversarial loss: 0.770558\n",
      "epoch 3; iter: 0; batch classifier loss: 0.217987; batch adversarial loss: 0.690080\n",
      "epoch 4; iter: 0; batch classifier loss: 0.182140; batch adversarial loss: 0.571200\n",
      "epoch 5; iter: 0; batch classifier loss: 0.218811; batch adversarial loss: 0.504431\n",
      "epoch 6; iter: 0; batch classifier loss: 0.198824; batch adversarial loss: 0.462173\n",
      "epoch 7; iter: 0; batch classifier loss: 0.231496; batch adversarial loss: 0.418298\n",
      "epoch 8; iter: 0; batch classifier loss: 0.234650; batch adversarial loss: 0.367101\n",
      "epoch 9; iter: 0; batch classifier loss: 0.239843; batch adversarial loss: 0.395486\n",
      "epoch 10; iter: 0; batch classifier loss: 0.129563; batch adversarial loss: 0.320923\n",
      "epoch 11; iter: 0; batch classifier loss: 0.233457; batch adversarial loss: 0.349029\n",
      "epoch 12; iter: 0; batch classifier loss: 0.328295; batch adversarial loss: 0.305048\n",
      "epoch 13; iter: 0; batch classifier loss: 0.263559; batch adversarial loss: 0.308725\n",
      "epoch 14; iter: 0; batch classifier loss: 0.267186; batch adversarial loss: 0.285692\n",
      "epoch 15; iter: 0; batch classifier loss: 0.180927; batch adversarial loss: 0.223028\n",
      "epoch 16; iter: 0; batch classifier loss: 0.222334; batch adversarial loss: 0.282023\n",
      "epoch 17; iter: 0; batch classifier loss: 0.259789; batch adversarial loss: 0.299820\n",
      "epoch 18; iter: 0; batch classifier loss: 0.180976; batch adversarial loss: 0.268700\n",
      "epoch 19; iter: 0; batch classifier loss: 0.165062; batch adversarial loss: 0.265114\n",
      "epoch 20; iter: 0; batch classifier loss: 0.200552; batch adversarial loss: 0.234197\n",
      "epoch 21; iter: 0; batch classifier loss: 0.191378; batch adversarial loss: 0.233772\n",
      "epoch 22; iter: 0; batch classifier loss: 0.211319; batch adversarial loss: 0.277860\n",
      "epoch 23; iter: 0; batch classifier loss: 0.185664; batch adversarial loss: 0.277662\n",
      "epoch 24; iter: 0; batch classifier loss: 0.138626; batch adversarial loss: 0.228973\n",
      "epoch 25; iter: 0; batch classifier loss: 0.229035; batch adversarial loss: 0.409827\n",
      "epoch 26; iter: 0; batch classifier loss: 0.244877; batch adversarial loss: 0.326606\n",
      "epoch 27; iter: 0; batch classifier loss: 0.218670; batch adversarial loss: 0.290274\n",
      "epoch 28; iter: 0; batch classifier loss: 0.196833; batch adversarial loss: 0.205375\n",
      "epoch 29; iter: 0; batch classifier loss: 0.156528; batch adversarial loss: 0.285388\n",
      "epoch 30; iter: 0; batch classifier loss: 0.214915; batch adversarial loss: 0.292068\n",
      "epoch 31; iter: 0; batch classifier loss: 0.203515; batch adversarial loss: 0.221781\n",
      "epoch 32; iter: 0; batch classifier loss: 0.170484; batch adversarial loss: 0.232106\n",
      "epoch 33; iter: 0; batch classifier loss: 0.266291; batch adversarial loss: 0.228316\n",
      "epoch 34; iter: 0; batch classifier loss: 0.224671; batch adversarial loss: 0.241082\n",
      "epoch 35; iter: 0; batch classifier loss: 0.231392; batch adversarial loss: 0.392231\n",
      "epoch 36; iter: 0; batch classifier loss: 0.226361; batch adversarial loss: 0.276585\n",
      "epoch 37; iter: 0; batch classifier loss: 0.199990; batch adversarial loss: 0.376640\n",
      "epoch 38; iter: 0; batch classifier loss: 0.231573; batch adversarial loss: 0.250417\n",
      "epoch 39; iter: 0; batch classifier loss: 0.165803; batch adversarial loss: 0.268263\n",
      "epoch 40; iter: 0; batch classifier loss: 0.253376; batch adversarial loss: 0.209031\n",
      "epoch 41; iter: 0; batch classifier loss: 0.237324; batch adversarial loss: 0.313324\n",
      "epoch 42; iter: 0; batch classifier loss: 0.247342; batch adversarial loss: 0.199348\n",
      "epoch 43; iter: 0; batch classifier loss: 0.300253; batch adversarial loss: 0.260656\n",
      "epoch 44; iter: 0; batch classifier loss: 0.167672; batch adversarial loss: 0.296184\n",
      "epoch 45; iter: 0; batch classifier loss: 0.232353; batch adversarial loss: 0.263330\n",
      "epoch 46; iter: 0; batch classifier loss: 0.190718; batch adversarial loss: 0.330960\n",
      "epoch 47; iter: 0; batch classifier loss: 0.166555; batch adversarial loss: 0.311061\n",
      "epoch 48; iter: 0; batch classifier loss: 0.165008; batch adversarial loss: 0.273883\n",
      "epoch 49; iter: 0; batch classifier loss: 0.180822; batch adversarial loss: 0.323186\n",
      "epoch 50; iter: 0; batch classifier loss: 0.262381; batch adversarial loss: 0.274126\n",
      "epoch 51; iter: 0; batch classifier loss: 0.203400; batch adversarial loss: 0.214594\n",
      "epoch 52; iter: 0; batch classifier loss: 0.193065; batch adversarial loss: 0.142952\n",
      "epoch 53; iter: 0; batch classifier loss: 0.225905; batch adversarial loss: 0.295622\n",
      "epoch 54; iter: 0; batch classifier loss: 0.269619; batch adversarial loss: 0.208489\n",
      "epoch 55; iter: 0; batch classifier loss: 0.167513; batch adversarial loss: 0.338583\n",
      "epoch 56; iter: 0; batch classifier loss: 0.151259; batch adversarial loss: 0.277031\n",
      "epoch 57; iter: 0; batch classifier loss: 0.191283; batch adversarial loss: 0.352576\n",
      "epoch 58; iter: 0; batch classifier loss: 0.174968; batch adversarial loss: 0.214477\n",
      "epoch 59; iter: 0; batch classifier loss: 0.217365; batch adversarial loss: 0.323050\n",
      "epoch 60; iter: 0; batch classifier loss: 0.195454; batch adversarial loss: 0.300758\n",
      "epoch 61; iter: 0; batch classifier loss: 0.220204; batch adversarial loss: 0.179001\n",
      "epoch 62; iter: 0; batch classifier loss: 0.121181; batch adversarial loss: 0.287619\n",
      "epoch 63; iter: 0; batch classifier loss: 0.285042; batch adversarial loss: 0.228997\n",
      "epoch 64; iter: 0; batch classifier loss: 0.218117; batch adversarial loss: 0.166350\n",
      "epoch 65; iter: 0; batch classifier loss: 0.213935; batch adversarial loss: 0.216675\n",
      "epoch 66; iter: 0; batch classifier loss: 0.210778; batch adversarial loss: 0.267237\n",
      "epoch 67; iter: 0; batch classifier loss: 0.214747; batch adversarial loss: 0.164920\n",
      "epoch 68; iter: 0; batch classifier loss: 0.170253; batch adversarial loss: 0.293235\n",
      "epoch 69; iter: 0; batch classifier loss: 0.227141; batch adversarial loss: 0.217634\n",
      "epoch 70; iter: 0; batch classifier loss: 0.214020; batch adversarial loss: 0.370278\n",
      "epoch 71; iter: 0; batch classifier loss: 0.184931; batch adversarial loss: 0.288292\n",
      "epoch 72; iter: 0; batch classifier loss: 0.206937; batch adversarial loss: 0.267338\n",
      "epoch 73; iter: 0; batch classifier loss: 0.177791; batch adversarial loss: 0.273025\n",
      "epoch 74; iter: 0; batch classifier loss: 0.241108; batch adversarial loss: 0.282893\n",
      "epoch 75; iter: 0; batch classifier loss: 0.164426; batch adversarial loss: 0.152558\n",
      "epoch 76; iter: 0; batch classifier loss: 0.216785; batch adversarial loss: 0.229338\n",
      "epoch 77; iter: 0; batch classifier loss: 0.145753; batch adversarial loss: 0.299055\n",
      "epoch 78; iter: 0; batch classifier loss: 0.245528; batch adversarial loss: 0.305944\n",
      "epoch 79; iter: 0; batch classifier loss: 0.199432; batch adversarial loss: 0.354153\n",
      "epoch 80; iter: 0; batch classifier loss: 0.195425; batch adversarial loss: 0.316902\n",
      "epoch 81; iter: 0; batch classifier loss: 0.227728; batch adversarial loss: 0.330800\n",
      "epoch 82; iter: 0; batch classifier loss: 0.222884; batch adversarial loss: 0.164050\n",
      "epoch 83; iter: 0; batch classifier loss: 0.248080; batch adversarial loss: 0.339773\n",
      "epoch 84; iter: 0; batch classifier loss: 0.256228; batch adversarial loss: 0.333138\n",
      "epoch 85; iter: 0; batch classifier loss: 0.227210; batch adversarial loss: 0.148712\n",
      "epoch 86; iter: 0; batch classifier loss: 0.115810; batch adversarial loss: 0.160943\n",
      "epoch 87; iter: 0; batch classifier loss: 0.137642; batch adversarial loss: 0.308049\n",
      "epoch 88; iter: 0; batch classifier loss: 0.234094; batch adversarial loss: 0.248203\n",
      "epoch 89; iter: 0; batch classifier loss: 0.208343; batch adversarial loss: 0.210102\n",
      "epoch 90; iter: 0; batch classifier loss: 0.257616; batch adversarial loss: 0.203419\n",
      "epoch 91; iter: 0; batch classifier loss: 0.236363; batch adversarial loss: 0.256322\n",
      "epoch 92; iter: 0; batch classifier loss: 0.151450; batch adversarial loss: 0.204604\n",
      "epoch 93; iter: 0; batch classifier loss: 0.195511; batch adversarial loss: 0.280912\n",
      "epoch 94; iter: 0; batch classifier loss: 0.237796; batch adversarial loss: 0.210926\n",
      "epoch 95; iter: 0; batch classifier loss: 0.245118; batch adversarial loss: 0.391783\n",
      "epoch 96; iter: 0; batch classifier loss: 0.216931; batch adversarial loss: 0.183065\n",
      "epoch 97; iter: 0; batch classifier loss: 0.172283; batch adversarial loss: 0.313537\n",
      "epoch 98; iter: 0; batch classifier loss: 0.243628; batch adversarial loss: 0.356275\n",
      "epoch 99; iter: 0; batch classifier loss: 0.193042; batch adversarial loss: 0.243669\n",
      "epoch 100; iter: 0; batch classifier loss: 0.222356; batch adversarial loss: 0.347816\n",
      "epoch 101; iter: 0; batch classifier loss: 0.249701; batch adversarial loss: 0.231064\n",
      "epoch 102; iter: 0; batch classifier loss: 0.179364; batch adversarial loss: 0.223530\n",
      "epoch 103; iter: 0; batch classifier loss: 0.168475; batch adversarial loss: 0.232409\n",
      "epoch 104; iter: 0; batch classifier loss: 0.184312; batch adversarial loss: 0.376039\n",
      "epoch 105; iter: 0; batch classifier loss: 0.194233; batch adversarial loss: 0.255684\n",
      "epoch 106; iter: 0; batch classifier loss: 0.138792; batch adversarial loss: 0.252294\n",
      "epoch 107; iter: 0; batch classifier loss: 0.268154; batch adversarial loss: 0.231588\n",
      "epoch 108; iter: 0; batch classifier loss: 0.305419; batch adversarial loss: 0.252902\n",
      "epoch 109; iter: 0; batch classifier loss: 0.242672; batch adversarial loss: 0.216632\n",
      "epoch 110; iter: 0; batch classifier loss: 0.195963; batch adversarial loss: 0.221382\n",
      "epoch 111; iter: 0; batch classifier loss: 0.211082; batch adversarial loss: 0.189671\n",
      "epoch 112; iter: 0; batch classifier loss: 0.260614; batch adversarial loss: 0.247478\n",
      "epoch 113; iter: 0; batch classifier loss: 0.201921; batch adversarial loss: 0.195180\n",
      "epoch 114; iter: 0; batch classifier loss: 0.161555; batch adversarial loss: 0.161923\n",
      "epoch 115; iter: 0; batch classifier loss: 0.161558; batch adversarial loss: 0.415103\n",
      "epoch 116; iter: 0; batch classifier loss: 0.246648; batch adversarial loss: 0.307182\n",
      "epoch 117; iter: 0; batch classifier loss: 0.232013; batch adversarial loss: 0.299516\n",
      "epoch 118; iter: 0; batch classifier loss: 0.251575; batch adversarial loss: 0.314958\n",
      "epoch 119; iter: 0; batch classifier loss: 0.184425; batch adversarial loss: 0.261288\n",
      "epoch 120; iter: 0; batch classifier loss: 0.160644; batch adversarial loss: 0.372681\n",
      "epoch 121; iter: 0; batch classifier loss: 0.207901; batch adversarial loss: 0.401089\n",
      "epoch 122; iter: 0; batch classifier loss: 0.207372; batch adversarial loss: 0.323794\n",
      "epoch 123; iter: 0; batch classifier loss: 0.173617; batch adversarial loss: 0.231886\n",
      "epoch 124; iter: 0; batch classifier loss: 0.216994; batch adversarial loss: 0.234031\n",
      "epoch 125; iter: 0; batch classifier loss: 0.145553; batch adversarial loss: 0.276096\n",
      "epoch 126; iter: 0; batch classifier loss: 0.207445; batch adversarial loss: 0.185932\n",
      "epoch 127; iter: 0; batch classifier loss: 0.157398; batch adversarial loss: 0.307460\n",
      "epoch 128; iter: 0; batch classifier loss: 0.239329; batch adversarial loss: 0.211853\n",
      "epoch 129; iter: 0; batch classifier loss: 0.218410; batch adversarial loss: 0.245466\n",
      "epoch 130; iter: 0; batch classifier loss: 0.231938; batch adversarial loss: 0.333489\n",
      "epoch 131; iter: 0; batch classifier loss: 0.206959; batch adversarial loss: 0.331710\n",
      "epoch 132; iter: 0; batch classifier loss: 0.295216; batch adversarial loss: 0.246690\n",
      "epoch 133; iter: 0; batch classifier loss: 0.238462; batch adversarial loss: 0.148927\n",
      "epoch 134; iter: 0; batch classifier loss: 0.133736; batch adversarial loss: 0.146005\n",
      "epoch 135; iter: 0; batch classifier loss: 0.172017; batch adversarial loss: 0.335240\n",
      "epoch 136; iter: 0; batch classifier loss: 0.154012; batch adversarial loss: 0.316949\n",
      "epoch 137; iter: 0; batch classifier loss: 0.156294; batch adversarial loss: 0.243540\n",
      "epoch 138; iter: 0; batch classifier loss: 0.151981; batch adversarial loss: 0.314985\n",
      "epoch 139; iter: 0; batch classifier loss: 0.250825; batch adversarial loss: 0.202211\n",
      "epoch 140; iter: 0; batch classifier loss: 0.235779; batch adversarial loss: 0.262620\n",
      "epoch 141; iter: 0; batch classifier loss: 0.241411; batch adversarial loss: 0.267699\n",
      "epoch 142; iter: 0; batch classifier loss: 0.234130; batch adversarial loss: 0.218299\n",
      "epoch 143; iter: 0; batch classifier loss: 0.144597; batch adversarial loss: 0.231720\n",
      "epoch 144; iter: 0; batch classifier loss: 0.191312; batch adversarial loss: 0.278734\n",
      "epoch 145; iter: 0; batch classifier loss: 0.215709; batch adversarial loss: 0.251342\n",
      "epoch 146; iter: 0; batch classifier loss: 0.203626; batch adversarial loss: 0.255018\n",
      "epoch 147; iter: 0; batch classifier loss: 0.178561; batch adversarial loss: 0.305405\n",
      "epoch 148; iter: 0; batch classifier loss: 0.182972; batch adversarial loss: 0.236773\n",
      "epoch 149; iter: 0; batch classifier loss: 0.182182; batch adversarial loss: 0.208145\n",
      "epoch 150; iter: 0; batch classifier loss: 0.215169; batch adversarial loss: 0.308460\n",
      "epoch 151; iter: 0; batch classifier loss: 0.191669; batch adversarial loss: 0.204927\n",
      "epoch 152; iter: 0; batch classifier loss: 0.267748; batch adversarial loss: 0.363922\n",
      "epoch 153; iter: 0; batch classifier loss: 0.153116; batch adversarial loss: 0.292703\n",
      "epoch 154; iter: 0; batch classifier loss: 0.131625; batch adversarial loss: 0.201548\n",
      "epoch 155; iter: 0; batch classifier loss: 0.240187; batch adversarial loss: 0.326373\n",
      "epoch 156; iter: 0; batch classifier loss: 0.127171; batch adversarial loss: 0.259583\n",
      "epoch 157; iter: 0; batch classifier loss: 0.197151; batch adversarial loss: 0.303625\n",
      "epoch 158; iter: 0; batch classifier loss: 0.248843; batch adversarial loss: 0.268473\n",
      "epoch 159; iter: 0; batch classifier loss: 0.141453; batch adversarial loss: 0.205743\n",
      "epoch 160; iter: 0; batch classifier loss: 0.204418; batch adversarial loss: 0.205840\n",
      "epoch 161; iter: 0; batch classifier loss: 0.322311; batch adversarial loss: 0.236081\n",
      "epoch 162; iter: 0; batch classifier loss: 0.222175; batch adversarial loss: 0.232883\n",
      "epoch 163; iter: 0; batch classifier loss: 0.243422; batch adversarial loss: 0.320572\n",
      "epoch 164; iter: 0; batch classifier loss: 0.152367; batch adversarial loss: 0.220845\n",
      "epoch 165; iter: 0; batch classifier loss: 0.136064; batch adversarial loss: 0.236937\n",
      "epoch 166; iter: 0; batch classifier loss: 0.212907; batch adversarial loss: 0.236405\n",
      "epoch 167; iter: 0; batch classifier loss: 0.179139; batch adversarial loss: 0.274810\n",
      "epoch 168; iter: 0; batch classifier loss: 0.213004; batch adversarial loss: 0.198901\n",
      "epoch 169; iter: 0; batch classifier loss: 0.282889; batch adversarial loss: 0.269656\n",
      "epoch 170; iter: 0; batch classifier loss: 0.168212; batch adversarial loss: 0.195947\n",
      "epoch 171; iter: 0; batch classifier loss: 0.225750; batch adversarial loss: 0.285515\n",
      "epoch 172; iter: 0; batch classifier loss: 0.177232; batch adversarial loss: 0.341809\n",
      "epoch 173; iter: 0; batch classifier loss: 0.162130; batch adversarial loss: 0.239887\n",
      "epoch 174; iter: 0; batch classifier loss: 0.275262; batch adversarial loss: 0.334305\n",
      "epoch 175; iter: 0; batch classifier loss: 0.148180; batch adversarial loss: 0.201765\n",
      "epoch 176; iter: 0; batch classifier loss: 0.215069; batch adversarial loss: 0.171509\n",
      "epoch 177; iter: 0; batch classifier loss: 0.117880; batch adversarial loss: 0.241335\n",
      "epoch 178; iter: 0; batch classifier loss: 0.124714; batch adversarial loss: 0.194922\n",
      "epoch 179; iter: 0; batch classifier loss: 0.214383; batch adversarial loss: 0.270548\n",
      "epoch 180; iter: 0; batch classifier loss: 0.251603; batch adversarial loss: 0.292388\n",
      "epoch 181; iter: 0; batch classifier loss: 0.165869; batch adversarial loss: 0.256197\n",
      "epoch 182; iter: 0; batch classifier loss: 0.117658; batch adversarial loss: 0.209660\n",
      "epoch 183; iter: 0; batch classifier loss: 0.201906; batch adversarial loss: 0.310535\n",
      "epoch 184; iter: 0; batch classifier loss: 0.193236; batch adversarial loss: 0.268357\n",
      "epoch 185; iter: 0; batch classifier loss: 0.128003; batch adversarial loss: 0.241677\n",
      "epoch 186; iter: 0; batch classifier loss: 0.209979; batch adversarial loss: 0.288791\n",
      "epoch 187; iter: 0; batch classifier loss: 0.133516; batch adversarial loss: 0.246606\n",
      "epoch 188; iter: 0; batch classifier loss: 0.219777; batch adversarial loss: 0.305123\n",
      "epoch 189; iter: 0; batch classifier loss: 0.281100; batch adversarial loss: 0.367110\n",
      "epoch 190; iter: 0; batch classifier loss: 0.089059; batch adversarial loss: 0.253775\n",
      "epoch 191; iter: 0; batch classifier loss: 0.146624; batch adversarial loss: 0.267008\n",
      "epoch 192; iter: 0; batch classifier loss: 0.203928; batch adversarial loss: 0.297307\n",
      "epoch 193; iter: 0; batch classifier loss: 0.181916; batch adversarial loss: 0.244201\n",
      "epoch 194; iter: 0; batch classifier loss: 0.237628; batch adversarial loss: 0.249440\n",
      "epoch 195; iter: 0; batch classifier loss: 0.178196; batch adversarial loss: 0.346103\n",
      "epoch 196; iter: 0; batch classifier loss: 0.210861; batch adversarial loss: 0.259031\n",
      "epoch 197; iter: 0; batch classifier loss: 0.302841; batch adversarial loss: 0.372193\n",
      "epoch 198; iter: 0; batch classifier loss: 0.198814; batch adversarial loss: 0.311366\n",
      "epoch 199; iter: 0; batch classifier loss: 0.155115; batch adversarial loss: 0.286972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:59:46.701737: W tensorflow/c/c_api.cc:305] Operation '{name:'39b915de-ae25-11ee-bc15-ae7d8bf09116/39b915de-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:14197 op device:{requested: '', assigned: ''} def:{{{node 39b915de-ae25-11ee-bc15-ae7d8bf09116/39b915de-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b915de-ae25-11ee-bc15-ae7d8bf09116/39b915de-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b915de-ae25-11ee-bc15-ae7d8bf09116/39b915de-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.658392; batch adversarial loss: 0.557374\n",
      "epoch 1; iter: 0; batch classifier loss: 0.294416; batch adversarial loss: 0.467630\n",
      "epoch 2; iter: 0; batch classifier loss: 0.276471; batch adversarial loss: 0.406063\n",
      "epoch 3; iter: 0; batch classifier loss: 0.299274; batch adversarial loss: 0.390990\n",
      "epoch 4; iter: 0; batch classifier loss: 0.229801; batch adversarial loss: 0.310087\n",
      "epoch 5; iter: 0; batch classifier loss: 0.203792; batch adversarial loss: 0.295117\n",
      "epoch 6; iter: 0; batch classifier loss: 0.277957; batch adversarial loss: 0.308744\n",
      "epoch 7; iter: 0; batch classifier loss: 0.309148; batch adversarial loss: 0.345995\n",
      "epoch 8; iter: 0; batch classifier loss: 0.239403; batch adversarial loss: 0.261856\n",
      "epoch 9; iter: 0; batch classifier loss: 0.243664; batch adversarial loss: 0.325299\n",
      "epoch 10; iter: 0; batch classifier loss: 0.217376; batch adversarial loss: 0.269087\n",
      "epoch 11; iter: 0; batch classifier loss: 0.255297; batch adversarial loss: 0.300926\n",
      "epoch 12; iter: 0; batch classifier loss: 0.253850; batch adversarial loss: 0.288925\n",
      "epoch 13; iter: 0; batch classifier loss: 0.224147; batch adversarial loss: 0.257136\n",
      "epoch 14; iter: 0; batch classifier loss: 0.191773; batch adversarial loss: 0.327266\n",
      "epoch 15; iter: 0; batch classifier loss: 0.256715; batch adversarial loss: 0.282966\n",
      "epoch 16; iter: 0; batch classifier loss: 0.257384; batch adversarial loss: 0.323395\n",
      "epoch 17; iter: 0; batch classifier loss: 0.259311; batch adversarial loss: 0.230978\n",
      "epoch 18; iter: 0; batch classifier loss: 0.279664; batch adversarial loss: 0.204534\n",
      "epoch 19; iter: 0; batch classifier loss: 0.247020; batch adversarial loss: 0.276965\n",
      "epoch 20; iter: 0; batch classifier loss: 0.228047; batch adversarial loss: 0.272792\n",
      "epoch 21; iter: 0; batch classifier loss: 0.246795; batch adversarial loss: 0.302588\n",
      "epoch 22; iter: 0; batch classifier loss: 0.261182; batch adversarial loss: 0.280218\n",
      "epoch 23; iter: 0; batch classifier loss: 0.176908; batch adversarial loss: 0.293564\n",
      "epoch 24; iter: 0; batch classifier loss: 0.181373; batch adversarial loss: 0.233895\n",
      "epoch 25; iter: 0; batch classifier loss: 0.213236; batch adversarial loss: 0.295878\n",
      "epoch 26; iter: 0; batch classifier loss: 0.225122; batch adversarial loss: 0.257404\n",
      "epoch 27; iter: 0; batch classifier loss: 0.219951; batch adversarial loss: 0.206918\n",
      "epoch 28; iter: 0; batch classifier loss: 0.254854; batch adversarial loss: 0.335775\n",
      "epoch 29; iter: 0; batch classifier loss: 0.172017; batch adversarial loss: 0.493252\n",
      "epoch 30; iter: 0; batch classifier loss: 0.194425; batch adversarial loss: 0.219109\n",
      "epoch 31; iter: 0; batch classifier loss: 0.230689; batch adversarial loss: 0.231820\n",
      "epoch 32; iter: 0; batch classifier loss: 0.218363; batch adversarial loss: 0.290549\n",
      "epoch 33; iter: 0; batch classifier loss: 0.180093; batch adversarial loss: 0.254252\n",
      "epoch 34; iter: 0; batch classifier loss: 0.274084; batch adversarial loss: 0.320664\n",
      "epoch 35; iter: 0; batch classifier loss: 0.213206; batch adversarial loss: 0.215428\n",
      "epoch 36; iter: 0; batch classifier loss: 0.295739; batch adversarial loss: 0.261103\n",
      "epoch 37; iter: 0; batch classifier loss: 0.229694; batch adversarial loss: 0.294062\n",
      "epoch 38; iter: 0; batch classifier loss: 0.168391; batch adversarial loss: 0.225328\n",
      "epoch 39; iter: 0; batch classifier loss: 0.183318; batch adversarial loss: 0.216684\n",
      "epoch 40; iter: 0; batch classifier loss: 0.267423; batch adversarial loss: 0.235489\n",
      "epoch 41; iter: 0; batch classifier loss: 0.238405; batch adversarial loss: 0.242998\n",
      "epoch 42; iter: 0; batch classifier loss: 0.260391; batch adversarial loss: 0.219670\n",
      "epoch 43; iter: 0; batch classifier loss: 0.224523; batch adversarial loss: 0.267809\n",
      "epoch 44; iter: 0; batch classifier loss: 0.186546; batch adversarial loss: 0.288088\n",
      "epoch 45; iter: 0; batch classifier loss: 0.245804; batch adversarial loss: 0.247326\n",
      "epoch 46; iter: 0; batch classifier loss: 0.229599; batch adversarial loss: 0.267572\n",
      "epoch 47; iter: 0; batch classifier loss: 0.211325; batch adversarial loss: 0.291496\n",
      "epoch 48; iter: 0; batch classifier loss: 0.210990; batch adversarial loss: 0.168466\n",
      "epoch 49; iter: 0; batch classifier loss: 0.228202; batch adversarial loss: 0.312172\n",
      "epoch 50; iter: 0; batch classifier loss: 0.192857; batch adversarial loss: 0.268641\n",
      "epoch 51; iter: 0; batch classifier loss: 0.185112; batch adversarial loss: 0.157375\n",
      "epoch 52; iter: 0; batch classifier loss: 0.258784; batch adversarial loss: 0.316477\n",
      "epoch 53; iter: 0; batch classifier loss: 0.245452; batch adversarial loss: 0.260049\n",
      "epoch 54; iter: 0; batch classifier loss: 0.184110; batch adversarial loss: 0.309293\n",
      "epoch 55; iter: 0; batch classifier loss: 0.172081; batch adversarial loss: 0.234480\n",
      "epoch 56; iter: 0; batch classifier loss: 0.224286; batch adversarial loss: 0.187426\n",
      "epoch 57; iter: 0; batch classifier loss: 0.230929; batch adversarial loss: 0.184602\n",
      "epoch 58; iter: 0; batch classifier loss: 0.205095; batch adversarial loss: 0.351969\n",
      "epoch 59; iter: 0; batch classifier loss: 0.237337; batch adversarial loss: 0.250051\n",
      "epoch 60; iter: 0; batch classifier loss: 0.173288; batch adversarial loss: 0.292723\n",
      "epoch 61; iter: 0; batch classifier loss: 0.178222; batch adversarial loss: 0.351332\n",
      "epoch 62; iter: 0; batch classifier loss: 0.142382; batch adversarial loss: 0.208069\n",
      "epoch 63; iter: 0; batch classifier loss: 0.195124; batch adversarial loss: 0.317774\n",
      "epoch 64; iter: 0; batch classifier loss: 0.201937; batch adversarial loss: 0.216823\n",
      "epoch 65; iter: 0; batch classifier loss: 0.159033; batch adversarial loss: 0.271040\n",
      "epoch 66; iter: 0; batch classifier loss: 0.152093; batch adversarial loss: 0.240959\n",
      "epoch 67; iter: 0; batch classifier loss: 0.207931; batch adversarial loss: 0.225286\n",
      "epoch 68; iter: 0; batch classifier loss: 0.250249; batch adversarial loss: 0.279191\n",
      "epoch 69; iter: 0; batch classifier loss: 0.212978; batch adversarial loss: 0.289433\n",
      "epoch 70; iter: 0; batch classifier loss: 0.186915; batch adversarial loss: 0.275534\n",
      "epoch 71; iter: 0; batch classifier loss: 0.194893; batch adversarial loss: 0.307280\n",
      "epoch 72; iter: 0; batch classifier loss: 0.255051; batch adversarial loss: 0.251218\n",
      "epoch 73; iter: 0; batch classifier loss: 0.278440; batch adversarial loss: 0.220401\n",
      "epoch 74; iter: 0; batch classifier loss: 0.241281; batch adversarial loss: 0.155124\n",
      "epoch 75; iter: 0; batch classifier loss: 0.156838; batch adversarial loss: 0.172063\n",
      "epoch 76; iter: 0; batch classifier loss: 0.227993; batch adversarial loss: 0.269587\n",
      "epoch 77; iter: 0; batch classifier loss: 0.241277; batch adversarial loss: 0.287797\n",
      "epoch 78; iter: 0; batch classifier loss: 0.207533; batch adversarial loss: 0.267891\n",
      "epoch 79; iter: 0; batch classifier loss: 0.275660; batch adversarial loss: 0.178062\n",
      "epoch 80; iter: 0; batch classifier loss: 0.163872; batch adversarial loss: 0.297674\n",
      "epoch 81; iter: 0; batch classifier loss: 0.187220; batch adversarial loss: 0.192609\n",
      "epoch 82; iter: 0; batch classifier loss: 0.195555; batch adversarial loss: 0.268995\n",
      "epoch 83; iter: 0; batch classifier loss: 0.142453; batch adversarial loss: 0.220993\n",
      "epoch 84; iter: 0; batch classifier loss: 0.189779; batch adversarial loss: 0.313859\n",
      "epoch 85; iter: 0; batch classifier loss: 0.267504; batch adversarial loss: 0.243580\n",
      "epoch 86; iter: 0; batch classifier loss: 0.189980; batch adversarial loss: 0.265176\n",
      "epoch 87; iter: 0; batch classifier loss: 0.216613; batch adversarial loss: 0.282658\n",
      "epoch 88; iter: 0; batch classifier loss: 0.161520; batch adversarial loss: 0.226231\n",
      "epoch 89; iter: 0; batch classifier loss: 0.261189; batch adversarial loss: 0.233518\n",
      "epoch 90; iter: 0; batch classifier loss: 0.259129; batch adversarial loss: 0.302562\n",
      "epoch 91; iter: 0; batch classifier loss: 0.222180; batch adversarial loss: 0.370281\n",
      "epoch 92; iter: 0; batch classifier loss: 0.199326; batch adversarial loss: 0.350839\n",
      "epoch 93; iter: 0; batch classifier loss: 0.177074; batch adversarial loss: 0.339785\n",
      "epoch 94; iter: 0; batch classifier loss: 0.213346; batch adversarial loss: 0.259182\n",
      "epoch 95; iter: 0; batch classifier loss: 0.294171; batch adversarial loss: 0.286142\n",
      "epoch 96; iter: 0; batch classifier loss: 0.216565; batch adversarial loss: 0.188571\n",
      "epoch 97; iter: 0; batch classifier loss: 0.232738; batch adversarial loss: 0.264757\n",
      "epoch 98; iter: 0; batch classifier loss: 0.201185; batch adversarial loss: 0.362637\n",
      "epoch 99; iter: 0; batch classifier loss: 0.243416; batch adversarial loss: 0.309996\n",
      "epoch 100; iter: 0; batch classifier loss: 0.213005; batch adversarial loss: 0.302476\n",
      "epoch 101; iter: 0; batch classifier loss: 0.190922; batch adversarial loss: 0.347152\n",
      "epoch 102; iter: 0; batch classifier loss: 0.181138; batch adversarial loss: 0.331807\n",
      "epoch 103; iter: 0; batch classifier loss: 0.254898; batch adversarial loss: 0.291515\n",
      "epoch 104; iter: 0; batch classifier loss: 0.300297; batch adversarial loss: 0.347256\n",
      "epoch 105; iter: 0; batch classifier loss: 0.172303; batch adversarial loss: 0.142345\n",
      "epoch 106; iter: 0; batch classifier loss: 0.233586; batch adversarial loss: 0.297156\n",
      "epoch 107; iter: 0; batch classifier loss: 0.223282; batch adversarial loss: 0.335728\n",
      "epoch 108; iter: 0; batch classifier loss: 0.144303; batch adversarial loss: 0.230084\n",
      "epoch 109; iter: 0; batch classifier loss: 0.264093; batch adversarial loss: 0.344576\n",
      "epoch 110; iter: 0; batch classifier loss: 0.293048; batch adversarial loss: 0.131741\n",
      "epoch 111; iter: 0; batch classifier loss: 0.187370; batch adversarial loss: 0.206522\n",
      "epoch 112; iter: 0; batch classifier loss: 0.221771; batch adversarial loss: 0.199525\n",
      "epoch 113; iter: 0; batch classifier loss: 0.177710; batch adversarial loss: 0.186873\n",
      "epoch 114; iter: 0; batch classifier loss: 0.219294; batch adversarial loss: 0.271064\n",
      "epoch 115; iter: 0; batch classifier loss: 0.125050; batch adversarial loss: 0.222379\n",
      "epoch 116; iter: 0; batch classifier loss: 0.262232; batch adversarial loss: 0.244563\n",
      "epoch 117; iter: 0; batch classifier loss: 0.186722; batch adversarial loss: 0.337154\n",
      "epoch 118; iter: 0; batch classifier loss: 0.177773; batch adversarial loss: 0.221887\n",
      "epoch 119; iter: 0; batch classifier loss: 0.223382; batch adversarial loss: 0.348940\n",
      "epoch 120; iter: 0; batch classifier loss: 0.191513; batch adversarial loss: 0.250512\n",
      "epoch 121; iter: 0; batch classifier loss: 0.258559; batch adversarial loss: 0.289049\n",
      "epoch 122; iter: 0; batch classifier loss: 0.151077; batch adversarial loss: 0.163947\n",
      "epoch 123; iter: 0; batch classifier loss: 0.295548; batch adversarial loss: 0.233488\n",
      "epoch 124; iter: 0; batch classifier loss: 0.218934; batch adversarial loss: 0.267455\n",
      "epoch 125; iter: 0; batch classifier loss: 0.170550; batch adversarial loss: 0.288708\n",
      "epoch 126; iter: 0; batch classifier loss: 0.228735; batch adversarial loss: 0.318025\n",
      "epoch 127; iter: 0; batch classifier loss: 0.234139; batch adversarial loss: 0.401062\n",
      "epoch 128; iter: 0; batch classifier loss: 0.118188; batch adversarial loss: 0.234069\n",
      "epoch 129; iter: 0; batch classifier loss: 0.206534; batch adversarial loss: 0.302767\n",
      "epoch 130; iter: 0; batch classifier loss: 0.210987; batch adversarial loss: 0.163160\n",
      "epoch 131; iter: 0; batch classifier loss: 0.216047; batch adversarial loss: 0.277820\n",
      "epoch 132; iter: 0; batch classifier loss: 0.200019; batch adversarial loss: 0.282573\n",
      "epoch 133; iter: 0; batch classifier loss: 0.273443; batch adversarial loss: 0.351664\n",
      "epoch 134; iter: 0; batch classifier loss: 0.181150; batch adversarial loss: 0.260554\n",
      "epoch 135; iter: 0; batch classifier loss: 0.193607; batch adversarial loss: 0.294210\n",
      "epoch 136; iter: 0; batch classifier loss: 0.182254; batch adversarial loss: 0.289611\n",
      "epoch 137; iter: 0; batch classifier loss: 0.211790; batch adversarial loss: 0.372088\n",
      "epoch 138; iter: 0; batch classifier loss: 0.130607; batch adversarial loss: 0.200199\n",
      "epoch 139; iter: 0; batch classifier loss: 0.163627; batch adversarial loss: 0.196004\n",
      "epoch 140; iter: 0; batch classifier loss: 0.197367; batch adversarial loss: 0.256979\n",
      "epoch 141; iter: 0; batch classifier loss: 0.147877; batch adversarial loss: 0.352808\n",
      "epoch 142; iter: 0; batch classifier loss: 0.161569; batch adversarial loss: 0.177177\n",
      "epoch 143; iter: 0; batch classifier loss: 0.228348; batch adversarial loss: 0.250329\n",
      "epoch 144; iter: 0; batch classifier loss: 0.220067; batch adversarial loss: 0.201614\n",
      "epoch 145; iter: 0; batch classifier loss: 0.274236; batch adversarial loss: 0.244104\n",
      "epoch 146; iter: 0; batch classifier loss: 0.218127; batch adversarial loss: 0.353390\n",
      "epoch 147; iter: 0; batch classifier loss: 0.181993; batch adversarial loss: 0.237843\n",
      "epoch 148; iter: 0; batch classifier loss: 0.198479; batch adversarial loss: 0.359784\n",
      "epoch 149; iter: 0; batch classifier loss: 0.223848; batch adversarial loss: 0.332548\n",
      "epoch 150; iter: 0; batch classifier loss: 0.314018; batch adversarial loss: 0.294525\n",
      "epoch 151; iter: 0; batch classifier loss: 0.251183; batch adversarial loss: 0.280689\n",
      "epoch 152; iter: 0; batch classifier loss: 0.215464; batch adversarial loss: 0.192436\n",
      "epoch 153; iter: 0; batch classifier loss: 0.292778; batch adversarial loss: 0.177721\n",
      "epoch 154; iter: 0; batch classifier loss: 0.211263; batch adversarial loss: 0.243390\n",
      "epoch 155; iter: 0; batch classifier loss: 0.223007; batch adversarial loss: 0.211269\n",
      "epoch 156; iter: 0; batch classifier loss: 0.168399; batch adversarial loss: 0.284154\n",
      "epoch 157; iter: 0; batch classifier loss: 0.188236; batch adversarial loss: 0.199814\n",
      "epoch 158; iter: 0; batch classifier loss: 0.249260; batch adversarial loss: 0.295457\n",
      "epoch 159; iter: 0; batch classifier loss: 0.168168; batch adversarial loss: 0.204921\n",
      "epoch 160; iter: 0; batch classifier loss: 0.268751; batch adversarial loss: 0.258309\n",
      "epoch 161; iter: 0; batch classifier loss: 0.279111; batch adversarial loss: 0.350250\n",
      "epoch 162; iter: 0; batch classifier loss: 0.141885; batch adversarial loss: 0.225940\n",
      "epoch 163; iter: 0; batch classifier loss: 0.213705; batch adversarial loss: 0.248522\n",
      "epoch 164; iter: 0; batch classifier loss: 0.122516; batch adversarial loss: 0.217912\n",
      "epoch 165; iter: 0; batch classifier loss: 0.239931; batch adversarial loss: 0.305649\n",
      "epoch 166; iter: 0; batch classifier loss: 0.236641; batch adversarial loss: 0.179047\n",
      "epoch 167; iter: 0; batch classifier loss: 0.181416; batch adversarial loss: 0.214718\n",
      "epoch 168; iter: 0; batch classifier loss: 0.145893; batch adversarial loss: 0.251069\n",
      "epoch 169; iter: 0; batch classifier loss: 0.208645; batch adversarial loss: 0.303579\n",
      "epoch 170; iter: 0; batch classifier loss: 0.223822; batch adversarial loss: 0.266993\n",
      "epoch 171; iter: 0; batch classifier loss: 0.118215; batch adversarial loss: 0.183729\n",
      "epoch 172; iter: 0; batch classifier loss: 0.263059; batch adversarial loss: 0.356005\n",
      "epoch 173; iter: 0; batch classifier loss: 0.212667; batch adversarial loss: 0.247836\n",
      "epoch 174; iter: 0; batch classifier loss: 0.204287; batch adversarial loss: 0.404460\n",
      "epoch 175; iter: 0; batch classifier loss: 0.182508; batch adversarial loss: 0.317990\n",
      "epoch 176; iter: 0; batch classifier loss: 0.235047; batch adversarial loss: 0.230648\n",
      "epoch 177; iter: 0; batch classifier loss: 0.217594; batch adversarial loss: 0.232309\n",
      "epoch 178; iter: 0; batch classifier loss: 0.190413; batch adversarial loss: 0.363367\n",
      "epoch 179; iter: 0; batch classifier loss: 0.165882; batch adversarial loss: 0.202465\n",
      "epoch 180; iter: 0; batch classifier loss: 0.278554; batch adversarial loss: 0.261441\n",
      "epoch 181; iter: 0; batch classifier loss: 0.154344; batch adversarial loss: 0.341279\n",
      "epoch 182; iter: 0; batch classifier loss: 0.227161; batch adversarial loss: 0.254700\n",
      "epoch 183; iter: 0; batch classifier loss: 0.207595; batch adversarial loss: 0.224281\n",
      "epoch 184; iter: 0; batch classifier loss: 0.283844; batch adversarial loss: 0.274081\n",
      "epoch 185; iter: 0; batch classifier loss: 0.228707; batch adversarial loss: 0.279828\n",
      "epoch 186; iter: 0; batch classifier loss: 0.194417; batch adversarial loss: 0.195142\n",
      "epoch 187; iter: 0; batch classifier loss: 0.200411; batch adversarial loss: 0.322977\n",
      "epoch 188; iter: 0; batch classifier loss: 0.154166; batch adversarial loss: 0.290624\n",
      "epoch 189; iter: 0; batch classifier loss: 0.155946; batch adversarial loss: 0.231897\n",
      "epoch 190; iter: 0; batch classifier loss: 0.185434; batch adversarial loss: 0.339633\n",
      "epoch 191; iter: 0; batch classifier loss: 0.162569; batch adversarial loss: 0.302623\n",
      "epoch 192; iter: 0; batch classifier loss: 0.153886; batch adversarial loss: 0.300993\n",
      "epoch 193; iter: 0; batch classifier loss: 0.137772; batch adversarial loss: 0.286632\n",
      "epoch 194; iter: 0; batch classifier loss: 0.206437; batch adversarial loss: 0.361808\n",
      "epoch 195; iter: 0; batch classifier loss: 0.255480; batch adversarial loss: 0.287329\n",
      "epoch 196; iter: 0; batch classifier loss: 0.188552; batch adversarial loss: 0.397139\n",
      "epoch 197; iter: 0; batch classifier loss: 0.243803; batch adversarial loss: 0.287617\n",
      "epoch 198; iter: 0; batch classifier loss: 0.221342; batch adversarial loss: 0.312673\n",
      "epoch 199; iter: 0; batch classifier loss: 0.194280; batch adversarial loss: 0.242285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 15:00:00.633140: W tensorflow/c/c_api.cc:305] Operation '{name:'39b91606-ae25-11ee-bc15-ae7d8bf09116/39b91606-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign' id:14987 op device:{requested: '', assigned: ''} def:{{{node 39b91606-ae25-11ee-bc15-ae7d8bf09116/39b91606-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](39b91606-ae25-11ee-bc15-ae7d8bf09116/39b91606-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1, 39b91606-ae25-11ee-bc15-ae7d8bf09116/39b91606-ae25-11ee-bc15-ae7d8bf09116/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.697510; batch adversarial loss: 0.821781\n",
      "epoch 1; iter: 0; batch classifier loss: 0.316984; batch adversarial loss: 0.792675\n",
      "epoch 2; iter: 0; batch classifier loss: 0.210076; batch adversarial loss: 0.683848\n",
      "epoch 3; iter: 0; batch classifier loss: 0.200424; batch adversarial loss: 0.582443\n",
      "epoch 4; iter: 0; batch classifier loss: 0.246268; batch adversarial loss: 0.522641\n",
      "epoch 5; iter: 0; batch classifier loss: 0.191786; batch adversarial loss: 0.466231\n",
      "epoch 6; iter: 0; batch classifier loss: 0.275744; batch adversarial loss: 0.391277\n",
      "epoch 7; iter: 0; batch classifier loss: 0.222185; batch adversarial loss: 0.373101\n",
      "epoch 8; iter: 0; batch classifier loss: 0.225707; batch adversarial loss: 0.356512\n",
      "epoch 9; iter: 0; batch classifier loss: 0.351194; batch adversarial loss: 0.399525\n",
      "epoch 10; iter: 0; batch classifier loss: 0.271632; batch adversarial loss: 0.330535\n",
      "epoch 11; iter: 0; batch classifier loss: 0.190482; batch adversarial loss: 0.297849\n",
      "epoch 12; iter: 0; batch classifier loss: 0.189120; batch adversarial loss: 0.270447\n",
      "epoch 13; iter: 0; batch classifier loss: 0.279010; batch adversarial loss: 0.359265\n",
      "epoch 14; iter: 0; batch classifier loss: 0.170236; batch adversarial loss: 0.259469\n",
      "epoch 15; iter: 0; batch classifier loss: 0.245176; batch adversarial loss: 0.414176\n",
      "epoch 16; iter: 0; batch classifier loss: 0.283864; batch adversarial loss: 0.298494\n",
      "epoch 17; iter: 0; batch classifier loss: 0.226985; batch adversarial loss: 0.228806\n",
      "epoch 18; iter: 0; batch classifier loss: 0.336016; batch adversarial loss: 0.299094\n",
      "epoch 19; iter: 0; batch classifier loss: 0.306488; batch adversarial loss: 0.283207\n",
      "epoch 20; iter: 0; batch classifier loss: 0.267881; batch adversarial loss: 0.159411\n",
      "epoch 21; iter: 0; batch classifier loss: 0.207874; batch adversarial loss: 0.302082\n",
      "epoch 22; iter: 0; batch classifier loss: 0.179378; batch adversarial loss: 0.236394\n",
      "epoch 23; iter: 0; batch classifier loss: 0.236292; batch adversarial loss: 0.280575\n",
      "epoch 24; iter: 0; batch classifier loss: 0.260458; batch adversarial loss: 0.254610\n",
      "epoch 25; iter: 0; batch classifier loss: 0.223268; batch adversarial loss: 0.305882\n",
      "epoch 26; iter: 0; batch classifier loss: 0.202032; batch adversarial loss: 0.367398\n",
      "epoch 27; iter: 0; batch classifier loss: 0.260282; batch adversarial loss: 0.364997\n",
      "epoch 28; iter: 0; batch classifier loss: 0.264792; batch adversarial loss: 0.289592\n",
      "epoch 29; iter: 0; batch classifier loss: 0.252751; batch adversarial loss: 0.227981\n",
      "epoch 30; iter: 0; batch classifier loss: 0.238905; batch adversarial loss: 0.211699\n",
      "epoch 31; iter: 0; batch classifier loss: 0.167091; batch adversarial loss: 0.259219\n",
      "epoch 32; iter: 0; batch classifier loss: 0.204290; batch adversarial loss: 0.205023\n",
      "epoch 33; iter: 0; batch classifier loss: 0.205025; batch adversarial loss: 0.328351\n",
      "epoch 34; iter: 0; batch classifier loss: 0.159849; batch adversarial loss: 0.255622\n",
      "epoch 35; iter: 0; batch classifier loss: 0.218785; batch adversarial loss: 0.298525\n",
      "epoch 36; iter: 0; batch classifier loss: 0.278632; batch adversarial loss: 0.221396\n",
      "epoch 37; iter: 0; batch classifier loss: 0.222772; batch adversarial loss: 0.216218\n",
      "epoch 38; iter: 0; batch classifier loss: 0.188096; batch adversarial loss: 0.351725\n",
      "epoch 39; iter: 0; batch classifier loss: 0.261680; batch adversarial loss: 0.220332\n",
      "epoch 40; iter: 0; batch classifier loss: 0.175518; batch adversarial loss: 0.278889\n",
      "epoch 41; iter: 0; batch classifier loss: 0.211405; batch adversarial loss: 0.245728\n",
      "epoch 42; iter: 0; batch classifier loss: 0.228821; batch adversarial loss: 0.265674\n",
      "epoch 43; iter: 0; batch classifier loss: 0.239783; batch adversarial loss: 0.230591\n",
      "epoch 44; iter: 0; batch classifier loss: 0.268344; batch adversarial loss: 0.252002\n",
      "epoch 45; iter: 0; batch classifier loss: 0.264210; batch adversarial loss: 0.173644\n",
      "epoch 46; iter: 0; batch classifier loss: 0.142946; batch adversarial loss: 0.182729\n",
      "epoch 47; iter: 0; batch classifier loss: 0.282957; batch adversarial loss: 0.265065\n",
      "epoch 48; iter: 0; batch classifier loss: 0.234082; batch adversarial loss: 0.228860\n",
      "epoch 49; iter: 0; batch classifier loss: 0.163682; batch adversarial loss: 0.220076\n",
      "epoch 50; iter: 0; batch classifier loss: 0.222447; batch adversarial loss: 0.297670\n",
      "epoch 51; iter: 0; batch classifier loss: 0.207979; batch adversarial loss: 0.271204\n",
      "epoch 52; iter: 0; batch classifier loss: 0.225037; batch adversarial loss: 0.280682\n",
      "epoch 53; iter: 0; batch classifier loss: 0.212448; batch adversarial loss: 0.275491\n",
      "epoch 54; iter: 0; batch classifier loss: 0.206405; batch adversarial loss: 0.295362\n",
      "epoch 55; iter: 0; batch classifier loss: 0.237038; batch adversarial loss: 0.181498\n",
      "epoch 56; iter: 0; batch classifier loss: 0.219294; batch adversarial loss: 0.201888\n",
      "epoch 57; iter: 0; batch classifier loss: 0.226359; batch adversarial loss: 0.208993\n",
      "epoch 58; iter: 0; batch classifier loss: 0.213192; batch adversarial loss: 0.267130\n",
      "epoch 59; iter: 0; batch classifier loss: 0.267910; batch adversarial loss: 0.339212\n",
      "epoch 60; iter: 0; batch classifier loss: 0.217639; batch adversarial loss: 0.319529\n",
      "epoch 61; iter: 0; batch classifier loss: 0.278790; batch adversarial loss: 0.194089\n",
      "epoch 62; iter: 0; batch classifier loss: 0.216709; batch adversarial loss: 0.211335\n",
      "epoch 63; iter: 0; batch classifier loss: 0.281295; batch adversarial loss: 0.317976\n",
      "epoch 64; iter: 0; batch classifier loss: 0.188069; batch adversarial loss: 0.275732\n",
      "epoch 65; iter: 0; batch classifier loss: 0.278212; batch adversarial loss: 0.270068\n",
      "epoch 66; iter: 0; batch classifier loss: 0.224738; batch adversarial loss: 0.331202\n",
      "epoch 67; iter: 0; batch classifier loss: 0.200621; batch adversarial loss: 0.306513\n",
      "epoch 68; iter: 0; batch classifier loss: 0.208701; batch adversarial loss: 0.280261\n",
      "epoch 69; iter: 0; batch classifier loss: 0.176780; batch adversarial loss: 0.228358\n",
      "epoch 70; iter: 0; batch classifier loss: 0.152069; batch adversarial loss: 0.226507\n",
      "epoch 71; iter: 0; batch classifier loss: 0.205333; batch adversarial loss: 0.306824\n",
      "epoch 72; iter: 0; batch classifier loss: 0.209918; batch adversarial loss: 0.304821\n",
      "epoch 73; iter: 0; batch classifier loss: 0.209360; batch adversarial loss: 0.236659\n",
      "epoch 74; iter: 0; batch classifier loss: 0.228664; batch adversarial loss: 0.204775\n",
      "epoch 75; iter: 0; batch classifier loss: 0.230873; batch adversarial loss: 0.263348\n",
      "epoch 76; iter: 0; batch classifier loss: 0.268229; batch adversarial loss: 0.200817\n",
      "epoch 77; iter: 0; batch classifier loss: 0.201526; batch adversarial loss: 0.365350\n",
      "epoch 78; iter: 0; batch classifier loss: 0.222389; batch adversarial loss: 0.206187\n",
      "epoch 79; iter: 0; batch classifier loss: 0.220696; batch adversarial loss: 0.240958\n",
      "epoch 80; iter: 0; batch classifier loss: 0.244281; batch adversarial loss: 0.233593\n",
      "epoch 81; iter: 0; batch classifier loss: 0.148152; batch adversarial loss: 0.273470\n",
      "epoch 82; iter: 0; batch classifier loss: 0.331372; batch adversarial loss: 0.270015\n",
      "epoch 83; iter: 0; batch classifier loss: 0.175941; batch adversarial loss: 0.220939\n",
      "epoch 84; iter: 0; batch classifier loss: 0.156267; batch adversarial loss: 0.261792\n",
      "epoch 85; iter: 0; batch classifier loss: 0.199566; batch adversarial loss: 0.212482\n",
      "epoch 86; iter: 0; batch classifier loss: 0.251626; batch adversarial loss: 0.288997\n",
      "epoch 87; iter: 0; batch classifier loss: 0.159140; batch adversarial loss: 0.235326\n",
      "epoch 88; iter: 0; batch classifier loss: 0.224485; batch adversarial loss: 0.194856\n",
      "epoch 89; iter: 0; batch classifier loss: 0.191757; batch adversarial loss: 0.207101\n",
      "epoch 90; iter: 0; batch classifier loss: 0.239238; batch adversarial loss: 0.270753\n",
      "epoch 91; iter: 0; batch classifier loss: 0.166776; batch adversarial loss: 0.148076\n",
      "epoch 92; iter: 0; batch classifier loss: 0.213047; batch adversarial loss: 0.222970\n",
      "epoch 93; iter: 0; batch classifier loss: 0.239940; batch adversarial loss: 0.227327\n",
      "epoch 94; iter: 0; batch classifier loss: 0.300050; batch adversarial loss: 0.169636\n",
      "epoch 95; iter: 0; batch classifier loss: 0.171256; batch adversarial loss: 0.314057\n",
      "epoch 96; iter: 0; batch classifier loss: 0.189744; batch adversarial loss: 0.189332\n",
      "epoch 97; iter: 0; batch classifier loss: 0.192404; batch adversarial loss: 0.327746\n",
      "epoch 98; iter: 0; batch classifier loss: 0.129369; batch adversarial loss: 0.268707\n",
      "epoch 99; iter: 0; batch classifier loss: 0.264059; batch adversarial loss: 0.296846\n",
      "epoch 100; iter: 0; batch classifier loss: 0.201528; batch adversarial loss: 0.351575\n",
      "epoch 101; iter: 0; batch classifier loss: 0.208046; batch adversarial loss: 0.186354\n",
      "epoch 102; iter: 0; batch classifier loss: 0.196168; batch adversarial loss: 0.229705\n",
      "epoch 103; iter: 0; batch classifier loss: 0.203029; batch adversarial loss: 0.257615\n",
      "epoch 104; iter: 0; batch classifier loss: 0.152287; batch adversarial loss: 0.322223\n",
      "epoch 105; iter: 0; batch classifier loss: 0.219515; batch adversarial loss: 0.306289\n",
      "epoch 106; iter: 0; batch classifier loss: 0.212171; batch adversarial loss: 0.310027\n",
      "epoch 107; iter: 0; batch classifier loss: 0.266662; batch adversarial loss: 0.236986\n",
      "epoch 108; iter: 0; batch classifier loss: 0.215077; batch adversarial loss: 0.286049\n",
      "epoch 109; iter: 0; batch classifier loss: 0.233421; batch adversarial loss: 0.256279\n",
      "epoch 110; iter: 0; batch classifier loss: 0.259957; batch adversarial loss: 0.297786\n",
      "epoch 111; iter: 0; batch classifier loss: 0.153318; batch adversarial loss: 0.245232\n",
      "epoch 112; iter: 0; batch classifier loss: 0.214448; batch adversarial loss: 0.207829\n",
      "epoch 113; iter: 0; batch classifier loss: 0.201391; batch adversarial loss: 0.274415\n",
      "epoch 114; iter: 0; batch classifier loss: 0.251563; batch adversarial loss: 0.348848\n",
      "epoch 115; iter: 0; batch classifier loss: 0.245770; batch adversarial loss: 0.355229\n",
      "epoch 116; iter: 0; batch classifier loss: 0.194171; batch adversarial loss: 0.184909\n",
      "epoch 117; iter: 0; batch classifier loss: 0.185421; batch adversarial loss: 0.185790\n",
      "epoch 118; iter: 0; batch classifier loss: 0.174679; batch adversarial loss: 0.284456\n",
      "epoch 119; iter: 0; batch classifier loss: 0.229468; batch adversarial loss: 0.229734\n",
      "epoch 120; iter: 0; batch classifier loss: 0.171588; batch adversarial loss: 0.295102\n",
      "epoch 121; iter: 0; batch classifier loss: 0.194133; batch adversarial loss: 0.250172\n",
      "epoch 122; iter: 0; batch classifier loss: 0.269748; batch adversarial loss: 0.291297\n",
      "epoch 123; iter: 0; batch classifier loss: 0.195651; batch adversarial loss: 0.394538\n",
      "epoch 124; iter: 0; batch classifier loss: 0.163497; batch adversarial loss: 0.221535\n",
      "epoch 125; iter: 0; batch classifier loss: 0.234808; batch adversarial loss: 0.318070\n",
      "epoch 126; iter: 0; batch classifier loss: 0.241556; batch adversarial loss: 0.369633\n",
      "epoch 127; iter: 0; batch classifier loss: 0.170778; batch adversarial loss: 0.362075\n",
      "epoch 128; iter: 0; batch classifier loss: 0.226360; batch adversarial loss: 0.290222\n",
      "epoch 129; iter: 0; batch classifier loss: 0.222103; batch adversarial loss: 0.322049\n",
      "epoch 130; iter: 0; batch classifier loss: 0.227427; batch adversarial loss: 0.391971\n",
      "epoch 131; iter: 0; batch classifier loss: 0.214994; batch adversarial loss: 0.279965\n",
      "epoch 132; iter: 0; batch classifier loss: 0.147887; batch adversarial loss: 0.226527\n",
      "epoch 133; iter: 0; batch classifier loss: 0.118037; batch adversarial loss: 0.098546\n",
      "epoch 134; iter: 0; batch classifier loss: 0.214638; batch adversarial loss: 0.397021\n",
      "epoch 135; iter: 0; batch classifier loss: 0.217650; batch adversarial loss: 0.212116\n",
      "epoch 136; iter: 0; batch classifier loss: 0.241962; batch adversarial loss: 0.319557\n",
      "epoch 137; iter: 0; batch classifier loss: 0.177742; batch adversarial loss: 0.221230\n",
      "epoch 138; iter: 0; batch classifier loss: 0.219546; batch adversarial loss: 0.238453\n",
      "epoch 139; iter: 0; batch classifier loss: 0.170765; batch adversarial loss: 0.251246\n",
      "epoch 140; iter: 0; batch classifier loss: 0.210993; batch adversarial loss: 0.221412\n",
      "epoch 141; iter: 0; batch classifier loss: 0.254250; batch adversarial loss: 0.237389\n",
      "epoch 142; iter: 0; batch classifier loss: 0.232541; batch adversarial loss: 0.196383\n",
      "epoch 143; iter: 0; batch classifier loss: 0.243446; batch adversarial loss: 0.415899\n",
      "epoch 144; iter: 0; batch classifier loss: 0.168428; batch adversarial loss: 0.236632\n",
      "epoch 145; iter: 0; batch classifier loss: 0.243662; batch adversarial loss: 0.306320\n",
      "epoch 146; iter: 0; batch classifier loss: 0.219754; batch adversarial loss: 0.317744\n",
      "epoch 147; iter: 0; batch classifier loss: 0.174851; batch adversarial loss: 0.288546\n",
      "epoch 148; iter: 0; batch classifier loss: 0.228092; batch adversarial loss: 0.336692\n",
      "epoch 149; iter: 0; batch classifier loss: 0.279085; batch adversarial loss: 0.195414\n",
      "epoch 150; iter: 0; batch classifier loss: 0.176584; batch adversarial loss: 0.245507\n",
      "epoch 151; iter: 0; batch classifier loss: 0.177501; batch adversarial loss: 0.312502\n",
      "epoch 152; iter: 0; batch classifier loss: 0.162292; batch adversarial loss: 0.183843\n",
      "epoch 153; iter: 0; batch classifier loss: 0.140331; batch adversarial loss: 0.223510\n",
      "epoch 154; iter: 0; batch classifier loss: 0.167980; batch adversarial loss: 0.212792\n",
      "epoch 155; iter: 0; batch classifier loss: 0.200035; batch adversarial loss: 0.252356\n",
      "epoch 156; iter: 0; batch classifier loss: 0.224470; batch adversarial loss: 0.271495\n",
      "epoch 157; iter: 0; batch classifier loss: 0.196989; batch adversarial loss: 0.290492\n",
      "epoch 158; iter: 0; batch classifier loss: 0.163265; batch adversarial loss: 0.210257\n",
      "epoch 159; iter: 0; batch classifier loss: 0.198409; batch adversarial loss: 0.310363\n",
      "epoch 160; iter: 0; batch classifier loss: 0.254904; batch adversarial loss: 0.206853\n",
      "epoch 161; iter: 0; batch classifier loss: 0.188552; batch adversarial loss: 0.244487\n",
      "epoch 162; iter: 0; batch classifier loss: 0.207082; batch adversarial loss: 0.260833\n",
      "epoch 163; iter: 0; batch classifier loss: 0.124634; batch adversarial loss: 0.230671\n",
      "epoch 164; iter: 0; batch classifier loss: 0.165164; batch adversarial loss: 0.151913\n",
      "epoch 165; iter: 0; batch classifier loss: 0.151476; batch adversarial loss: 0.239108\n",
      "epoch 166; iter: 0; batch classifier loss: 0.226379; batch adversarial loss: 0.252800\n",
      "epoch 167; iter: 0; batch classifier loss: 0.245839; batch adversarial loss: 0.374987\n",
      "epoch 168; iter: 0; batch classifier loss: 0.273451; batch adversarial loss: 0.219729\n",
      "epoch 169; iter: 0; batch classifier loss: 0.161923; batch adversarial loss: 0.344948\n",
      "epoch 170; iter: 0; batch classifier loss: 0.184182; batch adversarial loss: 0.269045\n",
      "epoch 171; iter: 0; batch classifier loss: 0.282137; batch adversarial loss: 0.236206\n",
      "epoch 172; iter: 0; batch classifier loss: 0.233829; batch adversarial loss: 0.283439\n",
      "epoch 173; iter: 0; batch classifier loss: 0.259650; batch adversarial loss: 0.351480\n",
      "epoch 174; iter: 0; batch classifier loss: 0.314360; batch adversarial loss: 0.178503\n",
      "epoch 175; iter: 0; batch classifier loss: 0.180845; batch adversarial loss: 0.244035\n",
      "epoch 176; iter: 0; batch classifier loss: 0.215917; batch adversarial loss: 0.200673\n",
      "epoch 177; iter: 0; batch classifier loss: 0.238141; batch adversarial loss: 0.205298\n",
      "epoch 178; iter: 0; batch classifier loss: 0.152737; batch adversarial loss: 0.236452\n",
      "epoch 179; iter: 0; batch classifier loss: 0.220583; batch adversarial loss: 0.262002\n",
      "epoch 180; iter: 0; batch classifier loss: 0.221908; batch adversarial loss: 0.274670\n",
      "epoch 181; iter: 0; batch classifier loss: 0.192421; batch adversarial loss: 0.341092\n",
      "epoch 182; iter: 0; batch classifier loss: 0.231356; batch adversarial loss: 0.207963\n",
      "epoch 183; iter: 0; batch classifier loss: 0.200895; batch adversarial loss: 0.305290\n",
      "epoch 184; iter: 0; batch classifier loss: 0.246225; batch adversarial loss: 0.231901\n",
      "epoch 185; iter: 0; batch classifier loss: 0.108883; batch adversarial loss: 0.184520\n",
      "epoch 186; iter: 0; batch classifier loss: 0.167189; batch adversarial loss: 0.301284\n",
      "epoch 187; iter: 0; batch classifier loss: 0.226091; batch adversarial loss: 0.247011\n",
      "epoch 188; iter: 0; batch classifier loss: 0.202977; batch adversarial loss: 0.331096\n",
      "epoch 189; iter: 0; batch classifier loss: 0.251261; batch adversarial loss: 0.261508\n",
      "epoch 190; iter: 0; batch classifier loss: 0.174347; batch adversarial loss: 0.264645\n",
      "epoch 191; iter: 0; batch classifier loss: 0.172748; batch adversarial loss: 0.278142\n",
      "epoch 192; iter: 0; batch classifier loss: 0.186318; batch adversarial loss: 0.239960\n",
      "epoch 193; iter: 0; batch classifier loss: 0.136994; batch adversarial loss: 0.293992\n",
      "epoch 194; iter: 0; batch classifier loss: 0.182270; batch adversarial loss: 0.269662\n",
      "epoch 195; iter: 0; batch classifier loss: 0.181457; batch adversarial loss: 0.301368\n",
      "epoch 196; iter: 0; batch classifier loss: 0.199523; batch adversarial loss: 0.239766\n",
      "epoch 197; iter: 0; batch classifier loss: 0.176631; batch adversarial loss: 0.361625\n",
      "epoch 198; iter: 0; batch classifier loss: 0.201212; batch adversarial loss: 0.244692\n",
      "epoch 199; iter: 0; batch classifier loss: 0.173786; batch adversarial loss: 0.245366\n"
     ]
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='LawSchoolDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d8270",
   "metadata": {},
   "source": [
    "### Experiment iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc40e12",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EXPERIMENT_SEEDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Configs for an experiment iteration\u001B[39;00m\n\u001B[1;32m      2\u001B[0m exp_iter_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m----> 3\u001B[0m experiment_seed \u001B[38;5;241m=\u001B[39m \u001B[43mEXPERIMENT_SEEDS\u001B[49m[exp_iter_num \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m      4\u001B[0m tuned_params_filenames \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      5\u001B[0m ]\n\u001B[1;32m      6\u001B[0m tuned_params_df_paths \u001B[38;5;241m=\u001B[39m [os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(ROOT_DIR, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiff_fairness_interventions_exp\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      7\u001B[0m                                       FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME, tuned_params_filename)\n\u001B[1;32m      8\u001B[0m                          \u001B[38;5;28;01mfor\u001B[39;00m tuned_params_filename \u001B[38;5;129;01min\u001B[39;00m tuned_params_filenames]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'EXPERIMENT_SEEDS' is not defined"
     ]
    }
   ],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 2\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be9a5b79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:56.249510Z",
     "start_time": "2024-01-04T20:53:56.233525Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 17:15:39 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 200,\n",
      " 'experiment_iteration': 'Exp_iter_2',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'intervention_param': 'True',\n",
      " 'model_init_seed': 200,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5645c6009d3b48d29619a238bc476502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 17:15:39 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-04 17:15:39 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([322, 293, 576, 300, 391, 343, 294, 558, 560, 439, 355, 440, 277,\n",
      "            492, 644, 639, 589, 259, 313, 129],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([322, 293, 576, 300, 391, 343, 294, 558, 560, 439, 355, 440, 277,\n",
      "            492, 644, 639, 589, 259, 313, 129],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 200, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c0277603884950a0c35cc9dcc3f539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7137564e8e0d4fcbbbfaaf03d73e170a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa30ab3dbbfe4acb8557a8c1e2d6556b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a36e32d08b44c1b01ae496881c6b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='LawSchoolDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834a569",
   "metadata": {},
   "source": [
    "### Experiment iteration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d130fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 3\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab940edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:46.750905Z",
     "start_time": "2024-01-04T20:53:46.744795Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 19:29:01 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 300,\n",
      " 'experiment_iteration': 'Exp_iter_3',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'model_init_seed': 300,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48b0aa26f024ad1b1089190e134193d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 19:29:01 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-04 19:29:01 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([250, 438, 479, 326,  46, 565, 534, 382, 377, 457,  97, 388, 123,\n",
      "            156, 430, 466,  38, 474, 167, 524],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([250, 438, 479, 326,  46, 565, 534, 382, 377, 457,  97, 388, 123,\n",
      "            156, 430, 466,  38, 474, 167, 524],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 300, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eddcae8f6fc4651885a3144088ae9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43ba8d965cc436a91f10cc39b3d5e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94495e7bec64c9ebf4c05cdd52e605d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f7167dd20740a68f8f90158ec19560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='LawSchoolDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9f5f7",
   "metadata": {},
   "source": [
    "### Experiment iteration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b363156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 4\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "048d1899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:37.675129Z",
     "start_time": "2024-01-04T20:53:37.670178Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 20:15:17 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 400,\n",
      " 'experiment_iteration': 'Exp_iter_4',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'intervention_param': 'True',\n",
      " 'model_init_seed': 400,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5b534568b14897b8686d22a9c79d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 20:15:17 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-04 20:15:17 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([331, 157, 559, 553, 580, 169, 561, 452, 180, 257, 160, 289, 197,\n",
      "             39, 290,  68,  56, 638,  54, 120],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([331, 157, 559, 553, 580, 169, 561, 452, 180, 257, 160, 289, 197,\n",
      "             39, 290,  68,  56, 638,  54, 120],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 400, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42164aebf0d74a5992f0ca8075639200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a128a0a62f40fb9bd0b88e41d55103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6300cf6db9cc4bcf95389b1f6e9b8108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e75feb8b5c4e00a1fc551544462dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='LawSchoolDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6ac04",
   "metadata": {},
   "source": [
    "### Experiment iteration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13c30aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 5\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4077068c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:27.080554Z",
     "start_time": "2024-01-04T20:53:27.072313Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 03:55:49 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 500,\n",
      " 'experiment_iteration': 'Exp_iter_5',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'model_init_seed': 500,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8257cfd4f743e8887bbc74613694af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 03:55:49 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-05 03:55:49 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([ 92, 640, 589, 519, 377, 478, 298, 336, 149, 278, 343, 573, 365,\n",
      "            174, 171, 219, 469, 162, 567, 203],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([ 92, 640, 589, 519, 377, 478, 298, 336, 149, 278, 343, 573, 365,\n",
      "            174, 171, 219, 469, 162, 567, 203],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 500, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2d40e5f54c4d67bf9f1e66289ae23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63535df3c576469e9e855b70194bc572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15688b8597f74cdaadb60cc93599720c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3165f4884fae4f29829de3acc475dac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='LawSchoolDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f1dfe",
   "metadata": {},
   "source": [
    "### Experiment iteration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e80e4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 6\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad31a7be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:16.632770Z",
     "start_time": "2024-01-04T20:53:16.629083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 04:49:43 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 600,\n",
      " 'experiment_iteration': 'Exp_iter_6',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'intervention_param': 'True',\n",
      " 'model_init_seed': 600,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2670abffe30e4a388a7a09306feb6673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 04:49:43 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-05 04:49:43 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([390, 397, 375, 507, 625, 369, 245, 428, 593, 102,  49, 621,  92,\n",
      "            158, 505, 612,  29, 452,  56, 145],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([390, 397, 375, 507, 625, 369, 245, 428, 593, 102,  49, 621,  92,\n",
      "            158, 505, 612,  29, 452,  56, 145],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 600, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f28080438649c0aa4a0f71f4b322c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511b20dacfe5420981d3cf173f60fc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40a989d937f483784b5138783479302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c5fe28a14a47118da3a7d485a77e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='LawSchoolDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb640e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
