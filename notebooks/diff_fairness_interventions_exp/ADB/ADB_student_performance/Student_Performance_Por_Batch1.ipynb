{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ded11654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5d85e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T10:55:54.052462Z",
     "start_time": "2024-01-06T10:55:54.038357Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall virny -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf024b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T10:56:09.679156Z",
     "start_time": "2024-01-06T10:56:09.668186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install using an HTTP link\n",
    "# !pip install git+https://github.com/DataResponsibly/Virny.git@feature/prepare_experiments_for_inprocessors\n",
    "\n",
    "# Install using an SSH link\n",
    "# !pip install git+ssh://git@github.com/DataResponsibly/Virny.git@feature/prepare_experiments_for_inprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39463327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:26.457257Z",
     "start_time": "2024-01-06T11:15:26.114625Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d83962",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:26.466361Z",
     "start_time": "2024-01-06T11:15:26.457627Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295621db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:26.478005Z",
     "start_time": "2024-01-06T11:15:26.467253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current location:  /home/denys_herasymuk/Research/NYU/Code/fairness-variance\n"
     ]
    }
   ],
   "source": [
    "cur_folder_name = os.getcwd().split('/')[-1]\n",
    "if cur_folder_name != \"fairness-variance\":\n",
    "    os.chdir(\"../../../..\")\n",
    "\n",
    "print('Current location: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d6dfa",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65442379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:31.734704Z",
     "start_time": "2024-01-06T11:15:28.036691Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n",
      "2024-01-08 08:22:00.950805: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-08 08:22:01.007694: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-08 08:22:01.008623: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-08 08:22:02.031392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from virny.utils.custom_initializers import create_config_obj\n",
    "from virny.datasets import StudentPerformancePortugueseDataset\n",
    "\n",
    "from configs.constants import TEST_SET_FRACTION, EXPERIMENT_SEEDS\n",
    "\n",
    "from source.experiment_interface import run_exp_iter_with_inprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb488976",
   "metadata": {},
   "source": [
    "## Define Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1f91c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:31.772286Z",
     "start_time": "2024-01-06T11:15:31.735883Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "EXPERIMENT_NAME = 'ADB_student_performance'\n",
    "DB_COLLECTION_NAME = 'one_repair_lvl_many_models'\n",
    "FAIRNESS_INTERVENTION_NAME = 'ADB'\n",
    "FAIR_INTERVENTION_PARAMS_LST = ['debiased_classifier']\n",
    "SAVE_RESULTS_DIR_PATH = os.path.join(ROOT_DIR, 'results', 'diff_fairness_interventions_exp',\n",
    "                                     FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME)\n",
    "\n",
    "config_yaml_path = os.path.join(ROOT_DIR, 'notebooks', 'diff_fairness_interventions_exp',\n",
    "                                FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME, 'student_performance_por_config.yaml')\n",
    "metrics_computation_config = create_config_obj(config_yaml_path=config_yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42b20f",
   "metadata": {},
   "source": [
    "## Define a db writer and custom fields to insert into your database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ed4e58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:31.813421Z",
     "start_time": "2024-01-06T11:15:31.771935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'fairness_variance'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./configs/secrets.env')\n",
    "os.getenv(\"DB_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2539023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:32.096974Z",
     "start_time": "2024-01-06T11:15:31.811395Z"
    }
   },
   "outputs": [],
   "source": [
    "from source.utils.db_functions import connect_to_mongodb\n",
    "\n",
    "client, collection_obj, db_writer_func = connect_to_mongodb(DB_COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e9682b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:32.138747Z",
     "start_time": "2024-01-06T11:15:32.097343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current session uuid:  45fc7ac3-18e1-4c93-8fd3-f144427b9fc4\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "custom_table_fields_dct = {\n",
    "    # 'session_uuid': str(uuid.uuid4()),\n",
    "    'session_uuid': '45fc7ac3-18e1-4c93-8fd3-f144427b9fc4',\n",
    "}\n",
    "print('Current session uuid: ', custom_table_fields_dct['session_uuid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b32200",
   "metadata": {},
   "source": [
    "## Initialize custom objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7470042d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:33.528732Z",
     "start_time": "2024-01-06T11:15:33.475702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   age  Medu  Fedu  traveltime  studytime  failures  famrel  freetime  goout  \\\n0   18     4     4           2          2         0       4         3      4   \n1   17     1     1           1          2         0       5         3      3   \n2   15     1     1           1          2         0       4         3      2   \n3   15     4     2           1          3         0       3         2      2   \n4   16     3     3           1          2         0       4         3      2   \n\n   Dalc  ...  reason  guardian  schoolsup  famsup  paid activities nursery  \\\n0     1  ...  course    mother        yes      no    no         no     yes   \n1     1  ...  course    father         no     yes    no         no      no   \n2     2  ...   other    mother        yes      no    no         no     yes   \n3     1  ...    home    mother         no     yes    no        yes     yes   \n4     1  ...    home    father         no     yes    no         no     yes   \n\n  higher internet romantic  \n0    yes       no       no  \n1    yes      yes       no  \n2    yes      yes       no  \n3    yes      yes      yes  \n4    yes       no       no  \n\n[5 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>Medu</th>\n      <th>Fedu</th>\n      <th>traveltime</th>\n      <th>studytime</th>\n      <th>failures</th>\n      <th>famrel</th>\n      <th>freetime</th>\n      <th>goout</th>\n      <th>Dalc</th>\n      <th>...</th>\n      <th>reason</th>\n      <th>guardian</th>\n      <th>schoolsup</th>\n      <th>famsup</th>\n      <th>paid</th>\n      <th>activities</th>\n      <th>nursery</th>\n      <th>higher</th>\n      <th>internet</th>\n      <th>romantic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>...</td>\n      <td>course</td>\n      <td>mother</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>no</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>...</td>\n      <td>course</td>\n      <td>father</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>no</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>other</td>\n      <td>mother</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>no</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>home</td>\n      <td>mother</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>home</td>\n      <td>father</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>no</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = StudentPerformancePortugueseDataset()\n",
    "data_loader.X_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cce54a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:34.580537Z",
     "start_time": "2024-01-06T11:15:34.538952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(649, 32)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.X_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c8f07",
   "metadata": {},
   "source": [
    "## Run experiment iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced15cb",
   "metadata": {},
   "source": [
    "### Experiment iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c9302f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:37.135031Z",
     "start_time": "2024-01-06T11:15:37.105079Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 1\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26d52fce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:44.618835Z",
     "start_time": "2024-01-06T11:15:43.745040Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:29 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 100,\n",
      " 'experiment_iteration': 'Exp_iter_1',\n",
      " 'fair_intervention_params_lst': \"['debiased_classifier']\",\n",
      " 'model_init_seed': 100,\n",
      " 'session_uuid': '45fc7ac3-18e1-4c93-8fd3-f144427b9fc4'}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "311d5b3f907d476abb0ff9ef1b221bc7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:29 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  debiased_classifier\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([518, 476, 240, 156, 366, 133, 103, 565, 567,  73, 642, 412, 490,\n",
      "            340, 287, 427, 142, 397, 267, 216],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([518, 476, 240, 156, 366, 133, 103, 565, 567,  73, 642, 412, 490,\n",
      "            340, 287, 427, 142, 397, 267, 216],\n",
      "           dtype='int64')\n",
      "Using AdversarialDebiasing postprocessor\n"
     ]
    },
    {
     "data": {
      "text/plain": "Analyze multiple models:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a19557fde21b4ef4bc266ec10bfef7fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Classifiers testing by bootstrap:   0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cea5bce365cf4504a63a76c0a026124e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/denys_herasymuk/Research/NYU/Code/fairness-variance/faact_venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2024-01-08 08:22:29.838746: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.859421; batch adversarial loss: 0.797385\n",
      "epoch 1; iter: 0; batch classifier loss: 0.712628; batch adversarial loss: 0.814061\n",
      "epoch 2; iter: 0; batch classifier loss: 0.647951; batch adversarial loss: 0.908682\n",
      "epoch 3; iter: 0; batch classifier loss: 0.518344; batch adversarial loss: 0.917374\n",
      "epoch 4; iter: 0; batch classifier loss: 0.474816; batch adversarial loss: 0.921436\n",
      "epoch 5; iter: 0; batch classifier loss: 0.421509; batch adversarial loss: 1.050132\n",
      "epoch 6; iter: 0; batch classifier loss: 0.380887; batch adversarial loss: 1.047989\n",
      "epoch 7; iter: 0; batch classifier loss: 0.328370; batch adversarial loss: 0.956158\n",
      "epoch 8; iter: 0; batch classifier loss: 0.327638; batch adversarial loss: 1.119557\n",
      "epoch 9; iter: 0; batch classifier loss: 0.308185; batch adversarial loss: 0.976770\n",
      "epoch 10; iter: 0; batch classifier loss: 0.232390; batch adversarial loss: 0.996092\n",
      "epoch 11; iter: 0; batch classifier loss: 0.311260; batch adversarial loss: 1.101273\n",
      "epoch 12; iter: 0; batch classifier loss: 0.218960; batch adversarial loss: 1.147074\n",
      "epoch 13; iter: 0; batch classifier loss: 0.226678; batch adversarial loss: 1.062270\n",
      "epoch 14; iter: 0; batch classifier loss: 0.213547; batch adversarial loss: 1.127749\n",
      "epoch 15; iter: 0; batch classifier loss: 0.181250; batch adversarial loss: 1.142426\n",
      "epoch 16; iter: 0; batch classifier loss: 0.188693; batch adversarial loss: 1.084773\n",
      "epoch 17; iter: 0; batch classifier loss: 0.217238; batch adversarial loss: 1.122229\n",
      "epoch 18; iter: 0; batch classifier loss: 0.155305; batch adversarial loss: 1.134814\n",
      "epoch 19; iter: 0; batch classifier loss: 0.176187; batch adversarial loss: 1.140853\n",
      "epoch 20; iter: 0; batch classifier loss: 0.147408; batch adversarial loss: 1.125422\n",
      "epoch 21; iter: 0; batch classifier loss: 0.161636; batch adversarial loss: 1.026333\n",
      "epoch 22; iter: 0; batch classifier loss: 0.130232; batch adversarial loss: 1.118147\n",
      "epoch 23; iter: 0; batch classifier loss: 0.127340; batch adversarial loss: 1.043319\n",
      "epoch 24; iter: 0; batch classifier loss: 0.123281; batch adversarial loss: 1.103807\n",
      "epoch 25; iter: 0; batch classifier loss: 0.107799; batch adversarial loss: 1.188526\n",
      "epoch 26; iter: 0; batch classifier loss: 0.138296; batch adversarial loss: 1.121267\n",
      "epoch 27; iter: 0; batch classifier loss: 0.127645; batch adversarial loss: 1.022050\n",
      "epoch 28; iter: 0; batch classifier loss: 0.112233; batch adversarial loss: 1.125267\n",
      "epoch 29; iter: 0; batch classifier loss: 0.109560; batch adversarial loss: 1.021606\n",
      "epoch 30; iter: 0; batch classifier loss: 0.103204; batch adversarial loss: 1.112685\n",
      "epoch 31; iter: 0; batch classifier loss: 0.127003; batch adversarial loss: 1.018174\n",
      "epoch 32; iter: 0; batch classifier loss: 0.130829; batch adversarial loss: 1.056712\n",
      "epoch 33; iter: 0; batch classifier loss: 0.127335; batch adversarial loss: 1.028772\n",
      "epoch 34; iter: 0; batch classifier loss: 0.080012; batch adversarial loss: 1.108402\n",
      "epoch 35; iter: 0; batch classifier loss: 0.090882; batch adversarial loss: 1.131028\n",
      "epoch 36; iter: 0; batch classifier loss: 0.110192; batch adversarial loss: 1.009525\n",
      "epoch 37; iter: 0; batch classifier loss: 0.095955; batch adversarial loss: 1.014652\n",
      "epoch 38; iter: 0; batch classifier loss: 0.103607; batch adversarial loss: 1.119784\n",
      "epoch 39; iter: 0; batch classifier loss: 0.075164; batch adversarial loss: 1.075910\n",
      "epoch 40; iter: 0; batch classifier loss: 0.082186; batch adversarial loss: 0.977574\n",
      "epoch 41; iter: 0; batch classifier loss: 0.085887; batch adversarial loss: 1.074690\n",
      "epoch 42; iter: 0; batch classifier loss: 0.087297; batch adversarial loss: 1.024078\n",
      "epoch 43; iter: 0; batch classifier loss: 0.067310; batch adversarial loss: 0.998022\n",
      "epoch 44; iter: 0; batch classifier loss: 0.080923; batch adversarial loss: 1.047373\n",
      "epoch 45; iter: 0; batch classifier loss: 0.071769; batch adversarial loss: 0.989431\n",
      "epoch 46; iter: 0; batch classifier loss: 0.081472; batch adversarial loss: 1.044459\n",
      "epoch 47; iter: 0; batch classifier loss: 0.066790; batch adversarial loss: 1.038337\n",
      "epoch 48; iter: 0; batch classifier loss: 0.053493; batch adversarial loss: 1.043154\n",
      "epoch 49; iter: 0; batch classifier loss: 0.058754; batch adversarial loss: 1.081035\n",
      "epoch 50; iter: 0; batch classifier loss: 0.054614; batch adversarial loss: 1.016324\n",
      "epoch 51; iter: 0; batch classifier loss: 0.082057; batch adversarial loss: 0.995694\n",
      "epoch 52; iter: 0; batch classifier loss: 0.049630; batch adversarial loss: 1.024744\n",
      "epoch 53; iter: 0; batch classifier loss: 0.054824; batch adversarial loss: 0.978492\n",
      "epoch 54; iter: 0; batch classifier loss: 0.061523; batch adversarial loss: 1.032812\n",
      "epoch 55; iter: 0; batch classifier loss: 0.044744; batch adversarial loss: 1.003747\n",
      "epoch 56; iter: 0; batch classifier loss: 0.046710; batch adversarial loss: 0.936380\n",
      "epoch 57; iter: 0; batch classifier loss: 0.057998; batch adversarial loss: 1.001612\n",
      "epoch 58; iter: 0; batch classifier loss: 0.045125; batch adversarial loss: 0.978205\n",
      "epoch 59; iter: 0; batch classifier loss: 0.045895; batch adversarial loss: 0.992589\n",
      "epoch 60; iter: 0; batch classifier loss: 0.049399; batch adversarial loss: 1.048289\n",
      "epoch 61; iter: 0; batch classifier loss: 0.042090; batch adversarial loss: 0.985336\n",
      "epoch 62; iter: 0; batch classifier loss: 0.029813; batch adversarial loss: 0.978576\n",
      "epoch 63; iter: 0; batch classifier loss: 0.047392; batch adversarial loss: 0.966037\n",
      "epoch 64; iter: 0; batch classifier loss: 0.028542; batch adversarial loss: 1.003989\n",
      "epoch 65; iter: 0; batch classifier loss: 0.057650; batch adversarial loss: 0.925582\n",
      "epoch 66; iter: 0; batch classifier loss: 0.042705; batch adversarial loss: 0.995541\n",
      "epoch 67; iter: 0; batch classifier loss: 0.042620; batch adversarial loss: 0.996627\n",
      "epoch 68; iter: 0; batch classifier loss: 0.054002; batch adversarial loss: 0.993796\n",
      "epoch 69; iter: 0; batch classifier loss: 0.028087; batch adversarial loss: 0.992145\n",
      "epoch 70; iter: 0; batch classifier loss: 0.030483; batch adversarial loss: 0.957186\n",
      "epoch 71; iter: 0; batch classifier loss: 0.049653; batch adversarial loss: 0.967102\n",
      "epoch 72; iter: 0; batch classifier loss: 0.042918; batch adversarial loss: 0.905235\n",
      "epoch 73; iter: 0; batch classifier loss: 0.042650; batch adversarial loss: 0.966253\n",
      "epoch 74; iter: 0; batch classifier loss: 0.031296; batch adversarial loss: 0.896870\n",
      "epoch 75; iter: 0; batch classifier loss: 0.038622; batch adversarial loss: 0.955555\n",
      "epoch 76; iter: 0; batch classifier loss: 0.028829; batch adversarial loss: 0.936691\n",
      "epoch 77; iter: 0; batch classifier loss: 0.033291; batch adversarial loss: 0.955819\n",
      "epoch 78; iter: 0; batch classifier loss: 0.044920; batch adversarial loss: 1.033992\n",
      "epoch 79; iter: 0; batch classifier loss: 0.048541; batch adversarial loss: 0.981379\n",
      "epoch 80; iter: 0; batch classifier loss: 0.027061; batch adversarial loss: 1.006946\n",
      "epoch 81; iter: 0; batch classifier loss: 0.043214; batch adversarial loss: 1.004808\n",
      "epoch 82; iter: 0; batch classifier loss: 0.023407; batch adversarial loss: 0.899087\n",
      "epoch 83; iter: 0; batch classifier loss: 0.034232; batch adversarial loss: 0.997768\n",
      "epoch 84; iter: 0; batch classifier loss: 0.038085; batch adversarial loss: 0.967726\n",
      "epoch 85; iter: 0; batch classifier loss: 0.027735; batch adversarial loss: 0.977797\n",
      "epoch 86; iter: 0; batch classifier loss: 0.021574; batch adversarial loss: 0.852418\n",
      "epoch 87; iter: 0; batch classifier loss: 0.024726; batch adversarial loss: 0.940579\n",
      "epoch 88; iter: 0; batch classifier loss: 0.032768; batch adversarial loss: 0.886046\n",
      "epoch 89; iter: 0; batch classifier loss: 0.028493; batch adversarial loss: 0.908876\n",
      "epoch 90; iter: 0; batch classifier loss: 0.030351; batch adversarial loss: 0.888904\n",
      "epoch 91; iter: 0; batch classifier loss: 0.023555; batch adversarial loss: 0.932851\n",
      "epoch 92; iter: 0; batch classifier loss: 0.027141; batch adversarial loss: 0.913657\n",
      "epoch 93; iter: 0; batch classifier loss: 0.017456; batch adversarial loss: 0.868541\n",
      "epoch 94; iter: 0; batch classifier loss: 0.018622; batch adversarial loss: 0.940978\n",
      "epoch 95; iter: 0; batch classifier loss: 0.033298; batch adversarial loss: 0.908269\n",
      "epoch 96; iter: 0; batch classifier loss: 0.033125; batch adversarial loss: 0.892098\n",
      "epoch 97; iter: 0; batch classifier loss: 0.022605; batch adversarial loss: 0.915290\n",
      "epoch 98; iter: 0; batch classifier loss: 0.023118; batch adversarial loss: 0.954322\n",
      "epoch 99; iter: 0; batch classifier loss: 0.047534; batch adversarial loss: 0.891805\n",
      "epoch 100; iter: 0; batch classifier loss: 0.029653; batch adversarial loss: 0.943148\n",
      "epoch 101; iter: 0; batch classifier loss: 0.016151; batch adversarial loss: 0.916658\n",
      "epoch 102; iter: 0; batch classifier loss: 0.026662; batch adversarial loss: 0.910737\n",
      "epoch 103; iter: 0; batch classifier loss: 0.032214; batch adversarial loss: 0.878582\n",
      "epoch 104; iter: 0; batch classifier loss: 0.017876; batch adversarial loss: 0.835941\n",
      "epoch 105; iter: 0; batch classifier loss: 0.019666; batch adversarial loss: 0.871001\n",
      "epoch 106; iter: 0; batch classifier loss: 0.024004; batch adversarial loss: 0.872466\n",
      "epoch 107; iter: 0; batch classifier loss: 0.022324; batch adversarial loss: 0.836335\n",
      "epoch 108; iter: 0; batch classifier loss: 0.029795; batch adversarial loss: 0.871003\n",
      "epoch 109; iter: 0; batch classifier loss: 0.023044; batch adversarial loss: 0.884734\n",
      "epoch 110; iter: 0; batch classifier loss: 0.029044; batch adversarial loss: 0.890401\n",
      "epoch 111; iter: 0; batch classifier loss: 0.024822; batch adversarial loss: 0.857627\n",
      "epoch 112; iter: 0; batch classifier loss: 0.029021; batch adversarial loss: 0.858468\n",
      "epoch 113; iter: 0; batch classifier loss: 0.016255; batch adversarial loss: 0.817353\n",
      "epoch 114; iter: 0; batch classifier loss: 0.021474; batch adversarial loss: 0.892374\n",
      "epoch 115; iter: 0; batch classifier loss: 0.012470; batch adversarial loss: 0.873849\n",
      "epoch 116; iter: 0; batch classifier loss: 0.017560; batch adversarial loss: 0.887202\n",
      "epoch 117; iter: 0; batch classifier loss: 0.028348; batch adversarial loss: 0.844105\n",
      "epoch 118; iter: 0; batch classifier loss: 0.023421; batch adversarial loss: 0.858430\n",
      "epoch 119; iter: 0; batch classifier loss: 0.016533; batch adversarial loss: 0.846732\n",
      "epoch 120; iter: 0; batch classifier loss: 0.007300; batch adversarial loss: 0.908197\n",
      "epoch 121; iter: 0; batch classifier loss: 0.019599; batch adversarial loss: 0.881108\n",
      "epoch 122; iter: 0; batch classifier loss: 0.018910; batch adversarial loss: 0.869304\n",
      "epoch 123; iter: 0; batch classifier loss: 0.012408; batch adversarial loss: 0.847879\n",
      "epoch 124; iter: 0; batch classifier loss: 0.016149; batch adversarial loss: 0.843350\n",
      "epoch 125; iter: 0; batch classifier loss: 0.021487; batch adversarial loss: 0.833033\n",
      "epoch 126; iter: 0; batch classifier loss: 0.018001; batch adversarial loss: 0.865777\n",
      "epoch 127; iter: 0; batch classifier loss: 0.012647; batch adversarial loss: 0.855268\n",
      "epoch 128; iter: 0; batch classifier loss: 0.018325; batch adversarial loss: 0.840267\n",
      "epoch 129; iter: 0; batch classifier loss: 0.010233; batch adversarial loss: 0.906570\n",
      "epoch 130; iter: 0; batch classifier loss: 0.011515; batch adversarial loss: 0.835385\n",
      "epoch 131; iter: 0; batch classifier loss: 0.010788; batch adversarial loss: 0.797089\n",
      "epoch 132; iter: 0; batch classifier loss: 0.016626; batch adversarial loss: 0.842504\n",
      "epoch 133; iter: 0; batch classifier loss: 0.014197; batch adversarial loss: 0.849342\n",
      "epoch 134; iter: 0; batch classifier loss: 0.016525; batch adversarial loss: 0.812344\n",
      "epoch 135; iter: 0; batch classifier loss: 0.021951; batch adversarial loss: 0.805808\n",
      "epoch 136; iter: 0; batch classifier loss: 0.014434; batch adversarial loss: 0.867005\n",
      "epoch 137; iter: 0; batch classifier loss: 0.014610; batch adversarial loss: 0.804780\n",
      "epoch 138; iter: 0; batch classifier loss: 0.016672; batch adversarial loss: 0.838416\n",
      "epoch 139; iter: 0; batch classifier loss: 0.025230; batch adversarial loss: 0.771952\n",
      "epoch 140; iter: 0; batch classifier loss: 0.013097; batch adversarial loss: 0.816384\n",
      "epoch 141; iter: 0; batch classifier loss: 0.019221; batch adversarial loss: 0.797829\n",
      "epoch 142; iter: 0; batch classifier loss: 0.009069; batch adversarial loss: 0.876118\n",
      "epoch 143; iter: 0; batch classifier loss: 0.012327; batch adversarial loss: 0.856969\n",
      "epoch 144; iter: 0; batch classifier loss: 0.014385; batch adversarial loss: 0.799090\n",
      "epoch 145; iter: 0; batch classifier loss: 0.015566; batch adversarial loss: 0.823430\n",
      "epoch 146; iter: 0; batch classifier loss: 0.005462; batch adversarial loss: 0.842801\n",
      "epoch 147; iter: 0; batch classifier loss: 0.017485; batch adversarial loss: 0.817397\n",
      "epoch 148; iter: 0; batch classifier loss: 0.006869; batch adversarial loss: 0.819978\n",
      "epoch 149; iter: 0; batch classifier loss: 0.018062; batch adversarial loss: 0.813267\n",
      "epoch 150; iter: 0; batch classifier loss: 0.008973; batch adversarial loss: 0.800797\n",
      "epoch 151; iter: 0; batch classifier loss: 0.013890; batch adversarial loss: 0.812824\n",
      "epoch 152; iter: 0; batch classifier loss: 0.004728; batch adversarial loss: 0.782996\n",
      "epoch 153; iter: 0; batch classifier loss: 0.011650; batch adversarial loss: 0.795261\n",
      "epoch 154; iter: 0; batch classifier loss: 0.012102; batch adversarial loss: 0.788565\n",
      "epoch 155; iter: 0; batch classifier loss: 0.012848; batch adversarial loss: 0.813844\n",
      "epoch 156; iter: 0; batch classifier loss: 0.009068; batch adversarial loss: 0.808735\n",
      "epoch 157; iter: 0; batch classifier loss: 0.006323; batch adversarial loss: 0.794829\n",
      "epoch 158; iter: 0; batch classifier loss: 0.012442; batch adversarial loss: 0.768026\n",
      "epoch 159; iter: 0; batch classifier loss: 0.006026; batch adversarial loss: 0.758968\n",
      "epoch 160; iter: 0; batch classifier loss: 0.007751; batch adversarial loss: 0.768163\n",
      "epoch 161; iter: 0; batch classifier loss: 0.007583; batch adversarial loss: 0.807834\n",
      "epoch 162; iter: 0; batch classifier loss: 0.005184; batch adversarial loss: 0.796171\n",
      "epoch 163; iter: 0; batch classifier loss: 0.007713; batch adversarial loss: 0.777208\n",
      "epoch 164; iter: 0; batch classifier loss: 0.008886; batch adversarial loss: 0.800370\n",
      "epoch 165; iter: 0; batch classifier loss: 0.019808; batch adversarial loss: 0.806139\n",
      "epoch 166; iter: 0; batch classifier loss: 0.007787; batch adversarial loss: 0.771416\n",
      "epoch 167; iter: 0; batch classifier loss: 0.008952; batch adversarial loss: 0.782783\n",
      "epoch 168; iter: 0; batch classifier loss: 0.006887; batch adversarial loss: 0.773382\n",
      "epoch 169; iter: 0; batch classifier loss: 0.006752; batch adversarial loss: 0.803341\n",
      "epoch 170; iter: 0; batch classifier loss: 0.008693; batch adversarial loss: 0.795780\n",
      "epoch 171; iter: 0; batch classifier loss: 0.003988; batch adversarial loss: 0.787611\n",
      "epoch 172; iter: 0; batch classifier loss: 0.014608; batch adversarial loss: 0.801381\n",
      "epoch 173; iter: 0; batch classifier loss: 0.009324; batch adversarial loss: 0.770365\n",
      "epoch 174; iter: 0; batch classifier loss: 0.006343; batch adversarial loss: 0.758737\n",
      "epoch 175; iter: 0; batch classifier loss: 0.012277; batch adversarial loss: 0.751648\n",
      "epoch 176; iter: 0; batch classifier loss: 0.007064; batch adversarial loss: 0.753125\n",
      "epoch 177; iter: 0; batch classifier loss: 0.006027; batch adversarial loss: 0.777951\n",
      "epoch 178; iter: 0; batch classifier loss: 0.005164; batch adversarial loss: 0.777716\n",
      "epoch 179; iter: 0; batch classifier loss: 0.011556; batch adversarial loss: 0.755903\n",
      "epoch 180; iter: 0; batch classifier loss: 0.008749; batch adversarial loss: 0.774290\n",
      "epoch 181; iter: 0; batch classifier loss: 0.008338; batch adversarial loss: 0.773308\n",
      "epoch 182; iter: 0; batch classifier loss: 0.012605; batch adversarial loss: 0.767359\n",
      "epoch 183; iter: 0; batch classifier loss: 0.008305; batch adversarial loss: 0.782246\n",
      "epoch 184; iter: 0; batch classifier loss: 0.016257; batch adversarial loss: 0.784304\n",
      "epoch 185; iter: 0; batch classifier loss: 0.008046; batch adversarial loss: 0.764785\n",
      "epoch 186; iter: 0; batch classifier loss: 0.006686; batch adversarial loss: 0.785021\n",
      "epoch 187; iter: 0; batch classifier loss: 0.006220; batch adversarial loss: 0.738228\n",
      "epoch 188; iter: 0; batch classifier loss: 0.006816; batch adversarial loss: 0.745888\n",
      "epoch 189; iter: 0; batch classifier loss: 0.005407; batch adversarial loss: 0.770489\n",
      "epoch 190; iter: 0; batch classifier loss: 0.003829; batch adversarial loss: 0.757201\n",
      "epoch 191; iter: 0; batch classifier loss: 0.007210; batch adversarial loss: 0.749100\n",
      "epoch 192; iter: 0; batch classifier loss: 0.011082; batch adversarial loss: 0.759375\n",
      "epoch 193; iter: 0; batch classifier loss: 0.006283; batch adversarial loss: 0.750888\n",
      "epoch 194; iter: 0; batch classifier loss: 0.004606; batch adversarial loss: 0.750087\n",
      "epoch 195; iter: 0; batch classifier loss: 0.006140; batch adversarial loss: 0.760627\n",
      "epoch 196; iter: 0; batch classifier loss: 0.003087; batch adversarial loss: 0.740286\n",
      "epoch 197; iter: 0; batch classifier loss: 0.006830; batch adversarial loss: 0.763256\n",
      "epoch 198; iter: 0; batch classifier loss: 0.006660; batch adversarial loss: 0.752855\n",
      "epoch 199; iter: 0; batch classifier loss: 0.010980; batch adversarial loss: 0.728790\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:31.368806: W tensorflow/c/c_api.cc:304] Operation '{name:'4c968858-adee-11ee-9362-a9d23602d4a3/4c968858-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:784 op device:{requested: '', assigned: ''} def:{{{node 4c968858-adee-11ee-9362-a9d23602d4a3/4c968858-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c968858-adee-11ee-9362-a9d23602d4a3/4c968858-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c968858-adee-11ee-9362-a9d23602d4a3/4c968858-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.695114; batch adversarial loss: 0.846313\n",
      "epoch 1; iter: 0; batch classifier loss: 0.617020; batch adversarial loss: 0.822914\n",
      "epoch 2; iter: 0; batch classifier loss: 0.497291; batch adversarial loss: 0.855172\n",
      "epoch 3; iter: 0; batch classifier loss: 0.472627; batch adversarial loss: 0.809875\n",
      "epoch 4; iter: 0; batch classifier loss: 0.397790; batch adversarial loss: 0.909134\n",
      "epoch 5; iter: 0; batch classifier loss: 0.338072; batch adversarial loss: 0.943514\n",
      "epoch 6; iter: 0; batch classifier loss: 0.285986; batch adversarial loss: 0.998233\n",
      "epoch 7; iter: 0; batch classifier loss: 0.287476; batch adversarial loss: 0.974600\n",
      "epoch 8; iter: 0; batch classifier loss: 0.274831; batch adversarial loss: 0.974906\n",
      "epoch 9; iter: 0; batch classifier loss: 0.190587; batch adversarial loss: 0.982216\n",
      "epoch 10; iter: 0; batch classifier loss: 0.160630; batch adversarial loss: 0.973268\n",
      "epoch 11; iter: 0; batch classifier loss: 0.166136; batch adversarial loss: 0.956363\n",
      "epoch 12; iter: 0; batch classifier loss: 0.173860; batch adversarial loss: 0.967300\n",
      "epoch 13; iter: 0; batch classifier loss: 0.144095; batch adversarial loss: 0.942496\n",
      "epoch 14; iter: 0; batch classifier loss: 0.136550; batch adversarial loss: 0.955070\n",
      "epoch 15; iter: 0; batch classifier loss: 0.169743; batch adversarial loss: 0.994492\n",
      "epoch 16; iter: 0; batch classifier loss: 0.126385; batch adversarial loss: 0.984001\n",
      "epoch 17; iter: 0; batch classifier loss: 0.124230; batch adversarial loss: 0.905388\n",
      "epoch 18; iter: 0; batch classifier loss: 0.142769; batch adversarial loss: 0.922689\n",
      "epoch 19; iter: 0; batch classifier loss: 0.145888; batch adversarial loss: 0.943199\n",
      "epoch 20; iter: 0; batch classifier loss: 0.162160; batch adversarial loss: 0.975777\n",
      "epoch 21; iter: 0; batch classifier loss: 0.117434; batch adversarial loss: 0.954077\n",
      "epoch 22; iter: 0; batch classifier loss: 0.133084; batch adversarial loss: 0.887560\n",
      "epoch 23; iter: 0; batch classifier loss: 0.116541; batch adversarial loss: 0.959635\n",
      "epoch 24; iter: 0; batch classifier loss: 0.107448; batch adversarial loss: 0.934442\n",
      "epoch 25; iter: 0; batch classifier loss: 0.102028; batch adversarial loss: 0.944699\n",
      "epoch 26; iter: 0; batch classifier loss: 0.083678; batch adversarial loss: 0.925307\n",
      "epoch 27; iter: 0; batch classifier loss: 0.095062; batch adversarial loss: 0.941053\n",
      "epoch 28; iter: 0; batch classifier loss: 0.083265; batch adversarial loss: 0.953195\n",
      "epoch 29; iter: 0; batch classifier loss: 0.132001; batch adversarial loss: 0.882704\n",
      "epoch 30; iter: 0; batch classifier loss: 0.067891; batch adversarial loss: 0.936494\n",
      "epoch 31; iter: 0; batch classifier loss: 0.086933; batch adversarial loss: 0.889752\n",
      "epoch 32; iter: 0; batch classifier loss: 0.093799; batch adversarial loss: 0.915741\n",
      "epoch 33; iter: 0; batch classifier loss: 0.074872; batch adversarial loss: 0.906499\n",
      "epoch 34; iter: 0; batch classifier loss: 0.095266; batch adversarial loss: 0.908263\n",
      "epoch 35; iter: 0; batch classifier loss: 0.060630; batch adversarial loss: 0.892200\n",
      "epoch 36; iter: 0; batch classifier loss: 0.076127; batch adversarial loss: 0.930999\n",
      "epoch 37; iter: 0; batch classifier loss: 0.056468; batch adversarial loss: 0.918237\n",
      "epoch 38; iter: 0; batch classifier loss: 0.074955; batch adversarial loss: 0.823424\n",
      "epoch 39; iter: 0; batch classifier loss: 0.062571; batch adversarial loss: 0.915623\n",
      "epoch 40; iter: 0; batch classifier loss: 0.087082; batch adversarial loss: 0.898513\n",
      "epoch 41; iter: 0; batch classifier loss: 0.054932; batch adversarial loss: 0.909486\n",
      "epoch 42; iter: 0; batch classifier loss: 0.063734; batch adversarial loss: 0.897914\n",
      "epoch 43; iter: 0; batch classifier loss: 0.063134; batch adversarial loss: 0.887614\n",
      "epoch 44; iter: 0; batch classifier loss: 0.089645; batch adversarial loss: 0.894076\n",
      "epoch 45; iter: 0; batch classifier loss: 0.055044; batch adversarial loss: 0.911777\n",
      "epoch 46; iter: 0; batch classifier loss: 0.077544; batch adversarial loss: 0.909825\n",
      "epoch 47; iter: 0; batch classifier loss: 0.045405; batch adversarial loss: 0.849125\n",
      "epoch 48; iter: 0; batch classifier loss: 0.064316; batch adversarial loss: 0.886035\n",
      "epoch 49; iter: 0; batch classifier loss: 0.046632; batch adversarial loss: 0.885783\n",
      "epoch 50; iter: 0; batch classifier loss: 0.033868; batch adversarial loss: 0.880277\n",
      "epoch 51; iter: 0; batch classifier loss: 0.057555; batch adversarial loss: 0.856777\n",
      "epoch 52; iter: 0; batch classifier loss: 0.051875; batch adversarial loss: 0.880712\n",
      "epoch 53; iter: 0; batch classifier loss: 0.045851; batch adversarial loss: 0.859115\n",
      "epoch 54; iter: 0; batch classifier loss: 0.049012; batch adversarial loss: 0.909361\n",
      "epoch 55; iter: 0; batch classifier loss: 0.028179; batch adversarial loss: 0.926988\n",
      "epoch 56; iter: 0; batch classifier loss: 0.049584; batch adversarial loss: 0.915034\n",
      "epoch 57; iter: 0; batch classifier loss: 0.038093; batch adversarial loss: 0.875075\n",
      "epoch 58; iter: 0; batch classifier loss: 0.045987; batch adversarial loss: 0.853451\n",
      "epoch 59; iter: 0; batch classifier loss: 0.046930; batch adversarial loss: 0.883938\n",
      "epoch 60; iter: 0; batch classifier loss: 0.045623; batch adversarial loss: 0.810888\n",
      "epoch 61; iter: 0; batch classifier loss: 0.029119; batch adversarial loss: 0.857582\n",
      "epoch 62; iter: 0; batch classifier loss: 0.032243; batch adversarial loss: 0.841860\n",
      "epoch 63; iter: 0; batch classifier loss: 0.043410; batch adversarial loss: 0.837321\n",
      "epoch 64; iter: 0; batch classifier loss: 0.024839; batch adversarial loss: 0.820162\n",
      "epoch 65; iter: 0; batch classifier loss: 0.021643; batch adversarial loss: 0.849878\n",
      "epoch 66; iter: 0; batch classifier loss: 0.037416; batch adversarial loss: 0.837672\n",
      "epoch 67; iter: 0; batch classifier loss: 0.037547; batch adversarial loss: 0.833073\n",
      "epoch 68; iter: 0; batch classifier loss: 0.030290; batch adversarial loss: 0.797705\n",
      "epoch 69; iter: 0; batch classifier loss: 0.021524; batch adversarial loss: 0.818913\n",
      "epoch 70; iter: 0; batch classifier loss: 0.028706; batch adversarial loss: 0.825750\n",
      "epoch 71; iter: 0; batch classifier loss: 0.025513; batch adversarial loss: 0.827454\n",
      "epoch 72; iter: 0; batch classifier loss: 0.029064; batch adversarial loss: 0.853092\n",
      "epoch 73; iter: 0; batch classifier loss: 0.032669; batch adversarial loss: 0.833417\n",
      "epoch 74; iter: 0; batch classifier loss: 0.027006; batch adversarial loss: 0.817416\n",
      "epoch 75; iter: 0; batch classifier loss: 0.023419; batch adversarial loss: 0.822387\n",
      "epoch 76; iter: 0; batch classifier loss: 0.027734; batch adversarial loss: 0.849545\n",
      "epoch 77; iter: 0; batch classifier loss: 0.034247; batch adversarial loss: 0.828987\n",
      "epoch 78; iter: 0; batch classifier loss: 0.032154; batch adversarial loss: 0.795875\n",
      "epoch 79; iter: 0; batch classifier loss: 0.031494; batch adversarial loss: 0.808193\n",
      "epoch 80; iter: 0; batch classifier loss: 0.029772; batch adversarial loss: 0.807010\n",
      "epoch 81; iter: 0; batch classifier loss: 0.030620; batch adversarial loss: 0.801921\n",
      "epoch 82; iter: 0; batch classifier loss: 0.029190; batch adversarial loss: 0.787350\n",
      "epoch 83; iter: 0; batch classifier loss: 0.021103; batch adversarial loss: 0.833630\n",
      "epoch 84; iter: 0; batch classifier loss: 0.031112; batch adversarial loss: 0.845425\n",
      "epoch 85; iter: 0; batch classifier loss: 0.019291; batch adversarial loss: 0.787161\n",
      "epoch 86; iter: 0; batch classifier loss: 0.020643; batch adversarial loss: 0.782950\n",
      "epoch 87; iter: 0; batch classifier loss: 0.031395; batch adversarial loss: 0.779979\n",
      "epoch 88; iter: 0; batch classifier loss: 0.027924; batch adversarial loss: 0.829970\n",
      "epoch 89; iter: 0; batch classifier loss: 0.025152; batch adversarial loss: 0.770540\n",
      "epoch 90; iter: 0; batch classifier loss: 0.029243; batch adversarial loss: 0.794768\n",
      "epoch 91; iter: 0; batch classifier loss: 0.034955; batch adversarial loss: 0.782560\n",
      "epoch 92; iter: 0; batch classifier loss: 0.014463; batch adversarial loss: 0.783293\n",
      "epoch 93; iter: 0; batch classifier loss: 0.016076; batch adversarial loss: 0.787369\n",
      "epoch 94; iter: 0; batch classifier loss: 0.014363; batch adversarial loss: 0.788785\n",
      "epoch 95; iter: 0; batch classifier loss: 0.023396; batch adversarial loss: 0.766394\n",
      "epoch 96; iter: 0; batch classifier loss: 0.020206; batch adversarial loss: 0.777252\n",
      "epoch 97; iter: 0; batch classifier loss: 0.021961; batch adversarial loss: 0.773131\n",
      "epoch 98; iter: 0; batch classifier loss: 0.013242; batch adversarial loss: 0.787227\n",
      "epoch 99; iter: 0; batch classifier loss: 0.021215; batch adversarial loss: 0.777107\n",
      "epoch 100; iter: 0; batch classifier loss: 0.013337; batch adversarial loss: 0.755984\n",
      "epoch 101; iter: 0; batch classifier loss: 0.014822; batch adversarial loss: 0.790263\n",
      "epoch 102; iter: 0; batch classifier loss: 0.025665; batch adversarial loss: 0.777015\n",
      "epoch 103; iter: 0; batch classifier loss: 0.014293; batch adversarial loss: 0.776583\n",
      "epoch 104; iter: 0; batch classifier loss: 0.020106; batch adversarial loss: 0.764553\n",
      "epoch 105; iter: 0; batch classifier loss: 0.021646; batch adversarial loss: 0.770273\n",
      "epoch 106; iter: 0; batch classifier loss: 0.021283; batch adversarial loss: 0.736827\n",
      "epoch 107; iter: 0; batch classifier loss: 0.022961; batch adversarial loss: 0.746955\n",
      "epoch 108; iter: 0; batch classifier loss: 0.024892; batch adversarial loss: 0.756162\n",
      "epoch 109; iter: 0; batch classifier loss: 0.017145; batch adversarial loss: 0.774150\n",
      "epoch 110; iter: 0; batch classifier loss: 0.015810; batch adversarial loss: 0.787468\n",
      "epoch 111; iter: 0; batch classifier loss: 0.019977; batch adversarial loss: 0.741994\n",
      "epoch 112; iter: 0; batch classifier loss: 0.019913; batch adversarial loss: 0.763970\n",
      "epoch 113; iter: 0; batch classifier loss: 0.017377; batch adversarial loss: 0.765778\n",
      "epoch 114; iter: 0; batch classifier loss: 0.022465; batch adversarial loss: 0.742807\n",
      "epoch 115; iter: 0; batch classifier loss: 0.012695; batch adversarial loss: 0.773916\n",
      "epoch 116; iter: 0; batch classifier loss: 0.024248; batch adversarial loss: 0.762087\n",
      "epoch 117; iter: 0; batch classifier loss: 0.007371; batch adversarial loss: 0.758823\n",
      "epoch 118; iter: 0; batch classifier loss: 0.024188; batch adversarial loss: 0.756142\n",
      "epoch 119; iter: 0; batch classifier loss: 0.020249; batch adversarial loss: 0.743922\n",
      "epoch 120; iter: 0; batch classifier loss: 0.013455; batch adversarial loss: 0.739359\n",
      "epoch 121; iter: 0; batch classifier loss: 0.006001; batch adversarial loss: 0.752640\n",
      "epoch 122; iter: 0; batch classifier loss: 0.022862; batch adversarial loss: 0.752605\n",
      "epoch 123; iter: 0; batch classifier loss: 0.014935; batch adversarial loss: 0.745960\n",
      "epoch 124; iter: 0; batch classifier loss: 0.010675; batch adversarial loss: 0.740083\n",
      "epoch 125; iter: 0; batch classifier loss: 0.011023; batch adversarial loss: 0.750049\n",
      "epoch 126; iter: 0; batch classifier loss: 0.020894; batch adversarial loss: 0.730556\n",
      "epoch 127; iter: 0; batch classifier loss: 0.013763; batch adversarial loss: 0.719641\n",
      "epoch 128; iter: 0; batch classifier loss: 0.006240; batch adversarial loss: 0.741643\n",
      "epoch 129; iter: 0; batch classifier loss: 0.023714; batch adversarial loss: 0.721321\n",
      "epoch 130; iter: 0; batch classifier loss: 0.010632; batch adversarial loss: 0.730820\n",
      "epoch 131; iter: 0; batch classifier loss: 0.008282; batch adversarial loss: 0.743571\n",
      "epoch 132; iter: 0; batch classifier loss: 0.016615; batch adversarial loss: 0.737761\n",
      "epoch 133; iter: 0; batch classifier loss: 0.006528; batch adversarial loss: 0.738347\n",
      "epoch 134; iter: 0; batch classifier loss: 0.006039; batch adversarial loss: 0.722241\n",
      "epoch 135; iter: 0; batch classifier loss: 0.006135; batch adversarial loss: 0.726330\n",
      "epoch 136; iter: 0; batch classifier loss: 0.021327; batch adversarial loss: 0.726553\n",
      "epoch 137; iter: 0; batch classifier loss: 0.010444; batch adversarial loss: 0.716632\n",
      "epoch 138; iter: 0; batch classifier loss: 0.013489; batch adversarial loss: 0.722659\n",
      "epoch 139; iter: 0; batch classifier loss: 0.013889; batch adversarial loss: 0.711609\n",
      "epoch 140; iter: 0; batch classifier loss: 0.020281; batch adversarial loss: 0.719808\n",
      "epoch 141; iter: 0; batch classifier loss: 0.013499; batch adversarial loss: 0.716341\n",
      "epoch 142; iter: 0; batch classifier loss: 0.006955; batch adversarial loss: 0.729950\n",
      "epoch 143; iter: 0; batch classifier loss: 0.023358; batch adversarial loss: 0.710748\n",
      "epoch 144; iter: 0; batch classifier loss: 0.011078; batch adversarial loss: 0.723751\n",
      "epoch 145; iter: 0; batch classifier loss: 0.008406; batch adversarial loss: 0.731620\n",
      "epoch 146; iter: 0; batch classifier loss: 0.021987; batch adversarial loss: 0.717277\n",
      "epoch 147; iter: 0; batch classifier loss: 0.007588; batch adversarial loss: 0.714427\n",
      "epoch 148; iter: 0; batch classifier loss: 0.007403; batch adversarial loss: 0.699090\n",
      "epoch 149; iter: 0; batch classifier loss: 0.009098; batch adversarial loss: 0.719669\n",
      "epoch 150; iter: 0; batch classifier loss: 0.013088; batch adversarial loss: 0.715478\n",
      "epoch 151; iter: 0; batch classifier loss: 0.015673; batch adversarial loss: 0.711431\n",
      "epoch 152; iter: 0; batch classifier loss: 0.010574; batch adversarial loss: 0.725823\n",
      "epoch 153; iter: 0; batch classifier loss: 0.011952; batch adversarial loss: 0.704011\n",
      "epoch 154; iter: 0; batch classifier loss: 0.022211; batch adversarial loss: 0.710439\n",
      "epoch 155; iter: 0; batch classifier loss: 0.010974; batch adversarial loss: 0.703503\n",
      "epoch 156; iter: 0; batch classifier loss: 0.010187; batch adversarial loss: 0.710339\n",
      "epoch 157; iter: 0; batch classifier loss: 0.018976; batch adversarial loss: 0.722407\n",
      "epoch 158; iter: 0; batch classifier loss: 0.014065; batch adversarial loss: 0.693488\n",
      "epoch 159; iter: 0; batch classifier loss: 0.005631; batch adversarial loss: 0.718333\n",
      "epoch 160; iter: 0; batch classifier loss: 0.010727; batch adversarial loss: 0.699238\n",
      "epoch 161; iter: 0; batch classifier loss: 0.013104; batch adversarial loss: 0.702990\n",
      "epoch 162; iter: 0; batch classifier loss: 0.018330; batch adversarial loss: 0.695534\n",
      "epoch 163; iter: 0; batch classifier loss: 0.006235; batch adversarial loss: 0.707973\n",
      "epoch 164; iter: 0; batch classifier loss: 0.006104; batch adversarial loss: 0.701149\n",
      "epoch 165; iter: 0; batch classifier loss: 0.016962; batch adversarial loss: 0.707014\n",
      "epoch 166; iter: 0; batch classifier loss: 0.008641; batch adversarial loss: 0.696473\n",
      "epoch 167; iter: 0; batch classifier loss: 0.010217; batch adversarial loss: 0.704997\n",
      "epoch 168; iter: 0; batch classifier loss: 0.017338; batch adversarial loss: 0.707587\n",
      "epoch 169; iter: 0; batch classifier loss: 0.003895; batch adversarial loss: 0.703249\n",
      "epoch 170; iter: 0; batch classifier loss: 0.014184; batch adversarial loss: 0.705268\n",
      "epoch 171; iter: 0; batch classifier loss: 0.005805; batch adversarial loss: 0.693211\n",
      "epoch 172; iter: 0; batch classifier loss: 0.007493; batch adversarial loss: 0.688482\n",
      "epoch 173; iter: 0; batch classifier loss: 0.017046; batch adversarial loss: 0.690072\n",
      "epoch 174; iter: 0; batch classifier loss: 0.017080; batch adversarial loss: 0.695306\n",
      "epoch 175; iter: 0; batch classifier loss: 0.005979; batch adversarial loss: 0.697434\n",
      "epoch 176; iter: 0; batch classifier loss: 0.008584; batch adversarial loss: 0.702880\n",
      "epoch 177; iter: 0; batch classifier loss: 0.010961; batch adversarial loss: 0.702405\n",
      "epoch 178; iter: 0; batch classifier loss: 0.007367; batch adversarial loss: 0.700310\n",
      "epoch 179; iter: 0; batch classifier loss: 0.004562; batch adversarial loss: 0.693917\n",
      "epoch 180; iter: 0; batch classifier loss: 0.010270; batch adversarial loss: 0.693288\n",
      "epoch 181; iter: 0; batch classifier loss: 0.004316; batch adversarial loss: 0.680835\n",
      "epoch 182; iter: 0; batch classifier loss: 0.011359; batch adversarial loss: 0.683596\n",
      "epoch 183; iter: 0; batch classifier loss: 0.005630; batch adversarial loss: 0.691544\n",
      "epoch 184; iter: 0; batch classifier loss: 0.008800; batch adversarial loss: 0.693832\n",
      "epoch 185; iter: 0; batch classifier loss: 0.006927; batch adversarial loss: 0.682389\n",
      "epoch 186; iter: 0; batch classifier loss: 0.004054; batch adversarial loss: 0.693589\n",
      "epoch 187; iter: 0; batch classifier loss: 0.002841; batch adversarial loss: 0.697922\n",
      "epoch 188; iter: 0; batch classifier loss: 0.009274; batch adversarial loss: 0.685416\n",
      "epoch 189; iter: 0; batch classifier loss: 0.004613; batch adversarial loss: 0.694743\n",
      "epoch 190; iter: 0; batch classifier loss: 0.007004; batch adversarial loss: 0.684980\n",
      "epoch 191; iter: 0; batch classifier loss: 0.009990; batch adversarial loss: 0.690605\n",
      "epoch 192; iter: 0; batch classifier loss: 0.002623; batch adversarial loss: 0.691844\n",
      "epoch 193; iter: 0; batch classifier loss: 0.008476; batch adversarial loss: 0.687987\n",
      "epoch 194; iter: 0; batch classifier loss: 0.005300; batch adversarial loss: 0.690102\n",
      "epoch 195; iter: 0; batch classifier loss: 0.003256; batch adversarial loss: 0.687015\n",
      "epoch 196; iter: 0; batch classifier loss: 0.007673; batch adversarial loss: 0.691413\n",
      "epoch 197; iter: 0; batch classifier loss: 0.008486; batch adversarial loss: 0.688810\n",
      "epoch 198; iter: 0; batch classifier loss: 0.009188; batch adversarial loss: 0.689112\n",
      "epoch 199; iter: 0; batch classifier loss: 0.004539; batch adversarial loss: 0.688249\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:33.221419: W tensorflow/c/c_api.cc:304] Operation '{name:'4c968f1a-adee-11ee-9362-a9d23602d4a3/4c968f1a-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:1591 op device:{requested: '', assigned: ''} def:{{{node 4c968f1a-adee-11ee-9362-a9d23602d4a3/4c968f1a-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c968f1a-adee-11ee-9362-a9d23602d4a3/4c968f1a-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c968f1a-adee-11ee-9362-a9d23602d4a3/4c968f1a-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.697751; batch adversarial loss: 0.694277\n",
      "epoch 1; iter: 0; batch classifier loss: 0.540909; batch adversarial loss: 0.707314\n",
      "epoch 2; iter: 0; batch classifier loss: 0.418468; batch adversarial loss: 0.673440\n",
      "epoch 3; iter: 0; batch classifier loss: 0.388767; batch adversarial loss: 0.693027\n",
      "epoch 4; iter: 0; batch classifier loss: 0.338899; batch adversarial loss: 0.715997\n",
      "epoch 5; iter: 0; batch classifier loss: 0.341698; batch adversarial loss: 0.698055\n",
      "epoch 6; iter: 0; batch classifier loss: 0.294522; batch adversarial loss: 0.731602\n",
      "epoch 7; iter: 0; batch classifier loss: 0.317531; batch adversarial loss: 0.719630\n",
      "epoch 8; iter: 0; batch classifier loss: 0.283082; batch adversarial loss: 0.735525\n",
      "epoch 9; iter: 0; batch classifier loss: 0.204961; batch adversarial loss: 0.708422\n",
      "epoch 10; iter: 0; batch classifier loss: 0.280128; batch adversarial loss: 0.719306\n",
      "epoch 11; iter: 0; batch classifier loss: 0.209676; batch adversarial loss: 0.716170\n",
      "epoch 12; iter: 0; batch classifier loss: 0.252523; batch adversarial loss: 0.705678\n",
      "epoch 13; iter: 0; batch classifier loss: 0.218159; batch adversarial loss: 0.707683\n",
      "epoch 14; iter: 0; batch classifier loss: 0.226577; batch adversarial loss: 0.716951\n",
      "epoch 15; iter: 0; batch classifier loss: 0.179553; batch adversarial loss: 0.716181\n",
      "epoch 16; iter: 0; batch classifier loss: 0.210407; batch adversarial loss: 0.706098\n",
      "epoch 17; iter: 0; batch classifier loss: 0.209779; batch adversarial loss: 0.706624\n",
      "epoch 18; iter: 0; batch classifier loss: 0.139788; batch adversarial loss: 0.691167\n",
      "epoch 19; iter: 0; batch classifier loss: 0.187357; batch adversarial loss: 0.705225\n",
      "epoch 20; iter: 0; batch classifier loss: 0.205201; batch adversarial loss: 0.686846\n",
      "epoch 21; iter: 0; batch classifier loss: 0.164581; batch adversarial loss: 0.695204\n",
      "epoch 22; iter: 0; batch classifier loss: 0.158514; batch adversarial loss: 0.712893\n",
      "epoch 23; iter: 0; batch classifier loss: 0.181337; batch adversarial loss: 0.740895\n",
      "epoch 24; iter: 0; batch classifier loss: 0.190846; batch adversarial loss: 0.682186\n",
      "epoch 25; iter: 0; batch classifier loss: 0.183451; batch adversarial loss: 0.734344\n",
      "epoch 26; iter: 0; batch classifier loss: 0.176980; batch adversarial loss: 0.693051\n",
      "epoch 27; iter: 0; batch classifier loss: 0.123440; batch adversarial loss: 0.694028\n",
      "epoch 28; iter: 0; batch classifier loss: 0.154511; batch adversarial loss: 0.757024\n",
      "epoch 29; iter: 0; batch classifier loss: 0.140859; batch adversarial loss: 0.681347\n",
      "epoch 30; iter: 0; batch classifier loss: 0.129080; batch adversarial loss: 0.719462\n",
      "epoch 31; iter: 0; batch classifier loss: 0.152817; batch adversarial loss: 0.705161\n",
      "epoch 32; iter: 0; batch classifier loss: 0.150626; batch adversarial loss: 0.741497\n",
      "epoch 33; iter: 0; batch classifier loss: 0.182605; batch adversarial loss: 0.717355\n",
      "epoch 34; iter: 0; batch classifier loss: 0.103423; batch adversarial loss: 0.710123\n",
      "epoch 35; iter: 0; batch classifier loss: 0.122622; batch adversarial loss: 0.696532\n",
      "epoch 36; iter: 0; batch classifier loss: 0.137604; batch adversarial loss: 0.686365\n",
      "epoch 37; iter: 0; batch classifier loss: 0.099707; batch adversarial loss: 0.702274\n",
      "epoch 38; iter: 0; batch classifier loss: 0.136572; batch adversarial loss: 0.701517\n",
      "epoch 39; iter: 0; batch classifier loss: 0.128572; batch adversarial loss: 0.712642\n",
      "epoch 40; iter: 0; batch classifier loss: 0.104260; batch adversarial loss: 0.731585\n",
      "epoch 41; iter: 0; batch classifier loss: 0.133319; batch adversarial loss: 0.700268\n",
      "epoch 42; iter: 0; batch classifier loss: 0.093179; batch adversarial loss: 0.703992\n",
      "epoch 43; iter: 0; batch classifier loss: 0.163620; batch adversarial loss: 0.726072\n",
      "epoch 44; iter: 0; batch classifier loss: 0.112889; batch adversarial loss: 0.724992\n",
      "epoch 45; iter: 0; batch classifier loss: 0.073813; batch adversarial loss: 0.691577\n",
      "epoch 46; iter: 0; batch classifier loss: 0.116442; batch adversarial loss: 0.730379\n",
      "epoch 47; iter: 0; batch classifier loss: 0.107673; batch adversarial loss: 0.726133\n",
      "epoch 48; iter: 0; batch classifier loss: 0.127058; batch adversarial loss: 0.739434\n",
      "epoch 49; iter: 0; batch classifier loss: 0.111077; batch adversarial loss: 0.721554\n",
      "epoch 50; iter: 0; batch classifier loss: 0.103400; batch adversarial loss: 0.714297\n",
      "epoch 51; iter: 0; batch classifier loss: 0.124301; batch adversarial loss: 0.740765\n",
      "epoch 52; iter: 0; batch classifier loss: 0.112067; batch adversarial loss: 0.726655\n",
      "epoch 53; iter: 0; batch classifier loss: 0.107901; batch adversarial loss: 0.748185\n",
      "epoch 54; iter: 0; batch classifier loss: 0.092008; batch adversarial loss: 0.701611\n",
      "epoch 55; iter: 0; batch classifier loss: 0.105733; batch adversarial loss: 0.725699\n",
      "epoch 56; iter: 0; batch classifier loss: 0.102504; batch adversarial loss: 0.720910\n",
      "epoch 57; iter: 0; batch classifier loss: 0.135035; batch adversarial loss: 0.732319\n",
      "epoch 58; iter: 0; batch classifier loss: 0.076860; batch adversarial loss: 0.714742\n",
      "epoch 59; iter: 0; batch classifier loss: 0.116273; batch adversarial loss: 0.737126\n",
      "epoch 60; iter: 0; batch classifier loss: 0.094235; batch adversarial loss: 0.717810\n",
      "epoch 61; iter: 0; batch classifier loss: 0.077422; batch adversarial loss: 0.687251\n",
      "epoch 62; iter: 0; batch classifier loss: 0.088867; batch adversarial loss: 0.697127\n",
      "epoch 63; iter: 0; batch classifier loss: 0.074688; batch adversarial loss: 0.698898\n",
      "epoch 64; iter: 0; batch classifier loss: 0.118056; batch adversarial loss: 0.743741\n",
      "epoch 65; iter: 0; batch classifier loss: 0.097786; batch adversarial loss: 0.717239\n",
      "epoch 66; iter: 0; batch classifier loss: 0.101337; batch adversarial loss: 0.720587\n",
      "epoch 67; iter: 0; batch classifier loss: 0.133078; batch adversarial loss: 0.726660\n",
      "epoch 68; iter: 0; batch classifier loss: 0.061511; batch adversarial loss: 0.730850\n",
      "epoch 69; iter: 0; batch classifier loss: 0.086843; batch adversarial loss: 0.715549\n",
      "epoch 70; iter: 0; batch classifier loss: 0.092379; batch adversarial loss: 0.740908\n",
      "epoch 71; iter: 0; batch classifier loss: 0.111609; batch adversarial loss: 0.715484\n",
      "epoch 72; iter: 0; batch classifier loss: 0.098141; batch adversarial loss: 0.722459\n",
      "epoch 73; iter: 0; batch classifier loss: 0.066904; batch adversarial loss: 0.691059\n",
      "epoch 74; iter: 0; batch classifier loss: 0.092375; batch adversarial loss: 0.731907\n",
      "epoch 75; iter: 0; batch classifier loss: 0.058671; batch adversarial loss: 0.713546\n",
      "epoch 76; iter: 0; batch classifier loss: 0.055217; batch adversarial loss: 0.701790\n",
      "epoch 77; iter: 0; batch classifier loss: 0.088792; batch adversarial loss: 0.710537\n",
      "epoch 78; iter: 0; batch classifier loss: 0.076900; batch adversarial loss: 0.725911\n",
      "epoch 79; iter: 0; batch classifier loss: 0.135233; batch adversarial loss: 0.763591\n",
      "epoch 80; iter: 0; batch classifier loss: 0.099523; batch adversarial loss: 0.723448\n",
      "epoch 81; iter: 0; batch classifier loss: 0.097037; batch adversarial loss: 0.731322\n",
      "epoch 82; iter: 0; batch classifier loss: 0.071158; batch adversarial loss: 0.688236\n",
      "epoch 83; iter: 0; batch classifier loss: 0.127550; batch adversarial loss: 0.744865\n",
      "epoch 84; iter: 0; batch classifier loss: 0.044514; batch adversarial loss: 0.693392\n",
      "epoch 85; iter: 0; batch classifier loss: 0.112508; batch adversarial loss: 0.734400\n",
      "epoch 86; iter: 0; batch classifier loss: 0.076642; batch adversarial loss: 0.712742\n",
      "epoch 87; iter: 0; batch classifier loss: 0.130061; batch adversarial loss: 0.760259\n",
      "epoch 88; iter: 0; batch classifier loss: 0.045923; batch adversarial loss: 0.700606\n",
      "epoch 89; iter: 0; batch classifier loss: 0.076940; batch adversarial loss: 0.745577\n",
      "epoch 90; iter: 0; batch classifier loss: 0.066557; batch adversarial loss: 0.700091\n",
      "epoch 91; iter: 0; batch classifier loss: 0.122840; batch adversarial loss: 0.752578\n",
      "epoch 92; iter: 0; batch classifier loss: 0.069054; batch adversarial loss: 0.690811\n",
      "epoch 93; iter: 0; batch classifier loss: 0.075242; batch adversarial loss: 0.711485\n",
      "epoch 94; iter: 0; batch classifier loss: 0.086255; batch adversarial loss: 0.716474\n",
      "epoch 95; iter: 0; batch classifier loss: 0.040510; batch adversarial loss: 0.686767\n",
      "epoch 96; iter: 0; batch classifier loss: 0.082837; batch adversarial loss: 0.739669\n",
      "epoch 97; iter: 0; batch classifier loss: 0.094892; batch adversarial loss: 0.697613\n",
      "epoch 98; iter: 0; batch classifier loss: 0.053243; batch adversarial loss: 0.716744\n",
      "epoch 99; iter: 0; batch classifier loss: 0.091727; batch adversarial loss: 0.725677\n",
      "epoch 100; iter: 0; batch classifier loss: 0.095190; batch adversarial loss: 0.721800\n",
      "epoch 101; iter: 0; batch classifier loss: 0.023036; batch adversarial loss: 0.692060\n",
      "epoch 102; iter: 0; batch classifier loss: 0.083446; batch adversarial loss: 0.736366\n",
      "epoch 103; iter: 0; batch classifier loss: 0.093078; batch adversarial loss: 0.715832\n",
      "epoch 104; iter: 0; batch classifier loss: 0.101636; batch adversarial loss: 0.718561\n",
      "epoch 105; iter: 0; batch classifier loss: 0.053272; batch adversarial loss: 0.705329\n",
      "epoch 106; iter: 0; batch classifier loss: 0.093511; batch adversarial loss: 0.704880\n",
      "epoch 107; iter: 0; batch classifier loss: 0.040835; batch adversarial loss: 0.697562\n",
      "epoch 108; iter: 0; batch classifier loss: 0.087597; batch adversarial loss: 0.720055\n",
      "epoch 109; iter: 0; batch classifier loss: 0.062307; batch adversarial loss: 0.713308\n",
      "epoch 110; iter: 0; batch classifier loss: 0.065909; batch adversarial loss: 0.738380\n",
      "epoch 111; iter: 0; batch classifier loss: 0.104128; batch adversarial loss: 0.711161\n",
      "epoch 112; iter: 0; batch classifier loss: 0.080146; batch adversarial loss: 0.708107\n",
      "epoch 113; iter: 0; batch classifier loss: 0.091547; batch adversarial loss: 0.727766\n",
      "epoch 114; iter: 0; batch classifier loss: 0.101375; batch adversarial loss: 0.713236\n",
      "epoch 115; iter: 0; batch classifier loss: 0.109906; batch adversarial loss: 0.720927\n",
      "epoch 116; iter: 0; batch classifier loss: 0.101734; batch adversarial loss: 0.718754\n",
      "epoch 117; iter: 0; batch classifier loss: 0.063048; batch adversarial loss: 0.685859\n",
      "epoch 118; iter: 0; batch classifier loss: 0.092310; batch adversarial loss: 0.723090\n",
      "epoch 119; iter: 0; batch classifier loss: 0.079344; batch adversarial loss: 0.743713\n",
      "epoch 120; iter: 0; batch classifier loss: 0.086190; batch adversarial loss: 0.727261\n",
      "epoch 121; iter: 0; batch classifier loss: 0.064200; batch adversarial loss: 0.667622\n",
      "epoch 122; iter: 0; batch classifier loss: 0.046978; batch adversarial loss: 0.684419\n",
      "epoch 123; iter: 0; batch classifier loss: 0.102693; batch adversarial loss: 0.728110\n",
      "epoch 124; iter: 0; batch classifier loss: 0.078496; batch adversarial loss: 0.711656\n",
      "epoch 125; iter: 0; batch classifier loss: 0.098376; batch adversarial loss: 0.714828\n",
      "epoch 126; iter: 0; batch classifier loss: 0.100739; batch adversarial loss: 0.741975\n",
      "epoch 127; iter: 0; batch classifier loss: 0.087088; batch adversarial loss: 0.693041\n",
      "epoch 128; iter: 0; batch classifier loss: 0.077648; batch adversarial loss: 0.722449\n",
      "epoch 129; iter: 0; batch classifier loss: 0.090807; batch adversarial loss: 0.736862\n",
      "epoch 130; iter: 0; batch classifier loss: 0.106079; batch adversarial loss: 0.718619\n",
      "epoch 131; iter: 0; batch classifier loss: 0.099134; batch adversarial loss: 0.742765\n",
      "epoch 132; iter: 0; batch classifier loss: 0.112625; batch adversarial loss: 0.717623\n",
      "epoch 133; iter: 0; batch classifier loss: 0.101876; batch adversarial loss: 0.718195\n",
      "epoch 134; iter: 0; batch classifier loss: 0.071823; batch adversarial loss: 0.729295\n",
      "epoch 135; iter: 0; batch classifier loss: 0.097453; batch adversarial loss: 0.697539\n",
      "epoch 136; iter: 0; batch classifier loss: 0.058951; batch adversarial loss: 0.703443\n",
      "epoch 137; iter: 0; batch classifier loss: 0.079817; batch adversarial loss: 0.708936\n",
      "epoch 138; iter: 0; batch classifier loss: 0.036987; batch adversarial loss: 0.703403\n",
      "epoch 139; iter: 0; batch classifier loss: 0.070626; batch adversarial loss: 0.704211\n",
      "epoch 140; iter: 0; batch classifier loss: 0.081082; batch adversarial loss: 0.727988\n",
      "epoch 141; iter: 0; batch classifier loss: 0.096873; batch adversarial loss: 0.716156\n",
      "epoch 142; iter: 0; batch classifier loss: 0.096522; batch adversarial loss: 0.707432\n",
      "epoch 143; iter: 0; batch classifier loss: 0.078370; batch adversarial loss: 0.694208\n",
      "epoch 144; iter: 0; batch classifier loss: 0.070078; batch adversarial loss: 0.715100\n",
      "epoch 145; iter: 0; batch classifier loss: 0.055153; batch adversarial loss: 0.719589\n",
      "epoch 146; iter: 0; batch classifier loss: 0.058197; batch adversarial loss: 0.697714\n",
      "epoch 147; iter: 0; batch classifier loss: 0.090800; batch adversarial loss: 0.708267\n",
      "epoch 148; iter: 0; batch classifier loss: 0.074181; batch adversarial loss: 0.689874\n",
      "epoch 149; iter: 0; batch classifier loss: 0.091143; batch adversarial loss: 0.695881\n",
      "epoch 150; iter: 0; batch classifier loss: 0.069413; batch adversarial loss: 0.706172\n",
      "epoch 151; iter: 0; batch classifier loss: 0.052958; batch adversarial loss: 0.682981\n",
      "epoch 152; iter: 0; batch classifier loss: 0.118366; batch adversarial loss: 0.751660\n",
      "epoch 153; iter: 0; batch classifier loss: 0.093745; batch adversarial loss: 0.726095\n",
      "epoch 154; iter: 0; batch classifier loss: 0.111137; batch adversarial loss: 0.718246\n",
      "epoch 155; iter: 0; batch classifier loss: 0.057729; batch adversarial loss: 0.681507\n",
      "epoch 156; iter: 0; batch classifier loss: 0.073604; batch adversarial loss: 0.705011\n",
      "epoch 157; iter: 0; batch classifier loss: 0.085185; batch adversarial loss: 0.717907\n",
      "epoch 158; iter: 0; batch classifier loss: 0.080271; batch adversarial loss: 0.724489\n",
      "epoch 159; iter: 0; batch classifier loss: 0.077622; batch adversarial loss: 0.703866\n",
      "epoch 160; iter: 0; batch classifier loss: 0.066542; batch adversarial loss: 0.683645\n",
      "epoch 161; iter: 0; batch classifier loss: 0.109908; batch adversarial loss: 0.735176\n",
      "epoch 162; iter: 0; batch classifier loss: 0.061212; batch adversarial loss: 0.713265\n",
      "epoch 163; iter: 0; batch classifier loss: 0.053890; batch adversarial loss: 0.702720\n",
      "epoch 164; iter: 0; batch classifier loss: 0.080921; batch adversarial loss: 0.702183\n",
      "epoch 165; iter: 0; batch classifier loss: 0.075369; batch adversarial loss: 0.694172\n",
      "epoch 166; iter: 0; batch classifier loss: 0.098137; batch adversarial loss: 0.726517\n",
      "epoch 167; iter: 0; batch classifier loss: 0.046590; batch adversarial loss: 0.669937\n",
      "epoch 168; iter: 0; batch classifier loss: 0.074118; batch adversarial loss: 0.688979\n",
      "epoch 169; iter: 0; batch classifier loss: 0.115685; batch adversarial loss: 0.726873\n",
      "epoch 170; iter: 0; batch classifier loss: 0.088786; batch adversarial loss: 0.681193\n",
      "epoch 171; iter: 0; batch classifier loss: 0.079836; batch adversarial loss: 0.698357\n",
      "epoch 172; iter: 0; batch classifier loss: 0.075160; batch adversarial loss: 0.710016\n",
      "epoch 173; iter: 0; batch classifier loss: 0.067453; batch adversarial loss: 0.707395\n",
      "epoch 174; iter: 0; batch classifier loss: 0.124743; batch adversarial loss: 0.729242\n",
      "epoch 175; iter: 0; batch classifier loss: 0.055009; batch adversarial loss: 0.704597\n",
      "epoch 176; iter: 0; batch classifier loss: 0.107625; batch adversarial loss: 0.713491\n",
      "epoch 177; iter: 0; batch classifier loss: 0.075985; batch adversarial loss: 0.716309\n",
      "epoch 178; iter: 0; batch classifier loss: 0.095952; batch adversarial loss: 0.732041\n",
      "epoch 179; iter: 0; batch classifier loss: 0.094935; batch adversarial loss: 0.696845\n",
      "epoch 180; iter: 0; batch classifier loss: 0.061480; batch adversarial loss: 0.698231\n",
      "epoch 181; iter: 0; batch classifier loss: 0.126360; batch adversarial loss: 0.724690\n",
      "epoch 182; iter: 0; batch classifier loss: 0.062391; batch adversarial loss: 0.700693\n",
      "epoch 183; iter: 0; batch classifier loss: 0.122667; batch adversarial loss: 0.722490\n",
      "epoch 184; iter: 0; batch classifier loss: 0.060602; batch adversarial loss: 0.697614\n",
      "epoch 185; iter: 0; batch classifier loss: 0.083927; batch adversarial loss: 0.699827\n",
      "epoch 186; iter: 0; batch classifier loss: 0.092653; batch adversarial loss: 0.727605\n",
      "epoch 187; iter: 0; batch classifier loss: 0.123126; batch adversarial loss: 0.712684\n",
      "epoch 188; iter: 0; batch classifier loss: 0.050944; batch adversarial loss: 0.691938\n",
      "epoch 189; iter: 0; batch classifier loss: 0.097921; batch adversarial loss: 0.719101\n",
      "epoch 190; iter: 0; batch classifier loss: 0.076290; batch adversarial loss: 0.696669\n",
      "epoch 191; iter: 0; batch classifier loss: 0.079888; batch adversarial loss: 0.693225\n",
      "epoch 192; iter: 0; batch classifier loss: 0.100928; batch adversarial loss: 0.694075\n",
      "epoch 193; iter: 0; batch classifier loss: 0.117321; batch adversarial loss: 0.705702\n",
      "epoch 194; iter: 0; batch classifier loss: 0.088562; batch adversarial loss: 0.719695\n",
      "epoch 195; iter: 0; batch classifier loss: 0.043787; batch adversarial loss: 0.699703\n",
      "epoch 196; iter: 0; batch classifier loss: 0.086608; batch adversarial loss: 0.709268\n",
      "epoch 197; iter: 0; batch classifier loss: 0.084579; batch adversarial loss: 0.695367\n",
      "epoch 198; iter: 0; batch classifier loss: 0.104285; batch adversarial loss: 0.721380\n",
      "epoch 199; iter: 0; batch classifier loss: 0.061182; batch adversarial loss: 0.733757\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:34.736118: W tensorflow/c/c_api.cc:304] Operation '{name:'4c9690d2-adee-11ee-9362-a9d23602d4a3/4c9690d2-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:2398 op device:{requested: '', assigned: ''} def:{{{node 4c9690d2-adee-11ee-9362-a9d23602d4a3/4c9690d2-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c9690d2-adee-11ee-9362-a9d23602d4a3/4c9690d2-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c9690d2-adee-11ee-9362-a9d23602d4a3/4c9690d2-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.879470; batch adversarial loss: 0.808271\n",
      "epoch 1; iter: 0; batch classifier loss: 0.717245; batch adversarial loss: 0.815287\n",
      "epoch 2; iter: 0; batch classifier loss: 0.618553; batch adversarial loss: 0.879413\n",
      "epoch 3; iter: 0; batch classifier loss: 0.519869; batch adversarial loss: 0.904010\n",
      "epoch 4; iter: 0; batch classifier loss: 0.473477; batch adversarial loss: 0.960223\n",
      "epoch 5; iter: 0; batch classifier loss: 0.382140; batch adversarial loss: 0.945891\n",
      "epoch 6; iter: 0; batch classifier loss: 0.316059; batch adversarial loss: 0.968777\n",
      "epoch 7; iter: 0; batch classifier loss: 0.295347; batch adversarial loss: 0.933319\n",
      "epoch 8; iter: 0; batch classifier loss: 0.267065; batch adversarial loss: 0.977028\n",
      "epoch 9; iter: 0; batch classifier loss: 0.317416; batch adversarial loss: 1.008264\n",
      "epoch 10; iter: 0; batch classifier loss: 0.292922; batch adversarial loss: 1.072437\n",
      "epoch 11; iter: 0; batch classifier loss: 0.177597; batch adversarial loss: 1.022541\n",
      "epoch 12; iter: 0; batch classifier loss: 0.164749; batch adversarial loss: 1.081207\n",
      "epoch 13; iter: 0; batch classifier loss: 0.186598; batch adversarial loss: 1.018539\n",
      "epoch 14; iter: 0; batch classifier loss: 0.226200; batch adversarial loss: 0.960558\n",
      "epoch 15; iter: 0; batch classifier loss: 0.196300; batch adversarial loss: 1.064701\n",
      "epoch 16; iter: 0; batch classifier loss: 0.182751; batch adversarial loss: 0.981113\n",
      "epoch 17; iter: 0; batch classifier loss: 0.136317; batch adversarial loss: 1.021368\n",
      "epoch 18; iter: 0; batch classifier loss: 0.154654; batch adversarial loss: 0.999974\n",
      "epoch 19; iter: 0; batch classifier loss: 0.146337; batch adversarial loss: 1.002056\n",
      "epoch 20; iter: 0; batch classifier loss: 0.151607; batch adversarial loss: 1.008939\n",
      "epoch 21; iter: 0; batch classifier loss: 0.114526; batch adversarial loss: 1.029086\n",
      "epoch 22; iter: 0; batch classifier loss: 0.151657; batch adversarial loss: 0.982186\n",
      "epoch 23; iter: 0; batch classifier loss: 0.133708; batch adversarial loss: 0.971003\n",
      "epoch 24; iter: 0; batch classifier loss: 0.120415; batch adversarial loss: 0.972520\n",
      "epoch 25; iter: 0; batch classifier loss: 0.119145; batch adversarial loss: 1.050326\n",
      "epoch 26; iter: 0; batch classifier loss: 0.126833; batch adversarial loss: 0.994462\n",
      "epoch 27; iter: 0; batch classifier loss: 0.098926; batch adversarial loss: 0.923929\n",
      "epoch 28; iter: 0; batch classifier loss: 0.098300; batch adversarial loss: 1.011398\n",
      "epoch 29; iter: 0; batch classifier loss: 0.115457; batch adversarial loss: 1.014182\n",
      "epoch 30; iter: 0; batch classifier loss: 0.134368; batch adversarial loss: 0.955426\n",
      "epoch 31; iter: 0; batch classifier loss: 0.112035; batch adversarial loss: 0.943782\n",
      "epoch 32; iter: 0; batch classifier loss: 0.116593; batch adversarial loss: 1.001127\n",
      "epoch 33; iter: 0; batch classifier loss: 0.076346; batch adversarial loss: 0.987934\n",
      "epoch 34; iter: 0; batch classifier loss: 0.073329; batch adversarial loss: 1.011129\n",
      "epoch 35; iter: 0; batch classifier loss: 0.088405; batch adversarial loss: 0.983287\n",
      "epoch 36; iter: 0; batch classifier loss: 0.117054; batch adversarial loss: 0.954711\n",
      "epoch 37; iter: 0; batch classifier loss: 0.069171; batch adversarial loss: 0.952915\n",
      "epoch 38; iter: 0; batch classifier loss: 0.101152; batch adversarial loss: 0.951870\n",
      "epoch 39; iter: 0; batch classifier loss: 0.094503; batch adversarial loss: 0.935672\n",
      "epoch 40; iter: 0; batch classifier loss: 0.094479; batch adversarial loss: 0.936721\n",
      "epoch 41; iter: 0; batch classifier loss: 0.068074; batch adversarial loss: 0.964529\n",
      "epoch 42; iter: 0; batch classifier loss: 0.083347; batch adversarial loss: 0.966331\n",
      "epoch 43; iter: 0; batch classifier loss: 0.067458; batch adversarial loss: 0.931143\n",
      "epoch 44; iter: 0; batch classifier loss: 0.093942; batch adversarial loss: 0.952886\n",
      "epoch 45; iter: 0; batch classifier loss: 0.075641; batch adversarial loss: 0.911421\n",
      "epoch 46; iter: 0; batch classifier loss: 0.078816; batch adversarial loss: 0.994918\n",
      "epoch 47; iter: 0; batch classifier loss: 0.061661; batch adversarial loss: 0.931285\n",
      "epoch 48; iter: 0; batch classifier loss: 0.079523; batch adversarial loss: 0.982192\n",
      "epoch 49; iter: 0; batch classifier loss: 0.041531; batch adversarial loss: 0.930790\n",
      "epoch 50; iter: 0; batch classifier loss: 0.069949; batch adversarial loss: 0.909958\n",
      "epoch 51; iter: 0; batch classifier loss: 0.110416; batch adversarial loss: 0.914736\n",
      "epoch 52; iter: 0; batch classifier loss: 0.069323; batch adversarial loss: 0.906707\n",
      "epoch 53; iter: 0; batch classifier loss: 0.063099; batch adversarial loss: 0.907646\n",
      "epoch 54; iter: 0; batch classifier loss: 0.084740; batch adversarial loss: 0.881162\n",
      "epoch 55; iter: 0; batch classifier loss: 0.080317; batch adversarial loss: 0.976040\n",
      "epoch 56; iter: 0; batch classifier loss: 0.075609; batch adversarial loss: 0.879941\n",
      "epoch 57; iter: 0; batch classifier loss: 0.083659; batch adversarial loss: 0.976473\n",
      "epoch 58; iter: 0; batch classifier loss: 0.058812; batch adversarial loss: 0.929309\n",
      "epoch 59; iter: 0; batch classifier loss: 0.061959; batch adversarial loss: 0.878660\n",
      "epoch 60; iter: 0; batch classifier loss: 0.050832; batch adversarial loss: 0.952636\n",
      "epoch 61; iter: 0; batch classifier loss: 0.069283; batch adversarial loss: 0.904477\n",
      "epoch 62; iter: 0; batch classifier loss: 0.088168; batch adversarial loss: 0.851846\n",
      "epoch 63; iter: 0; batch classifier loss: 0.058019; batch adversarial loss: 0.879069\n",
      "epoch 64; iter: 0; batch classifier loss: 0.082575; batch adversarial loss: 0.916733\n",
      "epoch 65; iter: 0; batch classifier loss: 0.062629; batch adversarial loss: 0.882692\n",
      "epoch 66; iter: 0; batch classifier loss: 0.081764; batch adversarial loss: 0.850400\n",
      "epoch 67; iter: 0; batch classifier loss: 0.050274; batch adversarial loss: 0.843832\n",
      "epoch 68; iter: 0; batch classifier loss: 0.086850; batch adversarial loss: 0.936624\n",
      "epoch 69; iter: 0; batch classifier loss: 0.043222; batch adversarial loss: 0.865574\n",
      "epoch 70; iter: 0; batch classifier loss: 0.051581; batch adversarial loss: 0.893036\n",
      "epoch 71; iter: 0; batch classifier loss: 0.048155; batch adversarial loss: 0.910332\n",
      "epoch 72; iter: 0; batch classifier loss: 0.038649; batch adversarial loss: 0.843806\n",
      "epoch 73; iter: 0; batch classifier loss: 0.071686; batch adversarial loss: 0.882212\n",
      "epoch 74; iter: 0; batch classifier loss: 0.025867; batch adversarial loss: 0.875462\n",
      "epoch 75; iter: 0; batch classifier loss: 0.058220; batch adversarial loss: 0.857424\n",
      "epoch 76; iter: 0; batch classifier loss: 0.031194; batch adversarial loss: 0.849720\n",
      "epoch 77; iter: 0; batch classifier loss: 0.057744; batch adversarial loss: 0.869666\n",
      "epoch 78; iter: 0; batch classifier loss: 0.062391; batch adversarial loss: 0.827890\n",
      "epoch 79; iter: 0; batch classifier loss: 0.072050; batch adversarial loss: 0.893232\n",
      "epoch 80; iter: 0; batch classifier loss: 0.071274; batch adversarial loss: 0.871099\n",
      "epoch 81; iter: 0; batch classifier loss: 0.087940; batch adversarial loss: 0.894931\n",
      "epoch 82; iter: 0; batch classifier loss: 0.049753; batch adversarial loss: 0.887273\n",
      "epoch 83; iter: 0; batch classifier loss: 0.045705; batch adversarial loss: 0.813038\n",
      "epoch 84; iter: 0; batch classifier loss: 0.027052; batch adversarial loss: 0.793734\n",
      "epoch 85; iter: 0; batch classifier loss: 0.051207; batch adversarial loss: 0.849766\n",
      "epoch 86; iter: 0; batch classifier loss: 0.064647; batch adversarial loss: 0.823933\n",
      "epoch 87; iter: 0; batch classifier loss: 0.039042; batch adversarial loss: 0.830891\n",
      "epoch 88; iter: 0; batch classifier loss: 0.069494; batch adversarial loss: 0.842972\n",
      "epoch 89; iter: 0; batch classifier loss: 0.068010; batch adversarial loss: 0.785603\n",
      "epoch 90; iter: 0; batch classifier loss: 0.047765; batch adversarial loss: 0.840220\n",
      "epoch 91; iter: 0; batch classifier loss: 0.084662; batch adversarial loss: 0.821755\n",
      "epoch 92; iter: 0; batch classifier loss: 0.037086; batch adversarial loss: 0.858022\n",
      "epoch 93; iter: 0; batch classifier loss: 0.084995; batch adversarial loss: 0.865876\n",
      "epoch 94; iter: 0; batch classifier loss: 0.057847; batch adversarial loss: 0.842543\n",
      "epoch 95; iter: 0; batch classifier loss: 0.063188; batch adversarial loss: 0.832197\n",
      "epoch 96; iter: 0; batch classifier loss: 0.028525; batch adversarial loss: 0.826880\n",
      "epoch 97; iter: 0; batch classifier loss: 0.052382; batch adversarial loss: 0.835150\n",
      "epoch 98; iter: 0; batch classifier loss: 0.053553; batch adversarial loss: 0.816278\n",
      "epoch 99; iter: 0; batch classifier loss: 0.059933; batch adversarial loss: 0.806603\n",
      "epoch 100; iter: 0; batch classifier loss: 0.064384; batch adversarial loss: 0.809891\n",
      "epoch 101; iter: 0; batch classifier loss: 0.026829; batch adversarial loss: 0.791571\n",
      "epoch 102; iter: 0; batch classifier loss: 0.050058; batch adversarial loss: 0.838592\n",
      "epoch 103; iter: 0; batch classifier loss: 0.063622; batch adversarial loss: 0.795520\n",
      "epoch 104; iter: 0; batch classifier loss: 0.047321; batch adversarial loss: 0.839573\n",
      "epoch 105; iter: 0; batch classifier loss: 0.084880; batch adversarial loss: 0.824771\n",
      "epoch 106; iter: 0; batch classifier loss: 0.032824; batch adversarial loss: 0.811707\n",
      "epoch 107; iter: 0; batch classifier loss: 0.040134; batch adversarial loss: 0.837030\n",
      "epoch 108; iter: 0; batch classifier loss: 0.045565; batch adversarial loss: 0.804323\n",
      "epoch 109; iter: 0; batch classifier loss: 0.053946; batch adversarial loss: 0.820073\n",
      "epoch 110; iter: 0; batch classifier loss: 0.034666; batch adversarial loss: 0.832223\n",
      "epoch 111; iter: 0; batch classifier loss: 0.039903; batch adversarial loss: 0.776234\n",
      "epoch 112; iter: 0; batch classifier loss: 0.038999; batch adversarial loss: 0.838230\n",
      "epoch 113; iter: 0; batch classifier loss: 0.062438; batch adversarial loss: 0.846926\n",
      "epoch 114; iter: 0; batch classifier loss: 0.067024; batch adversarial loss: 0.809845\n",
      "epoch 115; iter: 0; batch classifier loss: 0.042852; batch adversarial loss: 0.815522\n",
      "epoch 116; iter: 0; batch classifier loss: 0.066192; batch adversarial loss: 0.787399\n",
      "epoch 117; iter: 0; batch classifier loss: 0.073032; batch adversarial loss: 0.802328\n",
      "epoch 118; iter: 0; batch classifier loss: 0.033000; batch adversarial loss: 0.780549\n",
      "epoch 119; iter: 0; batch classifier loss: 0.089791; batch adversarial loss: 0.784257\n",
      "epoch 120; iter: 0; batch classifier loss: 0.083340; batch adversarial loss: 0.774577\n",
      "epoch 121; iter: 0; batch classifier loss: 0.071045; batch adversarial loss: 0.804103\n",
      "epoch 122; iter: 0; batch classifier loss: 0.052427; batch adversarial loss: 0.778850\n",
      "epoch 123; iter: 0; batch classifier loss: 0.040471; batch adversarial loss: 0.794697\n",
      "epoch 124; iter: 0; batch classifier loss: 0.038735; batch adversarial loss: 0.788965\n",
      "epoch 125; iter: 0; batch classifier loss: 0.067889; batch adversarial loss: 0.775606\n",
      "epoch 126; iter: 0; batch classifier loss: 0.060168; batch adversarial loss: 0.756973\n",
      "epoch 127; iter: 0; batch classifier loss: 0.033929; batch adversarial loss: 0.761025\n",
      "epoch 128; iter: 0; batch classifier loss: 0.070038; batch adversarial loss: 0.789439\n",
      "epoch 129; iter: 0; batch classifier loss: 0.057195; batch adversarial loss: 0.746933\n",
      "epoch 130; iter: 0; batch classifier loss: 0.049970; batch adversarial loss: 0.794454\n",
      "epoch 131; iter: 0; batch classifier loss: 0.043145; batch adversarial loss: 0.764983\n",
      "epoch 132; iter: 0; batch classifier loss: 0.028440; batch adversarial loss: 0.749933\n",
      "epoch 133; iter: 0; batch classifier loss: 0.062472; batch adversarial loss: 0.790224\n",
      "epoch 134; iter: 0; batch classifier loss: 0.054669; batch adversarial loss: 0.762070\n",
      "epoch 135; iter: 0; batch classifier loss: 0.045553; batch adversarial loss: 0.753779\n",
      "epoch 136; iter: 0; batch classifier loss: 0.031291; batch adversarial loss: 0.754555\n",
      "epoch 137; iter: 0; batch classifier loss: 0.042488; batch adversarial loss: 0.727495\n",
      "epoch 138; iter: 0; batch classifier loss: 0.062711; batch adversarial loss: 0.745174\n",
      "epoch 139; iter: 0; batch classifier loss: 0.011831; batch adversarial loss: 0.761598\n",
      "epoch 140; iter: 0; batch classifier loss: 0.044909; batch adversarial loss: 0.755072\n",
      "epoch 141; iter: 0; batch classifier loss: 0.071499; batch adversarial loss: 0.744838\n",
      "epoch 142; iter: 0; batch classifier loss: 0.045839; batch adversarial loss: 0.771697\n",
      "epoch 143; iter: 0; batch classifier loss: 0.025646; batch adversarial loss: 0.719418\n",
      "epoch 144; iter: 0; batch classifier loss: 0.033989; batch adversarial loss: 0.750547\n",
      "epoch 145; iter: 0; batch classifier loss: 0.059747; batch adversarial loss: 0.761928\n",
      "epoch 146; iter: 0; batch classifier loss: 0.046668; batch adversarial loss: 0.749938\n",
      "epoch 147; iter: 0; batch classifier loss: 0.045044; batch adversarial loss: 0.735560\n",
      "epoch 148; iter: 0; batch classifier loss: 0.059939; batch adversarial loss: 0.738521\n",
      "epoch 149; iter: 0; batch classifier loss: 0.025698; batch adversarial loss: 0.719265\n",
      "epoch 150; iter: 0; batch classifier loss: 0.047253; batch adversarial loss: 0.740831\n",
      "epoch 151; iter: 0; batch classifier loss: 0.030041; batch adversarial loss: 0.754653\n",
      "epoch 152; iter: 0; batch classifier loss: 0.046229; batch adversarial loss: 0.711128\n",
      "epoch 153; iter: 0; batch classifier loss: 0.047370; batch adversarial loss: 0.748061\n",
      "epoch 154; iter: 0; batch classifier loss: 0.073546; batch adversarial loss: 0.735434\n",
      "epoch 155; iter: 0; batch classifier loss: 0.034252; batch adversarial loss: 0.723150\n",
      "epoch 156; iter: 0; batch classifier loss: 0.045453; batch adversarial loss: 0.758700\n",
      "epoch 157; iter: 0; batch classifier loss: 0.037942; batch adversarial loss: 0.746091\n",
      "epoch 158; iter: 0; batch classifier loss: 0.043408; batch adversarial loss: 0.744584\n",
      "epoch 159; iter: 0; batch classifier loss: 0.036349; batch adversarial loss: 0.745613\n",
      "epoch 160; iter: 0; batch classifier loss: 0.026241; batch adversarial loss: 0.730479\n",
      "epoch 161; iter: 0; batch classifier loss: 0.050193; batch adversarial loss: 0.726855\n",
      "epoch 162; iter: 0; batch classifier loss: 0.019580; batch adversarial loss: 0.740245\n",
      "epoch 163; iter: 0; batch classifier loss: 0.048584; batch adversarial loss: 0.737031\n",
      "epoch 164; iter: 0; batch classifier loss: 0.060119; batch adversarial loss: 0.728168\n",
      "epoch 165; iter: 0; batch classifier loss: 0.033897; batch adversarial loss: 0.741481\n",
      "epoch 166; iter: 0; batch classifier loss: 0.055435; batch adversarial loss: 0.728699\n",
      "epoch 167; iter: 0; batch classifier loss: 0.039877; batch adversarial loss: 0.727372\n",
      "epoch 168; iter: 0; batch classifier loss: 0.089949; batch adversarial loss: 0.720032\n",
      "epoch 169; iter: 0; batch classifier loss: 0.028385; batch adversarial loss: 0.724677\n",
      "epoch 170; iter: 0; batch classifier loss: 0.045869; batch adversarial loss: 0.730479\n",
      "epoch 171; iter: 0; batch classifier loss: 0.047756; batch adversarial loss: 0.729206\n",
      "epoch 172; iter: 0; batch classifier loss: 0.038334; batch adversarial loss: 0.712056\n",
      "epoch 173; iter: 0; batch classifier loss: 0.044278; batch adversarial loss: 0.701109\n",
      "epoch 174; iter: 0; batch classifier loss: 0.038804; batch adversarial loss: 0.726320\n",
      "epoch 175; iter: 0; batch classifier loss: 0.045351; batch adversarial loss: 0.728342\n",
      "epoch 176; iter: 0; batch classifier loss: 0.038078; batch adversarial loss: 0.722347\n",
      "epoch 177; iter: 0; batch classifier loss: 0.027159; batch adversarial loss: 0.719737\n",
      "epoch 178; iter: 0; batch classifier loss: 0.051483; batch adversarial loss: 0.719332\n",
      "epoch 179; iter: 0; batch classifier loss: 0.056416; batch adversarial loss: 0.724159\n",
      "epoch 180; iter: 0; batch classifier loss: 0.033213; batch adversarial loss: 0.711967\n",
      "epoch 181; iter: 0; batch classifier loss: 0.043317; batch adversarial loss: 0.721084\n",
      "epoch 182; iter: 0; batch classifier loss: 0.044904; batch adversarial loss: 0.723400\n",
      "epoch 183; iter: 0; batch classifier loss: 0.018569; batch adversarial loss: 0.722367\n",
      "epoch 184; iter: 0; batch classifier loss: 0.047909; batch adversarial loss: 0.685594\n",
      "epoch 185; iter: 0; batch classifier loss: 0.047056; batch adversarial loss: 0.724835\n",
      "epoch 186; iter: 0; batch classifier loss: 0.036729; batch adversarial loss: 0.709077\n",
      "epoch 187; iter: 0; batch classifier loss: 0.041155; batch adversarial loss: 0.707818\n",
      "epoch 188; iter: 0; batch classifier loss: 0.038729; batch adversarial loss: 0.710592\n",
      "epoch 189; iter: 0; batch classifier loss: 0.044859; batch adversarial loss: 0.715632\n",
      "epoch 190; iter: 0; batch classifier loss: 0.042089; batch adversarial loss: 0.704757\n",
      "epoch 191; iter: 0; batch classifier loss: 0.020884; batch adversarial loss: 0.712139\n",
      "epoch 192; iter: 0; batch classifier loss: 0.040504; batch adversarial loss: 0.706855\n",
      "epoch 193; iter: 0; batch classifier loss: 0.029979; batch adversarial loss: 0.710148\n",
      "epoch 194; iter: 0; batch classifier loss: 0.033746; batch adversarial loss: 0.709453\n",
      "epoch 195; iter: 0; batch classifier loss: 0.025857; batch adversarial loss: 0.677302\n",
      "epoch 196; iter: 0; batch classifier loss: 0.056822; batch adversarial loss: 0.702844\n",
      "epoch 197; iter: 0; batch classifier loss: 0.042690; batch adversarial loss: 0.699613\n",
      "epoch 198; iter: 0; batch classifier loss: 0.041298; batch adversarial loss: 0.709650\n",
      "epoch 199; iter: 0; batch classifier loss: 0.041212; batch adversarial loss: 0.700317\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:36.278009: W tensorflow/c/c_api.cc:304] Operation '{name:'4c9691fe-adee-11ee-9362-a9d23602d4a3/4c9691fe-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:3205 op device:{requested: '', assigned: ''} def:{{{node 4c9691fe-adee-11ee-9362-a9d23602d4a3/4c9691fe-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c9691fe-adee-11ee-9362-a9d23602d4a3/4c9691fe-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c9691fe-adee-11ee-9362-a9d23602d4a3/4c9691fe-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.840337; batch adversarial loss: 0.692028\n",
      "epoch 1; iter: 0; batch classifier loss: 0.648993; batch adversarial loss: 0.689136\n",
      "epoch 2; iter: 0; batch classifier loss: 0.493219; batch adversarial loss: 0.690014\n",
      "epoch 3; iter: 0; batch classifier loss: 0.391114; batch adversarial loss: 0.687557\n",
      "epoch 4; iter: 0; batch classifier loss: 0.412032; batch adversarial loss: 0.684544\n",
      "epoch 5; iter: 0; batch classifier loss: 0.345594; batch adversarial loss: 0.687694\n",
      "epoch 6; iter: 0; batch classifier loss: 0.324120; batch adversarial loss: 0.689638\n",
      "epoch 7; iter: 0; batch classifier loss: 0.289144; batch adversarial loss: 0.684947\n",
      "epoch 8; iter: 0; batch classifier loss: 0.269119; batch adversarial loss: 0.684922\n",
      "epoch 9; iter: 0; batch classifier loss: 0.292010; batch adversarial loss: 0.687448\n",
      "epoch 10; iter: 0; batch classifier loss: 0.214266; batch adversarial loss: 0.687085\n",
      "epoch 11; iter: 0; batch classifier loss: 0.236060; batch adversarial loss: 0.683370\n",
      "epoch 12; iter: 0; batch classifier loss: 0.234895; batch adversarial loss: 0.690346\n",
      "epoch 13; iter: 0; batch classifier loss: 0.185098; batch adversarial loss: 0.681530\n",
      "epoch 14; iter: 0; batch classifier loss: 0.230475; batch adversarial loss: 0.685102\n",
      "epoch 15; iter: 0; batch classifier loss: 0.149822; batch adversarial loss: 0.684416\n",
      "epoch 16; iter: 0; batch classifier loss: 0.176746; batch adversarial loss: 0.682164\n",
      "epoch 17; iter: 0; batch classifier loss: 0.141410; batch adversarial loss: 0.682065\n",
      "epoch 18; iter: 0; batch classifier loss: 0.193404; batch adversarial loss: 0.676174\n",
      "epoch 19; iter: 0; batch classifier loss: 0.156986; batch adversarial loss: 0.674525\n",
      "epoch 20; iter: 0; batch classifier loss: 0.144350; batch adversarial loss: 0.680463\n",
      "epoch 21; iter: 0; batch classifier loss: 0.118980; batch adversarial loss: 0.681116\n",
      "epoch 22; iter: 0; batch classifier loss: 0.127782; batch adversarial loss: 0.685042\n",
      "epoch 23; iter: 0; batch classifier loss: 0.131617; batch adversarial loss: 0.674443\n",
      "epoch 24; iter: 0; batch classifier loss: 0.129047; batch adversarial loss: 0.684912\n",
      "epoch 25; iter: 0; batch classifier loss: 0.158674; batch adversarial loss: 0.682958\n",
      "epoch 26; iter: 0; batch classifier loss: 0.112431; batch adversarial loss: 0.681516\n",
      "epoch 27; iter: 0; batch classifier loss: 0.134687; batch adversarial loss: 0.678689\n",
      "epoch 28; iter: 0; batch classifier loss: 0.120204; batch adversarial loss: 0.688077\n",
      "epoch 29; iter: 0; batch classifier loss: 0.113626; batch adversarial loss: 0.689731\n",
      "epoch 30; iter: 0; batch classifier loss: 0.097257; batch adversarial loss: 0.680044\n",
      "epoch 31; iter: 0; batch classifier loss: 0.087406; batch adversarial loss: 0.686988\n",
      "epoch 32; iter: 0; batch classifier loss: 0.105966; batch adversarial loss: 0.677270\n",
      "epoch 33; iter: 0; batch classifier loss: 0.103192; batch adversarial loss: 0.684803\n",
      "epoch 34; iter: 0; batch classifier loss: 0.100584; batch adversarial loss: 0.688048\n",
      "epoch 35; iter: 0; batch classifier loss: 0.120434; batch adversarial loss: 0.678505\n",
      "epoch 36; iter: 0; batch classifier loss: 0.103323; batch adversarial loss: 0.673151\n",
      "epoch 37; iter: 0; batch classifier loss: 0.070183; batch adversarial loss: 0.662204\n",
      "epoch 38; iter: 0; batch classifier loss: 0.087260; batch adversarial loss: 0.671132\n",
      "epoch 39; iter: 0; batch classifier loss: 0.079793; batch adversarial loss: 0.682850\n",
      "epoch 40; iter: 0; batch classifier loss: 0.072544; batch adversarial loss: 0.680832\n",
      "epoch 41; iter: 0; batch classifier loss: 0.076539; batch adversarial loss: 0.671964\n",
      "epoch 42; iter: 0; batch classifier loss: 0.074877; batch adversarial loss: 0.677328\n",
      "epoch 43; iter: 0; batch classifier loss: 0.069987; batch adversarial loss: 0.681426\n",
      "epoch 44; iter: 0; batch classifier loss: 0.084078; batch adversarial loss: 0.670975\n",
      "epoch 45; iter: 0; batch classifier loss: 0.077725; batch adversarial loss: 0.679373\n",
      "epoch 46; iter: 0; batch classifier loss: 0.052678; batch adversarial loss: 0.662714\n",
      "epoch 47; iter: 0; batch classifier loss: 0.062068; batch adversarial loss: 0.674261\n",
      "epoch 48; iter: 0; batch classifier loss: 0.073907; batch adversarial loss: 0.686466\n",
      "epoch 49; iter: 0; batch classifier loss: 0.085090; batch adversarial loss: 0.671671\n",
      "epoch 50; iter: 0; batch classifier loss: 0.048654; batch adversarial loss: 0.681502\n",
      "epoch 51; iter: 0; batch classifier loss: 0.064465; batch adversarial loss: 0.683713\n",
      "epoch 52; iter: 0; batch classifier loss: 0.060555; batch adversarial loss: 0.669319\n",
      "epoch 53; iter: 0; batch classifier loss: 0.044708; batch adversarial loss: 0.686542\n",
      "epoch 54; iter: 0; batch classifier loss: 0.064966; batch adversarial loss: 0.679835\n",
      "epoch 55; iter: 0; batch classifier loss: 0.068268; batch adversarial loss: 0.664502\n",
      "epoch 56; iter: 0; batch classifier loss: 0.070108; batch adversarial loss: 0.691682\n",
      "epoch 57; iter: 0; batch classifier loss: 0.055021; batch adversarial loss: 0.682772\n",
      "epoch 58; iter: 0; batch classifier loss: 0.045947; batch adversarial loss: 0.677019\n",
      "epoch 59; iter: 0; batch classifier loss: 0.029427; batch adversarial loss: 0.680668\n",
      "epoch 60; iter: 0; batch classifier loss: 0.042569; batch adversarial loss: 0.669275\n",
      "epoch 61; iter: 0; batch classifier loss: 0.038541; batch adversarial loss: 0.671864\n",
      "epoch 62; iter: 0; batch classifier loss: 0.040106; batch adversarial loss: 0.697337\n",
      "epoch 63; iter: 0; batch classifier loss: 0.061225; batch adversarial loss: 0.659430\n",
      "epoch 64; iter: 0; batch classifier loss: 0.045826; batch adversarial loss: 0.683450\n",
      "epoch 65; iter: 0; batch classifier loss: 0.042854; batch adversarial loss: 0.679375\n",
      "epoch 66; iter: 0; batch classifier loss: 0.028513; batch adversarial loss: 0.672872\n",
      "epoch 67; iter: 0; batch classifier loss: 0.037239; batch adversarial loss: 0.687112\n",
      "epoch 68; iter: 0; batch classifier loss: 0.049349; batch adversarial loss: 0.682175\n",
      "epoch 69; iter: 0; batch classifier loss: 0.044142; batch adversarial loss: 0.669037\n",
      "epoch 70; iter: 0; batch classifier loss: 0.027808; batch adversarial loss: 0.692095\n",
      "epoch 71; iter: 0; batch classifier loss: 0.030873; batch adversarial loss: 0.671647\n",
      "epoch 72; iter: 0; batch classifier loss: 0.032347; batch adversarial loss: 0.666536\n",
      "epoch 73; iter: 0; batch classifier loss: 0.027544; batch adversarial loss: 0.690010\n",
      "epoch 74; iter: 0; batch classifier loss: 0.050893; batch adversarial loss: 0.683630\n",
      "epoch 75; iter: 0; batch classifier loss: 0.030344; batch adversarial loss: 0.675909\n",
      "epoch 76; iter: 0; batch classifier loss: 0.024919; batch adversarial loss: 0.694956\n",
      "epoch 77; iter: 0; batch classifier loss: 0.018633; batch adversarial loss: 0.692615\n",
      "epoch 78; iter: 0; batch classifier loss: 0.022203; batch adversarial loss: 0.674721\n",
      "epoch 79; iter: 0; batch classifier loss: 0.023198; batch adversarial loss: 0.674158\n",
      "epoch 80; iter: 0; batch classifier loss: 0.035118; batch adversarial loss: 0.672422\n",
      "epoch 81; iter: 0; batch classifier loss: 0.024622; batch adversarial loss: 0.673153\n",
      "epoch 82; iter: 0; batch classifier loss: 0.026215; batch adversarial loss: 0.660955\n",
      "epoch 83; iter: 0; batch classifier loss: 0.027949; batch adversarial loss: 0.688089\n",
      "epoch 84; iter: 0; batch classifier loss: 0.022998; batch adversarial loss: 0.677283\n",
      "epoch 85; iter: 0; batch classifier loss: 0.022770; batch adversarial loss: 0.689378\n",
      "epoch 86; iter: 0; batch classifier loss: 0.027122; batch adversarial loss: 0.675410\n",
      "epoch 87; iter: 0; batch classifier loss: 0.022159; batch adversarial loss: 0.670034\n",
      "epoch 88; iter: 0; batch classifier loss: 0.030818; batch adversarial loss: 0.674249\n",
      "epoch 89; iter: 0; batch classifier loss: 0.022275; batch adversarial loss: 0.679938\n",
      "epoch 90; iter: 0; batch classifier loss: 0.021443; batch adversarial loss: 0.677591\n",
      "epoch 91; iter: 0; batch classifier loss: 0.019965; batch adversarial loss: 0.671576\n",
      "epoch 92; iter: 0; batch classifier loss: 0.023803; batch adversarial loss: 0.677739\n",
      "epoch 93; iter: 0; batch classifier loss: 0.018873; batch adversarial loss: 0.673812\n",
      "epoch 94; iter: 0; batch classifier loss: 0.024489; batch adversarial loss: 0.690256\n",
      "epoch 95; iter: 0; batch classifier loss: 0.018015; batch adversarial loss: 0.669879\n",
      "epoch 96; iter: 0; batch classifier loss: 0.022167; batch adversarial loss: 0.688867\n",
      "epoch 97; iter: 0; batch classifier loss: 0.023698; batch adversarial loss: 0.658544\n",
      "epoch 98; iter: 0; batch classifier loss: 0.020533; batch adversarial loss: 0.661402\n",
      "epoch 99; iter: 0; batch classifier loss: 0.029283; batch adversarial loss: 0.677921\n",
      "epoch 100; iter: 0; batch classifier loss: 0.014743; batch adversarial loss: 0.671638\n",
      "epoch 101; iter: 0; batch classifier loss: 0.018288; batch adversarial loss: 0.691429\n",
      "epoch 102; iter: 0; batch classifier loss: 0.016269; batch adversarial loss: 0.693508\n",
      "epoch 103; iter: 0; batch classifier loss: 0.014109; batch adversarial loss: 0.672594\n",
      "epoch 104; iter: 0; batch classifier loss: 0.012613; batch adversarial loss: 0.660911\n",
      "epoch 105; iter: 0; batch classifier loss: 0.014073; batch adversarial loss: 0.674332\n",
      "epoch 106; iter: 0; batch classifier loss: 0.016919; batch adversarial loss: 0.683403\n",
      "epoch 107; iter: 0; batch classifier loss: 0.020023; batch adversarial loss: 0.683560\n",
      "epoch 108; iter: 0; batch classifier loss: 0.018262; batch adversarial loss: 0.701552\n",
      "epoch 109; iter: 0; batch classifier loss: 0.018540; batch adversarial loss: 0.688134\n",
      "epoch 110; iter: 0; batch classifier loss: 0.020595; batch adversarial loss: 0.699234\n",
      "epoch 111; iter: 0; batch classifier loss: 0.017176; batch adversarial loss: 0.685495\n",
      "epoch 112; iter: 0; batch classifier loss: 0.023547; batch adversarial loss: 0.626810\n",
      "epoch 113; iter: 0; batch classifier loss: 0.016162; batch adversarial loss: 0.655363\n",
      "epoch 114; iter: 0; batch classifier loss: 0.014141; batch adversarial loss: 0.688644\n",
      "epoch 115; iter: 0; batch classifier loss: 0.010887; batch adversarial loss: 0.663610\n",
      "epoch 116; iter: 0; batch classifier loss: 0.011140; batch adversarial loss: 0.679071\n",
      "epoch 117; iter: 0; batch classifier loss: 0.020896; batch adversarial loss: 0.656461\n",
      "epoch 118; iter: 0; batch classifier loss: 0.013899; batch adversarial loss: 0.678732\n",
      "epoch 119; iter: 0; batch classifier loss: 0.011665; batch adversarial loss: 0.696155\n",
      "epoch 120; iter: 0; batch classifier loss: 0.015363; batch adversarial loss: 0.664801\n",
      "epoch 121; iter: 0; batch classifier loss: 0.008636; batch adversarial loss: 0.675057\n",
      "epoch 122; iter: 0; batch classifier loss: 0.015386; batch adversarial loss: 0.664012\n",
      "epoch 123; iter: 0; batch classifier loss: 0.013240; batch adversarial loss: 0.656840\n",
      "epoch 124; iter: 0; batch classifier loss: 0.010186; batch adversarial loss: 0.670875\n",
      "epoch 125; iter: 0; batch classifier loss: 0.014920; batch adversarial loss: 0.682467\n",
      "epoch 126; iter: 0; batch classifier loss: 0.010527; batch adversarial loss: 0.674336\n",
      "epoch 127; iter: 0; batch classifier loss: 0.011825; batch adversarial loss: 0.685773\n",
      "epoch 128; iter: 0; batch classifier loss: 0.009223; batch adversarial loss: 0.670244\n",
      "epoch 129; iter: 0; batch classifier loss: 0.008299; batch adversarial loss: 0.676613\n",
      "epoch 130; iter: 0; batch classifier loss: 0.012749; batch adversarial loss: 0.662916\n",
      "epoch 131; iter: 0; batch classifier loss: 0.009345; batch adversarial loss: 0.679949\n",
      "epoch 132; iter: 0; batch classifier loss: 0.011548; batch adversarial loss: 0.675798\n",
      "epoch 133; iter: 0; batch classifier loss: 0.007352; batch adversarial loss: 0.658449\n",
      "epoch 134; iter: 0; batch classifier loss: 0.007900; batch adversarial loss: 0.684661\n",
      "epoch 135; iter: 0; batch classifier loss: 0.012056; batch adversarial loss: 0.669907\n",
      "epoch 136; iter: 0; batch classifier loss: 0.010993; batch adversarial loss: 0.685981\n",
      "epoch 137; iter: 0; batch classifier loss: 0.009190; batch adversarial loss: 0.688319\n",
      "epoch 138; iter: 0; batch classifier loss: 0.013100; batch adversarial loss: 0.680758\n",
      "epoch 139; iter: 0; batch classifier loss: 0.007128; batch adversarial loss: 0.659280\n",
      "epoch 140; iter: 0; batch classifier loss: 0.008895; batch adversarial loss: 0.684038\n",
      "epoch 141; iter: 0; batch classifier loss: 0.009269; batch adversarial loss: 0.697250\n",
      "epoch 142; iter: 0; batch classifier loss: 0.010991; batch adversarial loss: 0.701528\n",
      "epoch 143; iter: 0; batch classifier loss: 0.015199; batch adversarial loss: 0.679388\n",
      "epoch 144; iter: 0; batch classifier loss: 0.008701; batch adversarial loss: 0.671191\n",
      "epoch 145; iter: 0; batch classifier loss: 0.006558; batch adversarial loss: 0.687427\n",
      "epoch 146; iter: 0; batch classifier loss: 0.008832; batch adversarial loss: 0.682162\n",
      "epoch 147; iter: 0; batch classifier loss: 0.004747; batch adversarial loss: 0.668001\n",
      "epoch 148; iter: 0; batch classifier loss: 0.009225; batch adversarial loss: 0.671030\n",
      "epoch 149; iter: 0; batch classifier loss: 0.009968; batch adversarial loss: 0.659436\n",
      "epoch 150; iter: 0; batch classifier loss: 0.010910; batch adversarial loss: 0.686859\n",
      "epoch 151; iter: 0; batch classifier loss: 0.005279; batch adversarial loss: 0.676903\n",
      "epoch 152; iter: 0; batch classifier loss: 0.007402; batch adversarial loss: 0.648777\n",
      "epoch 153; iter: 0; batch classifier loss: 0.006725; batch adversarial loss: 0.675279\n",
      "epoch 154; iter: 0; batch classifier loss: 0.010501; batch adversarial loss: 0.680544\n",
      "epoch 155; iter: 0; batch classifier loss: 0.010764; batch adversarial loss: 0.692253\n",
      "epoch 156; iter: 0; batch classifier loss: 0.005291; batch adversarial loss: 0.678930\n",
      "epoch 157; iter: 0; batch classifier loss: 0.003760; batch adversarial loss: 0.674448\n",
      "epoch 158; iter: 0; batch classifier loss: 0.006355; batch adversarial loss: 0.695607\n",
      "epoch 159; iter: 0; batch classifier loss: 0.006288; batch adversarial loss: 0.664454\n",
      "epoch 160; iter: 0; batch classifier loss: 0.008738; batch adversarial loss: 0.685232\n",
      "epoch 161; iter: 0; batch classifier loss: 0.007245; batch adversarial loss: 0.648427\n",
      "epoch 162; iter: 0; batch classifier loss: 0.005134; batch adversarial loss: 0.697751\n",
      "epoch 163; iter: 0; batch classifier loss: 0.009469; batch adversarial loss: 0.692706\n",
      "epoch 164; iter: 0; batch classifier loss: 0.009434; batch adversarial loss: 0.665705\n",
      "epoch 165; iter: 0; batch classifier loss: 0.005328; batch adversarial loss: 0.680459\n",
      "epoch 166; iter: 0; batch classifier loss: 0.005809; batch adversarial loss: 0.673864\n",
      "epoch 167; iter: 0; batch classifier loss: 0.006138; batch adversarial loss: 0.665069\n",
      "epoch 168; iter: 0; batch classifier loss: 0.006303; batch adversarial loss: 0.675484\n",
      "epoch 169; iter: 0; batch classifier loss: 0.003307; batch adversarial loss: 0.662718\n",
      "epoch 170; iter: 0; batch classifier loss: 0.007898; batch adversarial loss: 0.675644\n",
      "epoch 171; iter: 0; batch classifier loss: 0.005815; batch adversarial loss: 0.686549\n",
      "epoch 172; iter: 0; batch classifier loss: 0.005539; batch adversarial loss: 0.674993\n",
      "epoch 173; iter: 0; batch classifier loss: 0.005619; batch adversarial loss: 0.681131\n",
      "epoch 174; iter: 0; batch classifier loss: 0.006988; batch adversarial loss: 0.672143\n",
      "epoch 175; iter: 0; batch classifier loss: 0.003967; batch adversarial loss: 0.660277\n",
      "epoch 176; iter: 0; batch classifier loss: 0.004839; batch adversarial loss: 0.661124\n",
      "epoch 177; iter: 0; batch classifier loss: 0.010247; batch adversarial loss: 0.670405\n",
      "epoch 178; iter: 0; batch classifier loss: 0.010271; batch adversarial loss: 0.668902\n",
      "epoch 179; iter: 0; batch classifier loss: 0.003947; batch adversarial loss: 0.673315\n",
      "epoch 180; iter: 0; batch classifier loss: 0.004201; batch adversarial loss: 0.674556\n",
      "epoch 181; iter: 0; batch classifier loss: 0.004685; batch adversarial loss: 0.655992\n",
      "epoch 182; iter: 0; batch classifier loss: 0.005684; batch adversarial loss: 0.690386\n",
      "epoch 183; iter: 0; batch classifier loss: 0.003420; batch adversarial loss: 0.717252\n",
      "epoch 184; iter: 0; batch classifier loss: 0.005229; batch adversarial loss: 0.673789\n",
      "epoch 185; iter: 0; batch classifier loss: 0.007510; batch adversarial loss: 0.695621\n",
      "epoch 186; iter: 0; batch classifier loss: 0.004948; batch adversarial loss: 0.671743\n",
      "epoch 187; iter: 0; batch classifier loss: 0.003620; batch adversarial loss: 0.676326\n",
      "epoch 188; iter: 0; batch classifier loss: 0.008396; batch adversarial loss: 0.681789\n",
      "epoch 189; iter: 0; batch classifier loss: 0.004148; batch adversarial loss: 0.661307\n",
      "epoch 190; iter: 0; batch classifier loss: 0.004094; batch adversarial loss: 0.680236\n",
      "epoch 191; iter: 0; batch classifier loss: 0.004491; batch adversarial loss: 0.667729\n",
      "epoch 192; iter: 0; batch classifier loss: 0.003724; batch adversarial loss: 0.689214\n",
      "epoch 193; iter: 0; batch classifier loss: 0.006190; batch adversarial loss: 0.679045\n",
      "epoch 194; iter: 0; batch classifier loss: 0.004405; batch adversarial loss: 0.663184\n",
      "epoch 195; iter: 0; batch classifier loss: 0.004772; batch adversarial loss: 0.698211\n",
      "epoch 196; iter: 0; batch classifier loss: 0.005902; batch adversarial loss: 0.645941\n",
      "epoch 197; iter: 0; batch classifier loss: 0.005783; batch adversarial loss: 0.690234\n",
      "epoch 198; iter: 0; batch classifier loss: 0.005631; batch adversarial loss: 0.677422\n",
      "epoch 199; iter: 0; batch classifier loss: 0.003653; batch adversarial loss: 0.675345\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:37.968966: W tensorflow/c/c_api.cc:304] Operation '{name:'4c969320-adee-11ee-9362-a9d23602d4a3/4c969320-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:4012 op device:{requested: '', assigned: ''} def:{{{node 4c969320-adee-11ee-9362-a9d23602d4a3/4c969320-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c969320-adee-11ee-9362-a9d23602d4a3/4c969320-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c969320-adee-11ee-9362-a9d23602d4a3/4c969320-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.566267; batch adversarial loss: 0.691371\n",
      "epoch 1; iter: 0; batch classifier loss: 0.475459; batch adversarial loss: 0.677016\n",
      "epoch 2; iter: 0; batch classifier loss: 0.409502; batch adversarial loss: 0.760778\n",
      "epoch 3; iter: 0; batch classifier loss: 0.354672; batch adversarial loss: 0.671862\n",
      "epoch 4; iter: 0; batch classifier loss: 0.349827; batch adversarial loss: 0.709402\n",
      "epoch 5; iter: 0; batch classifier loss: 0.287312; batch adversarial loss: 0.755954\n",
      "epoch 6; iter: 0; batch classifier loss: 0.320793; batch adversarial loss: 0.800123\n",
      "epoch 7; iter: 0; batch classifier loss: 0.226405; batch adversarial loss: 0.751595\n",
      "epoch 8; iter: 0; batch classifier loss: 0.223052; batch adversarial loss: 0.745249\n",
      "epoch 9; iter: 0; batch classifier loss: 0.215588; batch adversarial loss: 0.766763\n",
      "epoch 10; iter: 0; batch classifier loss: 0.238378; batch adversarial loss: 0.741343\n",
      "epoch 11; iter: 0; batch classifier loss: 0.189374; batch adversarial loss: 0.681048\n",
      "epoch 12; iter: 0; batch classifier loss: 0.162883; batch adversarial loss: 0.709056\n",
      "epoch 13; iter: 0; batch classifier loss: 0.169065; batch adversarial loss: 0.752979\n",
      "epoch 14; iter: 0; batch classifier loss: 0.177752; batch adversarial loss: 0.649935\n",
      "epoch 15; iter: 0; batch classifier loss: 0.184324; batch adversarial loss: 0.719683\n",
      "epoch 16; iter: 0; batch classifier loss: 0.171705; batch adversarial loss: 0.747611\n",
      "epoch 17; iter: 0; batch classifier loss: 0.150307; batch adversarial loss: 0.862245\n",
      "epoch 18; iter: 0; batch classifier loss: 0.173783; batch adversarial loss: 0.743238\n",
      "epoch 19; iter: 0; batch classifier loss: 0.114633; batch adversarial loss: 0.704738\n",
      "epoch 20; iter: 0; batch classifier loss: 0.114882; batch adversarial loss: 0.685393\n",
      "epoch 21; iter: 0; batch classifier loss: 0.147462; batch adversarial loss: 0.731477\n",
      "epoch 22; iter: 0; batch classifier loss: 0.092826; batch adversarial loss: 0.712890\n",
      "epoch 23; iter: 0; batch classifier loss: 0.154457; batch adversarial loss: 0.703372\n",
      "epoch 24; iter: 0; batch classifier loss: 0.111012; batch adversarial loss: 0.689201\n",
      "epoch 25; iter: 0; batch classifier loss: 0.098228; batch adversarial loss: 0.809362\n",
      "epoch 26; iter: 0; batch classifier loss: 0.086198; batch adversarial loss: 0.670429\n",
      "epoch 27; iter: 0; batch classifier loss: 0.081776; batch adversarial loss: 0.765439\n",
      "epoch 28; iter: 0; batch classifier loss: 0.087137; batch adversarial loss: 0.719679\n",
      "epoch 29; iter: 0; batch classifier loss: 0.093494; batch adversarial loss: 0.766464\n",
      "epoch 30; iter: 0; batch classifier loss: 0.107525; batch adversarial loss: 0.722812\n",
      "epoch 31; iter: 0; batch classifier loss: 0.086070; batch adversarial loss: 0.744428\n",
      "epoch 32; iter: 0; batch classifier loss: 0.091229; batch adversarial loss: 0.770432\n",
      "epoch 33; iter: 0; batch classifier loss: 0.078717; batch adversarial loss: 0.642482\n",
      "epoch 34; iter: 0; batch classifier loss: 0.052792; batch adversarial loss: 0.846606\n",
      "epoch 35; iter: 0; batch classifier loss: 0.049359; batch adversarial loss: 0.705970\n",
      "epoch 36; iter: 0; batch classifier loss: 0.084283; batch adversarial loss: 0.704502\n",
      "epoch 37; iter: 0; batch classifier loss: 0.064664; batch adversarial loss: 0.775121\n",
      "epoch 38; iter: 0; batch classifier loss: 0.081974; batch adversarial loss: 0.702193\n",
      "epoch 39; iter: 0; batch classifier loss: 0.075976; batch adversarial loss: 0.693170\n",
      "epoch 40; iter: 0; batch classifier loss: 0.051731; batch adversarial loss: 0.696048\n",
      "epoch 41; iter: 0; batch classifier loss: 0.076060; batch adversarial loss: 0.660647\n",
      "epoch 42; iter: 0; batch classifier loss: 0.071245; batch adversarial loss: 0.715454\n",
      "epoch 43; iter: 0; batch classifier loss: 0.069309; batch adversarial loss: 0.690085\n",
      "epoch 44; iter: 0; batch classifier loss: 0.061179; batch adversarial loss: 0.667043\n",
      "epoch 45; iter: 0; batch classifier loss: 0.054068; batch adversarial loss: 0.656869\n",
      "epoch 46; iter: 0; batch classifier loss: 0.044858; batch adversarial loss: 0.717980\n",
      "epoch 47; iter: 0; batch classifier loss: 0.062374; batch adversarial loss: 0.734722\n",
      "epoch 48; iter: 0; batch classifier loss: 0.058161; batch adversarial loss: 0.726292\n",
      "epoch 49; iter: 0; batch classifier loss: 0.051805; batch adversarial loss: 0.645862\n",
      "epoch 50; iter: 0; batch classifier loss: 0.042344; batch adversarial loss: 0.704205\n",
      "epoch 51; iter: 0; batch classifier loss: 0.053090; batch adversarial loss: 0.687328\n",
      "epoch 52; iter: 0; batch classifier loss: 0.031805; batch adversarial loss: 0.685833\n",
      "epoch 53; iter: 0; batch classifier loss: 0.050557; batch adversarial loss: 0.753071\n",
      "epoch 54; iter: 0; batch classifier loss: 0.059694; batch adversarial loss: 0.631297\n",
      "epoch 55; iter: 0; batch classifier loss: 0.054591; batch adversarial loss: 0.701325\n",
      "epoch 56; iter: 0; batch classifier loss: 0.047016; batch adversarial loss: 0.696076\n",
      "epoch 57; iter: 0; batch classifier loss: 0.036310; batch adversarial loss: 0.750130\n",
      "epoch 58; iter: 0; batch classifier loss: 0.040944; batch adversarial loss: 0.773558\n",
      "epoch 59; iter: 0; batch classifier loss: 0.031702; batch adversarial loss: 0.683549\n",
      "epoch 60; iter: 0; batch classifier loss: 0.024680; batch adversarial loss: 0.740012\n",
      "epoch 61; iter: 0; batch classifier loss: 0.028309; batch adversarial loss: 0.669096\n",
      "epoch 62; iter: 0; batch classifier loss: 0.023382; batch adversarial loss: 0.748814\n",
      "epoch 63; iter: 0; batch classifier loss: 0.050760; batch adversarial loss: 0.691603\n",
      "epoch 64; iter: 0; batch classifier loss: 0.035456; batch adversarial loss: 0.717539\n",
      "epoch 65; iter: 0; batch classifier loss: 0.028096; batch adversarial loss: 0.741606\n",
      "epoch 66; iter: 0; batch classifier loss: 0.036402; batch adversarial loss: 0.689188\n",
      "epoch 67; iter: 0; batch classifier loss: 0.024742; batch adversarial loss: 0.708405\n",
      "epoch 68; iter: 0; batch classifier loss: 0.027082; batch adversarial loss: 0.729639\n",
      "epoch 69; iter: 0; batch classifier loss: 0.032121; batch adversarial loss: 0.676676\n",
      "epoch 70; iter: 0; batch classifier loss: 0.027413; batch adversarial loss: 0.705922\n",
      "epoch 71; iter: 0; batch classifier loss: 0.032112; batch adversarial loss: 0.683233\n",
      "epoch 72; iter: 0; batch classifier loss: 0.026918; batch adversarial loss: 0.756924\n",
      "epoch 73; iter: 0; batch classifier loss: 0.030553; batch adversarial loss: 0.672974\n",
      "epoch 74; iter: 0; batch classifier loss: 0.029582; batch adversarial loss: 0.747148\n",
      "epoch 75; iter: 0; batch classifier loss: 0.018770; batch adversarial loss: 0.646326\n",
      "epoch 76; iter: 0; batch classifier loss: 0.028266; batch adversarial loss: 0.674929\n",
      "epoch 77; iter: 0; batch classifier loss: 0.020249; batch adversarial loss: 0.654891\n",
      "epoch 78; iter: 0; batch classifier loss: 0.025278; batch adversarial loss: 0.712189\n",
      "epoch 79; iter: 0; batch classifier loss: 0.022109; batch adversarial loss: 0.726214\n",
      "epoch 80; iter: 0; batch classifier loss: 0.016472; batch adversarial loss: 0.675152\n",
      "epoch 81; iter: 0; batch classifier loss: 0.017007; batch adversarial loss: 0.642027\n",
      "epoch 82; iter: 0; batch classifier loss: 0.017634; batch adversarial loss: 0.704263\n",
      "epoch 83; iter: 0; batch classifier loss: 0.023279; batch adversarial loss: 0.655563\n",
      "epoch 84; iter: 0; batch classifier loss: 0.017361; batch adversarial loss: 0.686247\n",
      "epoch 85; iter: 0; batch classifier loss: 0.019123; batch adversarial loss: 0.716662\n",
      "epoch 86; iter: 0; batch classifier loss: 0.025986; batch adversarial loss: 0.695223\n",
      "epoch 87; iter: 0; batch classifier loss: 0.018312; batch adversarial loss: 0.659341\n",
      "epoch 88; iter: 0; batch classifier loss: 0.011925; batch adversarial loss: 0.680540\n",
      "epoch 89; iter: 0; batch classifier loss: 0.026657; batch adversarial loss: 0.664227\n",
      "epoch 90; iter: 0; batch classifier loss: 0.011400; batch adversarial loss: 0.716024\n",
      "epoch 91; iter: 0; batch classifier loss: 0.017275; batch adversarial loss: 0.671020\n",
      "epoch 92; iter: 0; batch classifier loss: 0.016110; batch adversarial loss: 0.636864\n",
      "epoch 93; iter: 0; batch classifier loss: 0.016382; batch adversarial loss: 0.671287\n",
      "epoch 94; iter: 0; batch classifier loss: 0.013269; batch adversarial loss: 0.732138\n",
      "epoch 95; iter: 0; batch classifier loss: 0.012695; batch adversarial loss: 0.712807\n",
      "epoch 96; iter: 0; batch classifier loss: 0.014663; batch adversarial loss: 0.668529\n",
      "epoch 97; iter: 0; batch classifier loss: 0.012211; batch adversarial loss: 0.647273\n",
      "epoch 98; iter: 0; batch classifier loss: 0.010362; batch adversarial loss: 0.679509\n",
      "epoch 99; iter: 0; batch classifier loss: 0.011096; batch adversarial loss: 0.664423\n",
      "epoch 100; iter: 0; batch classifier loss: 0.012811; batch adversarial loss: 0.707721\n",
      "epoch 101; iter: 0; batch classifier loss: 0.013247; batch adversarial loss: 0.686360\n",
      "epoch 102; iter: 0; batch classifier loss: 0.014064; batch adversarial loss: 0.657178\n",
      "epoch 103; iter: 0; batch classifier loss: 0.012434; batch adversarial loss: 0.682438\n",
      "epoch 104; iter: 0; batch classifier loss: 0.014633; batch adversarial loss: 0.655579\n",
      "epoch 105; iter: 0; batch classifier loss: 0.009074; batch adversarial loss: 0.644834\n",
      "epoch 106; iter: 0; batch classifier loss: 0.012343; batch adversarial loss: 0.668677\n",
      "epoch 107; iter: 0; batch classifier loss: 0.013249; batch adversarial loss: 0.692995\n",
      "epoch 108; iter: 0; batch classifier loss: 0.006634; batch adversarial loss: 0.612876\n",
      "epoch 109; iter: 0; batch classifier loss: 0.011523; batch adversarial loss: 0.681624\n",
      "epoch 110; iter: 0; batch classifier loss: 0.009038; batch adversarial loss: 0.640292\n",
      "epoch 111; iter: 0; batch classifier loss: 0.008430; batch adversarial loss: 0.626531\n",
      "epoch 112; iter: 0; batch classifier loss: 0.008578; batch adversarial loss: 0.702642\n",
      "epoch 113; iter: 0; batch classifier loss: 0.009429; batch adversarial loss: 0.650639\n",
      "epoch 114; iter: 0; batch classifier loss: 0.007479; batch adversarial loss: 0.672963\n",
      "epoch 115; iter: 0; batch classifier loss: 0.008709; batch adversarial loss: 0.706575\n",
      "epoch 116; iter: 0; batch classifier loss: 0.006864; batch adversarial loss: 0.632100\n",
      "epoch 117; iter: 0; batch classifier loss: 0.010515; batch adversarial loss: 0.686291\n",
      "epoch 118; iter: 0; batch classifier loss: 0.007345; batch adversarial loss: 0.653269\n",
      "epoch 119; iter: 0; batch classifier loss: 0.011610; batch adversarial loss: 0.704668\n",
      "epoch 120; iter: 0; batch classifier loss: 0.018477; batch adversarial loss: 0.634845\n",
      "epoch 121; iter: 0; batch classifier loss: 0.013352; batch adversarial loss: 0.664520\n",
      "epoch 122; iter: 0; batch classifier loss: 0.007012; batch adversarial loss: 0.687161\n",
      "epoch 123; iter: 0; batch classifier loss: 0.008586; batch adversarial loss: 0.675204\n",
      "epoch 124; iter: 0; batch classifier loss: 0.007114; batch adversarial loss: 0.692565\n",
      "epoch 125; iter: 0; batch classifier loss: 0.008116; batch adversarial loss: 0.679636\n",
      "epoch 126; iter: 0; batch classifier loss: 0.007789; batch adversarial loss: 0.693665\n",
      "epoch 127; iter: 0; batch classifier loss: 0.009701; batch adversarial loss: 0.670550\n",
      "epoch 128; iter: 0; batch classifier loss: 0.005973; batch adversarial loss: 0.698273\n",
      "epoch 129; iter: 0; batch classifier loss: 0.008739; batch adversarial loss: 0.678360\n",
      "epoch 130; iter: 0; batch classifier loss: 0.005135; batch adversarial loss: 0.671694\n",
      "epoch 131; iter: 0; batch classifier loss: 0.007846; batch adversarial loss: 0.696854\n",
      "epoch 132; iter: 0; batch classifier loss: 0.006245; batch adversarial loss: 0.671939\n",
      "epoch 133; iter: 0; batch classifier loss: 0.007174; batch adversarial loss: 0.663163\n",
      "epoch 134; iter: 0; batch classifier loss: 0.007563; batch adversarial loss: 0.670345\n",
      "epoch 135; iter: 0; batch classifier loss: 0.009627; batch adversarial loss: 0.612392\n",
      "epoch 136; iter: 0; batch classifier loss: 0.007197; batch adversarial loss: 0.693711\n",
      "epoch 137; iter: 0; batch classifier loss: 0.006287; batch adversarial loss: 0.665470\n",
      "epoch 138; iter: 0; batch classifier loss: 0.005759; batch adversarial loss: 0.696764\n",
      "epoch 139; iter: 0; batch classifier loss: 0.006346; batch adversarial loss: 0.660360\n",
      "epoch 140; iter: 0; batch classifier loss: 0.006410; batch adversarial loss: 0.627464\n",
      "epoch 141; iter: 0; batch classifier loss: 0.004803; batch adversarial loss: 0.681073\n",
      "epoch 142; iter: 0; batch classifier loss: 0.004910; batch adversarial loss: 0.681424\n",
      "epoch 143; iter: 0; batch classifier loss: 0.005796; batch adversarial loss: 0.657520\n",
      "epoch 144; iter: 0; batch classifier loss: 0.006109; batch adversarial loss: 0.711050\n",
      "epoch 145; iter: 0; batch classifier loss: 0.006299; batch adversarial loss: 0.655013\n",
      "epoch 146; iter: 0; batch classifier loss: 0.006067; batch adversarial loss: 0.745945\n",
      "epoch 147; iter: 0; batch classifier loss: 0.005164; batch adversarial loss: 0.642666\n",
      "epoch 148; iter: 0; batch classifier loss: 0.008009; batch adversarial loss: 0.680937\n",
      "epoch 149; iter: 0; batch classifier loss: 0.003799; batch adversarial loss: 0.701150\n",
      "epoch 150; iter: 0; batch classifier loss: 0.003711; batch adversarial loss: 0.668557\n",
      "epoch 151; iter: 0; batch classifier loss: 0.004799; batch adversarial loss: 0.639665\n",
      "epoch 152; iter: 0; batch classifier loss: 0.004820; batch adversarial loss: 0.631117\n",
      "epoch 153; iter: 0; batch classifier loss: 0.005558; batch adversarial loss: 0.641040\n",
      "epoch 154; iter: 0; batch classifier loss: 0.003754; batch adversarial loss: 0.717137\n",
      "epoch 155; iter: 0; batch classifier loss: 0.005719; batch adversarial loss: 0.668234\n",
      "epoch 156; iter: 0; batch classifier loss: 0.004973; batch adversarial loss: 0.679695\n",
      "epoch 157; iter: 0; batch classifier loss: 0.004466; batch adversarial loss: 0.647775\n",
      "epoch 158; iter: 0; batch classifier loss: 0.006466; batch adversarial loss: 0.681051\n",
      "epoch 159; iter: 0; batch classifier loss: 0.006848; batch adversarial loss: 0.649380\n",
      "epoch 160; iter: 0; batch classifier loss: 0.005569; batch adversarial loss: 0.724005\n",
      "epoch 161; iter: 0; batch classifier loss: 0.004205; batch adversarial loss: 0.750724\n",
      "epoch 162; iter: 0; batch classifier loss: 0.004519; batch adversarial loss: 0.675064\n",
      "epoch 163; iter: 0; batch classifier loss: 0.005626; batch adversarial loss: 0.672215\n",
      "epoch 164; iter: 0; batch classifier loss: 0.005195; batch adversarial loss: 0.700083\n",
      "epoch 165; iter: 0; batch classifier loss: 0.004130; batch adversarial loss: 0.680358\n",
      "epoch 166; iter: 0; batch classifier loss: 0.004697; batch adversarial loss: 0.667612\n",
      "epoch 167; iter: 0; batch classifier loss: 0.003434; batch adversarial loss: 0.661621\n",
      "epoch 168; iter: 0; batch classifier loss: 0.003462; batch adversarial loss: 0.660061\n",
      "epoch 169; iter: 0; batch classifier loss: 0.004435; batch adversarial loss: 0.632152\n",
      "epoch 170; iter: 0; batch classifier loss: 0.004099; batch adversarial loss: 0.655856\n",
      "epoch 171; iter: 0; batch classifier loss: 0.006187; batch adversarial loss: 0.660528\n",
      "epoch 172; iter: 0; batch classifier loss: 0.006809; batch adversarial loss: 0.675487\n",
      "epoch 173; iter: 0; batch classifier loss: 0.005348; batch adversarial loss: 0.667088\n",
      "epoch 174; iter: 0; batch classifier loss: 0.005814; batch adversarial loss: 0.643798\n",
      "epoch 175; iter: 0; batch classifier loss: 0.003288; batch adversarial loss: 0.650071\n",
      "epoch 176; iter: 0; batch classifier loss: 0.003626; batch adversarial loss: 0.666485\n",
      "epoch 177; iter: 0; batch classifier loss: 0.005076; batch adversarial loss: 0.647721\n",
      "epoch 178; iter: 0; batch classifier loss: 0.003839; batch adversarial loss: 0.659859\n",
      "epoch 179; iter: 0; batch classifier loss: 0.003608; batch adversarial loss: 0.672140\n",
      "epoch 180; iter: 0; batch classifier loss: 0.002587; batch adversarial loss: 0.638568\n",
      "epoch 181; iter: 0; batch classifier loss: 0.004740; batch adversarial loss: 0.643130\n",
      "epoch 182; iter: 0; batch classifier loss: 0.002755; batch adversarial loss: 0.652791\n",
      "epoch 183; iter: 0; batch classifier loss: 0.004466; batch adversarial loss: 0.595831\n",
      "epoch 184; iter: 0; batch classifier loss: 0.002615; batch adversarial loss: 0.670561\n",
      "epoch 185; iter: 0; batch classifier loss: 0.006626; batch adversarial loss: 0.691937\n",
      "epoch 186; iter: 0; batch classifier loss: 0.004759; batch adversarial loss: 0.675536\n",
      "epoch 187; iter: 0; batch classifier loss: 0.004036; batch adversarial loss: 0.660219\n",
      "epoch 188; iter: 0; batch classifier loss: 0.003575; batch adversarial loss: 0.653221\n",
      "epoch 189; iter: 0; batch classifier loss: 0.002597; batch adversarial loss: 0.644884\n",
      "epoch 190; iter: 0; batch classifier loss: 0.002359; batch adversarial loss: 0.681328\n",
      "epoch 191; iter: 0; batch classifier loss: 0.002974; batch adversarial loss: 0.678221\n",
      "epoch 192; iter: 0; batch classifier loss: 0.003105; batch adversarial loss: 0.654763\n",
      "epoch 193; iter: 0; batch classifier loss: 0.004100; batch adversarial loss: 0.672517\n",
      "epoch 194; iter: 0; batch classifier loss: 0.003373; batch adversarial loss: 0.665022\n",
      "epoch 195; iter: 0; batch classifier loss: 0.003046; batch adversarial loss: 0.669233\n",
      "epoch 196; iter: 0; batch classifier loss: 0.003297; batch adversarial loss: 0.646309\n",
      "epoch 197; iter: 0; batch classifier loss: 0.002699; batch adversarial loss: 0.672263\n",
      "epoch 198; iter: 0; batch classifier loss: 0.003873; batch adversarial loss: 0.667772\n",
      "epoch 199; iter: 0; batch classifier loss: 0.002920; batch adversarial loss: 0.639398\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:39.819911: W tensorflow/c/c_api.cc:304] Operation '{name:'4c969442-adee-11ee-9362-a9d23602d4a3/4c969442-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:4819 op device:{requested: '', assigned: ''} def:{{{node 4c969442-adee-11ee-9362-a9d23602d4a3/4c969442-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c969442-adee-11ee-9362-a9d23602d4a3/4c969442-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c969442-adee-11ee-9362-a9d23602d4a3/4c969442-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.866830; batch adversarial loss: 0.755390\n",
      "epoch 1; iter: 0; batch classifier loss: 0.733741; batch adversarial loss: 0.772149\n",
      "epoch 2; iter: 0; batch classifier loss: 0.615200; batch adversarial loss: 0.753571\n",
      "epoch 3; iter: 0; batch classifier loss: 0.500723; batch adversarial loss: 0.760410\n",
      "epoch 4; iter: 0; batch classifier loss: 0.469184; batch adversarial loss: 0.812118\n",
      "epoch 5; iter: 0; batch classifier loss: 0.437361; batch adversarial loss: 0.788055\n",
      "epoch 6; iter: 0; batch classifier loss: 0.389405; batch adversarial loss: 0.784956\n",
      "epoch 7; iter: 0; batch classifier loss: 0.326490; batch adversarial loss: 0.771019\n",
      "epoch 8; iter: 0; batch classifier loss: 0.329304; batch adversarial loss: 0.845119\n",
      "epoch 9; iter: 0; batch classifier loss: 0.335333; batch adversarial loss: 0.851068\n",
      "epoch 10; iter: 0; batch classifier loss: 0.293556; batch adversarial loss: 0.830392\n",
      "epoch 11; iter: 0; batch classifier loss: 0.287934; batch adversarial loss: 0.800231\n",
      "epoch 12; iter: 0; batch classifier loss: 0.273874; batch adversarial loss: 0.806818\n",
      "epoch 13; iter: 0; batch classifier loss: 0.240646; batch adversarial loss: 0.844190\n",
      "epoch 14; iter: 0; batch classifier loss: 0.217073; batch adversarial loss: 0.810448\n",
      "epoch 15; iter: 0; batch classifier loss: 0.246425; batch adversarial loss: 0.817344\n",
      "epoch 16; iter: 0; batch classifier loss: 0.211097; batch adversarial loss: 0.788129\n",
      "epoch 17; iter: 0; batch classifier loss: 0.192787; batch adversarial loss: 0.835100\n",
      "epoch 18; iter: 0; batch classifier loss: 0.171095; batch adversarial loss: 0.796703\n",
      "epoch 19; iter: 0; batch classifier loss: 0.175123; batch adversarial loss: 0.789423\n",
      "epoch 20; iter: 0; batch classifier loss: 0.156069; batch adversarial loss: 0.816850\n",
      "epoch 21; iter: 0; batch classifier loss: 0.167877; batch adversarial loss: 0.792268\n",
      "epoch 22; iter: 0; batch classifier loss: 0.187511; batch adversarial loss: 0.826570\n",
      "epoch 23; iter: 0; batch classifier loss: 0.169246; batch adversarial loss: 0.819324\n",
      "epoch 24; iter: 0; batch classifier loss: 0.192041; batch adversarial loss: 0.817377\n",
      "epoch 25; iter: 0; batch classifier loss: 0.125690; batch adversarial loss: 0.793144\n",
      "epoch 26; iter: 0; batch classifier loss: 0.137077; batch adversarial loss: 0.824688\n",
      "epoch 27; iter: 0; batch classifier loss: 0.163833; batch adversarial loss: 0.826566\n",
      "epoch 28; iter: 0; batch classifier loss: 0.113364; batch adversarial loss: 0.797820\n",
      "epoch 29; iter: 0; batch classifier loss: 0.112824; batch adversarial loss: 0.771522\n",
      "epoch 30; iter: 0; batch classifier loss: 0.094429; batch adversarial loss: 0.815062\n",
      "epoch 31; iter: 0; batch classifier loss: 0.097246; batch adversarial loss: 0.828368\n",
      "epoch 32; iter: 0; batch classifier loss: 0.112247; batch adversarial loss: 0.774436\n",
      "epoch 33; iter: 0; batch classifier loss: 0.136758; batch adversarial loss: 0.787533\n",
      "epoch 34; iter: 0; batch classifier loss: 0.076135; batch adversarial loss: 0.806505\n",
      "epoch 35; iter: 0; batch classifier loss: 0.089963; batch adversarial loss: 0.753160\n",
      "epoch 36; iter: 0; batch classifier loss: 0.065472; batch adversarial loss: 0.797490\n",
      "epoch 37; iter: 0; batch classifier loss: 0.081852; batch adversarial loss: 0.798370\n",
      "epoch 38; iter: 0; batch classifier loss: 0.078639; batch adversarial loss: 0.746300\n",
      "epoch 39; iter: 0; batch classifier loss: 0.079424; batch adversarial loss: 0.765296\n",
      "epoch 40; iter: 0; batch classifier loss: 0.088451; batch adversarial loss: 0.763522\n",
      "epoch 41; iter: 0; batch classifier loss: 0.067302; batch adversarial loss: 0.770774\n",
      "epoch 42; iter: 0; batch classifier loss: 0.042836; batch adversarial loss: 0.769290\n",
      "epoch 43; iter: 0; batch classifier loss: 0.055260; batch adversarial loss: 0.772614\n",
      "epoch 44; iter: 0; batch classifier loss: 0.057715; batch adversarial loss: 0.755882\n",
      "epoch 45; iter: 0; batch classifier loss: 0.040903; batch adversarial loss: 0.760850\n",
      "epoch 46; iter: 0; batch classifier loss: 0.046607; batch adversarial loss: 0.757627\n",
      "epoch 47; iter: 0; batch classifier loss: 0.049206; batch adversarial loss: 0.740576\n",
      "epoch 48; iter: 0; batch classifier loss: 0.052165; batch adversarial loss: 0.762074\n",
      "epoch 49; iter: 0; batch classifier loss: 0.061246; batch adversarial loss: 0.764739\n",
      "epoch 50; iter: 0; batch classifier loss: 0.045374; batch adversarial loss: 0.796475\n",
      "epoch 51; iter: 0; batch classifier loss: 0.051101; batch adversarial loss: 0.748127\n",
      "epoch 52; iter: 0; batch classifier loss: 0.055602; batch adversarial loss: 0.771702\n",
      "epoch 53; iter: 0; batch classifier loss: 0.047264; batch adversarial loss: 0.765141\n",
      "epoch 54; iter: 0; batch classifier loss: 0.045948; batch adversarial loss: 0.750033\n",
      "epoch 55; iter: 0; batch classifier loss: 0.040603; batch adversarial loss: 0.779932\n",
      "epoch 56; iter: 0; batch classifier loss: 0.031076; batch adversarial loss: 0.759010\n",
      "epoch 57; iter: 0; batch classifier loss: 0.026688; batch adversarial loss: 0.745765\n",
      "epoch 58; iter: 0; batch classifier loss: 0.023945; batch adversarial loss: 0.747011\n",
      "epoch 59; iter: 0; batch classifier loss: 0.035758; batch adversarial loss: 0.756606\n",
      "epoch 60; iter: 0; batch classifier loss: 0.038429; batch adversarial loss: 0.734325\n",
      "epoch 61; iter: 0; batch classifier loss: 0.037026; batch adversarial loss: 0.738700\n",
      "epoch 62; iter: 0; batch classifier loss: 0.036231; batch adversarial loss: 0.738072\n",
      "epoch 63; iter: 0; batch classifier loss: 0.033998; batch adversarial loss: 0.728546\n",
      "epoch 64; iter: 0; batch classifier loss: 0.030302; batch adversarial loss: 0.729385\n",
      "epoch 65; iter: 0; batch classifier loss: 0.034250; batch adversarial loss: 0.748294\n",
      "epoch 66; iter: 0; batch classifier loss: 0.064483; batch adversarial loss: 0.741395\n",
      "epoch 67; iter: 0; batch classifier loss: 0.015061; batch adversarial loss: 0.727938\n",
      "epoch 68; iter: 0; batch classifier loss: 0.018782; batch adversarial loss: 0.753033\n",
      "epoch 69; iter: 0; batch classifier loss: 0.036188; batch adversarial loss: 0.756273\n",
      "epoch 70; iter: 0; batch classifier loss: 0.028608; batch adversarial loss: 0.736806\n",
      "epoch 71; iter: 0; batch classifier loss: 0.019804; batch adversarial loss: 0.723656\n",
      "epoch 72; iter: 0; batch classifier loss: 0.033111; batch adversarial loss: 0.731275\n",
      "epoch 73; iter: 0; batch classifier loss: 0.018237; batch adversarial loss: 0.736477\n",
      "epoch 74; iter: 0; batch classifier loss: 0.026192; batch adversarial loss: 0.737754\n",
      "epoch 75; iter: 0; batch classifier loss: 0.051563; batch adversarial loss: 0.717619\n",
      "epoch 76; iter: 0; batch classifier loss: 0.055816; batch adversarial loss: 0.742667\n",
      "epoch 77; iter: 0; batch classifier loss: 0.038204; batch adversarial loss: 0.716958\n",
      "epoch 78; iter: 0; batch classifier loss: 0.019168; batch adversarial loss: 0.727392\n",
      "epoch 79; iter: 0; batch classifier loss: 0.018696; batch adversarial loss: 0.721566\n",
      "epoch 80; iter: 0; batch classifier loss: 0.022265; batch adversarial loss: 0.730178\n",
      "epoch 81; iter: 0; batch classifier loss: 0.040965; batch adversarial loss: 0.719492\n",
      "epoch 82; iter: 0; batch classifier loss: 0.048655; batch adversarial loss: 0.734876\n",
      "epoch 83; iter: 0; batch classifier loss: 0.039651; batch adversarial loss: 0.722611\n",
      "epoch 84; iter: 0; batch classifier loss: 0.049171; batch adversarial loss: 0.728733\n",
      "epoch 85; iter: 0; batch classifier loss: 0.022240; batch adversarial loss: 0.721702\n",
      "epoch 86; iter: 0; batch classifier loss: 0.026852; batch adversarial loss: 0.727796\n",
      "epoch 87; iter: 0; batch classifier loss: 0.022950; batch adversarial loss: 0.729744\n",
      "epoch 88; iter: 0; batch classifier loss: 0.049534; batch adversarial loss: 0.732455\n",
      "epoch 89; iter: 0; batch classifier loss: 0.058166; batch adversarial loss: 0.721205\n",
      "epoch 90; iter: 0; batch classifier loss: 0.033946; batch adversarial loss: 0.727444\n",
      "epoch 91; iter: 0; batch classifier loss: 0.061441; batch adversarial loss: 0.724033\n",
      "epoch 92; iter: 0; batch classifier loss: 0.026341; batch adversarial loss: 0.705619\n",
      "epoch 93; iter: 0; batch classifier loss: 0.026936; batch adversarial loss: 0.722290\n",
      "epoch 94; iter: 0; batch classifier loss: 0.031829; batch adversarial loss: 0.722975\n",
      "epoch 95; iter: 0; batch classifier loss: 0.017936; batch adversarial loss: 0.715819\n",
      "epoch 96; iter: 0; batch classifier loss: 0.031598; batch adversarial loss: 0.708192\n",
      "epoch 97; iter: 0; batch classifier loss: 0.033679; batch adversarial loss: 0.715989\n",
      "epoch 98; iter: 0; batch classifier loss: 0.031020; batch adversarial loss: 0.717529\n",
      "epoch 99; iter: 0; batch classifier loss: 0.019641; batch adversarial loss: 0.710967\n",
      "epoch 100; iter: 0; batch classifier loss: 0.034094; batch adversarial loss: 0.707355\n",
      "epoch 101; iter: 0; batch classifier loss: 0.021353; batch adversarial loss: 0.712104\n",
      "epoch 102; iter: 0; batch classifier loss: 0.036404; batch adversarial loss: 0.707625\n",
      "epoch 103; iter: 0; batch classifier loss: 0.023012; batch adversarial loss: 0.705183\n",
      "epoch 104; iter: 0; batch classifier loss: 0.028894; batch adversarial loss: 0.709306\n",
      "epoch 105; iter: 0; batch classifier loss: 0.020578; batch adversarial loss: 0.700794\n",
      "epoch 106; iter: 0; batch classifier loss: 0.030365; batch adversarial loss: 0.709156\n",
      "epoch 107; iter: 0; batch classifier loss: 0.018711; batch adversarial loss: 0.699861\n",
      "epoch 108; iter: 0; batch classifier loss: 0.032473; batch adversarial loss: 0.709238\n",
      "epoch 109; iter: 0; batch classifier loss: 0.012745; batch adversarial loss: 0.702795\n",
      "epoch 110; iter: 0; batch classifier loss: 0.019097; batch adversarial loss: 0.707544\n",
      "epoch 111; iter: 0; batch classifier loss: 0.016156; batch adversarial loss: 0.699239\n",
      "epoch 112; iter: 0; batch classifier loss: 0.014269; batch adversarial loss: 0.699860\n",
      "epoch 113; iter: 0; batch classifier loss: 0.028678; batch adversarial loss: 0.707315\n",
      "epoch 114; iter: 0; batch classifier loss: 0.028498; batch adversarial loss: 0.707533\n",
      "epoch 115; iter: 0; batch classifier loss: 0.033597; batch adversarial loss: 0.705161\n",
      "epoch 116; iter: 0; batch classifier loss: 0.019697; batch adversarial loss: 0.703805\n",
      "epoch 117; iter: 0; batch classifier loss: 0.019922; batch adversarial loss: 0.693620\n",
      "epoch 118; iter: 0; batch classifier loss: 0.022577; batch adversarial loss: 0.700183\n",
      "epoch 119; iter: 0; batch classifier loss: 0.029442; batch adversarial loss: 0.697849\n",
      "epoch 120; iter: 0; batch classifier loss: 0.017840; batch adversarial loss: 0.695786\n",
      "epoch 121; iter: 0; batch classifier loss: 0.023782; batch adversarial loss: 0.694838\n",
      "epoch 122; iter: 0; batch classifier loss: 0.033366; batch adversarial loss: 0.706566\n",
      "epoch 123; iter: 0; batch classifier loss: 0.039896; batch adversarial loss: 0.707254\n",
      "epoch 124; iter: 0; batch classifier loss: 0.041356; batch adversarial loss: 0.703525\n",
      "epoch 125; iter: 0; batch classifier loss: 0.022262; batch adversarial loss: 0.692884\n",
      "epoch 126; iter: 0; batch classifier loss: 0.011205; batch adversarial loss: 0.693842\n",
      "epoch 127; iter: 0; batch classifier loss: 0.012185; batch adversarial loss: 0.694332\n",
      "epoch 128; iter: 0; batch classifier loss: 0.055576; batch adversarial loss: 0.710598\n",
      "epoch 129; iter: 0; batch classifier loss: 0.021554; batch adversarial loss: 0.697657\n",
      "epoch 130; iter: 0; batch classifier loss: 0.031893; batch adversarial loss: 0.699094\n",
      "epoch 131; iter: 0; batch classifier loss: 0.033361; batch adversarial loss: 0.702530\n",
      "epoch 132; iter: 0; batch classifier loss: 0.046706; batch adversarial loss: 0.706200\n",
      "epoch 133; iter: 0; batch classifier loss: 0.016067; batch adversarial loss: 0.695851\n",
      "epoch 134; iter: 0; batch classifier loss: 0.007525; batch adversarial loss: 0.694177\n",
      "epoch 135; iter: 0; batch classifier loss: 0.011630; batch adversarial loss: 0.686015\n",
      "epoch 136; iter: 0; batch classifier loss: 0.034860; batch adversarial loss: 0.697552\n",
      "epoch 137; iter: 0; batch classifier loss: 0.033141; batch adversarial loss: 0.690611\n",
      "epoch 138; iter: 0; batch classifier loss: 0.015174; batch adversarial loss: 0.686150\n",
      "epoch 139; iter: 0; batch classifier loss: 0.019720; batch adversarial loss: 0.693931\n",
      "epoch 140; iter: 0; batch classifier loss: 0.012713; batch adversarial loss: 0.690544\n",
      "epoch 141; iter: 0; batch classifier loss: 0.012440; batch adversarial loss: 0.678486\n",
      "epoch 142; iter: 0; batch classifier loss: 0.018913; batch adversarial loss: 0.692778\n",
      "epoch 143; iter: 0; batch classifier loss: 0.050617; batch adversarial loss: 0.692531\n",
      "epoch 144; iter: 0; batch classifier loss: 0.010538; batch adversarial loss: 0.689237\n",
      "epoch 145; iter: 0; batch classifier loss: 0.034885; batch adversarial loss: 0.686685\n",
      "epoch 146; iter: 0; batch classifier loss: 0.019493; batch adversarial loss: 0.687862\n",
      "epoch 147; iter: 0; batch classifier loss: 0.032053; batch adversarial loss: 0.685060\n",
      "epoch 148; iter: 0; batch classifier loss: 0.017029; batch adversarial loss: 0.691606\n",
      "epoch 149; iter: 0; batch classifier loss: 0.024893; batch adversarial loss: 0.689032\n",
      "epoch 150; iter: 0; batch classifier loss: 0.010566; batch adversarial loss: 0.672174\n",
      "epoch 151; iter: 0; batch classifier loss: 0.024218; batch adversarial loss: 0.684070\n",
      "epoch 152; iter: 0; batch classifier loss: 0.038223; batch adversarial loss: 0.696272\n",
      "epoch 153; iter: 0; batch classifier loss: 0.039552; batch adversarial loss: 0.698561\n",
      "epoch 154; iter: 0; batch classifier loss: 0.025602; batch adversarial loss: 0.691237\n",
      "epoch 155; iter: 0; batch classifier loss: 0.015880; batch adversarial loss: 0.682621\n",
      "epoch 156; iter: 0; batch classifier loss: 0.032393; batch adversarial loss: 0.690254\n",
      "epoch 157; iter: 0; batch classifier loss: 0.026786; batch adversarial loss: 0.685766\n",
      "epoch 158; iter: 0; batch classifier loss: 0.020918; batch adversarial loss: 0.699577\n",
      "epoch 159; iter: 0; batch classifier loss: 0.018963; batch adversarial loss: 0.685640\n",
      "epoch 160; iter: 0; batch classifier loss: 0.030691; batch adversarial loss: 0.680526\n",
      "epoch 161; iter: 0; batch classifier loss: 0.015087; batch adversarial loss: 0.687360\n",
      "epoch 162; iter: 0; batch classifier loss: 0.031268; batch adversarial loss: 0.688873\n",
      "epoch 163; iter: 0; batch classifier loss: 0.017307; batch adversarial loss: 0.684997\n",
      "epoch 164; iter: 0; batch classifier loss: 0.018479; batch adversarial loss: 0.677193\n",
      "epoch 165; iter: 0; batch classifier loss: 0.025396; batch adversarial loss: 0.697710\n",
      "epoch 166; iter: 0; batch classifier loss: 0.009425; batch adversarial loss: 0.677130\n",
      "epoch 167; iter: 0; batch classifier loss: 0.012849; batch adversarial loss: 0.677108\n",
      "epoch 168; iter: 0; batch classifier loss: 0.041881; batch adversarial loss: 0.698263\n",
      "epoch 169; iter: 0; batch classifier loss: 0.014254; batch adversarial loss: 0.692342\n",
      "epoch 170; iter: 0; batch classifier loss: 0.025444; batch adversarial loss: 0.697636\n",
      "epoch 171; iter: 0; batch classifier loss: 0.033473; batch adversarial loss: 0.682963\n",
      "epoch 172; iter: 0; batch classifier loss: 0.030879; batch adversarial loss: 0.670915\n",
      "epoch 173; iter: 0; batch classifier loss: 0.027787; batch adversarial loss: 0.678712\n",
      "epoch 174; iter: 0; batch classifier loss: 0.042556; batch adversarial loss: 0.689117\n",
      "epoch 175; iter: 0; batch classifier loss: 0.014292; batch adversarial loss: 0.678705\n",
      "epoch 176; iter: 0; batch classifier loss: 0.008073; batch adversarial loss: 0.683230\n",
      "epoch 177; iter: 0; batch classifier loss: 0.025553; batch adversarial loss: 0.679431\n",
      "epoch 178; iter: 0; batch classifier loss: 0.020726; batch adversarial loss: 0.682297\n",
      "epoch 179; iter: 0; batch classifier loss: 0.024913; batch adversarial loss: 0.699366\n",
      "epoch 180; iter: 0; batch classifier loss: 0.019059; batch adversarial loss: 0.674787\n",
      "epoch 181; iter: 0; batch classifier loss: 0.032538; batch adversarial loss: 0.700455\n",
      "epoch 182; iter: 0; batch classifier loss: 0.014678; batch adversarial loss: 0.688375\n",
      "epoch 183; iter: 0; batch classifier loss: 0.013553; batch adversarial loss: 0.677433\n",
      "epoch 184; iter: 0; batch classifier loss: 0.023573; batch adversarial loss: 0.685226\n",
      "epoch 185; iter: 0; batch classifier loss: 0.005539; batch adversarial loss: 0.680422\n",
      "epoch 186; iter: 0; batch classifier loss: 0.006769; batch adversarial loss: 0.672601\n",
      "epoch 187; iter: 0; batch classifier loss: 0.014641; batch adversarial loss: 0.679968\n",
      "epoch 188; iter: 0; batch classifier loss: 0.028802; batch adversarial loss: 0.694579\n",
      "epoch 189; iter: 0; batch classifier loss: 0.013037; batch adversarial loss: 0.689019\n",
      "epoch 190; iter: 0; batch classifier loss: 0.017036; batch adversarial loss: 0.679276\n",
      "epoch 191; iter: 0; batch classifier loss: 0.006533; batch adversarial loss: 0.679469\n",
      "epoch 192; iter: 0; batch classifier loss: 0.018112; batch adversarial loss: 0.665382\n",
      "epoch 193; iter: 0; batch classifier loss: 0.017218; batch adversarial loss: 0.679930\n",
      "epoch 194; iter: 0; batch classifier loss: 0.010275; batch adversarial loss: 0.688479\n",
      "epoch 195; iter: 0; batch classifier loss: 0.025704; batch adversarial loss: 0.672848\n",
      "epoch 196; iter: 0; batch classifier loss: 0.010034; batch adversarial loss: 0.685257\n",
      "epoch 197; iter: 0; batch classifier loss: 0.011091; batch adversarial loss: 0.676085\n",
      "epoch 198; iter: 0; batch classifier loss: 0.052979; batch adversarial loss: 0.698587\n",
      "epoch 199; iter: 0; batch classifier loss: 0.038492; batch adversarial loss: 0.685606\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:41.560999: W tensorflow/c/c_api.cc:304] Operation '{name:'4c96950a-adee-11ee-9362-a9d23602d4a3/4c96950a-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:5626 op device:{requested: '', assigned: ''} def:{{{node 4c96950a-adee-11ee-9362-a9d23602d4a3/4c96950a-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c96950a-adee-11ee-9362-a9d23602d4a3/4c96950a-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c96950a-adee-11ee-9362-a9d23602d4a3/4c96950a-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.808697; batch adversarial loss: 0.725733\n",
      "epoch 1; iter: 0; batch classifier loss: 0.655378; batch adversarial loss: 0.745727\n",
      "epoch 2; iter: 0; batch classifier loss: 0.561181; batch adversarial loss: 0.747609\n",
      "epoch 3; iter: 0; batch classifier loss: 0.464683; batch adversarial loss: 0.782491\n",
      "epoch 4; iter: 0; batch classifier loss: 0.463437; batch adversarial loss: 0.773044\n",
      "epoch 5; iter: 0; batch classifier loss: 0.381699; batch adversarial loss: 0.740482\n",
      "epoch 6; iter: 0; batch classifier loss: 0.348595; batch adversarial loss: 0.790121\n",
      "epoch 7; iter: 0; batch classifier loss: 0.339057; batch adversarial loss: 0.763509\n",
      "epoch 8; iter: 0; batch classifier loss: 0.307981; batch adversarial loss: 0.763820\n",
      "epoch 9; iter: 0; batch classifier loss: 0.290905; batch adversarial loss: 0.794237\n",
      "epoch 10; iter: 0; batch classifier loss: 0.276116; batch adversarial loss: 0.766558\n",
      "epoch 11; iter: 0; batch classifier loss: 0.235815; batch adversarial loss: 0.751882\n",
      "epoch 12; iter: 0; batch classifier loss: 0.233531; batch adversarial loss: 0.805449\n",
      "epoch 13; iter: 0; batch classifier loss: 0.275688; batch adversarial loss: 0.746633\n",
      "epoch 14; iter: 0; batch classifier loss: 0.177340; batch adversarial loss: 0.765786\n",
      "epoch 15; iter: 0; batch classifier loss: 0.189404; batch adversarial loss: 0.738862\n",
      "epoch 16; iter: 0; batch classifier loss: 0.241782; batch adversarial loss: 0.740131\n",
      "epoch 17; iter: 0; batch classifier loss: 0.173154; batch adversarial loss: 0.788204\n",
      "epoch 18; iter: 0; batch classifier loss: 0.150095; batch adversarial loss: 0.782632\n",
      "epoch 19; iter: 0; batch classifier loss: 0.118505; batch adversarial loss: 0.774357\n",
      "epoch 20; iter: 0; batch classifier loss: 0.174185; batch adversarial loss: 0.743852\n",
      "epoch 21; iter: 0; batch classifier loss: 0.194850; batch adversarial loss: 0.759606\n",
      "epoch 22; iter: 0; batch classifier loss: 0.140774; batch adversarial loss: 0.760700\n",
      "epoch 23; iter: 0; batch classifier loss: 0.094298; batch adversarial loss: 0.742975\n",
      "epoch 24; iter: 0; batch classifier loss: 0.134971; batch adversarial loss: 0.746410\n",
      "epoch 25; iter: 0; batch classifier loss: 0.151911; batch adversarial loss: 0.745391\n",
      "epoch 26; iter: 0; batch classifier loss: 0.104415; batch adversarial loss: 0.759527\n",
      "epoch 27; iter: 0; batch classifier loss: 0.099776; batch adversarial loss: 0.713093\n",
      "epoch 28; iter: 0; batch classifier loss: 0.126228; batch adversarial loss: 0.750679\n",
      "epoch 29; iter: 0; batch classifier loss: 0.097210; batch adversarial loss: 0.752998\n",
      "epoch 30; iter: 0; batch classifier loss: 0.102789; batch adversarial loss: 0.748557\n",
      "epoch 31; iter: 0; batch classifier loss: 0.135298; batch adversarial loss: 0.728067\n",
      "epoch 32; iter: 0; batch classifier loss: 0.094873; batch adversarial loss: 0.770459\n",
      "epoch 33; iter: 0; batch classifier loss: 0.109661; batch adversarial loss: 0.723213\n",
      "epoch 34; iter: 0; batch classifier loss: 0.086903; batch adversarial loss: 0.745018\n",
      "epoch 35; iter: 0; batch classifier loss: 0.058066; batch adversarial loss: 0.725881\n",
      "epoch 36; iter: 0; batch classifier loss: 0.108970; batch adversarial loss: 0.734649\n",
      "epoch 37; iter: 0; batch classifier loss: 0.072941; batch adversarial loss: 0.736345\n",
      "epoch 38; iter: 0; batch classifier loss: 0.089937; batch adversarial loss: 0.735675\n",
      "epoch 39; iter: 0; batch classifier loss: 0.075003; batch adversarial loss: 0.721380\n",
      "epoch 40; iter: 0; batch classifier loss: 0.071058; batch adversarial loss: 0.732831\n",
      "epoch 41; iter: 0; batch classifier loss: 0.071353; batch adversarial loss: 0.726553\n",
      "epoch 42; iter: 0; batch classifier loss: 0.074696; batch adversarial loss: 0.727128\n",
      "epoch 43; iter: 0; batch classifier loss: 0.069001; batch adversarial loss: 0.733917\n",
      "epoch 44; iter: 0; batch classifier loss: 0.076922; batch adversarial loss: 0.715941\n",
      "epoch 45; iter: 0; batch classifier loss: 0.096068; batch adversarial loss: 0.734216\n",
      "epoch 46; iter: 0; batch classifier loss: 0.056145; batch adversarial loss: 0.715028\n",
      "epoch 47; iter: 0; batch classifier loss: 0.059458; batch adversarial loss: 0.737334\n",
      "epoch 48; iter: 0; batch classifier loss: 0.069015; batch adversarial loss: 0.721542\n",
      "epoch 49; iter: 0; batch classifier loss: 0.072189; batch adversarial loss: 0.712969\n",
      "epoch 50; iter: 0; batch classifier loss: 0.058955; batch adversarial loss: 0.723870\n",
      "epoch 51; iter: 0; batch classifier loss: 0.055287; batch adversarial loss: 0.723867\n",
      "epoch 52; iter: 0; batch classifier loss: 0.067535; batch adversarial loss: 0.717709\n",
      "epoch 53; iter: 0; batch classifier loss: 0.058002; batch adversarial loss: 0.721698\n",
      "epoch 54; iter: 0; batch classifier loss: 0.069710; batch adversarial loss: 0.724909\n",
      "epoch 55; iter: 0; batch classifier loss: 0.041772; batch adversarial loss: 0.715963\n",
      "epoch 56; iter: 0; batch classifier loss: 0.075156; batch adversarial loss: 0.718758\n",
      "epoch 57; iter: 0; batch classifier loss: 0.060920; batch adversarial loss: 0.713908\n",
      "epoch 58; iter: 0; batch classifier loss: 0.049047; batch adversarial loss: 0.708735\n",
      "epoch 59; iter: 0; batch classifier loss: 0.061023; batch adversarial loss: 0.711901\n",
      "epoch 60; iter: 0; batch classifier loss: 0.044909; batch adversarial loss: 0.704111\n",
      "epoch 61; iter: 0; batch classifier loss: 0.058129; batch adversarial loss: 0.708838\n",
      "epoch 62; iter: 0; batch classifier loss: 0.044674; batch adversarial loss: 0.712123\n",
      "epoch 63; iter: 0; batch classifier loss: 0.051501; batch adversarial loss: 0.706844\n",
      "epoch 64; iter: 0; batch classifier loss: 0.045484; batch adversarial loss: 0.707081\n",
      "epoch 65; iter: 0; batch classifier loss: 0.050705; batch adversarial loss: 0.710335\n",
      "epoch 66; iter: 0; batch classifier loss: 0.046582; batch adversarial loss: 0.700428\n",
      "epoch 67; iter: 0; batch classifier loss: 0.057417; batch adversarial loss: 0.702271\n",
      "epoch 68; iter: 0; batch classifier loss: 0.044427; batch adversarial loss: 0.708361\n",
      "epoch 69; iter: 0; batch classifier loss: 0.034678; batch adversarial loss: 0.701885\n",
      "epoch 70; iter: 0; batch classifier loss: 0.052304; batch adversarial loss: 0.699011\n",
      "epoch 71; iter: 0; batch classifier loss: 0.036023; batch adversarial loss: 0.701443\n",
      "epoch 72; iter: 0; batch classifier loss: 0.044927; batch adversarial loss: 0.705150\n",
      "epoch 73; iter: 0; batch classifier loss: 0.044742; batch adversarial loss: 0.699258\n",
      "epoch 74; iter: 0; batch classifier loss: 0.039642; batch adversarial loss: 0.695970\n",
      "epoch 75; iter: 0; batch classifier loss: 0.022526; batch adversarial loss: 0.702419\n",
      "epoch 76; iter: 0; batch classifier loss: 0.042747; batch adversarial loss: 0.698204\n",
      "epoch 77; iter: 0; batch classifier loss: 0.016456; batch adversarial loss: 0.694428\n",
      "epoch 78; iter: 0; batch classifier loss: 0.045184; batch adversarial loss: 0.691079\n",
      "epoch 79; iter: 0; batch classifier loss: 0.035931; batch adversarial loss: 0.696185\n",
      "epoch 80; iter: 0; batch classifier loss: 0.023839; batch adversarial loss: 0.695203\n",
      "epoch 81; iter: 0; batch classifier loss: 0.037272; batch adversarial loss: 0.694725\n",
      "epoch 82; iter: 0; batch classifier loss: 0.034861; batch adversarial loss: 0.693935\n",
      "epoch 83; iter: 0; batch classifier loss: 0.041539; batch adversarial loss: 0.690601\n",
      "epoch 84; iter: 0; batch classifier loss: 0.036816; batch adversarial loss: 0.692933\n",
      "epoch 85; iter: 0; batch classifier loss: 0.025741; batch adversarial loss: 0.693752\n",
      "epoch 86; iter: 0; batch classifier loss: 0.020339; batch adversarial loss: 0.692731\n",
      "epoch 87; iter: 0; batch classifier loss: 0.034127; batch adversarial loss: 0.694949\n",
      "epoch 88; iter: 0; batch classifier loss: 0.044381; batch adversarial loss: 0.691434\n",
      "epoch 89; iter: 0; batch classifier loss: 0.039019; batch adversarial loss: 0.686215\n",
      "epoch 90; iter: 0; batch classifier loss: 0.039999; batch adversarial loss: 0.692995\n",
      "epoch 91; iter: 0; batch classifier loss: 0.028925; batch adversarial loss: 0.694683\n",
      "epoch 92; iter: 0; batch classifier loss: 0.035767; batch adversarial loss: 0.684809\n",
      "epoch 93; iter: 0; batch classifier loss: 0.017932; batch adversarial loss: 0.687350\n",
      "epoch 94; iter: 0; batch classifier loss: 0.033155; batch adversarial loss: 0.693428\n",
      "epoch 95; iter: 0; batch classifier loss: 0.018294; batch adversarial loss: 0.687751\n",
      "epoch 96; iter: 0; batch classifier loss: 0.021735; batch adversarial loss: 0.689482\n",
      "epoch 97; iter: 0; batch classifier loss: 0.016610; batch adversarial loss: 0.690956\n",
      "epoch 98; iter: 0; batch classifier loss: 0.031453; batch adversarial loss: 0.688668\n",
      "epoch 99; iter: 0; batch classifier loss: 0.021188; batch adversarial loss: 0.688621\n",
      "epoch 100; iter: 0; batch classifier loss: 0.037909; batch adversarial loss: 0.696687\n",
      "epoch 101; iter: 0; batch classifier loss: 0.011365; batch adversarial loss: 0.687775\n",
      "epoch 102; iter: 0; batch classifier loss: 0.029232; batch adversarial loss: 0.686303\n",
      "epoch 103; iter: 0; batch classifier loss: 0.016011; batch adversarial loss: 0.689215\n",
      "epoch 104; iter: 0; batch classifier loss: 0.027625; batch adversarial loss: 0.685919\n",
      "epoch 105; iter: 0; batch classifier loss: 0.019619; batch adversarial loss: 0.682464\n",
      "epoch 106; iter: 0; batch classifier loss: 0.018749; batch adversarial loss: 0.682371\n",
      "epoch 107; iter: 0; batch classifier loss: 0.037164; batch adversarial loss: 0.682769\n",
      "epoch 108; iter: 0; batch classifier loss: 0.027128; batch adversarial loss: 0.686646\n",
      "epoch 109; iter: 0; batch classifier loss: 0.033472; batch adversarial loss: 0.685719\n",
      "epoch 110; iter: 0; batch classifier loss: 0.031407; batch adversarial loss: 0.676270\n",
      "epoch 111; iter: 0; batch classifier loss: 0.039908; batch adversarial loss: 0.677512\n",
      "epoch 112; iter: 0; batch classifier loss: 0.016840; batch adversarial loss: 0.683871\n",
      "epoch 113; iter: 0; batch classifier loss: 0.014201; batch adversarial loss: 0.679374\n",
      "epoch 114; iter: 0; batch classifier loss: 0.040054; batch adversarial loss: 0.678565\n",
      "epoch 115; iter: 0; batch classifier loss: 0.025547; batch adversarial loss: 0.681148\n",
      "epoch 116; iter: 0; batch classifier loss: 0.019662; batch adversarial loss: 0.687971\n",
      "epoch 117; iter: 0; batch classifier loss: 0.023582; batch adversarial loss: 0.679016\n",
      "epoch 118; iter: 0; batch classifier loss: 0.024099; batch adversarial loss: 0.691566\n",
      "epoch 119; iter: 0; batch classifier loss: 0.017051; batch adversarial loss: 0.671617\n",
      "epoch 120; iter: 0; batch classifier loss: 0.026187; batch adversarial loss: 0.685007\n",
      "epoch 121; iter: 0; batch classifier loss: 0.011972; batch adversarial loss: 0.683089\n",
      "epoch 122; iter: 0; batch classifier loss: 0.032701; batch adversarial loss: 0.678073\n",
      "epoch 123; iter: 0; batch classifier loss: 0.014009; batch adversarial loss: 0.673370\n",
      "epoch 124; iter: 0; batch classifier loss: 0.020014; batch adversarial loss: 0.683003\n",
      "epoch 125; iter: 0; batch classifier loss: 0.039265; batch adversarial loss: 0.680519\n",
      "epoch 126; iter: 0; batch classifier loss: 0.026966; batch adversarial loss: 0.681501\n",
      "epoch 127; iter: 0; batch classifier loss: 0.020123; batch adversarial loss: 0.684351\n",
      "epoch 128; iter: 0; batch classifier loss: 0.011263; batch adversarial loss: 0.680632\n",
      "epoch 129; iter: 0; batch classifier loss: 0.023675; batch adversarial loss: 0.673050\n",
      "epoch 130; iter: 0; batch classifier loss: 0.028081; batch adversarial loss: 0.678451\n",
      "epoch 131; iter: 0; batch classifier loss: 0.023142; batch adversarial loss: 0.674928\n",
      "epoch 132; iter: 0; batch classifier loss: 0.020732; batch adversarial loss: 0.667762\n",
      "epoch 133; iter: 0; batch classifier loss: 0.020935; batch adversarial loss: 0.663217\n",
      "epoch 134; iter: 0; batch classifier loss: 0.013749; batch adversarial loss: 0.667127\n",
      "epoch 135; iter: 0; batch classifier loss: 0.021154; batch adversarial loss: 0.662343\n",
      "epoch 136; iter: 0; batch classifier loss: 0.017763; batch adversarial loss: 0.671140\n",
      "epoch 137; iter: 0; batch classifier loss: 0.014115; batch adversarial loss: 0.668602\n",
      "epoch 138; iter: 0; batch classifier loss: 0.016181; batch adversarial loss: 0.672859\n",
      "epoch 139; iter: 0; batch classifier loss: 0.023542; batch adversarial loss: 0.688320\n",
      "epoch 140; iter: 0; batch classifier loss: 0.013714; batch adversarial loss: 0.688887\n",
      "epoch 141; iter: 0; batch classifier loss: 0.014628; batch adversarial loss: 0.670220\n",
      "epoch 142; iter: 0; batch classifier loss: 0.009388; batch adversarial loss: 0.692173\n",
      "epoch 143; iter: 0; batch classifier loss: 0.013455; batch adversarial loss: 0.675929\n",
      "epoch 144; iter: 0; batch classifier loss: 0.007820; batch adversarial loss: 0.687464\n",
      "epoch 145; iter: 0; batch classifier loss: 0.023092; batch adversarial loss: 0.643412\n",
      "epoch 146; iter: 0; batch classifier loss: 0.013087; batch adversarial loss: 0.679745\n",
      "epoch 147; iter: 0; batch classifier loss: 0.025177; batch adversarial loss: 0.687904\n",
      "epoch 148; iter: 0; batch classifier loss: 0.018615; batch adversarial loss: 0.677101\n",
      "epoch 149; iter: 0; batch classifier loss: 0.013632; batch adversarial loss: 0.673191\n",
      "epoch 150; iter: 0; batch classifier loss: 0.011846; batch adversarial loss: 0.671212\n",
      "epoch 151; iter: 0; batch classifier loss: 0.015427; batch adversarial loss: 0.687707\n",
      "epoch 152; iter: 0; batch classifier loss: 0.010571; batch adversarial loss: 0.664069\n",
      "epoch 153; iter: 0; batch classifier loss: 0.008575; batch adversarial loss: 0.661914\n",
      "epoch 154; iter: 0; batch classifier loss: 0.011771; batch adversarial loss: 0.685563\n",
      "epoch 155; iter: 0; batch classifier loss: 0.025493; batch adversarial loss: 0.687367\n",
      "epoch 156; iter: 0; batch classifier loss: 0.013717; batch adversarial loss: 0.667781\n",
      "epoch 157; iter: 0; batch classifier loss: 0.009492; batch adversarial loss: 0.665472\n",
      "epoch 158; iter: 0; batch classifier loss: 0.010106; batch adversarial loss: 0.672006\n",
      "epoch 159; iter: 0; batch classifier loss: 0.010438; batch adversarial loss: 0.674215\n",
      "epoch 160; iter: 0; batch classifier loss: 0.008560; batch adversarial loss: 0.683280\n",
      "epoch 161; iter: 0; batch classifier loss: 0.004557; batch adversarial loss: 0.680936\n",
      "epoch 162; iter: 0; batch classifier loss: 0.008050; batch adversarial loss: 0.671586\n",
      "epoch 163; iter: 0; batch classifier loss: 0.006790; batch adversarial loss: 0.685623\n",
      "epoch 164; iter: 0; batch classifier loss: 0.014835; batch adversarial loss: 0.666512\n",
      "epoch 165; iter: 0; batch classifier loss: 0.009199; batch adversarial loss: 0.671273\n",
      "epoch 166; iter: 0; batch classifier loss: 0.008061; batch adversarial loss: 0.661502\n",
      "epoch 167; iter: 0; batch classifier loss: 0.007461; batch adversarial loss: 0.666279\n",
      "epoch 168; iter: 0; batch classifier loss: 0.020635; batch adversarial loss: 0.668324\n",
      "epoch 169; iter: 0; batch classifier loss: 0.004018; batch adversarial loss: 0.668648\n",
      "epoch 170; iter: 0; batch classifier loss: 0.020566; batch adversarial loss: 0.683134\n",
      "epoch 171; iter: 0; batch classifier loss: 0.014926; batch adversarial loss: 0.658464\n",
      "epoch 172; iter: 0; batch classifier loss: 0.004059; batch adversarial loss: 0.682943\n",
      "epoch 173; iter: 0; batch classifier loss: 0.020051; batch adversarial loss: 0.665505\n",
      "epoch 174; iter: 0; batch classifier loss: 0.007497; batch adversarial loss: 0.670400\n",
      "epoch 175; iter: 0; batch classifier loss: 0.004836; batch adversarial loss: 0.665180\n",
      "epoch 176; iter: 0; batch classifier loss: 0.006179; batch adversarial loss: 0.657584\n",
      "epoch 177; iter: 0; batch classifier loss: 0.016554; batch adversarial loss: 0.679974\n",
      "epoch 178; iter: 0; batch classifier loss: 0.004537; batch adversarial loss: 0.675168\n",
      "epoch 179; iter: 0; batch classifier loss: 0.009657; batch adversarial loss: 0.687649\n",
      "epoch 180; iter: 0; batch classifier loss: 0.007285; batch adversarial loss: 0.667788\n",
      "epoch 181; iter: 0; batch classifier loss: 0.003196; batch adversarial loss: 0.683137\n",
      "epoch 182; iter: 0; batch classifier loss: 0.008031; batch adversarial loss: 0.675056\n",
      "epoch 183; iter: 0; batch classifier loss: 0.003046; batch adversarial loss: 0.656729\n",
      "epoch 184; iter: 0; batch classifier loss: 0.015140; batch adversarial loss: 0.653801\n",
      "epoch 185; iter: 0; batch classifier loss: 0.009113; batch adversarial loss: 0.680631\n",
      "epoch 186; iter: 0; batch classifier loss: 0.010739; batch adversarial loss: 0.677472\n",
      "epoch 187; iter: 0; batch classifier loss: 0.011139; batch adversarial loss: 0.661539\n",
      "epoch 188; iter: 0; batch classifier loss: 0.013077; batch adversarial loss: 0.652949\n",
      "epoch 189; iter: 0; batch classifier loss: 0.007062; batch adversarial loss: 0.669503\n",
      "epoch 190; iter: 0; batch classifier loss: 0.004499; batch adversarial loss: 0.664311\n",
      "epoch 191; iter: 0; batch classifier loss: 0.004310; batch adversarial loss: 0.671883\n",
      "epoch 192; iter: 0; batch classifier loss: 0.005762; batch adversarial loss: 0.655624\n",
      "epoch 193; iter: 0; batch classifier loss: 0.005858; batch adversarial loss: 0.672507\n",
      "epoch 194; iter: 0; batch classifier loss: 0.003614; batch adversarial loss: 0.678034\n",
      "epoch 195; iter: 0; batch classifier loss: 0.003034; batch adversarial loss: 0.680425\n",
      "epoch 196; iter: 0; batch classifier loss: 0.002621; batch adversarial loss: 0.658287\n",
      "epoch 197; iter: 0; batch classifier loss: 0.002593; batch adversarial loss: 0.663455\n",
      "epoch 198; iter: 0; batch classifier loss: 0.012276; batch adversarial loss: 0.692302\n",
      "epoch 199; iter: 0; batch classifier loss: 0.004210; batch adversarial loss: 0.706438\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:43.388078: W tensorflow/c/c_api.cc:304] Operation '{name:'4c969596-adee-11ee-9362-a9d23602d4a3/4c969596-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:6433 op device:{requested: '', assigned: ''} def:{{{node 4c969596-adee-11ee-9362-a9d23602d4a3/4c969596-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c969596-adee-11ee-9362-a9d23602d4a3/4c969596-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c969596-adee-11ee-9362-a9d23602d4a3/4c969596-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.548547; batch adversarial loss: 0.763978\n",
      "epoch 1; iter: 0; batch classifier loss: 0.436328; batch adversarial loss: 0.727607\n",
      "epoch 2; iter: 0; batch classifier loss: 0.386549; batch adversarial loss: 0.751153\n",
      "epoch 3; iter: 0; batch classifier loss: 0.327824; batch adversarial loss: 0.743392\n",
      "epoch 4; iter: 0; batch classifier loss: 0.276491; batch adversarial loss: 0.800729\n",
      "epoch 5; iter: 0; batch classifier loss: 0.305604; batch adversarial loss: 0.770110\n",
      "epoch 6; iter: 0; batch classifier loss: 0.266101; batch adversarial loss: 0.748924\n",
      "epoch 7; iter: 0; batch classifier loss: 0.272713; batch adversarial loss: 0.767110\n",
      "epoch 8; iter: 0; batch classifier loss: 0.225397; batch adversarial loss: 0.755441\n",
      "epoch 9; iter: 0; batch classifier loss: 0.222557; batch adversarial loss: 0.739229\n",
      "epoch 10; iter: 0; batch classifier loss: 0.156488; batch adversarial loss: 0.743835\n",
      "epoch 11; iter: 0; batch classifier loss: 0.212657; batch adversarial loss: 0.761971\n",
      "epoch 12; iter: 0; batch classifier loss: 0.200198; batch adversarial loss: 0.759199\n",
      "epoch 13; iter: 0; batch classifier loss: 0.180080; batch adversarial loss: 0.745343\n",
      "epoch 14; iter: 0; batch classifier loss: 0.142935; batch adversarial loss: 0.733273\n",
      "epoch 15; iter: 0; batch classifier loss: 0.193059; batch adversarial loss: 0.729949\n",
      "epoch 16; iter: 0; batch classifier loss: 0.163465; batch adversarial loss: 0.743270\n",
      "epoch 17; iter: 0; batch classifier loss: 0.168263; batch adversarial loss: 0.742288\n",
      "epoch 18; iter: 0; batch classifier loss: 0.143064; batch adversarial loss: 0.728102\n",
      "epoch 19; iter: 0; batch classifier loss: 0.116314; batch adversarial loss: 0.748734\n",
      "epoch 20; iter: 0; batch classifier loss: 0.133727; batch adversarial loss: 0.729491\n",
      "epoch 21; iter: 0; batch classifier loss: 0.097013; batch adversarial loss: 0.723342\n",
      "epoch 22; iter: 0; batch classifier loss: 0.108003; batch adversarial loss: 0.744549\n",
      "epoch 23; iter: 0; batch classifier loss: 0.128360; batch adversarial loss: 0.721659\n",
      "epoch 24; iter: 0; batch classifier loss: 0.099230; batch adversarial loss: 0.715991\n",
      "epoch 25; iter: 0; batch classifier loss: 0.114399; batch adversarial loss: 0.730346\n",
      "epoch 26; iter: 0; batch classifier loss: 0.135261; batch adversarial loss: 0.727397\n",
      "epoch 27; iter: 0; batch classifier loss: 0.097512; batch adversarial loss: 0.753204\n",
      "epoch 28; iter: 0; batch classifier loss: 0.101209; batch adversarial loss: 0.709667\n",
      "epoch 29; iter: 0; batch classifier loss: 0.096174; batch adversarial loss: 0.729413\n",
      "epoch 30; iter: 0; batch classifier loss: 0.075215; batch adversarial loss: 0.742783\n",
      "epoch 31; iter: 0; batch classifier loss: 0.089562; batch adversarial loss: 0.711734\n",
      "epoch 32; iter: 0; batch classifier loss: 0.075349; batch adversarial loss: 0.739733\n",
      "epoch 33; iter: 0; batch classifier loss: 0.077159; batch adversarial loss: 0.711847\n",
      "epoch 34; iter: 0; batch classifier loss: 0.077270; batch adversarial loss: 0.749910\n",
      "epoch 35; iter: 0; batch classifier loss: 0.090727; batch adversarial loss: 0.729464\n",
      "epoch 36; iter: 0; batch classifier loss: 0.071644; batch adversarial loss: 0.728453\n",
      "epoch 37; iter: 0; batch classifier loss: 0.092319; batch adversarial loss: 0.739732\n",
      "epoch 38; iter: 0; batch classifier loss: 0.081384; batch adversarial loss: 0.712443\n",
      "epoch 39; iter: 0; batch classifier loss: 0.074892; batch adversarial loss: 0.733746\n",
      "epoch 40; iter: 0; batch classifier loss: 0.064778; batch adversarial loss: 0.709603\n",
      "epoch 41; iter: 0; batch classifier loss: 0.058526; batch adversarial loss: 0.711142\n",
      "epoch 42; iter: 0; batch classifier loss: 0.087968; batch adversarial loss: 0.725510\n",
      "epoch 43; iter: 0; batch classifier loss: 0.049010; batch adversarial loss: 0.713373\n",
      "epoch 44; iter: 0; batch classifier loss: 0.075435; batch adversarial loss: 0.725044\n",
      "epoch 45; iter: 0; batch classifier loss: 0.069577; batch adversarial loss: 0.716389\n",
      "epoch 46; iter: 0; batch classifier loss: 0.042654; batch adversarial loss: 0.698935\n",
      "epoch 47; iter: 0; batch classifier loss: 0.057277; batch adversarial loss: 0.696777\n",
      "epoch 48; iter: 0; batch classifier loss: 0.075841; batch adversarial loss: 0.727451\n",
      "epoch 49; iter: 0; batch classifier loss: 0.034547; batch adversarial loss: 0.705999\n",
      "epoch 50; iter: 0; batch classifier loss: 0.066116; batch adversarial loss: 0.701533\n",
      "epoch 51; iter: 0; batch classifier loss: 0.062555; batch adversarial loss: 0.718003\n",
      "epoch 52; iter: 0; batch classifier loss: 0.044439; batch adversarial loss: 0.710366\n",
      "epoch 53; iter: 0; batch classifier loss: 0.027452; batch adversarial loss: 0.718648\n",
      "epoch 54; iter: 0; batch classifier loss: 0.041529; batch adversarial loss: 0.713931\n",
      "epoch 55; iter: 0; batch classifier loss: 0.060006; batch adversarial loss: 0.700060\n",
      "epoch 56; iter: 0; batch classifier loss: 0.057294; batch adversarial loss: 0.693966\n",
      "epoch 57; iter: 0; batch classifier loss: 0.048315; batch adversarial loss: 0.712315\n",
      "epoch 58; iter: 0; batch classifier loss: 0.047721; batch adversarial loss: 0.698137\n",
      "epoch 59; iter: 0; batch classifier loss: 0.056853; batch adversarial loss: 0.701561\n",
      "epoch 60; iter: 0; batch classifier loss: 0.057765; batch adversarial loss: 0.700394\n",
      "epoch 61; iter: 0; batch classifier loss: 0.030219; batch adversarial loss: 0.707969\n",
      "epoch 62; iter: 0; batch classifier loss: 0.048652; batch adversarial loss: 0.709076\n",
      "epoch 63; iter: 0; batch classifier loss: 0.034250; batch adversarial loss: 0.696873\n",
      "epoch 64; iter: 0; batch classifier loss: 0.032026; batch adversarial loss: 0.703438\n",
      "epoch 65; iter: 0; batch classifier loss: 0.029479; batch adversarial loss: 0.691056\n",
      "epoch 66; iter: 0; batch classifier loss: 0.022365; batch adversarial loss: 0.699723\n",
      "epoch 67; iter: 0; batch classifier loss: 0.028968; batch adversarial loss: 0.696015\n",
      "epoch 68; iter: 0; batch classifier loss: 0.048374; batch adversarial loss: 0.704417\n",
      "epoch 69; iter: 0; batch classifier loss: 0.024224; batch adversarial loss: 0.710042\n",
      "epoch 70; iter: 0; batch classifier loss: 0.028068; batch adversarial loss: 0.688290\n",
      "epoch 71; iter: 0; batch classifier loss: 0.023652; batch adversarial loss: 0.706156\n",
      "epoch 72; iter: 0; batch classifier loss: 0.034169; batch adversarial loss: 0.705908\n",
      "epoch 73; iter: 0; batch classifier loss: 0.031194; batch adversarial loss: 0.701220\n",
      "epoch 74; iter: 0; batch classifier loss: 0.029269; batch adversarial loss: 0.694219\n",
      "epoch 75; iter: 0; batch classifier loss: 0.026841; batch adversarial loss: 0.700467\n",
      "epoch 76; iter: 0; batch classifier loss: 0.039070; batch adversarial loss: 0.701332\n",
      "epoch 77; iter: 0; batch classifier loss: 0.037513; batch adversarial loss: 0.694685\n",
      "epoch 78; iter: 0; batch classifier loss: 0.029320; batch adversarial loss: 0.693017\n",
      "epoch 79; iter: 0; batch classifier loss: 0.026379; batch adversarial loss: 0.697581\n",
      "epoch 80; iter: 0; batch classifier loss: 0.031623; batch adversarial loss: 0.694702\n",
      "epoch 81; iter: 0; batch classifier loss: 0.027190; batch adversarial loss: 0.690313\n",
      "epoch 82; iter: 0; batch classifier loss: 0.019168; batch adversarial loss: 0.691623\n",
      "epoch 83; iter: 0; batch classifier loss: 0.018185; batch adversarial loss: 0.694833\n",
      "epoch 84; iter: 0; batch classifier loss: 0.026116; batch adversarial loss: 0.695684\n",
      "epoch 85; iter: 0; batch classifier loss: 0.027178; batch adversarial loss: 0.699591\n",
      "epoch 86; iter: 0; batch classifier loss: 0.042044; batch adversarial loss: 0.689593\n",
      "epoch 87; iter: 0; batch classifier loss: 0.032578; batch adversarial loss: 0.688825\n",
      "epoch 88; iter: 0; batch classifier loss: 0.035972; batch adversarial loss: 0.687794\n",
      "epoch 89; iter: 0; batch classifier loss: 0.036258; batch adversarial loss: 0.688966\n",
      "epoch 90; iter: 0; batch classifier loss: 0.025358; batch adversarial loss: 0.687185\n",
      "epoch 91; iter: 0; batch classifier loss: 0.029306; batch adversarial loss: 0.690776\n",
      "epoch 92; iter: 0; batch classifier loss: 0.022272; batch adversarial loss: 0.696490\n",
      "epoch 93; iter: 0; batch classifier loss: 0.021904; batch adversarial loss: 0.689591\n",
      "epoch 94; iter: 0; batch classifier loss: 0.017717; batch adversarial loss: 0.689727\n",
      "epoch 95; iter: 0; batch classifier loss: 0.018464; batch adversarial loss: 0.690059\n",
      "epoch 96; iter: 0; batch classifier loss: 0.021288; batch adversarial loss: 0.688070\n",
      "epoch 97; iter: 0; batch classifier loss: 0.024315; batch adversarial loss: 0.687861\n",
      "epoch 98; iter: 0; batch classifier loss: 0.037351; batch adversarial loss: 0.682455\n",
      "epoch 99; iter: 0; batch classifier loss: 0.027024; batch adversarial loss: 0.687340\n",
      "epoch 100; iter: 0; batch classifier loss: 0.020418; batch adversarial loss: 0.690972\n",
      "epoch 101; iter: 0; batch classifier loss: 0.014015; batch adversarial loss: 0.687313\n",
      "epoch 102; iter: 0; batch classifier loss: 0.019430; batch adversarial loss: 0.687998\n",
      "epoch 103; iter: 0; batch classifier loss: 0.024085; batch adversarial loss: 0.694382\n",
      "epoch 104; iter: 0; batch classifier loss: 0.023498; batch adversarial loss: 0.688678\n",
      "epoch 105; iter: 0; batch classifier loss: 0.019310; batch adversarial loss: 0.683783\n",
      "epoch 106; iter: 0; batch classifier loss: 0.014555; batch adversarial loss: 0.693807\n",
      "epoch 107; iter: 0; batch classifier loss: 0.024413; batch adversarial loss: 0.684943\n",
      "epoch 108; iter: 0; batch classifier loss: 0.020751; batch adversarial loss: 0.683203\n",
      "epoch 109; iter: 0; batch classifier loss: 0.033571; batch adversarial loss: 0.684901\n",
      "epoch 110; iter: 0; batch classifier loss: 0.030032; batch adversarial loss: 0.682335\n",
      "epoch 111; iter: 0; batch classifier loss: 0.030071; batch adversarial loss: 0.681894\n",
      "epoch 112; iter: 0; batch classifier loss: 0.020149; batch adversarial loss: 0.680829\n",
      "epoch 113; iter: 0; batch classifier loss: 0.015737; batch adversarial loss: 0.683595\n",
      "epoch 114; iter: 0; batch classifier loss: 0.020400; batch adversarial loss: 0.687222\n",
      "epoch 115; iter: 0; batch classifier loss: 0.016500; batch adversarial loss: 0.689652\n",
      "epoch 116; iter: 0; batch classifier loss: 0.011587; batch adversarial loss: 0.685273\n",
      "epoch 117; iter: 0; batch classifier loss: 0.013480; batch adversarial loss: 0.689046\n",
      "epoch 118; iter: 0; batch classifier loss: 0.025053; batch adversarial loss: 0.686464\n",
      "epoch 119; iter: 0; batch classifier loss: 0.035850; batch adversarial loss: 0.677851\n",
      "epoch 120; iter: 0; batch classifier loss: 0.019412; batch adversarial loss: 0.684416\n",
      "epoch 121; iter: 0; batch classifier loss: 0.022902; batch adversarial loss: 0.681203\n",
      "epoch 122; iter: 0; batch classifier loss: 0.012406; batch adversarial loss: 0.682194\n",
      "epoch 123; iter: 0; batch classifier loss: 0.018405; batch adversarial loss: 0.683785\n",
      "epoch 124; iter: 0; batch classifier loss: 0.022631; batch adversarial loss: 0.677253\n",
      "epoch 125; iter: 0; batch classifier loss: 0.018320; batch adversarial loss: 0.678279\n",
      "epoch 126; iter: 0; batch classifier loss: 0.014352; batch adversarial loss: 0.681671\n",
      "epoch 127; iter: 0; batch classifier loss: 0.020341; batch adversarial loss: 0.685119\n",
      "epoch 128; iter: 0; batch classifier loss: 0.009395; batch adversarial loss: 0.677573\n",
      "epoch 129; iter: 0; batch classifier loss: 0.020422; batch adversarial loss: 0.683098\n",
      "epoch 130; iter: 0; batch classifier loss: 0.005868; batch adversarial loss: 0.684129\n",
      "epoch 131; iter: 0; batch classifier loss: 0.018945; batch adversarial loss: 0.678218\n",
      "epoch 132; iter: 0; batch classifier loss: 0.012584; batch adversarial loss: 0.681097\n",
      "epoch 133; iter: 0; batch classifier loss: 0.015278; batch adversarial loss: 0.679334\n",
      "epoch 134; iter: 0; batch classifier loss: 0.020000; batch adversarial loss: 0.677352\n",
      "epoch 135; iter: 0; batch classifier loss: 0.008735; batch adversarial loss: 0.682023\n",
      "epoch 136; iter: 0; batch classifier loss: 0.026085; batch adversarial loss: 0.691938\n",
      "epoch 137; iter: 0; batch classifier loss: 0.005228; batch adversarial loss: 0.688632\n",
      "epoch 138; iter: 0; batch classifier loss: 0.008551; batch adversarial loss: 0.688315\n",
      "epoch 139; iter: 0; batch classifier loss: 0.004846; batch adversarial loss: 0.693382\n",
      "epoch 140; iter: 0; batch classifier loss: 0.009073; batch adversarial loss: 0.677861\n",
      "epoch 141; iter: 0; batch classifier loss: 0.023434; batch adversarial loss: 0.680392\n",
      "epoch 142; iter: 0; batch classifier loss: 0.012729; batch adversarial loss: 0.679763\n",
      "epoch 143; iter: 0; batch classifier loss: 0.011510; batch adversarial loss: 0.679015\n",
      "epoch 144; iter: 0; batch classifier loss: 0.007687; batch adversarial loss: 0.694270\n",
      "epoch 145; iter: 0; batch classifier loss: 0.004415; batch adversarial loss: 0.679756\n",
      "epoch 146; iter: 0; batch classifier loss: 0.021593; batch adversarial loss: 0.678740\n",
      "epoch 147; iter: 0; batch classifier loss: 0.016408; batch adversarial loss: 0.689419\n",
      "epoch 148; iter: 0; batch classifier loss: 0.007455; batch adversarial loss: 0.685789\n",
      "epoch 149; iter: 0; batch classifier loss: 0.009318; batch adversarial loss: 0.683557\n",
      "epoch 150; iter: 0; batch classifier loss: 0.011315; batch adversarial loss: 0.679237\n",
      "epoch 151; iter: 0; batch classifier loss: 0.014070; batch adversarial loss: 0.684643\n",
      "epoch 152; iter: 0; batch classifier loss: 0.012807; batch adversarial loss: 0.683064\n",
      "epoch 153; iter: 0; batch classifier loss: 0.007039; batch adversarial loss: 0.688161\n",
      "epoch 154; iter: 0; batch classifier loss: 0.008237; batch adversarial loss: 0.678379\n",
      "epoch 155; iter: 0; batch classifier loss: 0.020363; batch adversarial loss: 0.673521\n",
      "epoch 156; iter: 0; batch classifier loss: 0.007604; batch adversarial loss: 0.672486\n",
      "epoch 157; iter: 0; batch classifier loss: 0.004771; batch adversarial loss: 0.677476\n",
      "epoch 158; iter: 0; batch classifier loss: 0.013201; batch adversarial loss: 0.675532\n",
      "epoch 159; iter: 0; batch classifier loss: 0.008655; batch adversarial loss: 0.676917\n",
      "epoch 160; iter: 0; batch classifier loss: 0.009038; batch adversarial loss: 0.683595\n",
      "epoch 161; iter: 0; batch classifier loss: 0.008745; batch adversarial loss: 0.671223\n",
      "epoch 162; iter: 0; batch classifier loss: 0.015901; batch adversarial loss: 0.684418\n",
      "epoch 163; iter: 0; batch classifier loss: 0.008007; batch adversarial loss: 0.676072\n",
      "epoch 164; iter: 0; batch classifier loss: 0.006024; batch adversarial loss: 0.684490\n",
      "epoch 165; iter: 0; batch classifier loss: 0.016270; batch adversarial loss: 0.674273\n",
      "epoch 166; iter: 0; batch classifier loss: 0.005193; batch adversarial loss: 0.708000\n",
      "epoch 167; iter: 0; batch classifier loss: 0.022454; batch adversarial loss: 0.685942\n",
      "epoch 168; iter: 0; batch classifier loss: 0.010290; batch adversarial loss: 0.690504\n",
      "epoch 169; iter: 0; batch classifier loss: 0.016815; batch adversarial loss: 0.682567\n",
      "epoch 170; iter: 0; batch classifier loss: 0.004173; batch adversarial loss: 0.676748\n",
      "epoch 171; iter: 0; batch classifier loss: 0.007956; batch adversarial loss: 0.695259\n",
      "epoch 172; iter: 0; batch classifier loss: 0.007208; batch adversarial loss: 0.673257\n",
      "epoch 173; iter: 0; batch classifier loss: 0.007109; batch adversarial loss: 0.671048\n",
      "epoch 174; iter: 0; batch classifier loss: 0.011697; batch adversarial loss: 0.688280\n",
      "epoch 175; iter: 0; batch classifier loss: 0.004495; batch adversarial loss: 0.676747\n",
      "epoch 176; iter: 0; batch classifier loss: 0.007259; batch adversarial loss: 0.684179\n",
      "epoch 177; iter: 0; batch classifier loss: 0.017138; batch adversarial loss: 0.671625\n",
      "epoch 178; iter: 0; batch classifier loss: 0.004012; batch adversarial loss: 0.691993\n",
      "epoch 179; iter: 0; batch classifier loss: 0.013343; batch adversarial loss: 0.676611\n",
      "epoch 180; iter: 0; batch classifier loss: 0.006016; batch adversarial loss: 0.679071\n",
      "epoch 181; iter: 0; batch classifier loss: 0.006253; batch adversarial loss: 0.681216\n",
      "epoch 182; iter: 0; batch classifier loss: 0.013825; batch adversarial loss: 0.675698\n",
      "epoch 183; iter: 0; batch classifier loss: 0.017677; batch adversarial loss: 0.679743\n",
      "epoch 184; iter: 0; batch classifier loss: 0.012090; batch adversarial loss: 0.670110\n",
      "epoch 185; iter: 0; batch classifier loss: 0.007029; batch adversarial loss: 0.693353\n",
      "epoch 186; iter: 0; batch classifier loss: 0.008188; batch adversarial loss: 0.684983\n",
      "epoch 187; iter: 0; batch classifier loss: 0.012716; batch adversarial loss: 0.671363\n",
      "epoch 188; iter: 0; batch classifier loss: 0.004095; batch adversarial loss: 0.706395\n",
      "epoch 189; iter: 0; batch classifier loss: 0.002872; batch adversarial loss: 0.691288\n",
      "epoch 190; iter: 0; batch classifier loss: 0.002259; batch adversarial loss: 0.672713\n",
      "epoch 191; iter: 0; batch classifier loss: 0.010409; batch adversarial loss: 0.700649\n",
      "epoch 192; iter: 0; batch classifier loss: 0.003288; batch adversarial loss: 0.690830\n",
      "epoch 193; iter: 0; batch classifier loss: 0.029639; batch adversarial loss: 0.679984\n",
      "epoch 194; iter: 0; batch classifier loss: 0.011019; batch adversarial loss: 0.680945\n",
      "epoch 195; iter: 0; batch classifier loss: 0.003375; batch adversarial loss: 0.689440\n",
      "epoch 196; iter: 0; batch classifier loss: 0.009172; batch adversarial loss: 0.690575\n",
      "epoch 197; iter: 0; batch classifier loss: 0.009053; batch adversarial loss: 0.678135\n",
      "epoch 198; iter: 0; batch classifier loss: 0.009917; batch adversarial loss: 0.668224\n",
      "epoch 199; iter: 0; batch classifier loss: 0.010147; batch adversarial loss: 0.695969\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:45.307323: W tensorflow/c/c_api.cc:304] Operation '{name:'4c969622-adee-11ee-9362-a9d23602d4a3/4c969622-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:7240 op device:{requested: '', assigned: ''} def:{{{node 4c969622-adee-11ee-9362-a9d23602d4a3/4c969622-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c969622-adee-11ee-9362-a9d23602d4a3/4c969622-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c969622-adee-11ee-9362-a9d23602d4a3/4c969622-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.581354; batch adversarial loss: 0.896448\n",
      "epoch 1; iter: 0; batch classifier loss: 0.513502; batch adversarial loss: 0.912638\n",
      "epoch 2; iter: 0; batch classifier loss: 0.463063; batch adversarial loss: 1.009861\n",
      "epoch 3; iter: 0; batch classifier loss: 0.423469; batch adversarial loss: 0.975788\n",
      "epoch 4; iter: 0; batch classifier loss: 0.337638; batch adversarial loss: 1.022497\n",
      "epoch 5; iter: 0; batch classifier loss: 0.304026; batch adversarial loss: 0.912642\n",
      "epoch 6; iter: 0; batch classifier loss: 0.306271; batch adversarial loss: 0.962317\n",
      "epoch 7; iter: 0; batch classifier loss: 0.274350; batch adversarial loss: 1.033191\n",
      "epoch 8; iter: 0; batch classifier loss: 0.217130; batch adversarial loss: 0.996767\n",
      "epoch 9; iter: 0; batch classifier loss: 0.278500; batch adversarial loss: 0.997239\n",
      "epoch 10; iter: 0; batch classifier loss: 0.178617; batch adversarial loss: 1.041247\n",
      "epoch 11; iter: 0; batch classifier loss: 0.224508; batch adversarial loss: 0.999412\n",
      "epoch 12; iter: 0; batch classifier loss: 0.190929; batch adversarial loss: 1.096553\n",
      "epoch 13; iter: 0; batch classifier loss: 0.156656; batch adversarial loss: 0.990293\n",
      "epoch 14; iter: 0; batch classifier loss: 0.211958; batch adversarial loss: 0.980588\n",
      "epoch 15; iter: 0; batch classifier loss: 0.127594; batch adversarial loss: 1.048319\n",
      "epoch 16; iter: 0; batch classifier loss: 0.167519; batch adversarial loss: 1.024037\n",
      "epoch 17; iter: 0; batch classifier loss: 0.120818; batch adversarial loss: 1.024875\n",
      "epoch 18; iter: 0; batch classifier loss: 0.146247; batch adversarial loss: 1.011636\n",
      "epoch 19; iter: 0; batch classifier loss: 0.133783; batch adversarial loss: 0.962414\n",
      "epoch 20; iter: 0; batch classifier loss: 0.132441; batch adversarial loss: 1.037777\n",
      "epoch 21; iter: 0; batch classifier loss: 0.117354; batch adversarial loss: 0.946872\n",
      "epoch 22; iter: 0; batch classifier loss: 0.131534; batch adversarial loss: 1.059844\n",
      "epoch 23; iter: 0; batch classifier loss: 0.098815; batch adversarial loss: 1.048831\n",
      "epoch 24; iter: 0; batch classifier loss: 0.127074; batch adversarial loss: 0.921215\n",
      "epoch 25; iter: 0; batch classifier loss: 0.122327; batch adversarial loss: 1.063457\n",
      "epoch 26; iter: 0; batch classifier loss: 0.107169; batch adversarial loss: 0.976748\n",
      "epoch 27; iter: 0; batch classifier loss: 0.097903; batch adversarial loss: 0.980154\n",
      "epoch 28; iter: 0; batch classifier loss: 0.114892; batch adversarial loss: 0.946667\n",
      "epoch 29; iter: 0; batch classifier loss: 0.109702; batch adversarial loss: 1.000578\n",
      "epoch 30; iter: 0; batch classifier loss: 0.091144; batch adversarial loss: 1.077550\n",
      "epoch 31; iter: 0; batch classifier loss: 0.086323; batch adversarial loss: 0.977662\n",
      "epoch 32; iter: 0; batch classifier loss: 0.076905; batch adversarial loss: 0.999747\n",
      "epoch 33; iter: 0; batch classifier loss: 0.106511; batch adversarial loss: 1.034592\n",
      "epoch 34; iter: 0; batch classifier loss: 0.070518; batch adversarial loss: 1.092448\n",
      "epoch 35; iter: 0; batch classifier loss: 0.072046; batch adversarial loss: 1.057423\n",
      "epoch 36; iter: 0; batch classifier loss: 0.067192; batch adversarial loss: 1.064473\n",
      "epoch 37; iter: 0; batch classifier loss: 0.072955; batch adversarial loss: 0.855205\n",
      "epoch 38; iter: 0; batch classifier loss: 0.072431; batch adversarial loss: 0.998193\n",
      "epoch 39; iter: 0; batch classifier loss: 0.070934; batch adversarial loss: 1.018957\n",
      "epoch 40; iter: 0; batch classifier loss: 0.066522; batch adversarial loss: 0.913521\n",
      "epoch 41; iter: 0; batch classifier loss: 0.074470; batch adversarial loss: 0.996026\n",
      "epoch 42; iter: 0; batch classifier loss: 0.085075; batch adversarial loss: 0.960938\n",
      "epoch 43; iter: 0; batch classifier loss: 0.079030; batch adversarial loss: 0.925578\n",
      "epoch 44; iter: 0; batch classifier loss: 0.049283; batch adversarial loss: 0.930322\n",
      "epoch 45; iter: 0; batch classifier loss: 0.055899; batch adversarial loss: 0.854879\n",
      "epoch 46; iter: 0; batch classifier loss: 0.067726; batch adversarial loss: 0.925135\n",
      "epoch 47; iter: 0; batch classifier loss: 0.061430; batch adversarial loss: 0.959066\n",
      "epoch 48; iter: 0; batch classifier loss: 0.036841; batch adversarial loss: 0.944388\n",
      "epoch 49; iter: 0; batch classifier loss: 0.071228; batch adversarial loss: 0.903134\n",
      "epoch 50; iter: 0; batch classifier loss: 0.048158; batch adversarial loss: 0.933315\n",
      "epoch 51; iter: 0; batch classifier loss: 0.042644; batch adversarial loss: 0.879721\n",
      "epoch 52; iter: 0; batch classifier loss: 0.032299; batch adversarial loss: 0.938362\n",
      "epoch 53; iter: 0; batch classifier loss: 0.058119; batch adversarial loss: 0.930805\n",
      "epoch 54; iter: 0; batch classifier loss: 0.026220; batch adversarial loss: 0.898356\n",
      "epoch 55; iter: 0; batch classifier loss: 0.041863; batch adversarial loss: 0.896895\n",
      "epoch 56; iter: 0; batch classifier loss: 0.044715; batch adversarial loss: 0.943187\n",
      "epoch 57; iter: 0; batch classifier loss: 0.059413; batch adversarial loss: 0.898004\n",
      "epoch 58; iter: 0; batch classifier loss: 0.045399; batch adversarial loss: 0.906469\n",
      "epoch 59; iter: 0; batch classifier loss: 0.040936; batch adversarial loss: 0.930920\n",
      "epoch 60; iter: 0; batch classifier loss: 0.037411; batch adversarial loss: 0.963789\n",
      "epoch 61; iter: 0; batch classifier loss: 0.027378; batch adversarial loss: 0.987750\n",
      "epoch 62; iter: 0; batch classifier loss: 0.039534; batch adversarial loss: 0.922756\n",
      "epoch 63; iter: 0; batch classifier loss: 0.041324; batch adversarial loss: 0.922212\n",
      "epoch 64; iter: 0; batch classifier loss: 0.050023; batch adversarial loss: 0.938806\n",
      "epoch 65; iter: 0; batch classifier loss: 0.035696; batch adversarial loss: 0.891759\n",
      "epoch 66; iter: 0; batch classifier loss: 0.039656; batch adversarial loss: 0.865685\n",
      "epoch 67; iter: 0; batch classifier loss: 0.046582; batch adversarial loss: 0.918037\n",
      "epoch 68; iter: 0; batch classifier loss: 0.049649; batch adversarial loss: 0.905442\n",
      "epoch 69; iter: 0; batch classifier loss: 0.047116; batch adversarial loss: 0.861586\n",
      "epoch 70; iter: 0; batch classifier loss: 0.034736; batch adversarial loss: 0.903134\n",
      "epoch 71; iter: 0; batch classifier loss: 0.032187; batch adversarial loss: 0.905156\n",
      "epoch 72; iter: 0; batch classifier loss: 0.046927; batch adversarial loss: 0.931820\n",
      "epoch 73; iter: 0; batch classifier loss: 0.023720; batch adversarial loss: 0.882824\n",
      "epoch 74; iter: 0; batch classifier loss: 0.034391; batch adversarial loss: 0.927931\n",
      "epoch 75; iter: 0; batch classifier loss: 0.029419; batch adversarial loss: 0.884922\n",
      "epoch 76; iter: 0; batch classifier loss: 0.029318; batch adversarial loss: 0.902662\n",
      "epoch 77; iter: 0; batch classifier loss: 0.041042; batch adversarial loss: 0.884377\n",
      "epoch 78; iter: 0; batch classifier loss: 0.057047; batch adversarial loss: 0.886423\n",
      "epoch 79; iter: 0; batch classifier loss: 0.017355; batch adversarial loss: 0.837301\n",
      "epoch 80; iter: 0; batch classifier loss: 0.029538; batch adversarial loss: 0.897305\n",
      "epoch 81; iter: 0; batch classifier loss: 0.044134; batch adversarial loss: 0.881957\n",
      "epoch 82; iter: 0; batch classifier loss: 0.039517; batch adversarial loss: 0.810939\n",
      "epoch 83; iter: 0; batch classifier loss: 0.032146; batch adversarial loss: 0.904833\n",
      "epoch 84; iter: 0; batch classifier loss: 0.059234; batch adversarial loss: 0.879283\n",
      "epoch 85; iter: 0; batch classifier loss: 0.033041; batch adversarial loss: 0.845491\n",
      "epoch 86; iter: 0; batch classifier loss: 0.034928; batch adversarial loss: 0.862704\n",
      "epoch 87; iter: 0; batch classifier loss: 0.026182; batch adversarial loss: 0.896409\n",
      "epoch 88; iter: 0; batch classifier loss: 0.021768; batch adversarial loss: 0.833512\n",
      "epoch 89; iter: 0; batch classifier loss: 0.033680; batch adversarial loss: 0.836987\n",
      "epoch 90; iter: 0; batch classifier loss: 0.038716; batch adversarial loss: 0.907564\n",
      "epoch 91; iter: 0; batch classifier loss: 0.042751; batch adversarial loss: 0.837582\n",
      "epoch 92; iter: 0; batch classifier loss: 0.045036; batch adversarial loss: 0.839557\n",
      "epoch 93; iter: 0; batch classifier loss: 0.024791; batch adversarial loss: 0.822299\n",
      "epoch 94; iter: 0; batch classifier loss: 0.026109; batch adversarial loss: 0.849319\n",
      "epoch 95; iter: 0; batch classifier loss: 0.038150; batch adversarial loss: 0.844403\n",
      "epoch 96; iter: 0; batch classifier loss: 0.043252; batch adversarial loss: 0.819866\n",
      "epoch 97; iter: 0; batch classifier loss: 0.038435; batch adversarial loss: 0.819604\n",
      "epoch 98; iter: 0; batch classifier loss: 0.028685; batch adversarial loss: 0.825490\n",
      "epoch 99; iter: 0; batch classifier loss: 0.055544; batch adversarial loss: 0.858940\n",
      "epoch 100; iter: 0; batch classifier loss: 0.026303; batch adversarial loss: 0.862473\n",
      "epoch 101; iter: 0; batch classifier loss: 0.032520; batch adversarial loss: 0.838143\n",
      "epoch 102; iter: 0; batch classifier loss: 0.049402; batch adversarial loss: 0.813420\n",
      "epoch 103; iter: 0; batch classifier loss: 0.025108; batch adversarial loss: 0.856145\n",
      "epoch 104; iter: 0; batch classifier loss: 0.016060; batch adversarial loss: 0.837464\n",
      "epoch 105; iter: 0; batch classifier loss: 0.040827; batch adversarial loss: 0.776286\n",
      "epoch 106; iter: 0; batch classifier loss: 0.022284; batch adversarial loss: 0.811959\n",
      "epoch 107; iter: 0; batch classifier loss: 0.040924; batch adversarial loss: 0.824593\n",
      "epoch 108; iter: 0; batch classifier loss: 0.039523; batch adversarial loss: 0.773460\n",
      "epoch 109; iter: 0; batch classifier loss: 0.048255; batch adversarial loss: 0.801446\n",
      "epoch 110; iter: 0; batch classifier loss: 0.032815; batch adversarial loss: 0.831443\n",
      "epoch 111; iter: 0; batch classifier loss: 0.031074; batch adversarial loss: 0.839399\n",
      "epoch 112; iter: 0; batch classifier loss: 0.023759; batch adversarial loss: 0.819576\n",
      "epoch 113; iter: 0; batch classifier loss: 0.011958; batch adversarial loss: 0.793849\n",
      "epoch 114; iter: 0; batch classifier loss: 0.035903; batch adversarial loss: 0.837482\n",
      "epoch 115; iter: 0; batch classifier loss: 0.033200; batch adversarial loss: 0.780671\n",
      "epoch 116; iter: 0; batch classifier loss: 0.049853; batch adversarial loss: 0.792475\n",
      "epoch 117; iter: 0; batch classifier loss: 0.011779; batch adversarial loss: 0.823744\n",
      "epoch 118; iter: 0; batch classifier loss: 0.047296; batch adversarial loss: 0.765980\n",
      "epoch 119; iter: 0; batch classifier loss: 0.011953; batch adversarial loss: 0.769787\n",
      "epoch 120; iter: 0; batch classifier loss: 0.029423; batch adversarial loss: 0.793262\n",
      "epoch 121; iter: 0; batch classifier loss: 0.038430; batch adversarial loss: 0.798069\n",
      "epoch 122; iter: 0; batch classifier loss: 0.038375; batch adversarial loss: 0.803929\n",
      "epoch 123; iter: 0; batch classifier loss: 0.017913; batch adversarial loss: 0.818422\n",
      "epoch 124; iter: 0; batch classifier loss: 0.014339; batch adversarial loss: 0.768715\n",
      "epoch 125; iter: 0; batch classifier loss: 0.025785; batch adversarial loss: 0.801926\n",
      "epoch 126; iter: 0; batch classifier loss: 0.012576; batch adversarial loss: 0.764301\n",
      "epoch 127; iter: 0; batch classifier loss: 0.039676; batch adversarial loss: 0.767693\n",
      "epoch 128; iter: 0; batch classifier loss: 0.050031; batch adversarial loss: 0.783111\n",
      "epoch 129; iter: 0; batch classifier loss: 0.027365; batch adversarial loss: 0.787067\n",
      "epoch 130; iter: 0; batch classifier loss: 0.037333; batch adversarial loss: 0.777078\n",
      "epoch 131; iter: 0; batch classifier loss: 0.023108; batch adversarial loss: 0.731263\n",
      "epoch 132; iter: 0; batch classifier loss: 0.039966; batch adversarial loss: 0.803773\n",
      "epoch 133; iter: 0; batch classifier loss: 0.009568; batch adversarial loss: 0.817546\n",
      "epoch 134; iter: 0; batch classifier loss: 0.068282; batch adversarial loss: 0.816659\n",
      "epoch 135; iter: 0; batch classifier loss: 0.021329; batch adversarial loss: 0.795493\n",
      "epoch 136; iter: 0; batch classifier loss: 0.025034; batch adversarial loss: 0.766744\n",
      "epoch 137; iter: 0; batch classifier loss: 0.051604; batch adversarial loss: 0.811876\n",
      "epoch 138; iter: 0; batch classifier loss: 0.017041; batch adversarial loss: 0.788715\n",
      "epoch 139; iter: 0; batch classifier loss: 0.031678; batch adversarial loss: 0.777836\n",
      "epoch 140; iter: 0; batch classifier loss: 0.026260; batch adversarial loss: 0.802551\n",
      "epoch 141; iter: 0; batch classifier loss: 0.022420; batch adversarial loss: 0.788321\n",
      "epoch 142; iter: 0; batch classifier loss: 0.040074; batch adversarial loss: 0.758968\n",
      "epoch 143; iter: 0; batch classifier loss: 0.053112; batch adversarial loss: 0.771167\n",
      "epoch 144; iter: 0; batch classifier loss: 0.036910; batch adversarial loss: 0.799887\n",
      "epoch 145; iter: 0; batch classifier loss: 0.047687; batch adversarial loss: 0.795741\n",
      "epoch 146; iter: 0; batch classifier loss: 0.030267; batch adversarial loss: 0.774621\n",
      "epoch 147; iter: 0; batch classifier loss: 0.039694; batch adversarial loss: 0.789327\n",
      "epoch 148; iter: 0; batch classifier loss: 0.035982; batch adversarial loss: 0.776121\n",
      "epoch 149; iter: 0; batch classifier loss: 0.044475; batch adversarial loss: 0.782016\n",
      "epoch 150; iter: 0; batch classifier loss: 0.037846; batch adversarial loss: 0.745668\n",
      "epoch 151; iter: 0; batch classifier loss: 0.042646; batch adversarial loss: 0.744847\n",
      "epoch 152; iter: 0; batch classifier loss: 0.043555; batch adversarial loss: 0.785858\n",
      "epoch 153; iter: 0; batch classifier loss: 0.029872; batch adversarial loss: 0.747254\n",
      "epoch 154; iter: 0; batch classifier loss: 0.033146; batch adversarial loss: 0.768543\n",
      "epoch 155; iter: 0; batch classifier loss: 0.020674; batch adversarial loss: 0.738374\n",
      "epoch 156; iter: 0; batch classifier loss: 0.024653; batch adversarial loss: 0.771797\n",
      "epoch 157; iter: 0; batch classifier loss: 0.030698; batch adversarial loss: 0.710275\n",
      "epoch 158; iter: 0; batch classifier loss: 0.046707; batch adversarial loss: 0.761830\n",
      "epoch 159; iter: 0; batch classifier loss: 0.037142; batch adversarial loss: 0.754460\n",
      "epoch 160; iter: 0; batch classifier loss: 0.012294; batch adversarial loss: 0.773570\n",
      "epoch 161; iter: 0; batch classifier loss: 0.008081; batch adversarial loss: 0.767040\n",
      "epoch 162; iter: 0; batch classifier loss: 0.034760; batch adversarial loss: 0.778480\n",
      "epoch 163; iter: 0; batch classifier loss: 0.022251; batch adversarial loss: 0.760777\n",
      "epoch 164; iter: 0; batch classifier loss: 0.017679; batch adversarial loss: 0.762107\n",
      "epoch 165; iter: 0; batch classifier loss: 0.032114; batch adversarial loss: 0.728312\n",
      "epoch 166; iter: 0; batch classifier loss: 0.025136; batch adversarial loss: 0.776708\n",
      "epoch 167; iter: 0; batch classifier loss: 0.024072; batch adversarial loss: 0.770319\n",
      "epoch 168; iter: 0; batch classifier loss: 0.027734; batch adversarial loss: 0.733980\n",
      "epoch 169; iter: 0; batch classifier loss: 0.034839; batch adversarial loss: 0.745141\n",
      "epoch 170; iter: 0; batch classifier loss: 0.027167; batch adversarial loss: 0.756196\n",
      "epoch 171; iter: 0; batch classifier loss: 0.036426; batch adversarial loss: 0.741150\n",
      "epoch 172; iter: 0; batch classifier loss: 0.022793; batch adversarial loss: 0.757402\n",
      "epoch 173; iter: 0; batch classifier loss: 0.025785; batch adversarial loss: 0.762981\n",
      "epoch 174; iter: 0; batch classifier loss: 0.023994; batch adversarial loss: 0.751859\n",
      "epoch 175; iter: 0; batch classifier loss: 0.021489; batch adversarial loss: 0.734986\n",
      "epoch 176; iter: 0; batch classifier loss: 0.026229; batch adversarial loss: 0.751325\n",
      "epoch 177; iter: 0; batch classifier loss: 0.042885; batch adversarial loss: 0.763836\n",
      "epoch 178; iter: 0; batch classifier loss: 0.016258; batch adversarial loss: 0.743823\n",
      "epoch 179; iter: 0; batch classifier loss: 0.018899; batch adversarial loss: 0.737897\n",
      "epoch 180; iter: 0; batch classifier loss: 0.025136; batch adversarial loss: 0.716126\n",
      "epoch 181; iter: 0; batch classifier loss: 0.033401; batch adversarial loss: 0.761707\n",
      "epoch 182; iter: 0; batch classifier loss: 0.023499; batch adversarial loss: 0.749004\n",
      "epoch 183; iter: 0; batch classifier loss: 0.015576; batch adversarial loss: 0.747259\n",
      "epoch 184; iter: 0; batch classifier loss: 0.017975; batch adversarial loss: 0.702265\n",
      "epoch 185; iter: 0; batch classifier loss: 0.034080; batch adversarial loss: 0.734239\n",
      "epoch 186; iter: 0; batch classifier loss: 0.037029; batch adversarial loss: 0.758444\n",
      "epoch 187; iter: 0; batch classifier loss: 0.030678; batch adversarial loss: 0.727069\n",
      "epoch 188; iter: 0; batch classifier loss: 0.033420; batch adversarial loss: 0.739533\n",
      "epoch 189; iter: 0; batch classifier loss: 0.023493; batch adversarial loss: 0.739755\n",
      "epoch 190; iter: 0; batch classifier loss: 0.023800; batch adversarial loss: 0.734844\n",
      "epoch 191; iter: 0; batch classifier loss: 0.015401; batch adversarial loss: 0.738808\n",
      "epoch 192; iter: 0; batch classifier loss: 0.006555; batch adversarial loss: 0.715489\n",
      "epoch 193; iter: 0; batch classifier loss: 0.031742; batch adversarial loss: 0.733080\n",
      "epoch 194; iter: 0; batch classifier loss: 0.032654; batch adversarial loss: 0.734006\n",
      "epoch 195; iter: 0; batch classifier loss: 0.019283; batch adversarial loss: 0.728731\n",
      "epoch 196; iter: 0; batch classifier loss: 0.024753; batch adversarial loss: 0.736405\n",
      "epoch 197; iter: 0; batch classifier loss: 0.034585; batch adversarial loss: 0.730791\n",
      "epoch 198; iter: 0; batch classifier loss: 0.046085; batch adversarial loss: 0.745465\n",
      "epoch 199; iter: 0; batch classifier loss: 0.027292; batch adversarial loss: 0.737036\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:47.407700: W tensorflow/c/c_api.cc:304] Operation '{name:'4c9696f4-adee-11ee-9362-a9d23602d4a3/4c9696f4-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:8047 op device:{requested: '', assigned: ''} def:{{{node 4c9696f4-adee-11ee-9362-a9d23602d4a3/4c9696f4-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c9696f4-adee-11ee-9362-a9d23602d4a3/4c9696f4-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c9696f4-adee-11ee-9362-a9d23602d4a3/4c9696f4-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.750276; batch adversarial loss: 0.704184\n",
      "epoch 1; iter: 0; batch classifier loss: 0.576510; batch adversarial loss: 0.716657\n",
      "epoch 2; iter: 0; batch classifier loss: 0.485160; batch adversarial loss: 0.681166\n",
      "epoch 3; iter: 0; batch classifier loss: 0.443013; batch adversarial loss: 0.719274\n",
      "epoch 4; iter: 0; batch classifier loss: 0.329226; batch adversarial loss: 0.702379\n",
      "epoch 5; iter: 0; batch classifier loss: 0.387916; batch adversarial loss: 0.732430\n",
      "epoch 6; iter: 0; batch classifier loss: 0.283026; batch adversarial loss: 0.712010\n",
      "epoch 7; iter: 0; batch classifier loss: 0.268203; batch adversarial loss: 0.737585\n",
      "epoch 8; iter: 0; batch classifier loss: 0.288157; batch adversarial loss: 0.712422\n",
      "epoch 9; iter: 0; batch classifier loss: 0.228359; batch adversarial loss: 0.729710\n",
      "epoch 10; iter: 0; batch classifier loss: 0.204427; batch adversarial loss: 0.696119\n",
      "epoch 11; iter: 0; batch classifier loss: 0.174669; batch adversarial loss: 0.710648\n",
      "epoch 12; iter: 0; batch classifier loss: 0.183170; batch adversarial loss: 0.712005\n",
      "epoch 13; iter: 0; batch classifier loss: 0.170640; batch adversarial loss: 0.724277\n",
      "epoch 14; iter: 0; batch classifier loss: 0.201422; batch adversarial loss: 0.701699\n",
      "epoch 15; iter: 0; batch classifier loss: 0.143971; batch adversarial loss: 0.715868\n",
      "epoch 16; iter: 0; batch classifier loss: 0.165018; batch adversarial loss: 0.709660\n",
      "epoch 17; iter: 0; batch classifier loss: 0.165825; batch adversarial loss: 0.696486\n",
      "epoch 18; iter: 0; batch classifier loss: 0.166855; batch adversarial loss: 0.724113\n",
      "epoch 19; iter: 0; batch classifier loss: 0.135082; batch adversarial loss: 0.711652\n",
      "epoch 20; iter: 0; batch classifier loss: 0.142485; batch adversarial loss: 0.711641\n",
      "epoch 21; iter: 0; batch classifier loss: 0.159982; batch adversarial loss: 0.696004\n",
      "epoch 22; iter: 0; batch classifier loss: 0.098817; batch adversarial loss: 0.707949\n",
      "epoch 23; iter: 0; batch classifier loss: 0.161826; batch adversarial loss: 0.707037\n",
      "epoch 24; iter: 0; batch classifier loss: 0.111128; batch adversarial loss: 0.710401\n",
      "epoch 25; iter: 0; batch classifier loss: 0.132957; batch adversarial loss: 0.711499\n",
      "epoch 26; iter: 0; batch classifier loss: 0.127186; batch adversarial loss: 0.694230\n",
      "epoch 27; iter: 0; batch classifier loss: 0.132039; batch adversarial loss: 0.697099\n",
      "epoch 28; iter: 0; batch classifier loss: 0.105254; batch adversarial loss: 0.700287\n",
      "epoch 29; iter: 0; batch classifier loss: 0.134467; batch adversarial loss: 0.698672\n",
      "epoch 30; iter: 0; batch classifier loss: 0.091783; batch adversarial loss: 0.695412\n",
      "epoch 31; iter: 0; batch classifier loss: 0.117996; batch adversarial loss: 0.701772\n",
      "epoch 32; iter: 0; batch classifier loss: 0.134515; batch adversarial loss: 0.699385\n",
      "epoch 33; iter: 0; batch classifier loss: 0.086713; batch adversarial loss: 0.686754\n",
      "epoch 34; iter: 0; batch classifier loss: 0.074588; batch adversarial loss: 0.697965\n",
      "epoch 35; iter: 0; batch classifier loss: 0.116354; batch adversarial loss: 0.695323\n",
      "epoch 36; iter: 0; batch classifier loss: 0.072103; batch adversarial loss: 0.695890\n",
      "epoch 37; iter: 0; batch classifier loss: 0.078942; batch adversarial loss: 0.690191\n",
      "epoch 38; iter: 0; batch classifier loss: 0.072994; batch adversarial loss: 0.684076\n",
      "epoch 39; iter: 0; batch classifier loss: 0.086759; batch adversarial loss: 0.689497\n",
      "epoch 40; iter: 0; batch classifier loss: 0.108639; batch adversarial loss: 0.700792\n",
      "epoch 41; iter: 0; batch classifier loss: 0.062358; batch adversarial loss: 0.692365\n",
      "epoch 42; iter: 0; batch classifier loss: 0.090465; batch adversarial loss: 0.694702\n",
      "epoch 43; iter: 0; batch classifier loss: 0.060465; batch adversarial loss: 0.684070\n",
      "epoch 44; iter: 0; batch classifier loss: 0.066467; batch adversarial loss: 0.687608\n",
      "epoch 45; iter: 0; batch classifier loss: 0.049988; batch adversarial loss: 0.684078\n",
      "epoch 46; iter: 0; batch classifier loss: 0.065677; batch adversarial loss: 0.689421\n",
      "epoch 47; iter: 0; batch classifier loss: 0.062541; batch adversarial loss: 0.693919\n",
      "epoch 48; iter: 0; batch classifier loss: 0.065438; batch adversarial loss: 0.684042\n",
      "epoch 49; iter: 0; batch classifier loss: 0.056840; batch adversarial loss: 0.683429\n",
      "epoch 50; iter: 0; batch classifier loss: 0.049305; batch adversarial loss: 0.687715\n",
      "epoch 51; iter: 0; batch classifier loss: 0.053564; batch adversarial loss: 0.690186\n",
      "epoch 52; iter: 0; batch classifier loss: 0.051337; batch adversarial loss: 0.682437\n",
      "epoch 53; iter: 0; batch classifier loss: 0.042890; batch adversarial loss: 0.683015\n",
      "epoch 54; iter: 0; batch classifier loss: 0.056317; batch adversarial loss: 0.678342\n",
      "epoch 55; iter: 0; batch classifier loss: 0.042580; batch adversarial loss: 0.684823\n",
      "epoch 56; iter: 0; batch classifier loss: 0.042973; batch adversarial loss: 0.676553\n",
      "epoch 57; iter: 0; batch classifier loss: 0.060349; batch adversarial loss: 0.683109\n",
      "epoch 58; iter: 0; batch classifier loss: 0.032494; batch adversarial loss: 0.688716\n",
      "epoch 59; iter: 0; batch classifier loss: 0.049920; batch adversarial loss: 0.681061\n",
      "epoch 60; iter: 0; batch classifier loss: 0.043597; batch adversarial loss: 0.687495\n",
      "epoch 61; iter: 0; batch classifier loss: 0.042999; batch adversarial loss: 0.677461\n",
      "epoch 62; iter: 0; batch classifier loss: 0.066638; batch adversarial loss: 0.691961\n",
      "epoch 63; iter: 0; batch classifier loss: 0.041866; batch adversarial loss: 0.669585\n",
      "epoch 64; iter: 0; batch classifier loss: 0.044943; batch adversarial loss: 0.685198\n",
      "epoch 65; iter: 0; batch classifier loss: 0.027712; batch adversarial loss: 0.680683\n",
      "epoch 66; iter: 0; batch classifier loss: 0.060570; batch adversarial loss: 0.681065\n",
      "epoch 67; iter: 0; batch classifier loss: 0.048149; batch adversarial loss: 0.670684\n",
      "epoch 68; iter: 0; batch classifier loss: 0.031280; batch adversarial loss: 0.681784\n",
      "epoch 69; iter: 0; batch classifier loss: 0.029581; batch adversarial loss: 0.670581\n",
      "epoch 70; iter: 0; batch classifier loss: 0.026378; batch adversarial loss: 0.680679\n",
      "epoch 71; iter: 0; batch classifier loss: 0.026691; batch adversarial loss: 0.665363\n",
      "epoch 72; iter: 0; batch classifier loss: 0.026013; batch adversarial loss: 0.677343\n",
      "epoch 73; iter: 0; batch classifier loss: 0.026047; batch adversarial loss: 0.672271\n",
      "epoch 74; iter: 0; batch classifier loss: 0.036017; batch adversarial loss: 0.675551\n",
      "epoch 75; iter: 0; batch classifier loss: 0.020966; batch adversarial loss: 0.678184\n",
      "epoch 76; iter: 0; batch classifier loss: 0.025757; batch adversarial loss: 0.674199\n",
      "epoch 77; iter: 0; batch classifier loss: 0.020551; batch adversarial loss: 0.673347\n",
      "epoch 78; iter: 0; batch classifier loss: 0.023563; batch adversarial loss: 0.672037\n",
      "epoch 79; iter: 0; batch classifier loss: 0.027549; batch adversarial loss: 0.677082\n",
      "epoch 80; iter: 0; batch classifier loss: 0.028995; batch adversarial loss: 0.675770\n",
      "epoch 81; iter: 0; batch classifier loss: 0.022688; batch adversarial loss: 0.673842\n",
      "epoch 82; iter: 0; batch classifier loss: 0.020923; batch adversarial loss: 0.652595\n",
      "epoch 83; iter: 0; batch classifier loss: 0.033368; batch adversarial loss: 0.681644\n",
      "epoch 84; iter: 0; batch classifier loss: 0.022253; batch adversarial loss: 0.663252\n",
      "epoch 85; iter: 0; batch classifier loss: 0.036845; batch adversarial loss: 0.674312\n",
      "epoch 86; iter: 0; batch classifier loss: 0.020722; batch adversarial loss: 0.682308\n",
      "epoch 87; iter: 0; batch classifier loss: 0.027931; batch adversarial loss: 0.673387\n",
      "epoch 88; iter: 0; batch classifier loss: 0.012007; batch adversarial loss: 0.672602\n",
      "epoch 89; iter: 0; batch classifier loss: 0.024347; batch adversarial loss: 0.670740\n",
      "epoch 90; iter: 0; batch classifier loss: 0.014266; batch adversarial loss: 0.672190\n",
      "epoch 91; iter: 0; batch classifier loss: 0.011547; batch adversarial loss: 0.671515\n",
      "epoch 92; iter: 0; batch classifier loss: 0.027048; batch adversarial loss: 0.665560\n",
      "epoch 93; iter: 0; batch classifier loss: 0.022047; batch adversarial loss: 0.679519\n",
      "epoch 94; iter: 0; batch classifier loss: 0.022704; batch adversarial loss: 0.679429\n",
      "epoch 95; iter: 0; batch classifier loss: 0.016131; batch adversarial loss: 0.676650\n",
      "epoch 96; iter: 0; batch classifier loss: 0.024111; batch adversarial loss: 0.693574\n",
      "epoch 97; iter: 0; batch classifier loss: 0.016634; batch adversarial loss: 0.654084\n",
      "epoch 98; iter: 0; batch classifier loss: 0.012790; batch adversarial loss: 0.655233\n",
      "epoch 99; iter: 0; batch classifier loss: 0.021932; batch adversarial loss: 0.660726\n",
      "epoch 100; iter: 0; batch classifier loss: 0.034382; batch adversarial loss: 0.665373\n",
      "epoch 101; iter: 0; batch classifier loss: 0.014443; batch adversarial loss: 0.666665\n",
      "epoch 102; iter: 0; batch classifier loss: 0.018585; batch adversarial loss: 0.655250\n",
      "epoch 103; iter: 0; batch classifier loss: 0.022736; batch adversarial loss: 0.678343\n",
      "epoch 104; iter: 0; batch classifier loss: 0.013550; batch adversarial loss: 0.688602\n",
      "epoch 105; iter: 0; batch classifier loss: 0.021673; batch adversarial loss: 0.685239\n",
      "epoch 106; iter: 0; batch classifier loss: 0.017274; batch adversarial loss: 0.663142\n",
      "epoch 107; iter: 0; batch classifier loss: 0.037253; batch adversarial loss: 0.694989\n",
      "epoch 108; iter: 0; batch classifier loss: 0.012227; batch adversarial loss: 0.662599\n",
      "epoch 109; iter: 0; batch classifier loss: 0.020354; batch adversarial loss: 0.676849\n",
      "epoch 110; iter: 0; batch classifier loss: 0.020450; batch adversarial loss: 0.665908\n",
      "epoch 111; iter: 0; batch classifier loss: 0.010843; batch adversarial loss: 0.682880\n",
      "epoch 112; iter: 0; batch classifier loss: 0.023798; batch adversarial loss: 0.677672\n",
      "epoch 113; iter: 0; batch classifier loss: 0.011525; batch adversarial loss: 0.670694\n",
      "epoch 114; iter: 0; batch classifier loss: 0.014263; batch adversarial loss: 0.666108\n",
      "epoch 115; iter: 0; batch classifier loss: 0.018350; batch adversarial loss: 0.650677\n",
      "epoch 116; iter: 0; batch classifier loss: 0.022164; batch adversarial loss: 0.652278\n",
      "epoch 117; iter: 0; batch classifier loss: 0.011438; batch adversarial loss: 0.651971\n",
      "epoch 118; iter: 0; batch classifier loss: 0.027995; batch adversarial loss: 0.692528\n",
      "epoch 119; iter: 0; batch classifier loss: 0.022320; batch adversarial loss: 0.673487\n",
      "epoch 120; iter: 0; batch classifier loss: 0.022989; batch adversarial loss: 0.677434\n",
      "epoch 121; iter: 0; batch classifier loss: 0.024477; batch adversarial loss: 0.678094\n",
      "epoch 122; iter: 0; batch classifier loss: 0.016206; batch adversarial loss: 0.663048\n",
      "epoch 123; iter: 0; batch classifier loss: 0.022944; batch adversarial loss: 0.668343\n",
      "epoch 124; iter: 0; batch classifier loss: 0.030837; batch adversarial loss: 0.654037\n",
      "epoch 125; iter: 0; batch classifier loss: 0.021287; batch adversarial loss: 0.656879\n",
      "epoch 126; iter: 0; batch classifier loss: 0.033481; batch adversarial loss: 0.667557\n",
      "epoch 127; iter: 0; batch classifier loss: 0.004190; batch adversarial loss: 0.675480\n",
      "epoch 128; iter: 0; batch classifier loss: 0.021335; batch adversarial loss: 0.683635\n",
      "epoch 129; iter: 0; batch classifier loss: 0.009862; batch adversarial loss: 0.671747\n",
      "epoch 130; iter: 0; batch classifier loss: 0.017030; batch adversarial loss: 0.672913\n",
      "epoch 131; iter: 0; batch classifier loss: 0.019098; batch adversarial loss: 0.676671\n",
      "epoch 132; iter: 0; batch classifier loss: 0.024142; batch adversarial loss: 0.662506\n",
      "epoch 133; iter: 0; batch classifier loss: 0.026171; batch adversarial loss: 0.678832\n",
      "epoch 134; iter: 0; batch classifier loss: 0.014611; batch adversarial loss: 0.668923\n",
      "epoch 135; iter: 0; batch classifier loss: 0.023476; batch adversarial loss: 0.647890\n",
      "epoch 136; iter: 0; batch classifier loss: 0.027179; batch adversarial loss: 0.679045\n",
      "epoch 137; iter: 0; batch classifier loss: 0.015742; batch adversarial loss: 0.676589\n",
      "epoch 138; iter: 0; batch classifier loss: 0.024685; batch adversarial loss: 0.697395\n",
      "epoch 139; iter: 0; batch classifier loss: 0.009844; batch adversarial loss: 0.654429\n",
      "epoch 140; iter: 0; batch classifier loss: 0.012416; batch adversarial loss: 0.696260\n",
      "epoch 141; iter: 0; batch classifier loss: 0.024642; batch adversarial loss: 0.689209\n",
      "epoch 142; iter: 0; batch classifier loss: 0.016979; batch adversarial loss: 0.673651\n",
      "epoch 143; iter: 0; batch classifier loss: 0.018320; batch adversarial loss: 0.679650\n",
      "epoch 144; iter: 0; batch classifier loss: 0.028534; batch adversarial loss: 0.681638\n",
      "epoch 145; iter: 0; batch classifier loss: 0.003093; batch adversarial loss: 0.681262\n",
      "epoch 146; iter: 0; batch classifier loss: 0.013958; batch adversarial loss: 0.667907\n",
      "epoch 147; iter: 0; batch classifier loss: 0.010162; batch adversarial loss: 0.671962\n",
      "epoch 148; iter: 0; batch classifier loss: 0.018980; batch adversarial loss: 0.666620\n",
      "epoch 149; iter: 0; batch classifier loss: 0.021944; batch adversarial loss: 0.674711\n",
      "epoch 150; iter: 0; batch classifier loss: 0.014732; batch adversarial loss: 0.667382\n",
      "epoch 151; iter: 0; batch classifier loss: 0.027735; batch adversarial loss: 0.687314\n",
      "epoch 152; iter: 0; batch classifier loss: 0.004498; batch adversarial loss: 0.660248\n",
      "epoch 153; iter: 0; batch classifier loss: 0.008969; batch adversarial loss: 0.689968\n",
      "epoch 154; iter: 0; batch classifier loss: 0.027055; batch adversarial loss: 0.679058\n",
      "epoch 155; iter: 0; batch classifier loss: 0.009928; batch adversarial loss: 0.674429\n",
      "epoch 156; iter: 0; batch classifier loss: 0.014105; batch adversarial loss: 0.674400\n",
      "epoch 157; iter: 0; batch classifier loss: 0.009541; batch adversarial loss: 0.672834\n",
      "epoch 158; iter: 0; batch classifier loss: 0.025784; batch adversarial loss: 0.693174\n",
      "epoch 159; iter: 0; batch classifier loss: 0.014447; batch adversarial loss: 0.678155\n",
      "epoch 160; iter: 0; batch classifier loss: 0.003876; batch adversarial loss: 0.663056\n",
      "epoch 161; iter: 0; batch classifier loss: 0.013304; batch adversarial loss: 0.665040\n",
      "epoch 162; iter: 0; batch classifier loss: 0.018241; batch adversarial loss: 0.694105\n",
      "epoch 163; iter: 0; batch classifier loss: 0.020560; batch adversarial loss: 0.670073\n",
      "epoch 164; iter: 0; batch classifier loss: 0.020385; batch adversarial loss: 0.664396\n",
      "epoch 165; iter: 0; batch classifier loss: 0.004062; batch adversarial loss: 0.664028\n",
      "epoch 166; iter: 0; batch classifier loss: 0.005009; batch adversarial loss: 0.667643\n",
      "epoch 167; iter: 0; batch classifier loss: 0.021608; batch adversarial loss: 0.657322\n",
      "epoch 168; iter: 0; batch classifier loss: 0.003560; batch adversarial loss: 0.667332\n",
      "epoch 169; iter: 0; batch classifier loss: 0.012538; batch adversarial loss: 0.660489\n",
      "epoch 170; iter: 0; batch classifier loss: 0.020012; batch adversarial loss: 0.646969\n",
      "epoch 171; iter: 0; batch classifier loss: 0.011098; batch adversarial loss: 0.691214\n",
      "epoch 172; iter: 0; batch classifier loss: 0.019248; batch adversarial loss: 0.669568\n",
      "epoch 173; iter: 0; batch classifier loss: 0.013535; batch adversarial loss: 0.686056\n",
      "epoch 174; iter: 0; batch classifier loss: 0.023641; batch adversarial loss: 0.703898\n",
      "epoch 175; iter: 0; batch classifier loss: 0.026926; batch adversarial loss: 0.681383\n",
      "epoch 176; iter: 0; batch classifier loss: 0.015116; batch adversarial loss: 0.665367\n",
      "epoch 177; iter: 0; batch classifier loss: 0.013467; batch adversarial loss: 0.670173\n",
      "epoch 178; iter: 0; batch classifier loss: 0.017307; batch adversarial loss: 0.651170\n",
      "epoch 179; iter: 0; batch classifier loss: 0.011147; batch adversarial loss: 0.644796\n",
      "epoch 180; iter: 0; batch classifier loss: 0.015801; batch adversarial loss: 0.670382\n",
      "epoch 181; iter: 0; batch classifier loss: 0.015004; batch adversarial loss: 0.679697\n",
      "epoch 182; iter: 0; batch classifier loss: 0.029506; batch adversarial loss: 0.680194\n",
      "epoch 183; iter: 0; batch classifier loss: 0.018091; batch adversarial loss: 0.658093\n",
      "epoch 184; iter: 0; batch classifier loss: 0.027207; batch adversarial loss: 0.684771\n",
      "epoch 185; iter: 0; batch classifier loss: 0.018578; batch adversarial loss: 0.683822\n",
      "epoch 186; iter: 0; batch classifier loss: 0.004364; batch adversarial loss: 0.649141\n",
      "epoch 187; iter: 0; batch classifier loss: 0.007634; batch adversarial loss: 0.661846\n",
      "epoch 188; iter: 0; batch classifier loss: 0.015653; batch adversarial loss: 0.692330\n",
      "epoch 189; iter: 0; batch classifier loss: 0.008533; batch adversarial loss: 0.672818\n",
      "epoch 190; iter: 0; batch classifier loss: 0.008208; batch adversarial loss: 0.647965\n",
      "epoch 191; iter: 0; batch classifier loss: 0.012371; batch adversarial loss: 0.639558\n",
      "epoch 192; iter: 0; batch classifier loss: 0.011200; batch adversarial loss: 0.687826\n",
      "epoch 193; iter: 0; batch classifier loss: 0.020683; batch adversarial loss: 0.709334\n",
      "epoch 194; iter: 0; batch classifier loss: 0.010474; batch adversarial loss: 0.677693\n",
      "epoch 195; iter: 0; batch classifier loss: 0.030096; batch adversarial loss: 0.686256\n",
      "epoch 196; iter: 0; batch classifier loss: 0.018538; batch adversarial loss: 0.683592\n",
      "epoch 197; iter: 0; batch classifier loss: 0.011765; batch adversarial loss: 0.682785\n",
      "epoch 198; iter: 0; batch classifier loss: 0.013622; batch adversarial loss: 0.666314\n",
      "epoch 199; iter: 0; batch classifier loss: 0.009700; batch adversarial loss: 0.669487\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:49.488787: W tensorflow/c/c_api.cc:304] Operation '{name:'4c969780-adee-11ee-9362-a9d23602d4a3/4c969780-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:8854 op device:{requested: '', assigned: ''} def:{{{node 4c969780-adee-11ee-9362-a9d23602d4a3/4c969780-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c969780-adee-11ee-9362-a9d23602d4a3/4c969780-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c969780-adee-11ee-9362-a9d23602d4a3/4c969780-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.750260; batch adversarial loss: 0.755826\n",
      "epoch 1; iter: 0; batch classifier loss: 0.669560; batch adversarial loss: 0.809290\n",
      "epoch 2; iter: 0; batch classifier loss: 0.572495; batch adversarial loss: 0.790162\n",
      "epoch 3; iter: 0; batch classifier loss: 0.454187; batch adversarial loss: 0.869685\n",
      "epoch 4; iter: 0; batch classifier loss: 0.426694; batch adversarial loss: 0.892906\n",
      "epoch 5; iter: 0; batch classifier loss: 0.394860; batch adversarial loss: 0.909597\n",
      "epoch 6; iter: 0; batch classifier loss: 0.377155; batch adversarial loss: 0.862572\n",
      "epoch 7; iter: 0; batch classifier loss: 0.306768; batch adversarial loss: 0.859667\n",
      "epoch 8; iter: 0; batch classifier loss: 0.335362; batch adversarial loss: 0.895325\n",
      "epoch 9; iter: 0; batch classifier loss: 0.288907; batch adversarial loss: 0.854396\n",
      "epoch 10; iter: 0; batch classifier loss: 0.272336; batch adversarial loss: 0.977394\n",
      "epoch 11; iter: 0; batch classifier loss: 0.245501; batch adversarial loss: 0.896467\n",
      "epoch 12; iter: 0; batch classifier loss: 0.230767; batch adversarial loss: 0.931708\n",
      "epoch 13; iter: 0; batch classifier loss: 0.267377; batch adversarial loss: 0.909556\n",
      "epoch 14; iter: 0; batch classifier loss: 0.249606; batch adversarial loss: 0.860058\n",
      "epoch 15; iter: 0; batch classifier loss: 0.191249; batch adversarial loss: 0.887166\n",
      "epoch 16; iter: 0; batch classifier loss: 0.178535; batch adversarial loss: 0.908947\n",
      "epoch 17; iter: 0; batch classifier loss: 0.142159; batch adversarial loss: 0.913476\n",
      "epoch 18; iter: 0; batch classifier loss: 0.197675; batch adversarial loss: 0.973088\n",
      "epoch 19; iter: 0; batch classifier loss: 0.113098; batch adversarial loss: 0.958892\n",
      "epoch 20; iter: 0; batch classifier loss: 0.134868; batch adversarial loss: 0.930283\n",
      "epoch 21; iter: 0; batch classifier loss: 0.160091; batch adversarial loss: 0.960102\n",
      "epoch 22; iter: 0; batch classifier loss: 0.100490; batch adversarial loss: 0.856628\n",
      "epoch 23; iter: 0; batch classifier loss: 0.158133; batch adversarial loss: 0.882577\n",
      "epoch 24; iter: 0; batch classifier loss: 0.110940; batch adversarial loss: 0.804672\n",
      "epoch 25; iter: 0; batch classifier loss: 0.123871; batch adversarial loss: 0.848835\n",
      "epoch 26; iter: 0; batch classifier loss: 0.137840; batch adversarial loss: 0.909816\n",
      "epoch 27; iter: 0; batch classifier loss: 0.119080; batch adversarial loss: 0.880833\n",
      "epoch 28; iter: 0; batch classifier loss: 0.109707; batch adversarial loss: 0.831409\n",
      "epoch 29; iter: 0; batch classifier loss: 0.125389; batch adversarial loss: 0.895359\n",
      "epoch 30; iter: 0; batch classifier loss: 0.093925; batch adversarial loss: 0.899377\n",
      "epoch 31; iter: 0; batch classifier loss: 0.081966; batch adversarial loss: 0.913848\n",
      "epoch 32; iter: 0; batch classifier loss: 0.083204; batch adversarial loss: 0.809399\n",
      "epoch 33; iter: 0; batch classifier loss: 0.100916; batch adversarial loss: 0.843028\n",
      "epoch 34; iter: 0; batch classifier loss: 0.105109; batch adversarial loss: 0.889717\n",
      "epoch 35; iter: 0; batch classifier loss: 0.108069; batch adversarial loss: 0.901916\n",
      "epoch 36; iter: 0; batch classifier loss: 0.094271; batch adversarial loss: 0.861197\n",
      "epoch 37; iter: 0; batch classifier loss: 0.082882; batch adversarial loss: 0.878160\n",
      "epoch 38; iter: 0; batch classifier loss: 0.074333; batch adversarial loss: 0.872572\n",
      "epoch 39; iter: 0; batch classifier loss: 0.102337; batch adversarial loss: 0.834301\n",
      "epoch 40; iter: 0; batch classifier loss: 0.046040; batch adversarial loss: 0.879759\n",
      "epoch 41; iter: 0; batch classifier loss: 0.085878; batch adversarial loss: 0.858387\n",
      "epoch 42; iter: 0; batch classifier loss: 0.065017; batch adversarial loss: 0.862794\n",
      "epoch 43; iter: 0; batch classifier loss: 0.055568; batch adversarial loss: 0.802323\n",
      "epoch 44; iter: 0; batch classifier loss: 0.047290; batch adversarial loss: 0.889829\n",
      "epoch 45; iter: 0; batch classifier loss: 0.074735; batch adversarial loss: 0.820528\n",
      "epoch 46; iter: 0; batch classifier loss: 0.059854; batch adversarial loss: 0.850992\n",
      "epoch 47; iter: 0; batch classifier loss: 0.088861; batch adversarial loss: 0.837034\n",
      "epoch 48; iter: 0; batch classifier loss: 0.059466; batch adversarial loss: 0.816355\n",
      "epoch 49; iter: 0; batch classifier loss: 0.057522; batch adversarial loss: 0.839394\n",
      "epoch 50; iter: 0; batch classifier loss: 0.063565; batch adversarial loss: 0.850871\n",
      "epoch 51; iter: 0; batch classifier loss: 0.062689; batch adversarial loss: 0.832251\n",
      "epoch 52; iter: 0; batch classifier loss: 0.064858; batch adversarial loss: 0.849909\n",
      "epoch 53; iter: 0; batch classifier loss: 0.048175; batch adversarial loss: 0.780219\n",
      "epoch 54; iter: 0; batch classifier loss: 0.043138; batch adversarial loss: 0.825852\n",
      "epoch 55; iter: 0; batch classifier loss: 0.039252; batch adversarial loss: 0.809576\n",
      "epoch 56; iter: 0; batch classifier loss: 0.045123; batch adversarial loss: 0.842027\n",
      "epoch 57; iter: 0; batch classifier loss: 0.029402; batch adversarial loss: 0.847693\n",
      "epoch 58; iter: 0; batch classifier loss: 0.053242; batch adversarial loss: 0.868323\n",
      "epoch 59; iter: 0; batch classifier loss: 0.046284; batch adversarial loss: 0.755247\n",
      "epoch 60; iter: 0; batch classifier loss: 0.048278; batch adversarial loss: 0.802142\n",
      "epoch 61; iter: 0; batch classifier loss: 0.038857; batch adversarial loss: 0.758747\n",
      "epoch 62; iter: 0; batch classifier loss: 0.030872; batch adversarial loss: 0.800964\n",
      "epoch 63; iter: 0; batch classifier loss: 0.028711; batch adversarial loss: 0.776492\n",
      "epoch 64; iter: 0; batch classifier loss: 0.026251; batch adversarial loss: 0.803019\n",
      "epoch 65; iter: 0; batch classifier loss: 0.034861; batch adversarial loss: 0.825335\n",
      "epoch 66; iter: 0; batch classifier loss: 0.030273; batch adversarial loss: 0.786698\n",
      "epoch 67; iter: 0; batch classifier loss: 0.021752; batch adversarial loss: 0.817995\n",
      "epoch 68; iter: 0; batch classifier loss: 0.029301; batch adversarial loss: 0.806007\n",
      "epoch 69; iter: 0; batch classifier loss: 0.026708; batch adversarial loss: 0.806397\n",
      "epoch 70; iter: 0; batch classifier loss: 0.029047; batch adversarial loss: 0.780238\n",
      "epoch 71; iter: 0; batch classifier loss: 0.037173; batch adversarial loss: 0.812769\n",
      "epoch 72; iter: 0; batch classifier loss: 0.037513; batch adversarial loss: 0.790135\n",
      "epoch 73; iter: 0; batch classifier loss: 0.038233; batch adversarial loss: 0.778057\n",
      "epoch 74; iter: 0; batch classifier loss: 0.026615; batch adversarial loss: 0.801648\n",
      "epoch 75; iter: 0; batch classifier loss: 0.027861; batch adversarial loss: 0.797048\n",
      "epoch 76; iter: 0; batch classifier loss: 0.023808; batch adversarial loss: 0.785453\n",
      "epoch 77; iter: 0; batch classifier loss: 0.021594; batch adversarial loss: 0.799058\n",
      "epoch 78; iter: 0; batch classifier loss: 0.031645; batch adversarial loss: 0.799517\n",
      "epoch 79; iter: 0; batch classifier loss: 0.032626; batch adversarial loss: 0.786018\n",
      "epoch 80; iter: 0; batch classifier loss: 0.022121; batch adversarial loss: 0.768220\n",
      "epoch 81; iter: 0; batch classifier loss: 0.021065; batch adversarial loss: 0.750577\n",
      "epoch 82; iter: 0; batch classifier loss: 0.020018; batch adversarial loss: 0.759410\n",
      "epoch 83; iter: 0; batch classifier loss: 0.018814; batch adversarial loss: 0.778957\n",
      "epoch 84; iter: 0; batch classifier loss: 0.016229; batch adversarial loss: 0.775773\n",
      "epoch 85; iter: 0; batch classifier loss: 0.009220; batch adversarial loss: 0.799248\n",
      "epoch 86; iter: 0; batch classifier loss: 0.032851; batch adversarial loss: 0.769179\n",
      "epoch 87; iter: 0; batch classifier loss: 0.017326; batch adversarial loss: 0.772699\n",
      "epoch 88; iter: 0; batch classifier loss: 0.024126; batch adversarial loss: 0.757374\n",
      "epoch 89; iter: 0; batch classifier loss: 0.019048; batch adversarial loss: 0.779955\n",
      "epoch 90; iter: 0; batch classifier loss: 0.016670; batch adversarial loss: 0.790859\n",
      "epoch 91; iter: 0; batch classifier loss: 0.030085; batch adversarial loss: 0.788433\n",
      "epoch 92; iter: 0; batch classifier loss: 0.023497; batch adversarial loss: 0.762038\n",
      "epoch 93; iter: 0; batch classifier loss: 0.016173; batch adversarial loss: 0.761713\n",
      "epoch 94; iter: 0; batch classifier loss: 0.018966; batch adversarial loss: 0.759908\n",
      "epoch 95; iter: 0; batch classifier loss: 0.021880; batch adversarial loss: 0.741903\n",
      "epoch 96; iter: 0; batch classifier loss: 0.016711; batch adversarial loss: 0.734504\n",
      "epoch 97; iter: 0; batch classifier loss: 0.014034; batch adversarial loss: 0.754378\n",
      "epoch 98; iter: 0; batch classifier loss: 0.019165; batch adversarial loss: 0.765281\n",
      "epoch 99; iter: 0; batch classifier loss: 0.025089; batch adversarial loss: 0.754165\n",
      "epoch 100; iter: 0; batch classifier loss: 0.014873; batch adversarial loss: 0.718636\n",
      "epoch 101; iter: 0; batch classifier loss: 0.012343; batch adversarial loss: 0.764667\n",
      "epoch 102; iter: 0; batch classifier loss: 0.018374; batch adversarial loss: 0.736914\n",
      "epoch 103; iter: 0; batch classifier loss: 0.009272; batch adversarial loss: 0.734148\n",
      "epoch 104; iter: 0; batch classifier loss: 0.008612; batch adversarial loss: 0.741827\n",
      "epoch 105; iter: 0; batch classifier loss: 0.010899; batch adversarial loss: 0.721366\n",
      "epoch 106; iter: 0; batch classifier loss: 0.014853; batch adversarial loss: 0.724533\n",
      "epoch 107; iter: 0; batch classifier loss: 0.022795; batch adversarial loss: 0.761257\n",
      "epoch 108; iter: 0; batch classifier loss: 0.011866; batch adversarial loss: 0.747914\n",
      "epoch 109; iter: 0; batch classifier loss: 0.017362; batch adversarial loss: 0.757949\n",
      "epoch 110; iter: 0; batch classifier loss: 0.024546; batch adversarial loss: 0.766967\n",
      "epoch 111; iter: 0; batch classifier loss: 0.017659; batch adversarial loss: 0.712680\n",
      "epoch 112; iter: 0; batch classifier loss: 0.018853; batch adversarial loss: 0.750333\n",
      "epoch 113; iter: 0; batch classifier loss: 0.013754; batch adversarial loss: 0.741847\n",
      "epoch 114; iter: 0; batch classifier loss: 0.011464; batch adversarial loss: 0.727472\n",
      "epoch 115; iter: 0; batch classifier loss: 0.015204; batch adversarial loss: 0.731590\n",
      "epoch 116; iter: 0; batch classifier loss: 0.017950; batch adversarial loss: 0.755910\n",
      "epoch 117; iter: 0; batch classifier loss: 0.016628; batch adversarial loss: 0.727689\n",
      "epoch 118; iter: 0; batch classifier loss: 0.028510; batch adversarial loss: 0.720749\n",
      "epoch 119; iter: 0; batch classifier loss: 0.005808; batch adversarial loss: 0.729338\n",
      "epoch 120; iter: 0; batch classifier loss: 0.011764; batch adversarial loss: 0.725441\n",
      "epoch 121; iter: 0; batch classifier loss: 0.009360; batch adversarial loss: 0.729384\n",
      "epoch 122; iter: 0; batch classifier loss: 0.012809; batch adversarial loss: 0.712623\n",
      "epoch 123; iter: 0; batch classifier loss: 0.018036; batch adversarial loss: 0.726789\n",
      "epoch 124; iter: 0; batch classifier loss: 0.011560; batch adversarial loss: 0.719567\n",
      "epoch 125; iter: 0; batch classifier loss: 0.004603; batch adversarial loss: 0.725545\n",
      "epoch 126; iter: 0; batch classifier loss: 0.014647; batch adversarial loss: 0.713702\n",
      "epoch 127; iter: 0; batch classifier loss: 0.013544; batch adversarial loss: 0.700567\n",
      "epoch 128; iter: 0; batch classifier loss: 0.023009; batch adversarial loss: 0.719505\n",
      "epoch 129; iter: 0; batch classifier loss: 0.022140; batch adversarial loss: 0.720779\n",
      "epoch 130; iter: 0; batch classifier loss: 0.020599; batch adversarial loss: 0.744917\n",
      "epoch 131; iter: 0; batch classifier loss: 0.010444; batch adversarial loss: 0.728040\n",
      "epoch 132; iter: 0; batch classifier loss: 0.022487; batch adversarial loss: 0.710315\n",
      "epoch 133; iter: 0; batch classifier loss: 0.016547; batch adversarial loss: 0.699271\n",
      "epoch 134; iter: 0; batch classifier loss: 0.004797; batch adversarial loss: 0.702988\n",
      "epoch 135; iter: 0; batch classifier loss: 0.015905; batch adversarial loss: 0.711767\n",
      "epoch 136; iter: 0; batch classifier loss: 0.013453; batch adversarial loss: 0.720518\n",
      "epoch 137; iter: 0; batch classifier loss: 0.008989; batch adversarial loss: 0.727445\n",
      "epoch 138; iter: 0; batch classifier loss: 0.016618; batch adversarial loss: 0.711382\n",
      "epoch 139; iter: 0; batch classifier loss: 0.013876; batch adversarial loss: 0.710057\n",
      "epoch 140; iter: 0; batch classifier loss: 0.004989; batch adversarial loss: 0.692637\n",
      "epoch 141; iter: 0; batch classifier loss: 0.005787; batch adversarial loss: 0.704588\n",
      "epoch 142; iter: 0; batch classifier loss: 0.016312; batch adversarial loss: 0.715216\n",
      "epoch 143; iter: 0; batch classifier loss: 0.003918; batch adversarial loss: 0.698947\n",
      "epoch 144; iter: 0; batch classifier loss: 0.037578; batch adversarial loss: 0.722271\n",
      "epoch 145; iter: 0; batch classifier loss: 0.021371; batch adversarial loss: 0.719861\n",
      "epoch 146; iter: 0; batch classifier loss: 0.005191; batch adversarial loss: 0.702365\n",
      "epoch 147; iter: 0; batch classifier loss: 0.020623; batch adversarial loss: 0.716661\n",
      "epoch 148; iter: 0; batch classifier loss: 0.014350; batch adversarial loss: 0.709883\n",
      "epoch 149; iter: 0; batch classifier loss: 0.016363; batch adversarial loss: 0.709649\n",
      "epoch 150; iter: 0; batch classifier loss: 0.005441; batch adversarial loss: 0.695309\n",
      "epoch 151; iter: 0; batch classifier loss: 0.021754; batch adversarial loss: 0.710361\n",
      "epoch 152; iter: 0; batch classifier loss: 0.009678; batch adversarial loss: 0.694797\n",
      "epoch 153; iter: 0; batch classifier loss: 0.005888; batch adversarial loss: 0.692942\n",
      "epoch 154; iter: 0; batch classifier loss: 0.003789; batch adversarial loss: 0.709147\n",
      "epoch 155; iter: 0; batch classifier loss: 0.009780; batch adversarial loss: 0.700552\n",
      "epoch 156; iter: 0; batch classifier loss: 0.002918; batch adversarial loss: 0.699641\n",
      "epoch 157; iter: 0; batch classifier loss: 0.014401; batch adversarial loss: 0.703528\n",
      "epoch 158; iter: 0; batch classifier loss: 0.021963; batch adversarial loss: 0.698089\n",
      "epoch 159; iter: 0; batch classifier loss: 0.018719; batch adversarial loss: 0.704060\n",
      "epoch 160; iter: 0; batch classifier loss: 0.009598; batch adversarial loss: 0.712627\n",
      "epoch 161; iter: 0; batch classifier loss: 0.002954; batch adversarial loss: 0.688629\n",
      "epoch 162; iter: 0; batch classifier loss: 0.004023; batch adversarial loss: 0.696626\n",
      "epoch 163; iter: 0; batch classifier loss: 0.002142; batch adversarial loss: 0.694765\n",
      "epoch 164; iter: 0; batch classifier loss: 0.009883; batch adversarial loss: 0.685757\n",
      "epoch 165; iter: 0; batch classifier loss: 0.004587; batch adversarial loss: 0.689629\n",
      "epoch 166; iter: 0; batch classifier loss: 0.016572; batch adversarial loss: 0.707119\n",
      "epoch 167; iter: 0; batch classifier loss: 0.013845; batch adversarial loss: 0.696391\n",
      "epoch 168; iter: 0; batch classifier loss: 0.017803; batch adversarial loss: 0.688552\n",
      "epoch 169; iter: 0; batch classifier loss: 0.028857; batch adversarial loss: 0.693775\n",
      "epoch 170; iter: 0; batch classifier loss: 0.010266; batch adversarial loss: 0.693001\n",
      "epoch 171; iter: 0; batch classifier loss: 0.016219; batch adversarial loss: 0.698089\n",
      "epoch 172; iter: 0; batch classifier loss: 0.006823; batch adversarial loss: 0.692068\n",
      "epoch 173; iter: 0; batch classifier loss: 0.007393; batch adversarial loss: 0.694186\n",
      "epoch 174; iter: 0; batch classifier loss: 0.007749; batch adversarial loss: 0.693987\n",
      "epoch 175; iter: 0; batch classifier loss: 0.003908; batch adversarial loss: 0.681756\n",
      "epoch 176; iter: 0; batch classifier loss: 0.013158; batch adversarial loss: 0.692526\n",
      "epoch 177; iter: 0; batch classifier loss: 0.020347; batch adversarial loss: 0.689582\n",
      "epoch 178; iter: 0; batch classifier loss: 0.004470; batch adversarial loss: 0.697373\n",
      "epoch 179; iter: 0; batch classifier loss: 0.006585; batch adversarial loss: 0.686075\n",
      "epoch 180; iter: 0; batch classifier loss: 0.013121; batch adversarial loss: 0.691056\n",
      "epoch 181; iter: 0; batch classifier loss: 0.008899; batch adversarial loss: 0.703467\n",
      "epoch 182; iter: 0; batch classifier loss: 0.010806; batch adversarial loss: 0.698007\n",
      "epoch 183; iter: 0; batch classifier loss: 0.006306; batch adversarial loss: 0.692181\n",
      "epoch 184; iter: 0; batch classifier loss: 0.001398; batch adversarial loss: 0.689862\n",
      "epoch 185; iter: 0; batch classifier loss: 0.023831; batch adversarial loss: 0.695231\n",
      "epoch 186; iter: 0; batch classifier loss: 0.016131; batch adversarial loss: 0.696391\n",
      "epoch 187; iter: 0; batch classifier loss: 0.008325; batch adversarial loss: 0.680688\n",
      "epoch 188; iter: 0; batch classifier loss: 0.009313; batch adversarial loss: 0.690066\n",
      "epoch 189; iter: 0; batch classifier loss: 0.009988; batch adversarial loss: 0.683538\n",
      "epoch 190; iter: 0; batch classifier loss: 0.017986; batch adversarial loss: 0.685024\n",
      "epoch 191; iter: 0; batch classifier loss: 0.005322; batch adversarial loss: 0.692722\n",
      "epoch 192; iter: 0; batch classifier loss: 0.003269; batch adversarial loss: 0.690952\n",
      "epoch 193; iter: 0; batch classifier loss: 0.016855; batch adversarial loss: 0.687107\n",
      "epoch 194; iter: 0; batch classifier loss: 0.021870; batch adversarial loss: 0.687431\n",
      "epoch 195; iter: 0; batch classifier loss: 0.004508; batch adversarial loss: 0.691670\n",
      "epoch 196; iter: 0; batch classifier loss: 0.023033; batch adversarial loss: 0.676960\n",
      "epoch 197; iter: 0; batch classifier loss: 0.012580; batch adversarial loss: 0.687377\n",
      "epoch 198; iter: 0; batch classifier loss: 0.024579; batch adversarial loss: 0.686688\n",
      "epoch 199; iter: 0; batch classifier loss: 0.017633; batch adversarial loss: 0.695675\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:51.649313: W tensorflow/c/c_api.cc:304] Operation '{name:'4c969816-adee-11ee-9362-a9d23602d4a3/4c969816-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:9661 op device:{requested: '', assigned: ''} def:{{{node 4c969816-adee-11ee-9362-a9d23602d4a3/4c969816-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c969816-adee-11ee-9362-a9d23602d4a3/4c969816-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c969816-adee-11ee-9362-a9d23602d4a3/4c969816-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.640795; batch adversarial loss: 0.941286\n",
      "epoch 1; iter: 0; batch classifier loss: 0.524475; batch adversarial loss: 0.902267\n",
      "epoch 2; iter: 0; batch classifier loss: 0.427866; batch adversarial loss: 0.965124\n",
      "epoch 3; iter: 0; batch classifier loss: 0.405965; batch adversarial loss: 1.071145\n",
      "epoch 4; iter: 0; batch classifier loss: 0.329336; batch adversarial loss: 1.175127\n",
      "epoch 5; iter: 0; batch classifier loss: 0.289305; batch adversarial loss: 1.150536\n",
      "epoch 6; iter: 0; batch classifier loss: 0.285748; batch adversarial loss: 1.043954\n",
      "epoch 7; iter: 0; batch classifier loss: 0.220672; batch adversarial loss: 1.141725\n",
      "epoch 8; iter: 0; batch classifier loss: 0.202247; batch adversarial loss: 1.193404\n",
      "epoch 9; iter: 0; batch classifier loss: 0.204086; batch adversarial loss: 1.121432\n",
      "epoch 10; iter: 0; batch classifier loss: 0.207617; batch adversarial loss: 1.167632\n",
      "epoch 11; iter: 0; batch classifier loss: 0.157643; batch adversarial loss: 1.161872\n",
      "epoch 12; iter: 0; batch classifier loss: 0.140011; batch adversarial loss: 1.124870\n",
      "epoch 13; iter: 0; batch classifier loss: 0.169772; batch adversarial loss: 1.083262\n",
      "epoch 14; iter: 0; batch classifier loss: 0.147669; batch adversarial loss: 1.025275\n",
      "epoch 15; iter: 0; batch classifier loss: 0.121112; batch adversarial loss: 1.194706\n",
      "epoch 16; iter: 0; batch classifier loss: 0.170463; batch adversarial loss: 1.085165\n",
      "epoch 17; iter: 0; batch classifier loss: 0.146405; batch adversarial loss: 1.107732\n",
      "epoch 18; iter: 0; batch classifier loss: 0.139113; batch adversarial loss: 1.173449\n",
      "epoch 19; iter: 0; batch classifier loss: 0.173214; batch adversarial loss: 1.095775\n",
      "epoch 20; iter: 0; batch classifier loss: 0.104735; batch adversarial loss: 1.102705\n",
      "epoch 21; iter: 0; batch classifier loss: 0.095736; batch adversarial loss: 1.087496\n",
      "epoch 22; iter: 0; batch classifier loss: 0.110339; batch adversarial loss: 1.068211\n",
      "epoch 23; iter: 0; batch classifier loss: 0.104807; batch adversarial loss: 1.144919\n",
      "epoch 24; iter: 0; batch classifier loss: 0.100487; batch adversarial loss: 1.089119\n",
      "epoch 25; iter: 0; batch classifier loss: 0.062009; batch adversarial loss: 1.016175\n",
      "epoch 26; iter: 0; batch classifier loss: 0.087091; batch adversarial loss: 1.113576\n",
      "epoch 27; iter: 0; batch classifier loss: 0.101053; batch adversarial loss: 1.179202\n",
      "epoch 28; iter: 0; batch classifier loss: 0.069949; batch adversarial loss: 1.061947\n",
      "epoch 29; iter: 0; batch classifier loss: 0.085187; batch adversarial loss: 1.236778\n",
      "epoch 30; iter: 0; batch classifier loss: 0.077905; batch adversarial loss: 1.198695\n",
      "epoch 31; iter: 0; batch classifier loss: 0.095187; batch adversarial loss: 1.118287\n",
      "epoch 32; iter: 0; batch classifier loss: 0.074831; batch adversarial loss: 1.131161\n",
      "epoch 33; iter: 0; batch classifier loss: 0.102712; batch adversarial loss: 1.124163\n",
      "epoch 34; iter: 0; batch classifier loss: 0.094306; batch adversarial loss: 1.037379\n",
      "epoch 35; iter: 0; batch classifier loss: 0.093572; batch adversarial loss: 1.138428\n",
      "epoch 36; iter: 0; batch classifier loss: 0.096006; batch adversarial loss: 1.131233\n",
      "epoch 37; iter: 0; batch classifier loss: 0.058865; batch adversarial loss: 1.173856\n",
      "epoch 38; iter: 0; batch classifier loss: 0.087473; batch adversarial loss: 1.055926\n",
      "epoch 39; iter: 0; batch classifier loss: 0.068226; batch adversarial loss: 1.126735\n",
      "epoch 40; iter: 0; batch classifier loss: 0.067527; batch adversarial loss: 1.055507\n",
      "epoch 41; iter: 0; batch classifier loss: 0.060033; batch adversarial loss: 1.039676\n",
      "epoch 42; iter: 0; batch classifier loss: 0.076461; batch adversarial loss: 1.123812\n",
      "epoch 43; iter: 0; batch classifier loss: 0.050125; batch adversarial loss: 1.125672\n",
      "epoch 44; iter: 0; batch classifier loss: 0.067234; batch adversarial loss: 1.104922\n",
      "epoch 45; iter: 0; batch classifier loss: 0.048406; batch adversarial loss: 1.043555\n",
      "epoch 46; iter: 0; batch classifier loss: 0.077204; batch adversarial loss: 1.081409\n",
      "epoch 47; iter: 0; batch classifier loss: 0.068344; batch adversarial loss: 1.026102\n",
      "epoch 48; iter: 0; batch classifier loss: 0.069310; batch adversarial loss: 1.112659\n",
      "epoch 49; iter: 0; batch classifier loss: 0.049906; batch adversarial loss: 1.049503\n",
      "epoch 50; iter: 0; batch classifier loss: 0.079433; batch adversarial loss: 0.996923\n",
      "epoch 51; iter: 0; batch classifier loss: 0.071847; batch adversarial loss: 1.113147\n",
      "epoch 52; iter: 0; batch classifier loss: 0.088164; batch adversarial loss: 1.000285\n",
      "epoch 53; iter: 0; batch classifier loss: 0.062136; batch adversarial loss: 1.047291\n",
      "epoch 54; iter: 0; batch classifier loss: 0.042247; batch adversarial loss: 1.053556\n",
      "epoch 55; iter: 0; batch classifier loss: 0.077930; batch adversarial loss: 1.016780\n",
      "epoch 56; iter: 0; batch classifier loss: 0.062159; batch adversarial loss: 1.017677\n",
      "epoch 57; iter: 0; batch classifier loss: 0.059454; batch adversarial loss: 1.046765\n",
      "epoch 58; iter: 0; batch classifier loss: 0.064349; batch adversarial loss: 1.117131\n",
      "epoch 59; iter: 0; batch classifier loss: 0.092650; batch adversarial loss: 0.983197\n",
      "epoch 60; iter: 0; batch classifier loss: 0.079035; batch adversarial loss: 1.023170\n",
      "epoch 61; iter: 0; batch classifier loss: 0.079116; batch adversarial loss: 1.035780\n",
      "epoch 62; iter: 0; batch classifier loss: 0.061914; batch adversarial loss: 0.976500\n",
      "epoch 63; iter: 0; batch classifier loss: 0.039805; batch adversarial loss: 1.074983\n",
      "epoch 64; iter: 0; batch classifier loss: 0.059674; batch adversarial loss: 1.057774\n",
      "epoch 65; iter: 0; batch classifier loss: 0.050854; batch adversarial loss: 0.921660\n",
      "epoch 66; iter: 0; batch classifier loss: 0.050740; batch adversarial loss: 1.040048\n",
      "epoch 67; iter: 0; batch classifier loss: 0.033920; batch adversarial loss: 1.017474\n",
      "epoch 68; iter: 0; batch classifier loss: 0.048509; batch adversarial loss: 0.954664\n",
      "epoch 69; iter: 0; batch classifier loss: 0.059175; batch adversarial loss: 0.995153\n",
      "epoch 70; iter: 0; batch classifier loss: 0.081828; batch adversarial loss: 1.034647\n",
      "epoch 71; iter: 0; batch classifier loss: 0.075162; batch adversarial loss: 1.031801\n",
      "epoch 72; iter: 0; batch classifier loss: 0.047587; batch adversarial loss: 0.974462\n",
      "epoch 73; iter: 0; batch classifier loss: 0.111024; batch adversarial loss: 0.990556\n",
      "epoch 74; iter: 0; batch classifier loss: 0.052003; batch adversarial loss: 1.027986\n",
      "epoch 75; iter: 0; batch classifier loss: 0.056635; batch adversarial loss: 0.973939\n",
      "epoch 76; iter: 0; batch classifier loss: 0.058784; batch adversarial loss: 0.951805\n",
      "epoch 77; iter: 0; batch classifier loss: 0.075369; batch adversarial loss: 0.928102\n",
      "epoch 78; iter: 0; batch classifier loss: 0.076086; batch adversarial loss: 1.031213\n",
      "epoch 79; iter: 0; batch classifier loss: 0.092011; batch adversarial loss: 1.010678\n",
      "epoch 80; iter: 0; batch classifier loss: 0.127501; batch adversarial loss: 0.991956\n",
      "epoch 81; iter: 0; batch classifier loss: 0.035965; batch adversarial loss: 0.925562\n",
      "epoch 82; iter: 0; batch classifier loss: 0.046129; batch adversarial loss: 1.041256\n",
      "epoch 83; iter: 0; batch classifier loss: 0.039595; batch adversarial loss: 0.959001\n",
      "epoch 84; iter: 0; batch classifier loss: 0.055199; batch adversarial loss: 0.897181\n",
      "epoch 85; iter: 0; batch classifier loss: 0.039662; batch adversarial loss: 0.962544\n",
      "epoch 86; iter: 0; batch classifier loss: 0.045136; batch adversarial loss: 0.944588\n",
      "epoch 87; iter: 0; batch classifier loss: 0.058244; batch adversarial loss: 0.926180\n",
      "epoch 88; iter: 0; batch classifier loss: 0.050180; batch adversarial loss: 0.873156\n",
      "epoch 89; iter: 0; batch classifier loss: 0.036491; batch adversarial loss: 0.935221\n",
      "epoch 90; iter: 0; batch classifier loss: 0.045236; batch adversarial loss: 0.983972\n",
      "epoch 91; iter: 0; batch classifier loss: 0.057442; batch adversarial loss: 0.894643\n",
      "epoch 92; iter: 0; batch classifier loss: 0.053607; batch adversarial loss: 0.907449\n",
      "epoch 93; iter: 0; batch classifier loss: 0.036152; batch adversarial loss: 0.975899\n",
      "epoch 94; iter: 0; batch classifier loss: 0.074159; batch adversarial loss: 0.939312\n",
      "epoch 95; iter: 0; batch classifier loss: 0.053830; batch adversarial loss: 0.945794\n",
      "epoch 96; iter: 0; batch classifier loss: 0.053274; batch adversarial loss: 0.896022\n",
      "epoch 97; iter: 0; batch classifier loss: 0.042132; batch adversarial loss: 0.977606\n",
      "epoch 98; iter: 0; batch classifier loss: 0.034046; batch adversarial loss: 0.946687\n",
      "epoch 99; iter: 0; batch classifier loss: 0.055433; batch adversarial loss: 0.925676\n",
      "epoch 100; iter: 0; batch classifier loss: 0.022758; batch adversarial loss: 0.926953\n",
      "epoch 101; iter: 0; batch classifier loss: 0.016685; batch adversarial loss: 0.921177\n",
      "epoch 102; iter: 0; batch classifier loss: 0.076371; batch adversarial loss: 0.882284\n",
      "epoch 103; iter: 0; batch classifier loss: 0.043360; batch adversarial loss: 0.868577\n",
      "epoch 104; iter: 0; batch classifier loss: 0.052662; batch adversarial loss: 0.914464\n",
      "epoch 105; iter: 0; batch classifier loss: 0.049230; batch adversarial loss: 0.876771\n",
      "epoch 106; iter: 0; batch classifier loss: 0.053282; batch adversarial loss: 0.862884\n",
      "epoch 107; iter: 0; batch classifier loss: 0.014845; batch adversarial loss: 0.900330\n",
      "epoch 108; iter: 0; batch classifier loss: 0.046627; batch adversarial loss: 0.899417\n",
      "epoch 109; iter: 0; batch classifier loss: 0.042422; batch adversarial loss: 0.921540\n",
      "epoch 110; iter: 0; batch classifier loss: 0.062962; batch adversarial loss: 0.934092\n",
      "epoch 111; iter: 0; batch classifier loss: 0.043754; batch adversarial loss: 0.896024\n",
      "epoch 112; iter: 0; batch classifier loss: 0.077772; batch adversarial loss: 0.936490\n",
      "epoch 113; iter: 0; batch classifier loss: 0.072306; batch adversarial loss: 0.863685\n",
      "epoch 114; iter: 0; batch classifier loss: 0.048607; batch adversarial loss: 0.870209\n",
      "epoch 115; iter: 0; batch classifier loss: 0.035745; batch adversarial loss: 0.869552\n",
      "epoch 116; iter: 0; batch classifier loss: 0.056984; batch adversarial loss: 0.918500\n",
      "epoch 117; iter: 0; batch classifier loss: 0.042931; batch adversarial loss: 0.903771\n",
      "epoch 118; iter: 0; batch classifier loss: 0.062217; batch adversarial loss: 0.895163\n",
      "epoch 119; iter: 0; batch classifier loss: 0.068710; batch adversarial loss: 0.862960\n",
      "epoch 120; iter: 0; batch classifier loss: 0.041129; batch adversarial loss: 0.866819\n",
      "epoch 121; iter: 0; batch classifier loss: 0.083011; batch adversarial loss: 0.868002\n",
      "epoch 122; iter: 0; batch classifier loss: 0.031366; batch adversarial loss: 0.829248\n",
      "epoch 123; iter: 0; batch classifier loss: 0.033326; batch adversarial loss: 0.863057\n",
      "epoch 124; iter: 0; batch classifier loss: 0.048242; batch adversarial loss: 0.844147\n",
      "epoch 125; iter: 0; batch classifier loss: 0.027357; batch adversarial loss: 0.847901\n",
      "epoch 126; iter: 0; batch classifier loss: 0.059186; batch adversarial loss: 0.888427\n",
      "epoch 127; iter: 0; batch classifier loss: 0.045634; batch adversarial loss: 0.882000\n",
      "epoch 128; iter: 0; batch classifier loss: 0.036226; batch adversarial loss: 0.838656\n",
      "epoch 129; iter: 0; batch classifier loss: 0.032158; batch adversarial loss: 0.839552\n",
      "epoch 130; iter: 0; batch classifier loss: 0.033102; batch adversarial loss: 0.859953\n",
      "epoch 131; iter: 0; batch classifier loss: 0.027563; batch adversarial loss: 0.861634\n",
      "epoch 132; iter: 0; batch classifier loss: 0.052795; batch adversarial loss: 0.880729\n",
      "epoch 133; iter: 0; batch classifier loss: 0.038948; batch adversarial loss: 0.842054\n",
      "epoch 134; iter: 0; batch classifier loss: 0.046845; batch adversarial loss: 0.863968\n",
      "epoch 135; iter: 0; batch classifier loss: 0.039108; batch adversarial loss: 0.821732\n",
      "epoch 136; iter: 0; batch classifier loss: 0.028108; batch adversarial loss: 0.824741\n",
      "epoch 137; iter: 0; batch classifier loss: 0.027852; batch adversarial loss: 0.844186\n",
      "epoch 138; iter: 0; batch classifier loss: 0.054001; batch adversarial loss: 0.861753\n",
      "epoch 139; iter: 0; batch classifier loss: 0.037059; batch adversarial loss: 0.825282\n",
      "epoch 140; iter: 0; batch classifier loss: 0.056443; batch adversarial loss: 0.887696\n",
      "epoch 141; iter: 0; batch classifier loss: 0.044404; batch adversarial loss: 0.837147\n",
      "epoch 142; iter: 0; batch classifier loss: 0.025082; batch adversarial loss: 0.797186\n",
      "epoch 143; iter: 0; batch classifier loss: 0.042285; batch adversarial loss: 0.818761\n",
      "epoch 144; iter: 0; batch classifier loss: 0.053996; batch adversarial loss: 0.810166\n",
      "epoch 145; iter: 0; batch classifier loss: 0.018680; batch adversarial loss: 0.851193\n",
      "epoch 146; iter: 0; batch classifier loss: 0.035177; batch adversarial loss: 0.774917\n",
      "epoch 147; iter: 0; batch classifier loss: 0.034046; batch adversarial loss: 0.854410\n",
      "epoch 148; iter: 0; batch classifier loss: 0.032752; batch adversarial loss: 0.770077\n",
      "epoch 149; iter: 0; batch classifier loss: 0.034390; batch adversarial loss: 0.829646\n",
      "epoch 150; iter: 0; batch classifier loss: 0.020940; batch adversarial loss: 0.840887\n",
      "epoch 151; iter: 0; batch classifier loss: 0.087034; batch adversarial loss: 0.809931\n",
      "epoch 152; iter: 0; batch classifier loss: 0.026724; batch adversarial loss: 0.841658\n",
      "epoch 153; iter: 0; batch classifier loss: 0.039919; batch adversarial loss: 0.848006\n",
      "epoch 154; iter: 0; batch classifier loss: 0.054331; batch adversarial loss: 0.833501\n",
      "epoch 155; iter: 0; batch classifier loss: 0.037666; batch adversarial loss: 0.805938\n",
      "epoch 156; iter: 0; batch classifier loss: 0.039534; batch adversarial loss: 0.808086\n",
      "epoch 157; iter: 0; batch classifier loss: 0.047061; batch adversarial loss: 0.818906\n",
      "epoch 158; iter: 0; batch classifier loss: 0.055241; batch adversarial loss: 0.803653\n",
      "epoch 159; iter: 0; batch classifier loss: 0.047209; batch adversarial loss: 0.790066\n",
      "epoch 160; iter: 0; batch classifier loss: 0.063418; batch adversarial loss: 0.799188\n",
      "epoch 161; iter: 0; batch classifier loss: 0.038039; batch adversarial loss: 0.834041\n",
      "epoch 162; iter: 0; batch classifier loss: 0.047675; batch adversarial loss: 0.833999\n",
      "epoch 163; iter: 0; batch classifier loss: 0.052444; batch adversarial loss: 0.804330\n",
      "epoch 164; iter: 0; batch classifier loss: 0.020314; batch adversarial loss: 0.811191\n",
      "epoch 165; iter: 0; batch classifier loss: 0.036240; batch adversarial loss: 0.830380\n",
      "epoch 166; iter: 0; batch classifier loss: 0.009840; batch adversarial loss: 0.764127\n",
      "epoch 167; iter: 0; batch classifier loss: 0.047351; batch adversarial loss: 0.849011\n",
      "epoch 168; iter: 0; batch classifier loss: 0.041265; batch adversarial loss: 0.830692\n",
      "epoch 169; iter: 0; batch classifier loss: 0.043947; batch adversarial loss: 0.766440\n",
      "epoch 170; iter: 0; batch classifier loss: 0.025504; batch adversarial loss: 0.781652\n",
      "epoch 171; iter: 0; batch classifier loss: 0.051533; batch adversarial loss: 0.807614\n",
      "epoch 172; iter: 0; batch classifier loss: 0.060815; batch adversarial loss: 0.770639\n",
      "epoch 173; iter: 0; batch classifier loss: 0.045638; batch adversarial loss: 0.796459\n",
      "epoch 174; iter: 0; batch classifier loss: 0.042729; batch adversarial loss: 0.782171\n",
      "epoch 175; iter: 0; batch classifier loss: 0.005865; batch adversarial loss: 0.778724\n",
      "epoch 176; iter: 0; batch classifier loss: 0.025688; batch adversarial loss: 0.778467\n",
      "epoch 177; iter: 0; batch classifier loss: 0.050494; batch adversarial loss: 0.784722\n",
      "epoch 178; iter: 0; batch classifier loss: 0.045864; batch adversarial loss: 0.806663\n",
      "epoch 179; iter: 0; batch classifier loss: 0.027668; batch adversarial loss: 0.773120\n",
      "epoch 180; iter: 0; batch classifier loss: 0.072773; batch adversarial loss: 0.780836\n",
      "epoch 181; iter: 0; batch classifier loss: 0.045892; batch adversarial loss: 0.783632\n",
      "epoch 182; iter: 0; batch classifier loss: 0.045825; batch adversarial loss: 0.798487\n",
      "epoch 183; iter: 0; batch classifier loss: 0.070807; batch adversarial loss: 0.791077\n",
      "epoch 184; iter: 0; batch classifier loss: 0.030156; batch adversarial loss: 0.769626\n",
      "epoch 185; iter: 0; batch classifier loss: 0.010749; batch adversarial loss: 0.778575\n",
      "epoch 186; iter: 0; batch classifier loss: 0.018496; batch adversarial loss: 0.787651\n",
      "epoch 187; iter: 0; batch classifier loss: 0.027458; batch adversarial loss: 0.770688\n",
      "epoch 188; iter: 0; batch classifier loss: 0.013622; batch adversarial loss: 0.752772\n",
      "epoch 189; iter: 0; batch classifier loss: 0.062444; batch adversarial loss: 0.759896\n",
      "epoch 190; iter: 0; batch classifier loss: 0.048250; batch adversarial loss: 0.773362\n",
      "epoch 191; iter: 0; batch classifier loss: 0.033794; batch adversarial loss: 0.767361\n",
      "epoch 192; iter: 0; batch classifier loss: 0.035675; batch adversarial loss: 0.765519\n",
      "epoch 193; iter: 0; batch classifier loss: 0.044539; batch adversarial loss: 0.790385\n",
      "epoch 194; iter: 0; batch classifier loss: 0.026916; batch adversarial loss: 0.768252\n",
      "epoch 195; iter: 0; batch classifier loss: 0.067789; batch adversarial loss: 0.768354\n",
      "epoch 196; iter: 0; batch classifier loss: 0.058029; batch adversarial loss: 0.793817\n",
      "epoch 197; iter: 0; batch classifier loss: 0.058263; batch adversarial loss: 0.796156\n",
      "epoch 198; iter: 0; batch classifier loss: 0.024857; batch adversarial loss: 0.787552\n",
      "epoch 199; iter: 0; batch classifier loss: 0.017578; batch adversarial loss: 0.769850\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:53.812110: W tensorflow/c/c_api.cc:304] Operation '{name:'4c9698a2-adee-11ee-9362-a9d23602d4a3/4c9698a2-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:10468 op device:{requested: '', assigned: ''} def:{{{node 4c9698a2-adee-11ee-9362-a9d23602d4a3/4c9698a2-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c9698a2-adee-11ee-9362-a9d23602d4a3/4c9698a2-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c9698a2-adee-11ee-9362-a9d23602d4a3/4c9698a2-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.739898; batch adversarial loss: 0.728499\n",
      "epoch 1; iter: 0; batch classifier loss: 0.605164; batch adversarial loss: 0.761668\n",
      "epoch 2; iter: 0; batch classifier loss: 0.449739; batch adversarial loss: 0.763656\n",
      "epoch 3; iter: 0; batch classifier loss: 0.413051; batch adversarial loss: 0.779400\n",
      "epoch 4; iter: 0; batch classifier loss: 0.360823; batch adversarial loss: 0.798642\n",
      "epoch 5; iter: 0; batch classifier loss: 0.311059; batch adversarial loss: 0.781240\n",
      "epoch 6; iter: 0; batch classifier loss: 0.279937; batch adversarial loss: 0.754069\n",
      "epoch 7; iter: 0; batch classifier loss: 0.280041; batch adversarial loss: 0.766264\n",
      "epoch 8; iter: 0; batch classifier loss: 0.288279; batch adversarial loss: 0.776569\n",
      "epoch 9; iter: 0; batch classifier loss: 0.256483; batch adversarial loss: 0.777971\n",
      "epoch 10; iter: 0; batch classifier loss: 0.177362; batch adversarial loss: 0.793302\n",
      "epoch 11; iter: 0; batch classifier loss: 0.190221; batch adversarial loss: 0.761681\n",
      "epoch 12; iter: 0; batch classifier loss: 0.173439; batch adversarial loss: 0.765025\n",
      "epoch 13; iter: 0; batch classifier loss: 0.216184; batch adversarial loss: 0.790870\n",
      "epoch 14; iter: 0; batch classifier loss: 0.178541; batch adversarial loss: 0.750559\n",
      "epoch 15; iter: 0; batch classifier loss: 0.164985; batch adversarial loss: 0.743906\n",
      "epoch 16; iter: 0; batch classifier loss: 0.194668; batch adversarial loss: 0.745946\n",
      "epoch 17; iter: 0; batch classifier loss: 0.198454; batch adversarial loss: 0.760776\n",
      "epoch 18; iter: 0; batch classifier loss: 0.118105; batch adversarial loss: 0.799904\n",
      "epoch 19; iter: 0; batch classifier loss: 0.146820; batch adversarial loss: 0.770230\n",
      "epoch 20; iter: 0; batch classifier loss: 0.152002; batch adversarial loss: 0.801025\n",
      "epoch 21; iter: 0; batch classifier loss: 0.115313; batch adversarial loss: 0.757028\n",
      "epoch 22; iter: 0; batch classifier loss: 0.124025; batch adversarial loss: 0.773040\n",
      "epoch 23; iter: 0; batch classifier loss: 0.118103; batch adversarial loss: 0.745206\n",
      "epoch 24; iter: 0; batch classifier loss: 0.130921; batch adversarial loss: 0.792448\n",
      "epoch 25; iter: 0; batch classifier loss: 0.112478; batch adversarial loss: 0.765476\n",
      "epoch 26; iter: 0; batch classifier loss: 0.114390; batch adversarial loss: 0.765057\n",
      "epoch 27; iter: 0; batch classifier loss: 0.113681; batch adversarial loss: 0.758453\n",
      "epoch 28; iter: 0; batch classifier loss: 0.072983; batch adversarial loss: 0.731178\n",
      "epoch 29; iter: 0; batch classifier loss: 0.083021; batch adversarial loss: 0.741994\n",
      "epoch 30; iter: 0; batch classifier loss: 0.110451; batch adversarial loss: 0.745461\n",
      "epoch 31; iter: 0; batch classifier loss: 0.082539; batch adversarial loss: 0.756935\n",
      "epoch 32; iter: 0; batch classifier loss: 0.113820; batch adversarial loss: 0.767481\n",
      "epoch 33; iter: 0; batch classifier loss: 0.077771; batch adversarial loss: 0.740474\n",
      "epoch 34; iter: 0; batch classifier loss: 0.092994; batch adversarial loss: 0.750116\n",
      "epoch 35; iter: 0; batch classifier loss: 0.062535; batch adversarial loss: 0.754518\n",
      "epoch 36; iter: 0; batch classifier loss: 0.077741; batch adversarial loss: 0.774011\n",
      "epoch 37; iter: 0; batch classifier loss: 0.078243; batch adversarial loss: 0.752959\n",
      "epoch 38; iter: 0; batch classifier loss: 0.100070; batch adversarial loss: 0.751497\n",
      "epoch 39; iter: 0; batch classifier loss: 0.055086; batch adversarial loss: 0.744448\n",
      "epoch 40; iter: 0; batch classifier loss: 0.045013; batch adversarial loss: 0.732532\n",
      "epoch 41; iter: 0; batch classifier loss: 0.077070; batch adversarial loss: 0.719242\n",
      "epoch 42; iter: 0; batch classifier loss: 0.062722; batch adversarial loss: 0.734353\n",
      "epoch 43; iter: 0; batch classifier loss: 0.083437; batch adversarial loss: 0.727690\n",
      "epoch 44; iter: 0; batch classifier loss: 0.056066; batch adversarial loss: 0.731687\n",
      "epoch 45; iter: 0; batch classifier loss: 0.058911; batch adversarial loss: 0.722926\n",
      "epoch 46; iter: 0; batch classifier loss: 0.081219; batch adversarial loss: 0.750500\n",
      "epoch 47; iter: 0; batch classifier loss: 0.040968; batch adversarial loss: 0.720716\n",
      "epoch 48; iter: 0; batch classifier loss: 0.049670; batch adversarial loss: 0.746662\n",
      "epoch 49; iter: 0; batch classifier loss: 0.079892; batch adversarial loss: 0.744308\n",
      "epoch 50; iter: 0; batch classifier loss: 0.050073; batch adversarial loss: 0.721723\n",
      "epoch 51; iter: 0; batch classifier loss: 0.046339; batch adversarial loss: 0.734378\n",
      "epoch 52; iter: 0; batch classifier loss: 0.059985; batch adversarial loss: 0.741792\n",
      "epoch 53; iter: 0; batch classifier loss: 0.064714; batch adversarial loss: 0.732457\n",
      "epoch 54; iter: 0; batch classifier loss: 0.055090; batch adversarial loss: 0.735348\n",
      "epoch 55; iter: 0; batch classifier loss: 0.044299; batch adversarial loss: 0.717415\n",
      "epoch 56; iter: 0; batch classifier loss: 0.045768; batch adversarial loss: 0.716272\n",
      "epoch 57; iter: 0; batch classifier loss: 0.084838; batch adversarial loss: 0.746575\n",
      "epoch 58; iter: 0; batch classifier loss: 0.042624; batch adversarial loss: 0.713911\n",
      "epoch 59; iter: 0; batch classifier loss: 0.057382; batch adversarial loss: 0.729284\n",
      "epoch 60; iter: 0; batch classifier loss: 0.050850; batch adversarial loss: 0.726731\n",
      "epoch 61; iter: 0; batch classifier loss: 0.061774; batch adversarial loss: 0.732521\n",
      "epoch 62; iter: 0; batch classifier loss: 0.070706; batch adversarial loss: 0.745899\n",
      "epoch 63; iter: 0; batch classifier loss: 0.034460; batch adversarial loss: 0.711826\n",
      "epoch 64; iter: 0; batch classifier loss: 0.036077; batch adversarial loss: 0.713236\n",
      "epoch 65; iter: 0; batch classifier loss: 0.041647; batch adversarial loss: 0.712651\n",
      "epoch 66; iter: 0; batch classifier loss: 0.054869; batch adversarial loss: 0.716587\n",
      "epoch 67; iter: 0; batch classifier loss: 0.047397; batch adversarial loss: 0.718945\n",
      "epoch 68; iter: 0; batch classifier loss: 0.060949; batch adversarial loss: 0.722659\n",
      "epoch 69; iter: 0; batch classifier loss: 0.061357; batch adversarial loss: 0.728478\n",
      "epoch 70; iter: 0; batch classifier loss: 0.021790; batch adversarial loss: 0.693649\n",
      "epoch 71; iter: 0; batch classifier loss: 0.069524; batch adversarial loss: 0.722856\n",
      "epoch 72; iter: 0; batch classifier loss: 0.037593; batch adversarial loss: 0.707039\n",
      "epoch 73; iter: 0; batch classifier loss: 0.040141; batch adversarial loss: 0.712570\n",
      "epoch 74; iter: 0; batch classifier loss: 0.041692; batch adversarial loss: 0.718356\n",
      "epoch 75; iter: 0; batch classifier loss: 0.038967; batch adversarial loss: 0.711120\n",
      "epoch 76; iter: 0; batch classifier loss: 0.044997; batch adversarial loss: 0.712045\n",
      "epoch 77; iter: 0; batch classifier loss: 0.055384; batch adversarial loss: 0.718148\n",
      "epoch 78; iter: 0; batch classifier loss: 0.048346; batch adversarial loss: 0.711960\n",
      "epoch 79; iter: 0; batch classifier loss: 0.044746; batch adversarial loss: 0.716376\n",
      "epoch 80; iter: 0; batch classifier loss: 0.043223; batch adversarial loss: 0.704300\n",
      "epoch 81; iter: 0; batch classifier loss: 0.073415; batch adversarial loss: 0.730686\n",
      "epoch 82; iter: 0; batch classifier loss: 0.033493; batch adversarial loss: 0.702177\n",
      "epoch 83; iter: 0; batch classifier loss: 0.049942; batch adversarial loss: 0.716031\n",
      "epoch 84; iter: 0; batch classifier loss: 0.014298; batch adversarial loss: 0.691675\n",
      "epoch 85; iter: 0; batch classifier loss: 0.074252; batch adversarial loss: 0.724429\n",
      "epoch 86; iter: 0; batch classifier loss: 0.047812; batch adversarial loss: 0.708458\n",
      "epoch 87; iter: 0; batch classifier loss: 0.089967; batch adversarial loss: 0.737499\n",
      "epoch 88; iter: 0; batch classifier loss: 0.033511; batch adversarial loss: 0.698798\n",
      "epoch 89; iter: 0; batch classifier loss: 0.026646; batch adversarial loss: 0.693370\n",
      "epoch 90; iter: 0; batch classifier loss: 0.024089; batch adversarial loss: 0.693676\n",
      "epoch 91; iter: 0; batch classifier loss: 0.057386; batch adversarial loss: 0.717304\n",
      "epoch 92; iter: 0; batch classifier loss: 0.027154; batch adversarial loss: 0.692171\n",
      "epoch 93; iter: 0; batch classifier loss: 0.020622; batch adversarial loss: 0.693013\n",
      "epoch 94; iter: 0; batch classifier loss: 0.035019; batch adversarial loss: 0.697909\n",
      "epoch 95; iter: 0; batch classifier loss: 0.071749; batch adversarial loss: 0.722581\n",
      "epoch 96; iter: 0; batch classifier loss: 0.041943; batch adversarial loss: 0.699207\n",
      "epoch 97; iter: 0; batch classifier loss: 0.044525; batch adversarial loss: 0.706644\n",
      "epoch 98; iter: 0; batch classifier loss: 0.042991; batch adversarial loss: 0.704538\n",
      "epoch 99; iter: 0; batch classifier loss: 0.023438; batch adversarial loss: 0.682848\n",
      "epoch 100; iter: 0; batch classifier loss: 0.058169; batch adversarial loss: 0.709460\n",
      "epoch 101; iter: 0; batch classifier loss: 0.030579; batch adversarial loss: 0.691312\n",
      "epoch 102; iter: 0; batch classifier loss: 0.050095; batch adversarial loss: 0.706225\n",
      "epoch 103; iter: 0; batch classifier loss: 0.050065; batch adversarial loss: 0.709531\n",
      "epoch 104; iter: 0; batch classifier loss: 0.026505; batch adversarial loss: 0.694196\n",
      "epoch 105; iter: 0; batch classifier loss: 0.057206; batch adversarial loss: 0.710722\n",
      "epoch 106; iter: 0; batch classifier loss: 0.065614; batch adversarial loss: 0.716747\n",
      "epoch 107; iter: 0; batch classifier loss: 0.024574; batch adversarial loss: 0.685181\n",
      "epoch 108; iter: 0; batch classifier loss: 0.045491; batch adversarial loss: 0.701509\n",
      "epoch 109; iter: 0; batch classifier loss: 0.057613; batch adversarial loss: 0.705567\n",
      "epoch 110; iter: 0; batch classifier loss: 0.036126; batch adversarial loss: 0.696290\n",
      "epoch 111; iter: 0; batch classifier loss: 0.036477; batch adversarial loss: 0.694428\n",
      "epoch 112; iter: 0; batch classifier loss: 0.031051; batch adversarial loss: 0.688005\n",
      "epoch 113; iter: 0; batch classifier loss: 0.040643; batch adversarial loss: 0.694224\n",
      "epoch 114; iter: 0; batch classifier loss: 0.041279; batch adversarial loss: 0.690616\n",
      "epoch 115; iter: 0; batch classifier loss: 0.053062; batch adversarial loss: 0.706118\n",
      "epoch 116; iter: 0; batch classifier loss: 0.056710; batch adversarial loss: 0.708745\n",
      "epoch 117; iter: 0; batch classifier loss: 0.026981; batch adversarial loss: 0.688393\n",
      "epoch 118; iter: 0; batch classifier loss: 0.061192; batch adversarial loss: 0.691983\n",
      "epoch 119; iter: 0; batch classifier loss: 0.051655; batch adversarial loss: 0.691865\n",
      "epoch 120; iter: 0; batch classifier loss: 0.035352; batch adversarial loss: 0.695275\n",
      "epoch 121; iter: 0; batch classifier loss: 0.035951; batch adversarial loss: 0.676932\n",
      "epoch 122; iter: 0; batch classifier loss: 0.038465; batch adversarial loss: 0.688497\n",
      "epoch 123; iter: 0; batch classifier loss: 0.015136; batch adversarial loss: 0.673646\n",
      "epoch 124; iter: 0; batch classifier loss: 0.039313; batch adversarial loss: 0.691183\n",
      "epoch 125; iter: 0; batch classifier loss: 0.048496; batch adversarial loss: 0.709405\n",
      "epoch 126; iter: 0; batch classifier loss: 0.055865; batch adversarial loss: 0.703533\n",
      "epoch 127; iter: 0; batch classifier loss: 0.070822; batch adversarial loss: 0.703467\n",
      "epoch 128; iter: 0; batch classifier loss: 0.039215; batch adversarial loss: 0.689405\n",
      "epoch 129; iter: 0; batch classifier loss: 0.047706; batch adversarial loss: 0.710835\n",
      "epoch 130; iter: 0; batch classifier loss: 0.034585; batch adversarial loss: 0.682852\n",
      "epoch 131; iter: 0; batch classifier loss: 0.037262; batch adversarial loss: 0.712053\n",
      "epoch 132; iter: 0; batch classifier loss: 0.050884; batch adversarial loss: 0.690665\n",
      "epoch 133; iter: 0; batch classifier loss: 0.081275; batch adversarial loss: 0.721502\n",
      "epoch 134; iter: 0; batch classifier loss: 0.039656; batch adversarial loss: 0.696587\n",
      "epoch 135; iter: 0; batch classifier loss: 0.067618; batch adversarial loss: 0.707491\n",
      "epoch 136; iter: 0; batch classifier loss: 0.035769; batch adversarial loss: 0.682748\n",
      "epoch 137; iter: 0; batch classifier loss: 0.041526; batch adversarial loss: 0.688134\n",
      "epoch 138; iter: 0; batch classifier loss: 0.062779; batch adversarial loss: 0.712152\n",
      "epoch 139; iter: 0; batch classifier loss: 0.024380; batch adversarial loss: 0.688202\n",
      "epoch 140; iter: 0; batch classifier loss: 0.029609; batch adversarial loss: 0.667555\n",
      "epoch 141; iter: 0; batch classifier loss: 0.058916; batch adversarial loss: 0.710286\n",
      "epoch 142; iter: 0; batch classifier loss: 0.022202; batch adversarial loss: 0.688875\n",
      "epoch 143; iter: 0; batch classifier loss: 0.050407; batch adversarial loss: 0.714769\n",
      "epoch 144; iter: 0; batch classifier loss: 0.033410; batch adversarial loss: 0.680337\n",
      "epoch 145; iter: 0; batch classifier loss: 0.034229; batch adversarial loss: 0.691170\n",
      "epoch 146; iter: 0; batch classifier loss: 0.060491; batch adversarial loss: 0.703506\n",
      "epoch 147; iter: 0; batch classifier loss: 0.034258; batch adversarial loss: 0.690566\n",
      "epoch 148; iter: 0; batch classifier loss: 0.049289; batch adversarial loss: 0.662786\n",
      "epoch 149; iter: 0; batch classifier loss: 0.028655; batch adversarial loss: 0.680128\n",
      "epoch 150; iter: 0; batch classifier loss: 0.044985; batch adversarial loss: 0.682039\n",
      "epoch 151; iter: 0; batch classifier loss: 0.040130; batch adversarial loss: 0.680339\n",
      "epoch 152; iter: 0; batch classifier loss: 0.032113; batch adversarial loss: 0.701813\n",
      "epoch 153; iter: 0; batch classifier loss: 0.030145; batch adversarial loss: 0.670636\n",
      "epoch 154; iter: 0; batch classifier loss: 0.041317; batch adversarial loss: 0.689284\n",
      "epoch 155; iter: 0; batch classifier loss: 0.032568; batch adversarial loss: 0.685364\n",
      "epoch 156; iter: 0; batch classifier loss: 0.050541; batch adversarial loss: 0.705597\n",
      "epoch 157; iter: 0; batch classifier loss: 0.017309; batch adversarial loss: 0.670810\n",
      "epoch 158; iter: 0; batch classifier loss: 0.040183; batch adversarial loss: 0.685271\n",
      "epoch 159; iter: 0; batch classifier loss: 0.031665; batch adversarial loss: 0.677795\n",
      "epoch 160; iter: 0; batch classifier loss: 0.025690; batch adversarial loss: 0.668185\n",
      "epoch 161; iter: 0; batch classifier loss: 0.045806; batch adversarial loss: 0.698418\n",
      "epoch 162; iter: 0; batch classifier loss: 0.056277; batch adversarial loss: 0.705508\n",
      "epoch 163; iter: 0; batch classifier loss: 0.008241; batch adversarial loss: 0.669592\n",
      "epoch 164; iter: 0; batch classifier loss: 0.062576; batch adversarial loss: 0.712013\n",
      "epoch 165; iter: 0; batch classifier loss: 0.045635; batch adversarial loss: 0.685745\n",
      "epoch 166; iter: 0; batch classifier loss: 0.035891; batch adversarial loss: 0.696877\n",
      "epoch 167; iter: 0; batch classifier loss: 0.040970; batch adversarial loss: 0.699980\n",
      "epoch 168; iter: 0; batch classifier loss: 0.056150; batch adversarial loss: 0.712112\n",
      "epoch 169; iter: 0; batch classifier loss: 0.026330; batch adversarial loss: 0.673261\n",
      "epoch 170; iter: 0; batch classifier loss: 0.029425; batch adversarial loss: 0.690239\n",
      "epoch 171; iter: 0; batch classifier loss: 0.033243; batch adversarial loss: 0.652237\n",
      "epoch 172; iter: 0; batch classifier loss: 0.052492; batch adversarial loss: 0.680156\n",
      "epoch 173; iter: 0; batch classifier loss: 0.046601; batch adversarial loss: 0.710918\n",
      "epoch 174; iter: 0; batch classifier loss: 0.034154; batch adversarial loss: 0.674238\n",
      "epoch 175; iter: 0; batch classifier loss: 0.011388; batch adversarial loss: 0.663233\n",
      "epoch 176; iter: 0; batch classifier loss: 0.017234; batch adversarial loss: 0.672653\n",
      "epoch 177; iter: 0; batch classifier loss: 0.025088; batch adversarial loss: 0.683086\n",
      "epoch 178; iter: 0; batch classifier loss: 0.023715; batch adversarial loss: 0.667393\n",
      "epoch 179; iter: 0; batch classifier loss: 0.042310; batch adversarial loss: 0.691149\n",
      "epoch 180; iter: 0; batch classifier loss: 0.031061; batch adversarial loss: 0.675339\n",
      "epoch 181; iter: 0; batch classifier loss: 0.048725; batch adversarial loss: 0.685371\n",
      "epoch 182; iter: 0; batch classifier loss: 0.042225; batch adversarial loss: 0.711387\n",
      "epoch 183; iter: 0; batch classifier loss: 0.041319; batch adversarial loss: 0.670929\n",
      "epoch 184; iter: 0; batch classifier loss: 0.058772; batch adversarial loss: 0.685859\n",
      "epoch 185; iter: 0; batch classifier loss: 0.027331; batch adversarial loss: 0.685022\n",
      "epoch 186; iter: 0; batch classifier loss: 0.060156; batch adversarial loss: 0.688374\n",
      "epoch 187; iter: 0; batch classifier loss: 0.056360; batch adversarial loss: 0.694535\n",
      "epoch 188; iter: 0; batch classifier loss: 0.024056; batch adversarial loss: 0.667049\n",
      "epoch 189; iter: 0; batch classifier loss: 0.041131; batch adversarial loss: 0.675097\n",
      "epoch 190; iter: 0; batch classifier loss: 0.031514; batch adversarial loss: 0.677424\n",
      "epoch 191; iter: 0; batch classifier loss: 0.057815; batch adversarial loss: 0.683602\n",
      "epoch 192; iter: 0; batch classifier loss: 0.038984; batch adversarial loss: 0.685179\n",
      "epoch 193; iter: 0; batch classifier loss: 0.061183; batch adversarial loss: 0.687612\n",
      "epoch 194; iter: 0; batch classifier loss: 0.049362; batch adversarial loss: 0.677419\n",
      "epoch 195; iter: 0; batch classifier loss: 0.026334; batch adversarial loss: 0.649705\n",
      "epoch 196; iter: 0; batch classifier loss: 0.035326; batch adversarial loss: 0.692537\n",
      "epoch 197; iter: 0; batch classifier loss: 0.025917; batch adversarial loss: 0.665656\n",
      "epoch 198; iter: 0; batch classifier loss: 0.061228; batch adversarial loss: 0.692760\n",
      "epoch 199; iter: 0; batch classifier loss: 0.022873; batch adversarial loss: 0.670042\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:56.111227: W tensorflow/c/c_api.cc:304] Operation '{name:'4c96992e-adee-11ee-9362-a9d23602d4a3/4c96992e-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:11275 op device:{requested: '', assigned: ''} def:{{{node 4c96992e-adee-11ee-9362-a9d23602d4a3/4c96992e-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c96992e-adee-11ee-9362-a9d23602d4a3/4c96992e-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c96992e-adee-11ee-9362-a9d23602d4a3/4c96992e-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.776898; batch adversarial loss: 0.692694\n",
      "epoch 1; iter: 0; batch classifier loss: 0.575537; batch adversarial loss: 0.694673\n",
      "epoch 2; iter: 0; batch classifier loss: 0.429709; batch adversarial loss: 0.712355\n",
      "epoch 3; iter: 0; batch classifier loss: 0.397408; batch adversarial loss: 0.747398\n",
      "epoch 4; iter: 0; batch classifier loss: 0.313891; batch adversarial loss: 0.692392\n",
      "epoch 5; iter: 0; batch classifier loss: 0.251133; batch adversarial loss: 0.670692\n",
      "epoch 6; iter: 0; batch classifier loss: 0.198722; batch adversarial loss: 0.712146\n",
      "epoch 7; iter: 0; batch classifier loss: 0.258412; batch adversarial loss: 0.740424\n",
      "epoch 8; iter: 0; batch classifier loss: 0.274745; batch adversarial loss: 0.716179\n",
      "epoch 9; iter: 0; batch classifier loss: 0.269117; batch adversarial loss: 0.758409\n",
      "epoch 10; iter: 0; batch classifier loss: 0.247299; batch adversarial loss: 0.704065\n",
      "epoch 11; iter: 0; batch classifier loss: 0.214753; batch adversarial loss: 0.711038\n",
      "epoch 12; iter: 0; batch classifier loss: 0.214990; batch adversarial loss: 0.749917\n",
      "epoch 13; iter: 0; batch classifier loss: 0.173198; batch adversarial loss: 0.704085\n",
      "epoch 14; iter: 0; batch classifier loss: 0.188299; batch adversarial loss: 0.716472\n",
      "epoch 15; iter: 0; batch classifier loss: 0.176763; batch adversarial loss: 0.706527\n",
      "epoch 16; iter: 0; batch classifier loss: 0.193992; batch adversarial loss: 0.711477\n",
      "epoch 17; iter: 0; batch classifier loss: 0.196870; batch adversarial loss: 0.726090\n",
      "epoch 18; iter: 0; batch classifier loss: 0.180754; batch adversarial loss: 0.734651\n",
      "epoch 19; iter: 0; batch classifier loss: 0.184022; batch adversarial loss: 0.728620\n",
      "epoch 20; iter: 0; batch classifier loss: 0.125499; batch adversarial loss: 0.713746\n",
      "epoch 21; iter: 0; batch classifier loss: 0.100140; batch adversarial loss: 0.701657\n",
      "epoch 22; iter: 0; batch classifier loss: 0.138377; batch adversarial loss: 0.700113\n",
      "epoch 23; iter: 0; batch classifier loss: 0.133586; batch adversarial loss: 0.705953\n",
      "epoch 24; iter: 0; batch classifier loss: 0.174575; batch adversarial loss: 0.706275\n",
      "epoch 25; iter: 0; batch classifier loss: 0.151763; batch adversarial loss: 0.749439\n",
      "epoch 26; iter: 0; batch classifier loss: 0.173376; batch adversarial loss: 0.737625\n",
      "epoch 27; iter: 0; batch classifier loss: 0.164460; batch adversarial loss: 0.722648\n",
      "epoch 28; iter: 0; batch classifier loss: 0.134155; batch adversarial loss: 0.709750\n",
      "epoch 29; iter: 0; batch classifier loss: 0.119717; batch adversarial loss: 0.689989\n",
      "epoch 30; iter: 0; batch classifier loss: 0.143135; batch adversarial loss: 0.731741\n",
      "epoch 31; iter: 0; batch classifier loss: 0.126180; batch adversarial loss: 0.728825\n",
      "epoch 32; iter: 0; batch classifier loss: 0.101777; batch adversarial loss: 0.711262\n",
      "epoch 33; iter: 0; batch classifier loss: 0.132736; batch adversarial loss: 0.734954\n",
      "epoch 34; iter: 0; batch classifier loss: 0.134226; batch adversarial loss: 0.712940\n",
      "epoch 35; iter: 0; batch classifier loss: 0.145293; batch adversarial loss: 0.729766\n",
      "epoch 36; iter: 0; batch classifier loss: 0.127836; batch adversarial loss: 0.737062\n",
      "epoch 37; iter: 0; batch classifier loss: 0.132655; batch adversarial loss: 0.726821\n",
      "epoch 38; iter: 0; batch classifier loss: 0.124311; batch adversarial loss: 0.727637\n",
      "epoch 39; iter: 0; batch classifier loss: 0.105640; batch adversarial loss: 0.720585\n",
      "epoch 40; iter: 0; batch classifier loss: 0.135934; batch adversarial loss: 0.730117\n",
      "epoch 41; iter: 0; batch classifier loss: 0.108101; batch adversarial loss: 0.725319\n",
      "epoch 42; iter: 0; batch classifier loss: 0.116649; batch adversarial loss: 0.709349\n",
      "epoch 43; iter: 0; batch classifier loss: 0.125670; batch adversarial loss: 0.742964\n",
      "epoch 44; iter: 0; batch classifier loss: 0.123882; batch adversarial loss: 0.732305\n",
      "epoch 45; iter: 0; batch classifier loss: 0.112096; batch adversarial loss: 0.733753\n",
      "epoch 46; iter: 0; batch classifier loss: 0.121014; batch adversarial loss: 0.728662\n",
      "epoch 47; iter: 0; batch classifier loss: 0.124798; batch adversarial loss: 0.743150\n",
      "epoch 48; iter: 0; batch classifier loss: 0.112520; batch adversarial loss: 0.727515\n",
      "epoch 49; iter: 0; batch classifier loss: 0.101018; batch adversarial loss: 0.717458\n",
      "epoch 50; iter: 0; batch classifier loss: 0.089061; batch adversarial loss: 0.721486\n",
      "epoch 51; iter: 0; batch classifier loss: 0.128130; batch adversarial loss: 0.738228\n",
      "epoch 52; iter: 0; batch classifier loss: 0.078388; batch adversarial loss: 0.703474\n",
      "epoch 53; iter: 0; batch classifier loss: 0.115439; batch adversarial loss: 0.711194\n",
      "epoch 54; iter: 0; batch classifier loss: 0.096850; batch adversarial loss: 0.722147\n",
      "epoch 55; iter: 0; batch classifier loss: 0.099073; batch adversarial loss: 0.721493\n",
      "epoch 56; iter: 0; batch classifier loss: 0.091166; batch adversarial loss: 0.731873\n",
      "epoch 57; iter: 0; batch classifier loss: 0.063191; batch adversarial loss: 0.712200\n",
      "epoch 58; iter: 0; batch classifier loss: 0.084866; batch adversarial loss: 0.715973\n",
      "epoch 59; iter: 0; batch classifier loss: 0.139659; batch adversarial loss: 0.741607\n",
      "epoch 60; iter: 0; batch classifier loss: 0.105929; batch adversarial loss: 0.722359\n",
      "epoch 61; iter: 0; batch classifier loss: 0.165386; batch adversarial loss: 0.773963\n",
      "epoch 62; iter: 0; batch classifier loss: 0.099764; batch adversarial loss: 0.726563\n",
      "epoch 63; iter: 0; batch classifier loss: 0.106318; batch adversarial loss: 0.727545\n",
      "epoch 64; iter: 0; batch classifier loss: 0.111655; batch adversarial loss: 0.721523\n",
      "epoch 65; iter: 0; batch classifier loss: 0.086171; batch adversarial loss: 0.715944\n",
      "epoch 66; iter: 0; batch classifier loss: 0.125432; batch adversarial loss: 0.744524\n",
      "epoch 67; iter: 0; batch classifier loss: 0.106758; batch adversarial loss: 0.724834\n",
      "epoch 68; iter: 0; batch classifier loss: 0.072984; batch adversarial loss: 0.704923\n",
      "epoch 69; iter: 0; batch classifier loss: 0.115288; batch adversarial loss: 0.721914\n",
      "epoch 70; iter: 0; batch classifier loss: 0.064320; batch adversarial loss: 0.701269\n",
      "epoch 71; iter: 0; batch classifier loss: 0.076685; batch adversarial loss: 0.721447\n",
      "epoch 72; iter: 0; batch classifier loss: 0.069224; batch adversarial loss: 0.715763\n",
      "epoch 73; iter: 0; batch classifier loss: 0.121537; batch adversarial loss: 0.738051\n",
      "epoch 74; iter: 0; batch classifier loss: 0.092682; batch adversarial loss: 0.720356\n",
      "epoch 75; iter: 0; batch classifier loss: 0.109799; batch adversarial loss: 0.731766\n",
      "epoch 76; iter: 0; batch classifier loss: 0.100879; batch adversarial loss: 0.725797\n",
      "epoch 77; iter: 0; batch classifier loss: 0.088705; batch adversarial loss: 0.716099\n",
      "epoch 78; iter: 0; batch classifier loss: 0.106222; batch adversarial loss: 0.722844\n",
      "epoch 79; iter: 0; batch classifier loss: 0.099218; batch adversarial loss: 0.709639\n",
      "epoch 80; iter: 0; batch classifier loss: 0.105502; batch adversarial loss: 0.726064\n",
      "epoch 81; iter: 0; batch classifier loss: 0.090161; batch adversarial loss: 0.711102\n",
      "epoch 82; iter: 0; batch classifier loss: 0.063540; batch adversarial loss: 0.716618\n",
      "epoch 83; iter: 0; batch classifier loss: 0.104411; batch adversarial loss: 0.717854\n",
      "epoch 84; iter: 0; batch classifier loss: 0.092659; batch adversarial loss: 0.712453\n",
      "epoch 85; iter: 0; batch classifier loss: 0.089127; batch adversarial loss: 0.715793\n",
      "epoch 86; iter: 0; batch classifier loss: 0.134852; batch adversarial loss: 0.743546\n",
      "epoch 87; iter: 0; batch classifier loss: 0.082736; batch adversarial loss: 0.726273\n",
      "epoch 88; iter: 0; batch classifier loss: 0.094276; batch adversarial loss: 0.727215\n",
      "epoch 89; iter: 0; batch classifier loss: 0.093889; batch adversarial loss: 0.734307\n",
      "epoch 90; iter: 0; batch classifier loss: 0.063798; batch adversarial loss: 0.680898\n",
      "epoch 91; iter: 0; batch classifier loss: 0.083861; batch adversarial loss: 0.728993\n",
      "epoch 92; iter: 0; batch classifier loss: 0.077157; batch adversarial loss: 0.721474\n",
      "epoch 93; iter: 0; batch classifier loss: 0.098710; batch adversarial loss: 0.713306\n",
      "epoch 94; iter: 0; batch classifier loss: 0.099755; batch adversarial loss: 0.714264\n",
      "epoch 95; iter: 0; batch classifier loss: 0.107746; batch adversarial loss: 0.744833\n",
      "epoch 96; iter: 0; batch classifier loss: 0.088116; batch adversarial loss: 0.723999\n",
      "epoch 97; iter: 0; batch classifier loss: 0.137805; batch adversarial loss: 0.739431\n",
      "epoch 98; iter: 0; batch classifier loss: 0.080641; batch adversarial loss: 0.717657\n",
      "epoch 99; iter: 0; batch classifier loss: 0.048683; batch adversarial loss: 0.708178\n",
      "epoch 100; iter: 0; batch classifier loss: 0.114018; batch adversarial loss: 0.735979\n",
      "epoch 101; iter: 0; batch classifier loss: 0.097361; batch adversarial loss: 0.719371\n",
      "epoch 102; iter: 0; batch classifier loss: 0.086444; batch adversarial loss: 0.715502\n",
      "epoch 103; iter: 0; batch classifier loss: 0.073132; batch adversarial loss: 0.704264\n",
      "epoch 104; iter: 0; batch classifier loss: 0.057821; batch adversarial loss: 0.704190\n",
      "epoch 105; iter: 0; batch classifier loss: 0.097026; batch adversarial loss: 0.707047\n",
      "epoch 106; iter: 0; batch classifier loss: 0.057984; batch adversarial loss: 0.704150\n",
      "epoch 107; iter: 0; batch classifier loss: 0.081541; batch adversarial loss: 0.725916\n",
      "epoch 108; iter: 0; batch classifier loss: 0.115818; batch adversarial loss: 0.717976\n",
      "epoch 109; iter: 0; batch classifier loss: 0.077767; batch adversarial loss: 0.712578\n",
      "epoch 110; iter: 0; batch classifier loss: 0.082035; batch adversarial loss: 0.701957\n",
      "epoch 111; iter: 0; batch classifier loss: 0.093380; batch adversarial loss: 0.709909\n",
      "epoch 112; iter: 0; batch classifier loss: 0.077539; batch adversarial loss: 0.719156\n",
      "epoch 113; iter: 0; batch classifier loss: 0.068974; batch adversarial loss: 0.702627\n",
      "epoch 114; iter: 0; batch classifier loss: 0.112540; batch adversarial loss: 0.737175\n",
      "epoch 115; iter: 0; batch classifier loss: 0.040453; batch adversarial loss: 0.689239\n",
      "epoch 116; iter: 0; batch classifier loss: 0.063201; batch adversarial loss: 0.698882\n",
      "epoch 117; iter: 0; batch classifier loss: 0.107927; batch adversarial loss: 0.738663\n",
      "epoch 118; iter: 0; batch classifier loss: 0.069319; batch adversarial loss: 0.715216\n",
      "epoch 119; iter: 0; batch classifier loss: 0.092074; batch adversarial loss: 0.729036\n",
      "epoch 120; iter: 0; batch classifier loss: 0.092175; batch adversarial loss: 0.705736\n",
      "epoch 121; iter: 0; batch classifier loss: 0.120373; batch adversarial loss: 0.731828\n",
      "epoch 122; iter: 0; batch classifier loss: 0.078973; batch adversarial loss: 0.729662\n",
      "epoch 123; iter: 0; batch classifier loss: 0.130878; batch adversarial loss: 0.722941\n",
      "epoch 124; iter: 0; batch classifier loss: 0.082332; batch adversarial loss: 0.712253\n",
      "epoch 125; iter: 0; batch classifier loss: 0.106576; batch adversarial loss: 0.720114\n",
      "epoch 126; iter: 0; batch classifier loss: 0.091455; batch adversarial loss: 0.710164\n",
      "epoch 127; iter: 0; batch classifier loss: 0.131943; batch adversarial loss: 0.733029\n",
      "epoch 128; iter: 0; batch classifier loss: 0.092221; batch adversarial loss: 0.718239\n",
      "epoch 129; iter: 0; batch classifier loss: 0.074878; batch adversarial loss: 0.715556\n",
      "epoch 130; iter: 0; batch classifier loss: 0.089704; batch adversarial loss: 0.730599\n",
      "epoch 131; iter: 0; batch classifier loss: 0.095677; batch adversarial loss: 0.720942\n",
      "epoch 132; iter: 0; batch classifier loss: 0.101139; batch adversarial loss: 0.703032\n",
      "epoch 133; iter: 0; batch classifier loss: 0.137004; batch adversarial loss: 0.736787\n",
      "epoch 134; iter: 0; batch classifier loss: 0.119811; batch adversarial loss: 0.723696\n",
      "epoch 135; iter: 0; batch classifier loss: 0.090364; batch adversarial loss: 0.694496\n",
      "epoch 136; iter: 0; batch classifier loss: 0.079706; batch adversarial loss: 0.703521\n",
      "epoch 137; iter: 0; batch classifier loss: 0.081165; batch adversarial loss: 0.722515\n",
      "epoch 138; iter: 0; batch classifier loss: 0.101750; batch adversarial loss: 0.704877\n",
      "epoch 139; iter: 0; batch classifier loss: 0.101788; batch adversarial loss: 0.717220\n",
      "epoch 140; iter: 0; batch classifier loss: 0.070975; batch adversarial loss: 0.698646\n",
      "epoch 141; iter: 0; batch classifier loss: 0.076148; batch adversarial loss: 0.707920\n",
      "epoch 142; iter: 0; batch classifier loss: 0.146697; batch adversarial loss: 0.725857\n",
      "epoch 143; iter: 0; batch classifier loss: 0.085945; batch adversarial loss: 0.724662\n",
      "epoch 144; iter: 0; batch classifier loss: 0.102107; batch adversarial loss: 0.720842\n",
      "epoch 145; iter: 0; batch classifier loss: 0.056162; batch adversarial loss: 0.695019\n",
      "epoch 146; iter: 0; batch classifier loss: 0.122702; batch adversarial loss: 0.729356\n",
      "epoch 147; iter: 0; batch classifier loss: 0.102997; batch adversarial loss: 0.732616\n",
      "epoch 148; iter: 0; batch classifier loss: 0.090473; batch adversarial loss: 0.686693\n",
      "epoch 149; iter: 0; batch classifier loss: 0.053032; batch adversarial loss: 0.678535\n",
      "epoch 150; iter: 0; batch classifier loss: 0.106521; batch adversarial loss: 0.710915\n",
      "epoch 151; iter: 0; batch classifier loss: 0.119714; batch adversarial loss: 0.724724\n",
      "epoch 152; iter: 0; batch classifier loss: 0.082975; batch adversarial loss: 0.697699\n",
      "epoch 153; iter: 0; batch classifier loss: 0.073683; batch adversarial loss: 0.697156\n",
      "epoch 154; iter: 0; batch classifier loss: 0.090930; batch adversarial loss: 0.692076\n",
      "epoch 155; iter: 0; batch classifier loss: 0.119488; batch adversarial loss: 0.712163\n",
      "epoch 156; iter: 0; batch classifier loss: 0.082104; batch adversarial loss: 0.702291\n",
      "epoch 157; iter: 0; batch classifier loss: 0.101828; batch adversarial loss: 0.723579\n",
      "epoch 158; iter: 0; batch classifier loss: 0.098723; batch adversarial loss: 0.723841\n",
      "epoch 159; iter: 0; batch classifier loss: 0.086737; batch adversarial loss: 0.689134\n",
      "epoch 160; iter: 0; batch classifier loss: 0.119217; batch adversarial loss: 0.723697\n",
      "epoch 161; iter: 0; batch classifier loss: 0.043895; batch adversarial loss: 0.693694\n",
      "epoch 162; iter: 0; batch classifier loss: 0.065296; batch adversarial loss: 0.696144\n",
      "epoch 163; iter: 0; batch classifier loss: 0.057762; batch adversarial loss: 0.695441\n",
      "epoch 164; iter: 0; batch classifier loss: 0.116655; batch adversarial loss: 0.729230\n",
      "epoch 165; iter: 0; batch classifier loss: 0.114715; batch adversarial loss: 0.714485\n",
      "epoch 166; iter: 0; batch classifier loss: 0.069996; batch adversarial loss: 0.711086\n",
      "epoch 167; iter: 0; batch classifier loss: 0.047299; batch adversarial loss: 0.693513\n",
      "epoch 168; iter: 0; batch classifier loss: 0.096727; batch adversarial loss: 0.726983\n",
      "epoch 169; iter: 0; batch classifier loss: 0.061314; batch adversarial loss: 0.694564\n",
      "epoch 170; iter: 0; batch classifier loss: 0.116637; batch adversarial loss: 0.714747\n",
      "epoch 171; iter: 0; batch classifier loss: 0.048644; batch adversarial loss: 0.703316\n",
      "epoch 172; iter: 0; batch classifier loss: 0.146665; batch adversarial loss: 0.743630\n",
      "epoch 173; iter: 0; batch classifier loss: 0.085731; batch adversarial loss: 0.713836\n",
      "epoch 174; iter: 0; batch classifier loss: 0.099630; batch adversarial loss: 0.709455\n",
      "epoch 175; iter: 0; batch classifier loss: 0.086029; batch adversarial loss: 0.712963\n",
      "epoch 176; iter: 0; batch classifier loss: 0.078936; batch adversarial loss: 0.705852\n",
      "epoch 177; iter: 0; batch classifier loss: 0.076797; batch adversarial loss: 0.705003\n",
      "epoch 178; iter: 0; batch classifier loss: 0.095710; batch adversarial loss: 0.709465\n",
      "epoch 179; iter: 0; batch classifier loss: 0.077083; batch adversarial loss: 0.699210\n",
      "epoch 180; iter: 0; batch classifier loss: 0.079492; batch adversarial loss: 0.700272\n",
      "epoch 181; iter: 0; batch classifier loss: 0.079640; batch adversarial loss: 0.705967\n",
      "epoch 182; iter: 0; batch classifier loss: 0.142448; batch adversarial loss: 0.728743\n",
      "epoch 183; iter: 0; batch classifier loss: 0.083245; batch adversarial loss: 0.712492\n",
      "epoch 184; iter: 0; batch classifier loss: 0.082382; batch adversarial loss: 0.701071\n",
      "epoch 185; iter: 0; batch classifier loss: 0.113554; batch adversarial loss: 0.699043\n",
      "epoch 186; iter: 0; batch classifier loss: 0.086990; batch adversarial loss: 0.684969\n",
      "epoch 187; iter: 0; batch classifier loss: 0.089526; batch adversarial loss: 0.688443\n",
      "epoch 188; iter: 0; batch classifier loss: 0.076472; batch adversarial loss: 0.696680\n",
      "epoch 189; iter: 0; batch classifier loss: 0.082209; batch adversarial loss: 0.701012\n",
      "epoch 190; iter: 0; batch classifier loss: 0.112588; batch adversarial loss: 0.706558\n",
      "epoch 191; iter: 0; batch classifier loss: 0.056097; batch adversarial loss: 0.691356\n",
      "epoch 192; iter: 0; batch classifier loss: 0.115710; batch adversarial loss: 0.710467\n",
      "epoch 193; iter: 0; batch classifier loss: 0.099980; batch adversarial loss: 0.714794\n",
      "epoch 194; iter: 0; batch classifier loss: 0.089825; batch adversarial loss: 0.700092\n",
      "epoch 195; iter: 0; batch classifier loss: 0.104460; batch adversarial loss: 0.703010\n",
      "epoch 196; iter: 0; batch classifier loss: 0.101297; batch adversarial loss: 0.686248\n",
      "epoch 197; iter: 0; batch classifier loss: 0.068725; batch adversarial loss: 0.715808\n",
      "epoch 198; iter: 0; batch classifier loss: 0.095263; batch adversarial loss: 0.707788\n",
      "epoch 199; iter: 0; batch classifier loss: 0.073092; batch adversarial loss: 0.697209\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:22:58.462015: W tensorflow/c/c_api.cc:304] Operation '{name:'4c9699b0-adee-11ee-9362-a9d23602d4a3/4c9699b0-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:12082 op device:{requested: '', assigned: ''} def:{{{node 4c9699b0-adee-11ee-9362-a9d23602d4a3/4c9699b0-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c9699b0-adee-11ee-9362-a9d23602d4a3/4c9699b0-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c9699b0-adee-11ee-9362-a9d23602d4a3/4c9699b0-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.681997; batch adversarial loss: 0.777764\n",
      "epoch 1; iter: 0; batch classifier loss: 0.595547; batch adversarial loss: 0.793548\n",
      "epoch 2; iter: 0; batch classifier loss: 0.541081; batch adversarial loss: 0.813110\n",
      "epoch 3; iter: 0; batch classifier loss: 0.427878; batch adversarial loss: 0.874333\n",
      "epoch 4; iter: 0; batch classifier loss: 0.442272; batch adversarial loss: 0.839510\n",
      "epoch 5; iter: 0; batch classifier loss: 0.369399; batch adversarial loss: 0.855605\n",
      "epoch 6; iter: 0; batch classifier loss: 0.311137; batch adversarial loss: 0.846744\n",
      "epoch 7; iter: 0; batch classifier loss: 0.304850; batch adversarial loss: 0.853323\n",
      "epoch 8; iter: 0; batch classifier loss: 0.342820; batch adversarial loss: 0.837143\n",
      "epoch 9; iter: 0; batch classifier loss: 0.296717; batch adversarial loss: 0.808959\n",
      "epoch 10; iter: 0; batch classifier loss: 0.280409; batch adversarial loss: 0.835757\n",
      "epoch 11; iter: 0; batch classifier loss: 0.290132; batch adversarial loss: 0.858983\n",
      "epoch 12; iter: 0; batch classifier loss: 0.243914; batch adversarial loss: 0.821814\n",
      "epoch 13; iter: 0; batch classifier loss: 0.194541; batch adversarial loss: 0.852963\n",
      "epoch 14; iter: 0; batch classifier loss: 0.244923; batch adversarial loss: 0.831012\n",
      "epoch 15; iter: 0; batch classifier loss: 0.191999; batch adversarial loss: 0.865864\n",
      "epoch 16; iter: 0; batch classifier loss: 0.181994; batch adversarial loss: 0.867535\n",
      "epoch 17; iter: 0; batch classifier loss: 0.219598; batch adversarial loss: 0.858091\n",
      "epoch 18; iter: 0; batch classifier loss: 0.153398; batch adversarial loss: 0.892504\n",
      "epoch 19; iter: 0; batch classifier loss: 0.189119; batch adversarial loss: 0.816311\n",
      "epoch 20; iter: 0; batch classifier loss: 0.152020; batch adversarial loss: 0.877601\n",
      "epoch 21; iter: 0; batch classifier loss: 0.179452; batch adversarial loss: 0.867893\n",
      "epoch 22; iter: 0; batch classifier loss: 0.116311; batch adversarial loss: 0.858549\n",
      "epoch 23; iter: 0; batch classifier loss: 0.135971; batch adversarial loss: 0.825152\n",
      "epoch 24; iter: 0; batch classifier loss: 0.147241; batch adversarial loss: 0.839477\n",
      "epoch 25; iter: 0; batch classifier loss: 0.144128; batch adversarial loss: 0.822519\n",
      "epoch 26; iter: 0; batch classifier loss: 0.164776; batch adversarial loss: 0.845627\n",
      "epoch 27; iter: 0; batch classifier loss: 0.124570; batch adversarial loss: 0.838774\n",
      "epoch 28; iter: 0; batch classifier loss: 0.102570; batch adversarial loss: 0.843490\n",
      "epoch 29; iter: 0; batch classifier loss: 0.108265; batch adversarial loss: 0.836881\n",
      "epoch 30; iter: 0; batch classifier loss: 0.087320; batch adversarial loss: 0.868006\n",
      "epoch 31; iter: 0; batch classifier loss: 0.090190; batch adversarial loss: 0.815240\n",
      "epoch 32; iter: 0; batch classifier loss: 0.096762; batch adversarial loss: 0.836307\n",
      "epoch 33; iter: 0; batch classifier loss: 0.084619; batch adversarial loss: 0.805914\n",
      "epoch 34; iter: 0; batch classifier loss: 0.109767; batch adversarial loss: 0.794864\n",
      "epoch 35; iter: 0; batch classifier loss: 0.093875; batch adversarial loss: 0.811674\n",
      "epoch 36; iter: 0; batch classifier loss: 0.093347; batch adversarial loss: 0.772578\n",
      "epoch 37; iter: 0; batch classifier loss: 0.114499; batch adversarial loss: 0.834874\n",
      "epoch 38; iter: 0; batch classifier loss: 0.105026; batch adversarial loss: 0.783694\n",
      "epoch 39; iter: 0; batch classifier loss: 0.101293; batch adversarial loss: 0.812629\n",
      "epoch 40; iter: 0; batch classifier loss: 0.086079; batch adversarial loss: 0.778432\n",
      "epoch 41; iter: 0; batch classifier loss: 0.092337; batch adversarial loss: 0.770177\n",
      "epoch 42; iter: 0; batch classifier loss: 0.076437; batch adversarial loss: 0.798332\n",
      "epoch 43; iter: 0; batch classifier loss: 0.084914; batch adversarial loss: 0.826428\n",
      "epoch 44; iter: 0; batch classifier loss: 0.053624; batch adversarial loss: 0.795729\n",
      "epoch 45; iter: 0; batch classifier loss: 0.081915; batch adversarial loss: 0.777260\n",
      "epoch 46; iter: 0; batch classifier loss: 0.064851; batch adversarial loss: 0.817157\n",
      "epoch 47; iter: 0; batch classifier loss: 0.075916; batch adversarial loss: 0.776985\n",
      "epoch 48; iter: 0; batch classifier loss: 0.042056; batch adversarial loss: 0.797160\n",
      "epoch 49; iter: 0; batch classifier loss: 0.053841; batch adversarial loss: 0.812662\n",
      "epoch 50; iter: 0; batch classifier loss: 0.075986; batch adversarial loss: 0.784427\n",
      "epoch 51; iter: 0; batch classifier loss: 0.066117; batch adversarial loss: 0.774935\n",
      "epoch 52; iter: 0; batch classifier loss: 0.035960; batch adversarial loss: 0.776244\n",
      "epoch 53; iter: 0; batch classifier loss: 0.068172; batch adversarial loss: 0.777804\n",
      "epoch 54; iter: 0; batch classifier loss: 0.049596; batch adversarial loss: 0.788179\n",
      "epoch 55; iter: 0; batch classifier loss: 0.047593; batch adversarial loss: 0.750326\n",
      "epoch 56; iter: 0; batch classifier loss: 0.046411; batch adversarial loss: 0.755404\n",
      "epoch 57; iter: 0; batch classifier loss: 0.064676; batch adversarial loss: 0.774906\n",
      "epoch 58; iter: 0; batch classifier loss: 0.047005; batch adversarial loss: 0.724920\n",
      "epoch 59; iter: 0; batch classifier loss: 0.062435; batch adversarial loss: 0.758128\n",
      "epoch 60; iter: 0; batch classifier loss: 0.052934; batch adversarial loss: 0.766396\n",
      "epoch 61; iter: 0; batch classifier loss: 0.033019; batch adversarial loss: 0.777830\n",
      "epoch 62; iter: 0; batch classifier loss: 0.045760; batch adversarial loss: 0.756818\n",
      "epoch 63; iter: 0; batch classifier loss: 0.037424; batch adversarial loss: 0.756116\n",
      "epoch 64; iter: 0; batch classifier loss: 0.052096; batch adversarial loss: 0.767998\n",
      "epoch 65; iter: 0; batch classifier loss: 0.042575; batch adversarial loss: 0.784169\n",
      "epoch 66; iter: 0; batch classifier loss: 0.055803; batch adversarial loss: 0.728354\n",
      "epoch 67; iter: 0; batch classifier loss: 0.060687; batch adversarial loss: 0.769077\n",
      "epoch 68; iter: 0; batch classifier loss: 0.032740; batch adversarial loss: 0.771474\n",
      "epoch 69; iter: 0; batch classifier loss: 0.039424; batch adversarial loss: 0.747841\n",
      "epoch 70; iter: 0; batch classifier loss: 0.018489; batch adversarial loss: 0.755803\n",
      "epoch 71; iter: 0; batch classifier loss: 0.044666; batch adversarial loss: 0.742153\n",
      "epoch 72; iter: 0; batch classifier loss: 0.029599; batch adversarial loss: 0.766794\n",
      "epoch 73; iter: 0; batch classifier loss: 0.033565; batch adversarial loss: 0.751504\n",
      "epoch 74; iter: 0; batch classifier loss: 0.025648; batch adversarial loss: 0.755804\n",
      "epoch 75; iter: 0; batch classifier loss: 0.049206; batch adversarial loss: 0.736264\n",
      "epoch 76; iter: 0; batch classifier loss: 0.038920; batch adversarial loss: 0.731744\n",
      "epoch 77; iter: 0; batch classifier loss: 0.036127; batch adversarial loss: 0.730698\n",
      "epoch 78; iter: 0; batch classifier loss: 0.038589; batch adversarial loss: 0.740435\n",
      "epoch 79; iter: 0; batch classifier loss: 0.040144; batch adversarial loss: 0.746759\n",
      "epoch 80; iter: 0; batch classifier loss: 0.044968; batch adversarial loss: 0.726016\n",
      "epoch 81; iter: 0; batch classifier loss: 0.027929; batch adversarial loss: 0.727603\n",
      "epoch 82; iter: 0; batch classifier loss: 0.042556; batch adversarial loss: 0.726147\n",
      "epoch 83; iter: 0; batch classifier loss: 0.043148; batch adversarial loss: 0.743605\n",
      "epoch 84; iter: 0; batch classifier loss: 0.027544; batch adversarial loss: 0.759483\n",
      "epoch 85; iter: 0; batch classifier loss: 0.038997; batch adversarial loss: 0.738663\n",
      "epoch 86; iter: 0; batch classifier loss: 0.037948; batch adversarial loss: 0.739675\n",
      "epoch 87; iter: 0; batch classifier loss: 0.027378; batch adversarial loss: 0.738598\n",
      "epoch 88; iter: 0; batch classifier loss: 0.030452; batch adversarial loss: 0.725273\n",
      "epoch 89; iter: 0; batch classifier loss: 0.021998; batch adversarial loss: 0.724016\n",
      "epoch 90; iter: 0; batch classifier loss: 0.028916; batch adversarial loss: 0.727253\n",
      "epoch 91; iter: 0; batch classifier loss: 0.026825; batch adversarial loss: 0.725004\n",
      "epoch 92; iter: 0; batch classifier loss: 0.030168; batch adversarial loss: 0.719569\n",
      "epoch 93; iter: 0; batch classifier loss: 0.031028; batch adversarial loss: 0.714866\n",
      "epoch 94; iter: 0; batch classifier loss: 0.032887; batch adversarial loss: 0.717292\n",
      "epoch 95; iter: 0; batch classifier loss: 0.020230; batch adversarial loss: 0.732346\n",
      "epoch 96; iter: 0; batch classifier loss: 0.024571; batch adversarial loss: 0.724605\n",
      "epoch 97; iter: 0; batch classifier loss: 0.016894; batch adversarial loss: 0.717809\n",
      "epoch 98; iter: 0; batch classifier loss: 0.024820; batch adversarial loss: 0.717098\n",
      "epoch 99; iter: 0; batch classifier loss: 0.021819; batch adversarial loss: 0.713454\n",
      "epoch 100; iter: 0; batch classifier loss: 0.031549; batch adversarial loss: 0.720100\n",
      "epoch 101; iter: 0; batch classifier loss: 0.017559; batch adversarial loss: 0.709592\n",
      "epoch 102; iter: 0; batch classifier loss: 0.037350; batch adversarial loss: 0.726168\n",
      "epoch 103; iter: 0; batch classifier loss: 0.010663; batch adversarial loss: 0.714537\n",
      "epoch 104; iter: 0; batch classifier loss: 0.013356; batch adversarial loss: 0.707307\n",
      "epoch 105; iter: 0; batch classifier loss: 0.024728; batch adversarial loss: 0.711815\n",
      "epoch 106; iter: 0; batch classifier loss: 0.033282; batch adversarial loss: 0.709308\n",
      "epoch 107; iter: 0; batch classifier loss: 0.016738; batch adversarial loss: 0.710284\n",
      "epoch 108; iter: 0; batch classifier loss: 0.019335; batch adversarial loss: 0.701109\n",
      "epoch 109; iter: 0; batch classifier loss: 0.016252; batch adversarial loss: 0.710562\n",
      "epoch 110; iter: 0; batch classifier loss: 0.015528; batch adversarial loss: 0.699856\n",
      "epoch 111; iter: 0; batch classifier loss: 0.032160; batch adversarial loss: 0.706690\n",
      "epoch 112; iter: 0; batch classifier loss: 0.010149; batch adversarial loss: 0.698380\n",
      "epoch 113; iter: 0; batch classifier loss: 0.017552; batch adversarial loss: 0.703494\n",
      "epoch 114; iter: 0; batch classifier loss: 0.022424; batch adversarial loss: 0.697399\n",
      "epoch 115; iter: 0; batch classifier loss: 0.018603; batch adversarial loss: 0.704850\n",
      "epoch 116; iter: 0; batch classifier loss: 0.031060; batch adversarial loss: 0.704956\n",
      "epoch 117; iter: 0; batch classifier loss: 0.017105; batch adversarial loss: 0.699363\n",
      "epoch 118; iter: 0; batch classifier loss: 0.030720; batch adversarial loss: 0.693699\n",
      "epoch 119; iter: 0; batch classifier loss: 0.027532; batch adversarial loss: 0.702925\n",
      "epoch 120; iter: 0; batch classifier loss: 0.012277; batch adversarial loss: 0.696552\n",
      "epoch 121; iter: 0; batch classifier loss: 0.030041; batch adversarial loss: 0.702553\n",
      "epoch 122; iter: 0; batch classifier loss: 0.031933; batch adversarial loss: 0.691405\n",
      "epoch 123; iter: 0; batch classifier loss: 0.016246; batch adversarial loss: 0.687500\n",
      "epoch 124; iter: 0; batch classifier loss: 0.031207; batch adversarial loss: 0.696292\n",
      "epoch 125; iter: 0; batch classifier loss: 0.013555; batch adversarial loss: 0.704161\n",
      "epoch 126; iter: 0; batch classifier loss: 0.016376; batch adversarial loss: 0.696919\n",
      "epoch 127; iter: 0; batch classifier loss: 0.020336; batch adversarial loss: 0.696359\n",
      "epoch 128; iter: 0; batch classifier loss: 0.021899; batch adversarial loss: 0.686871\n",
      "epoch 129; iter: 0; batch classifier loss: 0.021292; batch adversarial loss: 0.698545\n",
      "epoch 130; iter: 0; batch classifier loss: 0.029813; batch adversarial loss: 0.695220\n",
      "epoch 131; iter: 0; batch classifier loss: 0.018298; batch adversarial loss: 0.688005\n",
      "epoch 132; iter: 0; batch classifier loss: 0.024707; batch adversarial loss: 0.686767\n",
      "epoch 133; iter: 0; batch classifier loss: 0.016977; batch adversarial loss: 0.694440\n",
      "epoch 134; iter: 0; batch classifier loss: 0.025331; batch adversarial loss: 0.692217\n",
      "epoch 135; iter: 0; batch classifier loss: 0.029770; batch adversarial loss: 0.693766\n",
      "epoch 136; iter: 0; batch classifier loss: 0.014689; batch adversarial loss: 0.689768\n",
      "epoch 137; iter: 0; batch classifier loss: 0.012917; batch adversarial loss: 0.685781\n",
      "epoch 138; iter: 0; batch classifier loss: 0.027541; batch adversarial loss: 0.686944\n",
      "epoch 139; iter: 0; batch classifier loss: 0.022865; batch adversarial loss: 0.695830\n",
      "epoch 140; iter: 0; batch classifier loss: 0.014989; batch adversarial loss: 0.688182\n",
      "epoch 141; iter: 0; batch classifier loss: 0.025067; batch adversarial loss: 0.688435\n",
      "epoch 142; iter: 0; batch classifier loss: 0.015575; batch adversarial loss: 0.690146\n",
      "epoch 143; iter: 0; batch classifier loss: 0.032088; batch adversarial loss: 0.687949\n",
      "epoch 144; iter: 0; batch classifier loss: 0.019549; batch adversarial loss: 0.688076\n",
      "epoch 145; iter: 0; batch classifier loss: 0.022259; batch adversarial loss: 0.684024\n",
      "epoch 146; iter: 0; batch classifier loss: 0.019793; batch adversarial loss: 0.682541\n",
      "epoch 147; iter: 0; batch classifier loss: 0.017530; batch adversarial loss: 0.690374\n",
      "epoch 148; iter: 0; batch classifier loss: 0.013531; batch adversarial loss: 0.679841\n",
      "epoch 149; iter: 0; batch classifier loss: 0.011308; batch adversarial loss: 0.684828\n",
      "epoch 150; iter: 0; batch classifier loss: 0.024543; batch adversarial loss: 0.691026\n",
      "epoch 151; iter: 0; batch classifier loss: 0.020785; batch adversarial loss: 0.685307\n",
      "epoch 152; iter: 0; batch classifier loss: 0.030801; batch adversarial loss: 0.677072\n",
      "epoch 153; iter: 0; batch classifier loss: 0.016413; batch adversarial loss: 0.672980\n",
      "epoch 154; iter: 0; batch classifier loss: 0.030065; batch adversarial loss: 0.680047\n",
      "epoch 155; iter: 0; batch classifier loss: 0.020214; batch adversarial loss: 0.680069\n",
      "epoch 156; iter: 0; batch classifier loss: 0.023364; batch adversarial loss: 0.682564\n",
      "epoch 157; iter: 0; batch classifier loss: 0.016797; batch adversarial loss: 0.686778\n",
      "epoch 158; iter: 0; batch classifier loss: 0.010995; batch adversarial loss: 0.682660\n",
      "epoch 159; iter: 0; batch classifier loss: 0.017870; batch adversarial loss: 0.684927\n",
      "epoch 160; iter: 0; batch classifier loss: 0.017111; batch adversarial loss: 0.684608\n",
      "epoch 161; iter: 0; batch classifier loss: 0.010789; batch adversarial loss: 0.680550\n",
      "epoch 162; iter: 0; batch classifier loss: 0.033269; batch adversarial loss: 0.669793\n",
      "epoch 163; iter: 0; batch classifier loss: 0.009892; batch adversarial loss: 0.666744\n",
      "epoch 164; iter: 0; batch classifier loss: 0.011451; batch adversarial loss: 0.679117\n",
      "epoch 165; iter: 0; batch classifier loss: 0.007401; batch adversarial loss: 0.666349\n",
      "epoch 166; iter: 0; batch classifier loss: 0.013990; batch adversarial loss: 0.673891\n",
      "epoch 167; iter: 0; batch classifier loss: 0.007502; batch adversarial loss: 0.681312\n",
      "epoch 168; iter: 0; batch classifier loss: 0.007945; batch adversarial loss: 0.675339\n",
      "epoch 169; iter: 0; batch classifier loss: 0.010763; batch adversarial loss: 0.675763\n",
      "epoch 170; iter: 0; batch classifier loss: 0.022930; batch adversarial loss: 0.685177\n",
      "epoch 171; iter: 0; batch classifier loss: 0.004317; batch adversarial loss: 0.670312\n",
      "epoch 172; iter: 0; batch classifier loss: 0.025421; batch adversarial loss: 0.681061\n",
      "epoch 173; iter: 0; batch classifier loss: 0.028035; batch adversarial loss: 0.669896\n",
      "epoch 174; iter: 0; batch classifier loss: 0.021983; batch adversarial loss: 0.676063\n",
      "epoch 175; iter: 0; batch classifier loss: 0.027029; batch adversarial loss: 0.674087\n",
      "epoch 176; iter: 0; batch classifier loss: 0.010908; batch adversarial loss: 0.671279\n",
      "epoch 177; iter: 0; batch classifier loss: 0.020917; batch adversarial loss: 0.670260\n",
      "epoch 178; iter: 0; batch classifier loss: 0.012420; batch adversarial loss: 0.684606\n",
      "epoch 179; iter: 0; batch classifier loss: 0.030436; batch adversarial loss: 0.685863\n",
      "epoch 180; iter: 0; batch classifier loss: 0.003421; batch adversarial loss: 0.671440\n",
      "epoch 181; iter: 0; batch classifier loss: 0.015735; batch adversarial loss: 0.672699\n",
      "epoch 182; iter: 0; batch classifier loss: 0.016563; batch adversarial loss: 0.677905\n",
      "epoch 183; iter: 0; batch classifier loss: 0.014548; batch adversarial loss: 0.676690\n",
      "epoch 184; iter: 0; batch classifier loss: 0.014812; batch adversarial loss: 0.677789\n",
      "epoch 185; iter: 0; batch classifier loss: 0.015569; batch adversarial loss: 0.689202\n",
      "epoch 186; iter: 0; batch classifier loss: 0.008196; batch adversarial loss: 0.651426\n",
      "epoch 187; iter: 0; batch classifier loss: 0.005057; batch adversarial loss: 0.667973\n",
      "epoch 188; iter: 0; batch classifier loss: 0.019336; batch adversarial loss: 0.673920\n",
      "epoch 189; iter: 0; batch classifier loss: 0.009753; batch adversarial loss: 0.682229\n",
      "epoch 190; iter: 0; batch classifier loss: 0.010801; batch adversarial loss: 0.679929\n",
      "epoch 191; iter: 0; batch classifier loss: 0.014412; batch adversarial loss: 0.671332\n",
      "epoch 192; iter: 0; batch classifier loss: 0.020182; batch adversarial loss: 0.668574\n",
      "epoch 193; iter: 0; batch classifier loss: 0.011478; batch adversarial loss: 0.678266\n",
      "epoch 194; iter: 0; batch classifier loss: 0.012916; batch adversarial loss: 0.660851\n",
      "epoch 195; iter: 0; batch classifier loss: 0.014506; batch adversarial loss: 0.674394\n",
      "epoch 196; iter: 0; batch classifier loss: 0.012803; batch adversarial loss: 0.664410\n",
      "epoch 197; iter: 0; batch classifier loss: 0.009728; batch adversarial loss: 0.673667\n",
      "epoch 198; iter: 0; batch classifier loss: 0.019663; batch adversarial loss: 0.670105\n",
      "epoch 199; iter: 0; batch classifier loss: 0.003663; batch adversarial loss: 0.681915\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:23:00.877600: W tensorflow/c/c_api.cc:304] Operation '{name:'4c969a32-adee-11ee-9362-a9d23602d4a3/4c969a32-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:12889 op device:{requested: '', assigned: ''} def:{{{node 4c969a32-adee-11ee-9362-a9d23602d4a3/4c969a32-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c969a32-adee-11ee-9362-a9d23602d4a3/4c969a32-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c969a32-adee-11ee-9362-a9d23602d4a3/4c969a32-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.633510; batch adversarial loss: 0.852231\n",
      "epoch 1; iter: 0; batch classifier loss: 0.579872; batch adversarial loss: 0.841032\n",
      "epoch 2; iter: 0; batch classifier loss: 0.524106; batch adversarial loss: 0.883281\n",
      "epoch 3; iter: 0; batch classifier loss: 0.489025; batch adversarial loss: 0.898109\n",
      "epoch 4; iter: 0; batch classifier loss: 0.461554; batch adversarial loss: 0.830656\n",
      "epoch 5; iter: 0; batch classifier loss: 0.388446; batch adversarial loss: 0.849947\n",
      "epoch 6; iter: 0; batch classifier loss: 0.404112; batch adversarial loss: 0.819480\n",
      "epoch 7; iter: 0; batch classifier loss: 0.388131; batch adversarial loss: 0.911285\n",
      "epoch 8; iter: 0; batch classifier loss: 0.349508; batch adversarial loss: 0.881613\n",
      "epoch 9; iter: 0; batch classifier loss: 0.332973; batch adversarial loss: 0.916285\n",
      "epoch 10; iter: 0; batch classifier loss: 0.324949; batch adversarial loss: 0.971501\n",
      "epoch 11; iter: 0; batch classifier loss: 0.308364; batch adversarial loss: 0.879702\n",
      "epoch 12; iter: 0; batch classifier loss: 0.246224; batch adversarial loss: 0.950747\n",
      "epoch 13; iter: 0; batch classifier loss: 0.282510; batch adversarial loss: 0.898496\n",
      "epoch 14; iter: 0; batch classifier loss: 0.227038; batch adversarial loss: 0.883523\n",
      "epoch 15; iter: 0; batch classifier loss: 0.239855; batch adversarial loss: 0.860081\n",
      "epoch 16; iter: 0; batch classifier loss: 0.209011; batch adversarial loss: 0.942385\n",
      "epoch 17; iter: 0; batch classifier loss: 0.222139; batch adversarial loss: 0.888514\n",
      "epoch 18; iter: 0; batch classifier loss: 0.209726; batch adversarial loss: 0.868607\n",
      "epoch 19; iter: 0; batch classifier loss: 0.171917; batch adversarial loss: 0.895473\n",
      "epoch 20; iter: 0; batch classifier loss: 0.175218; batch adversarial loss: 0.812533\n",
      "epoch 21; iter: 0; batch classifier loss: 0.203979; batch adversarial loss: 0.864644\n",
      "epoch 22; iter: 0; batch classifier loss: 0.139671; batch adversarial loss: 0.888284\n",
      "epoch 23; iter: 0; batch classifier loss: 0.218487; batch adversarial loss: 0.869693\n",
      "epoch 24; iter: 0; batch classifier loss: 0.179698; batch adversarial loss: 0.922114\n",
      "epoch 25; iter: 0; batch classifier loss: 0.141441; batch adversarial loss: 0.939412\n",
      "epoch 26; iter: 0; batch classifier loss: 0.140276; batch adversarial loss: 0.964045\n",
      "epoch 27; iter: 0; batch classifier loss: 0.131480; batch adversarial loss: 0.905841\n",
      "epoch 28; iter: 0; batch classifier loss: 0.127978; batch adversarial loss: 0.872060\n",
      "epoch 29; iter: 0; batch classifier loss: 0.123843; batch adversarial loss: 0.871355\n",
      "epoch 30; iter: 0; batch classifier loss: 0.107910; batch adversarial loss: 0.878816\n",
      "epoch 31; iter: 0; batch classifier loss: 0.091610; batch adversarial loss: 0.908794\n",
      "epoch 32; iter: 0; batch classifier loss: 0.096573; batch adversarial loss: 0.933535\n",
      "epoch 33; iter: 0; batch classifier loss: 0.095985; batch adversarial loss: 0.858869\n",
      "epoch 34; iter: 0; batch classifier loss: 0.100230; batch adversarial loss: 0.855743\n",
      "epoch 35; iter: 0; batch classifier loss: 0.092939; batch adversarial loss: 0.910718\n",
      "epoch 36; iter: 0; batch classifier loss: 0.069756; batch adversarial loss: 0.849441\n",
      "epoch 37; iter: 0; batch classifier loss: 0.080652; batch adversarial loss: 0.822077\n",
      "epoch 38; iter: 0; batch classifier loss: 0.076349; batch adversarial loss: 0.902554\n",
      "epoch 39; iter: 0; batch classifier loss: 0.087597; batch adversarial loss: 0.828435\n",
      "epoch 40; iter: 0; batch classifier loss: 0.099648; batch adversarial loss: 0.842369\n",
      "epoch 41; iter: 0; batch classifier loss: 0.084167; batch adversarial loss: 0.850329\n",
      "epoch 42; iter: 0; batch classifier loss: 0.077672; batch adversarial loss: 0.822273\n",
      "epoch 43; iter: 0; batch classifier loss: 0.070113; batch adversarial loss: 0.900891\n",
      "epoch 44; iter: 0; batch classifier loss: 0.062883; batch adversarial loss: 0.865877\n",
      "epoch 45; iter: 0; batch classifier loss: 0.056425; batch adversarial loss: 0.791991\n",
      "epoch 46; iter: 0; batch classifier loss: 0.080555; batch adversarial loss: 0.860248\n",
      "epoch 47; iter: 0; batch classifier loss: 0.048640; batch adversarial loss: 0.827141\n",
      "epoch 48; iter: 0; batch classifier loss: 0.063359; batch adversarial loss: 0.866028\n",
      "epoch 49; iter: 0; batch classifier loss: 0.054416; batch adversarial loss: 0.821769\n",
      "epoch 50; iter: 0; batch classifier loss: 0.068382; batch adversarial loss: 0.816916\n",
      "epoch 51; iter: 0; batch classifier loss: 0.076964; batch adversarial loss: 0.812137\n",
      "epoch 52; iter: 0; batch classifier loss: 0.050189; batch adversarial loss: 0.852808\n",
      "epoch 53; iter: 0; batch classifier loss: 0.050173; batch adversarial loss: 0.839946\n",
      "epoch 54; iter: 0; batch classifier loss: 0.062353; batch adversarial loss: 0.832933\n",
      "epoch 55; iter: 0; batch classifier loss: 0.045215; batch adversarial loss: 0.857263\n",
      "epoch 56; iter: 0; batch classifier loss: 0.062751; batch adversarial loss: 0.813215\n",
      "epoch 57; iter: 0; batch classifier loss: 0.052342; batch adversarial loss: 0.800661\n",
      "epoch 58; iter: 0; batch classifier loss: 0.053246; batch adversarial loss: 0.804643\n",
      "epoch 59; iter: 0; batch classifier loss: 0.035887; batch adversarial loss: 0.804669\n",
      "epoch 60; iter: 0; batch classifier loss: 0.041426; batch adversarial loss: 0.834450\n",
      "epoch 61; iter: 0; batch classifier loss: 0.043715; batch adversarial loss: 0.823530\n",
      "epoch 62; iter: 0; batch classifier loss: 0.046784; batch adversarial loss: 0.805233\n",
      "epoch 63; iter: 0; batch classifier loss: 0.032062; batch adversarial loss: 0.814555\n",
      "epoch 64; iter: 0; batch classifier loss: 0.046256; batch adversarial loss: 0.854139\n",
      "epoch 65; iter: 0; batch classifier loss: 0.054630; batch adversarial loss: 0.791840\n",
      "epoch 66; iter: 0; batch classifier loss: 0.039064; batch adversarial loss: 0.778691\n",
      "epoch 67; iter: 0; batch classifier loss: 0.041547; batch adversarial loss: 0.806399\n",
      "epoch 68; iter: 0; batch classifier loss: 0.027883; batch adversarial loss: 0.773520\n",
      "epoch 69; iter: 0; batch classifier loss: 0.038855; batch adversarial loss: 0.823413\n",
      "epoch 70; iter: 0; batch classifier loss: 0.030625; batch adversarial loss: 0.803177\n",
      "epoch 71; iter: 0; batch classifier loss: 0.050720; batch adversarial loss: 0.817739\n",
      "epoch 72; iter: 0; batch classifier loss: 0.033950; batch adversarial loss: 0.774031\n",
      "epoch 73; iter: 0; batch classifier loss: 0.033899; batch adversarial loss: 0.767625\n",
      "epoch 74; iter: 0; batch classifier loss: 0.043143; batch adversarial loss: 0.804306\n",
      "epoch 75; iter: 0; batch classifier loss: 0.040936; batch adversarial loss: 0.797318\n",
      "epoch 76; iter: 0; batch classifier loss: 0.020595; batch adversarial loss: 0.831953\n",
      "epoch 77; iter: 0; batch classifier loss: 0.043694; batch adversarial loss: 0.777351\n",
      "epoch 78; iter: 0; batch classifier loss: 0.034821; batch adversarial loss: 0.807054\n",
      "epoch 79; iter: 0; batch classifier loss: 0.035597; batch adversarial loss: 0.788688\n",
      "epoch 80; iter: 0; batch classifier loss: 0.025188; batch adversarial loss: 0.770129\n",
      "epoch 81; iter: 0; batch classifier loss: 0.020805; batch adversarial loss: 0.797382\n",
      "epoch 82; iter: 0; batch classifier loss: 0.037390; batch adversarial loss: 0.774080\n",
      "epoch 83; iter: 0; batch classifier loss: 0.018949; batch adversarial loss: 0.765803\n",
      "epoch 84; iter: 0; batch classifier loss: 0.030990; batch adversarial loss: 0.790179\n",
      "epoch 85; iter: 0; batch classifier loss: 0.029904; batch adversarial loss: 0.778364\n",
      "epoch 86; iter: 0; batch classifier loss: 0.027859; batch adversarial loss: 0.745433\n",
      "epoch 87; iter: 0; batch classifier loss: 0.038619; batch adversarial loss: 0.761521\n",
      "epoch 88; iter: 0; batch classifier loss: 0.029592; batch adversarial loss: 0.768907\n",
      "epoch 89; iter: 0; batch classifier loss: 0.036757; batch adversarial loss: 0.816354\n",
      "epoch 90; iter: 0; batch classifier loss: 0.028765; batch adversarial loss: 0.777317\n",
      "epoch 91; iter: 0; batch classifier loss: 0.039144; batch adversarial loss: 0.763055\n",
      "epoch 92; iter: 0; batch classifier loss: 0.017580; batch adversarial loss: 0.759283\n",
      "epoch 93; iter: 0; batch classifier loss: 0.030403; batch adversarial loss: 0.765441\n",
      "epoch 94; iter: 0; batch classifier loss: 0.049659; batch adversarial loss: 0.765363\n",
      "epoch 95; iter: 0; batch classifier loss: 0.029392; batch adversarial loss: 0.741495\n",
      "epoch 96; iter: 0; batch classifier loss: 0.021493; batch adversarial loss: 0.760574\n",
      "epoch 97; iter: 0; batch classifier loss: 0.016033; batch adversarial loss: 0.758848\n",
      "epoch 98; iter: 0; batch classifier loss: 0.018948; batch adversarial loss: 0.742390\n",
      "epoch 99; iter: 0; batch classifier loss: 0.030483; batch adversarial loss: 0.762745\n",
      "epoch 100; iter: 0; batch classifier loss: 0.032213; batch adversarial loss: 0.757274\n",
      "epoch 101; iter: 0; batch classifier loss: 0.029787; batch adversarial loss: 0.766244\n",
      "epoch 102; iter: 0; batch classifier loss: 0.022045; batch adversarial loss: 0.732883\n",
      "epoch 103; iter: 0; batch classifier loss: 0.027295; batch adversarial loss: 0.749167\n",
      "epoch 104; iter: 0; batch classifier loss: 0.024707; batch adversarial loss: 0.732324\n",
      "epoch 105; iter: 0; batch classifier loss: 0.013743; batch adversarial loss: 0.729364\n",
      "epoch 106; iter: 0; batch classifier loss: 0.018194; batch adversarial loss: 0.757834\n",
      "epoch 107; iter: 0; batch classifier loss: 0.017330; batch adversarial loss: 0.752612\n",
      "epoch 108; iter: 0; batch classifier loss: 0.016761; batch adversarial loss: 0.768818\n",
      "epoch 109; iter: 0; batch classifier loss: 0.026708; batch adversarial loss: 0.738651\n",
      "epoch 110; iter: 0; batch classifier loss: 0.019101; batch adversarial loss: 0.726743\n",
      "epoch 111; iter: 0; batch classifier loss: 0.021640; batch adversarial loss: 0.739166\n",
      "epoch 112; iter: 0; batch classifier loss: 0.022912; batch adversarial loss: 0.733220\n",
      "epoch 113; iter: 0; batch classifier loss: 0.012486; batch adversarial loss: 0.731542\n",
      "epoch 114; iter: 0; batch classifier loss: 0.019414; batch adversarial loss: 0.729352\n",
      "epoch 115; iter: 0; batch classifier loss: 0.021079; batch adversarial loss: 0.744826\n",
      "epoch 116; iter: 0; batch classifier loss: 0.013568; batch adversarial loss: 0.763474\n",
      "epoch 117; iter: 0; batch classifier loss: 0.022547; batch adversarial loss: 0.723086\n",
      "epoch 118; iter: 0; batch classifier loss: 0.012729; batch adversarial loss: 0.729935\n",
      "epoch 119; iter: 0; batch classifier loss: 0.014580; batch adversarial loss: 0.720177\n",
      "epoch 120; iter: 0; batch classifier loss: 0.023652; batch adversarial loss: 0.750231\n",
      "epoch 121; iter: 0; batch classifier loss: 0.013583; batch adversarial loss: 0.736145\n",
      "epoch 122; iter: 0; batch classifier loss: 0.013943; batch adversarial loss: 0.732878\n",
      "epoch 123; iter: 0; batch classifier loss: 0.013759; batch adversarial loss: 0.741550\n",
      "epoch 124; iter: 0; batch classifier loss: 0.018337; batch adversarial loss: 0.731156\n",
      "epoch 125; iter: 0; batch classifier loss: 0.017774; batch adversarial loss: 0.719353\n",
      "epoch 126; iter: 0; batch classifier loss: 0.026236; batch adversarial loss: 0.724082\n",
      "epoch 127; iter: 0; batch classifier loss: 0.014074; batch adversarial loss: 0.737438\n",
      "epoch 128; iter: 0; batch classifier loss: 0.012571; batch adversarial loss: 0.717790\n",
      "epoch 129; iter: 0; batch classifier loss: 0.009558; batch adversarial loss: 0.724321\n",
      "epoch 130; iter: 0; batch classifier loss: 0.023043; batch adversarial loss: 0.719512\n",
      "epoch 131; iter: 0; batch classifier loss: 0.017277; batch adversarial loss: 0.708009\n",
      "epoch 132; iter: 0; batch classifier loss: 0.011418; batch adversarial loss: 0.714299\n",
      "epoch 133; iter: 0; batch classifier loss: 0.007082; batch adversarial loss: 0.736054\n",
      "epoch 134; iter: 0; batch classifier loss: 0.011209; batch adversarial loss: 0.724548\n",
      "epoch 135; iter: 0; batch classifier loss: 0.014520; batch adversarial loss: 0.710153\n",
      "epoch 136; iter: 0; batch classifier loss: 0.012271; batch adversarial loss: 0.717154\n",
      "epoch 137; iter: 0; batch classifier loss: 0.006927; batch adversarial loss: 0.717893\n",
      "epoch 138; iter: 0; batch classifier loss: 0.015265; batch adversarial loss: 0.713475\n",
      "epoch 139; iter: 0; batch classifier loss: 0.004448; batch adversarial loss: 0.719579\n",
      "epoch 140; iter: 0; batch classifier loss: 0.010477; batch adversarial loss: 0.718411\n",
      "epoch 141; iter: 0; batch classifier loss: 0.010825; batch adversarial loss: 0.713627\n",
      "epoch 142; iter: 0; batch classifier loss: 0.022463; batch adversarial loss: 0.713922\n",
      "epoch 143; iter: 0; batch classifier loss: 0.008166; batch adversarial loss: 0.719056\n",
      "epoch 144; iter: 0; batch classifier loss: 0.010530; batch adversarial loss: 0.721593\n",
      "epoch 145; iter: 0; batch classifier loss: 0.012102; batch adversarial loss: 0.709951\n",
      "epoch 146; iter: 0; batch classifier loss: 0.011715; batch adversarial loss: 0.718993\n",
      "epoch 147; iter: 0; batch classifier loss: 0.015016; batch adversarial loss: 0.706884\n",
      "epoch 148; iter: 0; batch classifier loss: 0.008321; batch adversarial loss: 0.712750\n",
      "epoch 149; iter: 0; batch classifier loss: 0.012665; batch adversarial loss: 0.711574\n",
      "epoch 150; iter: 0; batch classifier loss: 0.012576; batch adversarial loss: 0.703280\n",
      "epoch 151; iter: 0; batch classifier loss: 0.013695; batch adversarial loss: 0.700928\n",
      "epoch 152; iter: 0; batch classifier loss: 0.009730; batch adversarial loss: 0.712658\n",
      "epoch 153; iter: 0; batch classifier loss: 0.011172; batch adversarial loss: 0.701119\n",
      "epoch 154; iter: 0; batch classifier loss: 0.007945; batch adversarial loss: 0.689441\n",
      "epoch 155; iter: 0; batch classifier loss: 0.008186; batch adversarial loss: 0.708810\n",
      "epoch 156; iter: 0; batch classifier loss: 0.013796; batch adversarial loss: 0.715460\n",
      "epoch 157; iter: 0; batch classifier loss: 0.006550; batch adversarial loss: 0.721515\n",
      "epoch 158; iter: 0; batch classifier loss: 0.003555; batch adversarial loss: 0.704215\n",
      "epoch 159; iter: 0; batch classifier loss: 0.003826; batch adversarial loss: 0.692324\n",
      "epoch 160; iter: 0; batch classifier loss: 0.023061; batch adversarial loss: 0.707194\n",
      "epoch 161; iter: 0; batch classifier loss: 0.010938; batch adversarial loss: 0.707937\n",
      "epoch 162; iter: 0; batch classifier loss: 0.008843; batch adversarial loss: 0.698288\n",
      "epoch 163; iter: 0; batch classifier loss: 0.005912; batch adversarial loss: 0.701704\n",
      "epoch 164; iter: 0; batch classifier loss: 0.009810; batch adversarial loss: 0.698822\n",
      "epoch 165; iter: 0; batch classifier loss: 0.009367; batch adversarial loss: 0.692802\n",
      "epoch 166; iter: 0; batch classifier loss: 0.006671; batch adversarial loss: 0.700342\n",
      "epoch 167; iter: 0; batch classifier loss: 0.008913; batch adversarial loss: 0.715727\n",
      "epoch 168; iter: 0; batch classifier loss: 0.016313; batch adversarial loss: 0.703957\n",
      "epoch 169; iter: 0; batch classifier loss: 0.003171; batch adversarial loss: 0.695088\n",
      "epoch 170; iter: 0; batch classifier loss: 0.004078; batch adversarial loss: 0.691004\n",
      "epoch 171; iter: 0; batch classifier loss: 0.002759; batch adversarial loss: 0.706757\n",
      "epoch 172; iter: 0; batch classifier loss: 0.008351; batch adversarial loss: 0.687459\n",
      "epoch 173; iter: 0; batch classifier loss: 0.012534; batch adversarial loss: 0.692790\n",
      "epoch 174; iter: 0; batch classifier loss: 0.006511; batch adversarial loss: 0.687388\n",
      "epoch 175; iter: 0; batch classifier loss: 0.004165; batch adversarial loss: 0.690950\n",
      "epoch 176; iter: 0; batch classifier loss: 0.008859; batch adversarial loss: 0.685545\n",
      "epoch 177; iter: 0; batch classifier loss: 0.007348; batch adversarial loss: 0.684765\n",
      "epoch 178; iter: 0; batch classifier loss: 0.004238; batch adversarial loss: 0.695967\n",
      "epoch 179; iter: 0; batch classifier loss: 0.013411; batch adversarial loss: 0.699148\n",
      "epoch 180; iter: 0; batch classifier loss: 0.009163; batch adversarial loss: 0.693900\n",
      "epoch 181; iter: 0; batch classifier loss: 0.010694; batch adversarial loss: 0.700212\n",
      "epoch 182; iter: 0; batch classifier loss: 0.017174; batch adversarial loss: 0.681424\n",
      "epoch 183; iter: 0; batch classifier loss: 0.007066; batch adversarial loss: 0.701605\n",
      "epoch 184; iter: 0; batch classifier loss: 0.015605; batch adversarial loss: 0.681089\n",
      "epoch 185; iter: 0; batch classifier loss: 0.006861; batch adversarial loss: 0.691672\n",
      "epoch 186; iter: 0; batch classifier loss: 0.005637; batch adversarial loss: 0.679100\n",
      "epoch 187; iter: 0; batch classifier loss: 0.005018; batch adversarial loss: 0.684154\n",
      "epoch 188; iter: 0; batch classifier loss: 0.004880; batch adversarial loss: 0.695443\n",
      "epoch 189; iter: 0; batch classifier loss: 0.008640; batch adversarial loss: 0.691013\n",
      "epoch 190; iter: 0; batch classifier loss: 0.003619; batch adversarial loss: 0.687424\n",
      "epoch 191; iter: 0; batch classifier loss: 0.005375; batch adversarial loss: 0.685041\n",
      "epoch 192; iter: 0; batch classifier loss: 0.009678; batch adversarial loss: 0.685598\n",
      "epoch 193; iter: 0; batch classifier loss: 0.007320; batch adversarial loss: 0.689586\n",
      "epoch 194; iter: 0; batch classifier loss: 0.007575; batch adversarial loss: 0.703493\n",
      "epoch 195; iter: 0; batch classifier loss: 0.007238; batch adversarial loss: 0.679931\n",
      "epoch 196; iter: 0; batch classifier loss: 0.007275; batch adversarial loss: 0.667529\n",
      "epoch 197; iter: 0; batch classifier loss: 0.006499; batch adversarial loss: 0.690254\n",
      "epoch 198; iter: 0; batch classifier loss: 0.009811; batch adversarial loss: 0.693571\n",
      "epoch 199; iter: 0; batch classifier loss: 0.004818; batch adversarial loss: 0.681249\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:23:03.479543: W tensorflow/c/c_api.cc:304] Operation '{name:'4c969abe-adee-11ee-9362-a9d23602d4a3/4c969abe-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:13696 op device:{requested: '', assigned: ''} def:{{{node 4c969abe-adee-11ee-9362-a9d23602d4a3/4c969abe-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c969abe-adee-11ee-9362-a9d23602d4a3/4c969abe-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c969abe-adee-11ee-9362-a9d23602d4a3/4c969abe-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.649626; batch adversarial loss: 0.783785\n",
      "epoch 1; iter: 0; batch classifier loss: 0.560425; batch adversarial loss: 0.794939\n",
      "epoch 2; iter: 0; batch classifier loss: 0.495022; batch adversarial loss: 0.781254\n",
      "epoch 3; iter: 0; batch classifier loss: 0.412965; batch adversarial loss: 0.757836\n",
      "epoch 4; iter: 0; batch classifier loss: 0.387241; batch adversarial loss: 0.799653\n",
      "epoch 5; iter: 0; batch classifier loss: 0.336759; batch adversarial loss: 0.777567\n",
      "epoch 6; iter: 0; batch classifier loss: 0.324549; batch adversarial loss: 0.756473\n",
      "epoch 7; iter: 0; batch classifier loss: 0.292962; batch adversarial loss: 0.792304\n",
      "epoch 8; iter: 0; batch classifier loss: 0.319721; batch adversarial loss: 0.768641\n",
      "epoch 9; iter: 0; batch classifier loss: 0.266903; batch adversarial loss: 0.783641\n",
      "epoch 10; iter: 0; batch classifier loss: 0.232107; batch adversarial loss: 0.790859\n",
      "epoch 11; iter: 0; batch classifier loss: 0.243938; batch adversarial loss: 0.783306\n",
      "epoch 12; iter: 0; batch classifier loss: 0.206463; batch adversarial loss: 0.779106\n",
      "epoch 13; iter: 0; batch classifier loss: 0.229089; batch adversarial loss: 0.805575\n",
      "epoch 14; iter: 0; batch classifier loss: 0.175064; batch adversarial loss: 0.790186\n",
      "epoch 15; iter: 0; batch classifier loss: 0.174830; batch adversarial loss: 0.777527\n",
      "epoch 16; iter: 0; batch classifier loss: 0.201971; batch adversarial loss: 0.786589\n",
      "epoch 17; iter: 0; batch classifier loss: 0.137376; batch adversarial loss: 0.796147\n",
      "epoch 18; iter: 0; batch classifier loss: 0.187936; batch adversarial loss: 0.738296\n",
      "epoch 19; iter: 0; batch classifier loss: 0.133644; batch adversarial loss: 0.786751\n",
      "epoch 20; iter: 0; batch classifier loss: 0.183467; batch adversarial loss: 0.725633\n",
      "epoch 21; iter: 0; batch classifier loss: 0.163801; batch adversarial loss: 0.768236\n",
      "epoch 22; iter: 0; batch classifier loss: 0.114894; batch adversarial loss: 0.817035\n",
      "epoch 23; iter: 0; batch classifier loss: 0.102810; batch adversarial loss: 0.791516\n",
      "epoch 24; iter: 0; batch classifier loss: 0.148360; batch adversarial loss: 0.791965\n",
      "epoch 25; iter: 0; batch classifier loss: 0.150202; batch adversarial loss: 0.784177\n",
      "epoch 26; iter: 0; batch classifier loss: 0.100822; batch adversarial loss: 0.780871\n",
      "epoch 27; iter: 0; batch classifier loss: 0.121728; batch adversarial loss: 0.739941\n",
      "epoch 28; iter: 0; batch classifier loss: 0.121789; batch adversarial loss: 0.775948\n",
      "epoch 29; iter: 0; batch classifier loss: 0.090741; batch adversarial loss: 0.768869\n",
      "epoch 30; iter: 0; batch classifier loss: 0.096288; batch adversarial loss: 0.760547\n",
      "epoch 31; iter: 0; batch classifier loss: 0.081985; batch adversarial loss: 0.750607\n",
      "epoch 32; iter: 0; batch classifier loss: 0.073412; batch adversarial loss: 0.733871\n",
      "epoch 33; iter: 0; batch classifier loss: 0.089913; batch adversarial loss: 0.768252\n",
      "epoch 34; iter: 0; batch classifier loss: 0.087061; batch adversarial loss: 0.758538\n",
      "epoch 35; iter: 0; batch classifier loss: 0.070436; batch adversarial loss: 0.778688\n",
      "epoch 36; iter: 0; batch classifier loss: 0.077621; batch adversarial loss: 0.719543\n",
      "epoch 37; iter: 0; batch classifier loss: 0.063485; batch adversarial loss: 0.761149\n",
      "epoch 38; iter: 0; batch classifier loss: 0.078025; batch adversarial loss: 0.750007\n",
      "epoch 39; iter: 0; batch classifier loss: 0.074273; batch adversarial loss: 0.786024\n",
      "epoch 40; iter: 0; batch classifier loss: 0.073462; batch adversarial loss: 0.762575\n",
      "epoch 41; iter: 0; batch classifier loss: 0.063133; batch adversarial loss: 0.759886\n",
      "epoch 42; iter: 0; batch classifier loss: 0.057882; batch adversarial loss: 0.739922\n",
      "epoch 43; iter: 0; batch classifier loss: 0.056997; batch adversarial loss: 0.755238\n",
      "epoch 44; iter: 0; batch classifier loss: 0.049299; batch adversarial loss: 0.758258\n",
      "epoch 45; iter: 0; batch classifier loss: 0.062656; batch adversarial loss: 0.724918\n",
      "epoch 46; iter: 0; batch classifier loss: 0.050814; batch adversarial loss: 0.751942\n",
      "epoch 47; iter: 0; batch classifier loss: 0.054034; batch adversarial loss: 0.724893\n",
      "epoch 48; iter: 0; batch classifier loss: 0.050925; batch adversarial loss: 0.754675\n",
      "epoch 49; iter: 0; batch classifier loss: 0.043205; batch adversarial loss: 0.749160\n",
      "epoch 50; iter: 0; batch classifier loss: 0.052250; batch adversarial loss: 0.727428\n",
      "epoch 51; iter: 0; batch classifier loss: 0.045074; batch adversarial loss: 0.745676\n",
      "epoch 52; iter: 0; batch classifier loss: 0.036016; batch adversarial loss: 0.730929\n",
      "epoch 53; iter: 0; batch classifier loss: 0.034383; batch adversarial loss: 0.728931\n",
      "epoch 54; iter: 0; batch classifier loss: 0.062361; batch adversarial loss: 0.734236\n",
      "epoch 55; iter: 0; batch classifier loss: 0.052397; batch adversarial loss: 0.743899\n",
      "epoch 56; iter: 0; batch classifier loss: 0.033194; batch adversarial loss: 0.741168\n",
      "epoch 57; iter: 0; batch classifier loss: 0.042130; batch adversarial loss: 0.739812\n",
      "epoch 58; iter: 0; batch classifier loss: 0.027975; batch adversarial loss: 0.738574\n",
      "epoch 59; iter: 0; batch classifier loss: 0.041094; batch adversarial loss: 0.739495\n",
      "epoch 60; iter: 0; batch classifier loss: 0.036749; batch adversarial loss: 0.715060\n",
      "epoch 61; iter: 0; batch classifier loss: 0.035892; batch adversarial loss: 0.733485\n",
      "epoch 62; iter: 0; batch classifier loss: 0.056935; batch adversarial loss: 0.736744\n",
      "epoch 63; iter: 0; batch classifier loss: 0.045219; batch adversarial loss: 0.724202\n",
      "epoch 64; iter: 0; batch classifier loss: 0.030636; batch adversarial loss: 0.737027\n",
      "epoch 65; iter: 0; batch classifier loss: 0.031495; batch adversarial loss: 0.720893\n",
      "epoch 66; iter: 0; batch classifier loss: 0.039357; batch adversarial loss: 0.724940\n",
      "epoch 67; iter: 0; batch classifier loss: 0.036157; batch adversarial loss: 0.733430\n",
      "epoch 68; iter: 0; batch classifier loss: 0.017882; batch adversarial loss: 0.735535\n",
      "epoch 69; iter: 0; batch classifier loss: 0.047447; batch adversarial loss: 0.726251\n",
      "epoch 70; iter: 0; batch classifier loss: 0.029360; batch adversarial loss: 0.711763\n",
      "epoch 71; iter: 0; batch classifier loss: 0.033012; batch adversarial loss: 0.704004\n",
      "epoch 72; iter: 0; batch classifier loss: 0.023381; batch adversarial loss: 0.714205\n",
      "epoch 73; iter: 0; batch classifier loss: 0.028900; batch adversarial loss: 0.715970\n",
      "epoch 74; iter: 0; batch classifier loss: 0.032493; batch adversarial loss: 0.712018\n",
      "epoch 75; iter: 0; batch classifier loss: 0.021667; batch adversarial loss: 0.720379\n",
      "epoch 76; iter: 0; batch classifier loss: 0.047447; batch adversarial loss: 0.715926\n",
      "epoch 77; iter: 0; batch classifier loss: 0.036260; batch adversarial loss: 0.724294\n",
      "epoch 78; iter: 0; batch classifier loss: 0.016175; batch adversarial loss: 0.713567\n",
      "epoch 79; iter: 0; batch classifier loss: 0.019013; batch adversarial loss: 0.714217\n",
      "epoch 80; iter: 0; batch classifier loss: 0.030102; batch adversarial loss: 0.714119\n",
      "epoch 81; iter: 0; batch classifier loss: 0.028116; batch adversarial loss: 0.709889\n",
      "epoch 82; iter: 0; batch classifier loss: 0.022175; batch adversarial loss: 0.713727\n",
      "epoch 83; iter: 0; batch classifier loss: 0.020211; batch adversarial loss: 0.704340\n",
      "epoch 84; iter: 0; batch classifier loss: 0.027484; batch adversarial loss: 0.711651\n",
      "epoch 85; iter: 0; batch classifier loss: 0.015840; batch adversarial loss: 0.703756\n",
      "epoch 86; iter: 0; batch classifier loss: 0.026429; batch adversarial loss: 0.705590\n",
      "epoch 87; iter: 0; batch classifier loss: 0.024007; batch adversarial loss: 0.705906\n",
      "epoch 88; iter: 0; batch classifier loss: 0.020017; batch adversarial loss: 0.706787\n",
      "epoch 89; iter: 0; batch classifier loss: 0.030137; batch adversarial loss: 0.712324\n",
      "epoch 90; iter: 0; batch classifier loss: 0.031635; batch adversarial loss: 0.708953\n",
      "epoch 91; iter: 0; batch classifier loss: 0.012458; batch adversarial loss: 0.705086\n",
      "epoch 92; iter: 0; batch classifier loss: 0.024704; batch adversarial loss: 0.710588\n",
      "epoch 93; iter: 0; batch classifier loss: 0.015032; batch adversarial loss: 0.703738\n",
      "epoch 94; iter: 0; batch classifier loss: 0.022350; batch adversarial loss: 0.704523\n",
      "epoch 95; iter: 0; batch classifier loss: 0.031930; batch adversarial loss: 0.709870\n",
      "epoch 96; iter: 0; batch classifier loss: 0.019592; batch adversarial loss: 0.710238\n",
      "epoch 97; iter: 0; batch classifier loss: 0.028359; batch adversarial loss: 0.706199\n",
      "epoch 98; iter: 0; batch classifier loss: 0.031127; batch adversarial loss: 0.701798\n",
      "epoch 99; iter: 0; batch classifier loss: 0.022266; batch adversarial loss: 0.701809\n",
      "epoch 100; iter: 0; batch classifier loss: 0.018418; batch adversarial loss: 0.705103\n",
      "epoch 101; iter: 0; batch classifier loss: 0.009798; batch adversarial loss: 0.695985\n",
      "epoch 102; iter: 0; batch classifier loss: 0.027335; batch adversarial loss: 0.699935\n",
      "epoch 103; iter: 0; batch classifier loss: 0.026547; batch adversarial loss: 0.702516\n",
      "epoch 104; iter: 0; batch classifier loss: 0.014463; batch adversarial loss: 0.697595\n",
      "epoch 105; iter: 0; batch classifier loss: 0.019804; batch adversarial loss: 0.699166\n",
      "epoch 106; iter: 0; batch classifier loss: 0.018609; batch adversarial loss: 0.694028\n",
      "epoch 107; iter: 0; batch classifier loss: 0.020670; batch adversarial loss: 0.697332\n",
      "epoch 108; iter: 0; batch classifier loss: 0.026700; batch adversarial loss: 0.696125\n",
      "epoch 109; iter: 0; batch classifier loss: 0.026657; batch adversarial loss: 0.696404\n",
      "epoch 110; iter: 0; batch classifier loss: 0.012904; batch adversarial loss: 0.696446\n",
      "epoch 111; iter: 0; batch classifier loss: 0.021865; batch adversarial loss: 0.699071\n",
      "epoch 112; iter: 0; batch classifier loss: 0.005197; batch adversarial loss: 0.698035\n",
      "epoch 113; iter: 0; batch classifier loss: 0.026580; batch adversarial loss: 0.696492\n",
      "epoch 114; iter: 0; batch classifier loss: 0.045033; batch adversarial loss: 0.706129\n",
      "epoch 115; iter: 0; batch classifier loss: 0.030934; batch adversarial loss: 0.695467\n",
      "epoch 116; iter: 0; batch classifier loss: 0.028237; batch adversarial loss: 0.704237\n",
      "epoch 117; iter: 0; batch classifier loss: 0.024466; batch adversarial loss: 0.697491\n",
      "epoch 118; iter: 0; batch classifier loss: 0.010254; batch adversarial loss: 0.696476\n",
      "epoch 119; iter: 0; batch classifier loss: 0.025926; batch adversarial loss: 0.703216\n",
      "epoch 120; iter: 0; batch classifier loss: 0.017532; batch adversarial loss: 0.691771\n",
      "epoch 121; iter: 0; batch classifier loss: 0.047470; batch adversarial loss: 0.704347\n",
      "epoch 122; iter: 0; batch classifier loss: 0.023031; batch adversarial loss: 0.705629\n",
      "epoch 123; iter: 0; batch classifier loss: 0.015116; batch adversarial loss: 0.692449\n",
      "epoch 124; iter: 0; batch classifier loss: 0.011207; batch adversarial loss: 0.697735\n",
      "epoch 125; iter: 0; batch classifier loss: 0.012789; batch adversarial loss: 0.691861\n",
      "epoch 126; iter: 0; batch classifier loss: 0.022397; batch adversarial loss: 0.694706\n",
      "epoch 127; iter: 0; batch classifier loss: 0.013448; batch adversarial loss: 0.685727\n",
      "epoch 128; iter: 0; batch classifier loss: 0.025124; batch adversarial loss: 0.692044\n",
      "epoch 129; iter: 0; batch classifier loss: 0.023447; batch adversarial loss: 0.692936\n",
      "epoch 130; iter: 0; batch classifier loss: 0.022230; batch adversarial loss: 0.697877\n",
      "epoch 131; iter: 0; batch classifier loss: 0.007841; batch adversarial loss: 0.699383\n",
      "epoch 132; iter: 0; batch classifier loss: 0.014021; batch adversarial loss: 0.689368\n",
      "epoch 133; iter: 0; batch classifier loss: 0.036972; batch adversarial loss: 0.686283\n",
      "epoch 134; iter: 0; batch classifier loss: 0.015494; batch adversarial loss: 0.688236\n",
      "epoch 135; iter: 0; batch classifier loss: 0.028610; batch adversarial loss: 0.685089\n",
      "epoch 136; iter: 0; batch classifier loss: 0.007358; batch adversarial loss: 0.668501\n",
      "epoch 137; iter: 0; batch classifier loss: 0.005018; batch adversarial loss: 0.687234\n",
      "epoch 138; iter: 0; batch classifier loss: 0.018139; batch adversarial loss: 0.685435\n",
      "epoch 139; iter: 0; batch classifier loss: 0.025012; batch adversarial loss: 0.700442\n",
      "epoch 140; iter: 0; batch classifier loss: 0.009547; batch adversarial loss: 0.695050\n",
      "epoch 141; iter: 0; batch classifier loss: 0.013848; batch adversarial loss: 0.693908\n",
      "epoch 142; iter: 0; batch classifier loss: 0.020070; batch adversarial loss: 0.691972\n",
      "epoch 143; iter: 0; batch classifier loss: 0.041626; batch adversarial loss: 0.691235\n",
      "epoch 144; iter: 0; batch classifier loss: 0.003464; batch adversarial loss: 0.680022\n",
      "epoch 145; iter: 0; batch classifier loss: 0.021704; batch adversarial loss: 0.697035\n",
      "epoch 146; iter: 0; batch classifier loss: 0.011184; batch adversarial loss: 0.673002\n",
      "epoch 147; iter: 0; batch classifier loss: 0.024701; batch adversarial loss: 0.687181\n",
      "epoch 148; iter: 0; batch classifier loss: 0.012052; batch adversarial loss: 0.680742\n",
      "epoch 149; iter: 0; batch classifier loss: 0.009504; batch adversarial loss: 0.679691\n",
      "epoch 150; iter: 0; batch classifier loss: 0.019551; batch adversarial loss: 0.689257\n",
      "epoch 151; iter: 0; batch classifier loss: 0.024098; batch adversarial loss: 0.690222\n",
      "epoch 152; iter: 0; batch classifier loss: 0.019221; batch adversarial loss: 0.684251\n",
      "epoch 153; iter: 0; batch classifier loss: 0.015699; batch adversarial loss: 0.688209\n",
      "epoch 154; iter: 0; batch classifier loss: 0.019205; batch adversarial loss: 0.688617\n",
      "epoch 155; iter: 0; batch classifier loss: 0.013917; batch adversarial loss: 0.678296\n",
      "epoch 156; iter: 0; batch classifier loss: 0.006283; batch adversarial loss: 0.679077\n",
      "epoch 157; iter: 0; batch classifier loss: 0.009467; batch adversarial loss: 0.673803\n",
      "epoch 158; iter: 0; batch classifier loss: 0.020250; batch adversarial loss: 0.685232\n",
      "epoch 159; iter: 0; batch classifier loss: 0.010635; batch adversarial loss: 0.681766\n",
      "epoch 160; iter: 0; batch classifier loss: 0.004934; batch adversarial loss: 0.680767\n",
      "epoch 161; iter: 0; batch classifier loss: 0.017510; batch adversarial loss: 0.680135\n",
      "epoch 162; iter: 0; batch classifier loss: 0.005464; batch adversarial loss: 0.667934\n",
      "epoch 163; iter: 0; batch classifier loss: 0.026676; batch adversarial loss: 0.674925\n",
      "epoch 164; iter: 0; batch classifier loss: 0.022420; batch adversarial loss: 0.686882\n",
      "epoch 165; iter: 0; batch classifier loss: 0.019881; batch adversarial loss: 0.713273\n",
      "epoch 166; iter: 0; batch classifier loss: 0.015554; batch adversarial loss: 0.691138\n",
      "epoch 167; iter: 0; batch classifier loss: 0.012525; batch adversarial loss: 0.687798\n",
      "epoch 168; iter: 0; batch classifier loss: 0.018485; batch adversarial loss: 0.671415\n",
      "epoch 169; iter: 0; batch classifier loss: 0.018426; batch adversarial loss: 0.691484\n",
      "epoch 170; iter: 0; batch classifier loss: 0.017201; batch adversarial loss: 0.684114\n",
      "epoch 171; iter: 0; batch classifier loss: 0.011947; batch adversarial loss: 0.676667\n",
      "epoch 172; iter: 0; batch classifier loss: 0.030814; batch adversarial loss: 0.691128\n",
      "epoch 173; iter: 0; batch classifier loss: 0.019610; batch adversarial loss: 0.688201\n",
      "epoch 174; iter: 0; batch classifier loss: 0.019969; batch adversarial loss: 0.678447\n",
      "epoch 175; iter: 0; batch classifier loss: 0.019059; batch adversarial loss: 0.670554\n",
      "epoch 176; iter: 0; batch classifier loss: 0.009597; batch adversarial loss: 0.675144\n",
      "epoch 177; iter: 0; batch classifier loss: 0.010965; batch adversarial loss: 0.686924\n",
      "epoch 178; iter: 0; batch classifier loss: 0.005751; batch adversarial loss: 0.682338\n",
      "epoch 179; iter: 0; batch classifier loss: 0.018253; batch adversarial loss: 0.667588\n",
      "epoch 180; iter: 0; batch classifier loss: 0.021711; batch adversarial loss: 0.681227\n",
      "epoch 181; iter: 0; batch classifier loss: 0.015952; batch adversarial loss: 0.673475\n",
      "epoch 182; iter: 0; batch classifier loss: 0.005296; batch adversarial loss: 0.676880\n",
      "epoch 183; iter: 0; batch classifier loss: 0.009553; batch adversarial loss: 0.688671\n",
      "epoch 184; iter: 0; batch classifier loss: 0.028343; batch adversarial loss: 0.694185\n",
      "epoch 185; iter: 0; batch classifier loss: 0.034421; batch adversarial loss: 0.686360\n",
      "epoch 186; iter: 0; batch classifier loss: 0.007601; batch adversarial loss: 0.681704\n",
      "epoch 187; iter: 0; batch classifier loss: 0.020720; batch adversarial loss: 0.690159\n",
      "epoch 188; iter: 0; batch classifier loss: 0.016480; batch adversarial loss: 0.673801\n",
      "epoch 189; iter: 0; batch classifier loss: 0.014622; batch adversarial loss: 0.688201\n",
      "epoch 190; iter: 0; batch classifier loss: 0.005798; batch adversarial loss: 0.686499\n",
      "epoch 191; iter: 0; batch classifier loss: 0.008139; batch adversarial loss: 0.702287\n",
      "epoch 192; iter: 0; batch classifier loss: 0.017091; batch adversarial loss: 0.673669\n",
      "epoch 193; iter: 0; batch classifier loss: 0.015913; batch adversarial loss: 0.685585\n",
      "epoch 194; iter: 0; batch classifier loss: 0.007903; batch adversarial loss: 0.681225\n",
      "epoch 195; iter: 0; batch classifier loss: 0.022667; batch adversarial loss: 0.677769\n",
      "epoch 196; iter: 0; batch classifier loss: 0.010486; batch adversarial loss: 0.688393\n",
      "epoch 197; iter: 0; batch classifier loss: 0.002433; batch adversarial loss: 0.683412\n",
      "epoch 198; iter: 0; batch classifier loss: 0.012685; batch adversarial loss: 0.677776\n",
      "epoch 199; iter: 0; batch classifier loss: 0.013309; batch adversarial loss: 0.677578\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:23:06.091479: W tensorflow/c/c_api.cc:304] Operation '{name:'4c969b4a-adee-11ee-9362-a9d23602d4a3/4c969b4a-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:14503 op device:{requested: '', assigned: ''} def:{{{node 4c969b4a-adee-11ee-9362-a9d23602d4a3/4c969b4a-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c969b4a-adee-11ee-9362-a9d23602d4a3/4c969b4a-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c969b4a-adee-11ee-9362-a9d23602d4a3/4c969b4a-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.646266; batch adversarial loss: 0.665701\n",
      "epoch 1; iter: 0; batch classifier loss: 0.496685; batch adversarial loss: 0.663493\n",
      "epoch 2; iter: 0; batch classifier loss: 0.402631; batch adversarial loss: 0.663290\n",
      "epoch 3; iter: 0; batch classifier loss: 0.385195; batch adversarial loss: 0.667229\n",
      "epoch 4; iter: 0; batch classifier loss: 0.330596; batch adversarial loss: 0.653912\n",
      "epoch 5; iter: 0; batch classifier loss: 0.249872; batch adversarial loss: 0.649121\n",
      "epoch 6; iter: 0; batch classifier loss: 0.324769; batch adversarial loss: 0.692267\n",
      "epoch 7; iter: 0; batch classifier loss: 0.293226; batch adversarial loss: 0.664789\n",
      "epoch 8; iter: 0; batch classifier loss: 0.255443; batch adversarial loss: 0.657296\n",
      "epoch 9; iter: 0; batch classifier loss: 0.293167; batch adversarial loss: 0.674661\n",
      "epoch 10; iter: 0; batch classifier loss: 0.234990; batch adversarial loss: 0.650382\n",
      "epoch 11; iter: 0; batch classifier loss: 0.205525; batch adversarial loss: 0.677015\n",
      "epoch 12; iter: 0; batch classifier loss: 0.238059; batch adversarial loss: 0.659206\n",
      "epoch 13; iter: 0; batch classifier loss: 0.181404; batch adversarial loss: 0.674298\n",
      "epoch 14; iter: 0; batch classifier loss: 0.209309; batch adversarial loss: 0.649083\n",
      "epoch 15; iter: 0; batch classifier loss: 0.177092; batch adversarial loss: 0.637570\n",
      "epoch 16; iter: 0; batch classifier loss: 0.204663; batch adversarial loss: 0.666597\n",
      "epoch 17; iter: 0; batch classifier loss: 0.159426; batch adversarial loss: 0.664293\n",
      "epoch 18; iter: 0; batch classifier loss: 0.193208; batch adversarial loss: 0.659040\n",
      "epoch 19; iter: 0; batch classifier loss: 0.169037; batch adversarial loss: 0.646789\n",
      "epoch 20; iter: 0; batch classifier loss: 0.153032; batch adversarial loss: 0.656842\n",
      "epoch 21; iter: 0; batch classifier loss: 0.172445; batch adversarial loss: 0.649217\n",
      "epoch 22; iter: 0; batch classifier loss: 0.159657; batch adversarial loss: 0.656971\n",
      "epoch 23; iter: 0; batch classifier loss: 0.149420; batch adversarial loss: 0.665376\n",
      "epoch 24; iter: 0; batch classifier loss: 0.112891; batch adversarial loss: 0.657915\n",
      "epoch 25; iter: 0; batch classifier loss: 0.151806; batch adversarial loss: 0.675311\n",
      "epoch 26; iter: 0; batch classifier loss: 0.109667; batch adversarial loss: 0.673147\n",
      "epoch 27; iter: 0; batch classifier loss: 0.132486; batch adversarial loss: 0.659240\n",
      "epoch 28; iter: 0; batch classifier loss: 0.142658; batch adversarial loss: 0.661583\n",
      "epoch 29; iter: 0; batch classifier loss: 0.130792; batch adversarial loss: 0.650667\n",
      "epoch 30; iter: 0; batch classifier loss: 0.134370; batch adversarial loss: 0.644368\n",
      "epoch 31; iter: 0; batch classifier loss: 0.113352; batch adversarial loss: 0.659101\n",
      "epoch 32; iter: 0; batch classifier loss: 0.111935; batch adversarial loss: 0.646397\n",
      "epoch 33; iter: 0; batch classifier loss: 0.082706; batch adversarial loss: 0.634084\n",
      "epoch 34; iter: 0; batch classifier loss: 0.108891; batch adversarial loss: 0.675616\n",
      "epoch 35; iter: 0; batch classifier loss: 0.077009; batch adversarial loss: 0.652207\n",
      "epoch 36; iter: 0; batch classifier loss: 0.081710; batch adversarial loss: 0.649072\n",
      "epoch 37; iter: 0; batch classifier loss: 0.110608; batch adversarial loss: 0.631065\n",
      "epoch 38; iter: 0; batch classifier loss: 0.104372; batch adversarial loss: 0.648316\n",
      "epoch 39; iter: 0; batch classifier loss: 0.119489; batch adversarial loss: 0.664716\n",
      "epoch 40; iter: 0; batch classifier loss: 0.089868; batch adversarial loss: 0.662176\n",
      "epoch 41; iter: 0; batch classifier loss: 0.086033; batch adversarial loss: 0.614488\n",
      "epoch 42; iter: 0; batch classifier loss: 0.074961; batch adversarial loss: 0.650300\n",
      "epoch 43; iter: 0; batch classifier loss: 0.065411; batch adversarial loss: 0.667134\n",
      "epoch 44; iter: 0; batch classifier loss: 0.095273; batch adversarial loss: 0.656623\n",
      "epoch 45; iter: 0; batch classifier loss: 0.064119; batch adversarial loss: 0.639628\n",
      "epoch 46; iter: 0; batch classifier loss: 0.059657; batch adversarial loss: 0.663112\n",
      "epoch 47; iter: 0; batch classifier loss: 0.080480; batch adversarial loss: 0.626120\n",
      "epoch 48; iter: 0; batch classifier loss: 0.062631; batch adversarial loss: 0.654332\n",
      "epoch 49; iter: 0; batch classifier loss: 0.070009; batch adversarial loss: 0.668974\n",
      "epoch 50; iter: 0; batch classifier loss: 0.049263; batch adversarial loss: 0.689366\n",
      "epoch 51; iter: 0; batch classifier loss: 0.061770; batch adversarial loss: 0.677432\n",
      "epoch 52; iter: 0; batch classifier loss: 0.059079; batch adversarial loss: 0.654775\n",
      "epoch 53; iter: 0; batch classifier loss: 0.055074; batch adversarial loss: 0.699200\n",
      "epoch 54; iter: 0; batch classifier loss: 0.065381; batch adversarial loss: 0.610020\n",
      "epoch 55; iter: 0; batch classifier loss: 0.062473; batch adversarial loss: 0.650511\n",
      "epoch 56; iter: 0; batch classifier loss: 0.053437; batch adversarial loss: 0.660692\n",
      "epoch 57; iter: 0; batch classifier loss: 0.076799; batch adversarial loss: 0.652425\n",
      "epoch 58; iter: 0; batch classifier loss: 0.055479; batch adversarial loss: 0.672858\n",
      "epoch 59; iter: 0; batch classifier loss: 0.043225; batch adversarial loss: 0.641444\n",
      "epoch 60; iter: 0; batch classifier loss: 0.051742; batch adversarial loss: 0.675320\n",
      "epoch 61; iter: 0; batch classifier loss: 0.051901; batch adversarial loss: 0.666116\n",
      "epoch 62; iter: 0; batch classifier loss: 0.045775; batch adversarial loss: 0.670182\n",
      "epoch 63; iter: 0; batch classifier loss: 0.049776; batch adversarial loss: 0.666498\n",
      "epoch 64; iter: 0; batch classifier loss: 0.038756; batch adversarial loss: 0.642845\n",
      "epoch 65; iter: 0; batch classifier loss: 0.040404; batch adversarial loss: 0.643200\n",
      "epoch 66; iter: 0; batch classifier loss: 0.050842; batch adversarial loss: 0.662650\n",
      "epoch 67; iter: 0; batch classifier loss: 0.027725; batch adversarial loss: 0.657509\n",
      "epoch 68; iter: 0; batch classifier loss: 0.040794; batch adversarial loss: 0.646488\n",
      "epoch 69; iter: 0; batch classifier loss: 0.032254; batch adversarial loss: 0.654051\n",
      "epoch 70; iter: 0; batch classifier loss: 0.051028; batch adversarial loss: 0.648748\n",
      "epoch 71; iter: 0; batch classifier loss: 0.054134; batch adversarial loss: 0.641240\n",
      "epoch 72; iter: 0; batch classifier loss: 0.034241; batch adversarial loss: 0.635783\n",
      "epoch 73; iter: 0; batch classifier loss: 0.029836; batch adversarial loss: 0.660635\n",
      "epoch 74; iter: 0; batch classifier loss: 0.035261; batch adversarial loss: 0.623698\n",
      "epoch 75; iter: 0; batch classifier loss: 0.034305; batch adversarial loss: 0.647592\n",
      "epoch 76; iter: 0; batch classifier loss: 0.024496; batch adversarial loss: 0.641325\n",
      "epoch 77; iter: 0; batch classifier loss: 0.030254; batch adversarial loss: 0.653907\n",
      "epoch 78; iter: 0; batch classifier loss: 0.036064; batch adversarial loss: 0.666879\n",
      "epoch 79; iter: 0; batch classifier loss: 0.034123; batch adversarial loss: 0.637430\n",
      "epoch 80; iter: 0; batch classifier loss: 0.039779; batch adversarial loss: 0.636479\n",
      "epoch 81; iter: 0; batch classifier loss: 0.021634; batch adversarial loss: 0.638791\n",
      "epoch 82; iter: 0; batch classifier loss: 0.030632; batch adversarial loss: 0.615954\n",
      "epoch 83; iter: 0; batch classifier loss: 0.038269; batch adversarial loss: 0.653261\n",
      "epoch 84; iter: 0; batch classifier loss: 0.047342; batch adversarial loss: 0.678595\n",
      "epoch 85; iter: 0; batch classifier loss: 0.031439; batch adversarial loss: 0.631688\n",
      "epoch 86; iter: 0; batch classifier loss: 0.037789; batch adversarial loss: 0.679319\n",
      "epoch 87; iter: 0; batch classifier loss: 0.046670; batch adversarial loss: 0.661556\n",
      "epoch 88; iter: 0; batch classifier loss: 0.029951; batch adversarial loss: 0.669092\n",
      "epoch 89; iter: 0; batch classifier loss: 0.029250; batch adversarial loss: 0.631168\n",
      "epoch 90; iter: 0; batch classifier loss: 0.029893; batch adversarial loss: 0.663551\n",
      "epoch 91; iter: 0; batch classifier loss: 0.031928; batch adversarial loss: 0.646286\n",
      "epoch 92; iter: 0; batch classifier loss: 0.039579; batch adversarial loss: 0.656406\n",
      "epoch 93; iter: 0; batch classifier loss: 0.026060; batch adversarial loss: 0.644861\n",
      "epoch 94; iter: 0; batch classifier loss: 0.028067; batch adversarial loss: 0.624975\n",
      "epoch 95; iter: 0; batch classifier loss: 0.033341; batch adversarial loss: 0.687107\n",
      "epoch 96; iter: 0; batch classifier loss: 0.023340; batch adversarial loss: 0.669569\n",
      "epoch 97; iter: 0; batch classifier loss: 0.024449; batch adversarial loss: 0.644342\n",
      "epoch 98; iter: 0; batch classifier loss: 0.018186; batch adversarial loss: 0.649280\n",
      "epoch 99; iter: 0; batch classifier loss: 0.032624; batch adversarial loss: 0.690410\n",
      "epoch 100; iter: 0; batch classifier loss: 0.023888; batch adversarial loss: 0.700644\n",
      "epoch 101; iter: 0; batch classifier loss: 0.017617; batch adversarial loss: 0.671881\n",
      "epoch 102; iter: 0; batch classifier loss: 0.032141; batch adversarial loss: 0.655479\n",
      "epoch 103; iter: 0; batch classifier loss: 0.024669; batch adversarial loss: 0.650399\n",
      "epoch 104; iter: 0; batch classifier loss: 0.030782; batch adversarial loss: 0.662411\n",
      "epoch 105; iter: 0; batch classifier loss: 0.030040; batch adversarial loss: 0.648890\n",
      "epoch 106; iter: 0; batch classifier loss: 0.020406; batch adversarial loss: 0.685412\n",
      "epoch 107; iter: 0; batch classifier loss: 0.027409; batch adversarial loss: 0.655490\n",
      "epoch 108; iter: 0; batch classifier loss: 0.030527; batch adversarial loss: 0.648283\n",
      "epoch 109; iter: 0; batch classifier loss: 0.016830; batch adversarial loss: 0.631469\n",
      "epoch 110; iter: 0; batch classifier loss: 0.018828; batch adversarial loss: 0.658445\n",
      "epoch 111; iter: 0; batch classifier loss: 0.021591; batch adversarial loss: 0.646816\n",
      "epoch 112; iter: 0; batch classifier loss: 0.032093; batch adversarial loss: 0.658541\n",
      "epoch 113; iter: 0; batch classifier loss: 0.023661; batch adversarial loss: 0.668646\n",
      "epoch 114; iter: 0; batch classifier loss: 0.015431; batch adversarial loss: 0.676462\n",
      "epoch 115; iter: 0; batch classifier loss: 0.020449; batch adversarial loss: 0.650848\n",
      "epoch 116; iter: 0; batch classifier loss: 0.018962; batch adversarial loss: 0.681070\n",
      "epoch 117; iter: 0; batch classifier loss: 0.021656; batch adversarial loss: 0.644343\n",
      "epoch 118; iter: 0; batch classifier loss: 0.022540; batch adversarial loss: 0.641712\n",
      "epoch 119; iter: 0; batch classifier loss: 0.019509; batch adversarial loss: 0.663689\n",
      "epoch 120; iter: 0; batch classifier loss: 0.019891; batch adversarial loss: 0.629189\n",
      "epoch 121; iter: 0; batch classifier loss: 0.009038; batch adversarial loss: 0.645624\n",
      "epoch 122; iter: 0; batch classifier loss: 0.024651; batch adversarial loss: 0.673366\n",
      "epoch 123; iter: 0; batch classifier loss: 0.020843; batch adversarial loss: 0.665976\n",
      "epoch 124; iter: 0; batch classifier loss: 0.017120; batch adversarial loss: 0.642565\n",
      "epoch 125; iter: 0; batch classifier loss: 0.025612; batch adversarial loss: 0.673279\n",
      "epoch 126; iter: 0; batch classifier loss: 0.017469; batch adversarial loss: 0.662820\n",
      "epoch 127; iter: 0; batch classifier loss: 0.011788; batch adversarial loss: 0.640678\n",
      "epoch 128; iter: 0; batch classifier loss: 0.018186; batch adversarial loss: 0.672376\n",
      "epoch 129; iter: 0; batch classifier loss: 0.008498; batch adversarial loss: 0.669273\n",
      "epoch 130; iter: 0; batch classifier loss: 0.016716; batch adversarial loss: 0.653926\n",
      "epoch 131; iter: 0; batch classifier loss: 0.020279; batch adversarial loss: 0.614007\n",
      "epoch 132; iter: 0; batch classifier loss: 0.022146; batch adversarial loss: 0.676435\n",
      "epoch 133; iter: 0; batch classifier loss: 0.013250; batch adversarial loss: 0.637556\n",
      "epoch 134; iter: 0; batch classifier loss: 0.012157; batch adversarial loss: 0.672337\n",
      "epoch 135; iter: 0; batch classifier loss: 0.016951; batch adversarial loss: 0.629884\n",
      "epoch 136; iter: 0; batch classifier loss: 0.009650; batch adversarial loss: 0.654504\n",
      "epoch 137; iter: 0; batch classifier loss: 0.014215; batch adversarial loss: 0.659288\n",
      "epoch 138; iter: 0; batch classifier loss: 0.007984; batch adversarial loss: 0.688362\n",
      "epoch 139; iter: 0; batch classifier loss: 0.015046; batch adversarial loss: 0.657732\n",
      "epoch 140; iter: 0; batch classifier loss: 0.014636; batch adversarial loss: 0.664767\n",
      "epoch 141; iter: 0; batch classifier loss: 0.012500; batch adversarial loss: 0.636417\n",
      "epoch 142; iter: 0; batch classifier loss: 0.019377; batch adversarial loss: 0.650996\n",
      "epoch 143; iter: 0; batch classifier loss: 0.015511; batch adversarial loss: 0.654683\n",
      "epoch 144; iter: 0; batch classifier loss: 0.015267; batch adversarial loss: 0.684756\n",
      "epoch 145; iter: 0; batch classifier loss: 0.008463; batch adversarial loss: 0.636345\n",
      "epoch 146; iter: 0; batch classifier loss: 0.010979; batch adversarial loss: 0.662364\n",
      "epoch 147; iter: 0; batch classifier loss: 0.007460; batch adversarial loss: 0.655634\n",
      "epoch 148; iter: 0; batch classifier loss: 0.019637; batch adversarial loss: 0.672390\n",
      "epoch 149; iter: 0; batch classifier loss: 0.014429; batch adversarial loss: 0.654761\n",
      "epoch 150; iter: 0; batch classifier loss: 0.017983; batch adversarial loss: 0.666154\n",
      "epoch 151; iter: 0; batch classifier loss: 0.016384; batch adversarial loss: 0.662846\n",
      "epoch 152; iter: 0; batch classifier loss: 0.012858; batch adversarial loss: 0.666028\n",
      "epoch 153; iter: 0; batch classifier loss: 0.010724; batch adversarial loss: 0.674936\n",
      "epoch 154; iter: 0; batch classifier loss: 0.020077; batch adversarial loss: 0.641812\n",
      "epoch 155; iter: 0; batch classifier loss: 0.006141; batch adversarial loss: 0.663247\n",
      "epoch 156; iter: 0; batch classifier loss: 0.008915; batch adversarial loss: 0.665823\n",
      "epoch 157; iter: 0; batch classifier loss: 0.010969; batch adversarial loss: 0.645447\n",
      "epoch 158; iter: 0; batch classifier loss: 0.014221; batch adversarial loss: 0.644361\n",
      "epoch 159; iter: 0; batch classifier loss: 0.011564; batch adversarial loss: 0.645488\n",
      "epoch 160; iter: 0; batch classifier loss: 0.007064; batch adversarial loss: 0.671619\n",
      "epoch 161; iter: 0; batch classifier loss: 0.008433; batch adversarial loss: 0.667819\n",
      "epoch 162; iter: 0; batch classifier loss: 0.016702; batch adversarial loss: 0.607051\n",
      "epoch 163; iter: 0; batch classifier loss: 0.011679; batch adversarial loss: 0.673957\n",
      "epoch 164; iter: 0; batch classifier loss: 0.016781; batch adversarial loss: 0.645477\n",
      "epoch 165; iter: 0; batch classifier loss: 0.014059; batch adversarial loss: 0.649011\n",
      "epoch 166; iter: 0; batch classifier loss: 0.008412; batch adversarial loss: 0.649463\n",
      "epoch 167; iter: 0; batch classifier loss: 0.003784; batch adversarial loss: 0.663738\n",
      "epoch 168; iter: 0; batch classifier loss: 0.025767; batch adversarial loss: 0.638751\n",
      "epoch 169; iter: 0; batch classifier loss: 0.006636; batch adversarial loss: 0.671684\n",
      "epoch 170; iter: 0; batch classifier loss: 0.008627; batch adversarial loss: 0.638579\n",
      "epoch 171; iter: 0; batch classifier loss: 0.012819; batch adversarial loss: 0.628625\n",
      "epoch 172; iter: 0; batch classifier loss: 0.008157; batch adversarial loss: 0.695784\n",
      "epoch 173; iter: 0; batch classifier loss: 0.008410; batch adversarial loss: 0.661007\n",
      "epoch 174; iter: 0; batch classifier loss: 0.006329; batch adversarial loss: 0.674906\n",
      "epoch 175; iter: 0; batch classifier loss: 0.009124; batch adversarial loss: 0.667474\n",
      "epoch 176; iter: 0; batch classifier loss: 0.012277; batch adversarial loss: 0.658859\n",
      "epoch 177; iter: 0; batch classifier loss: 0.012494; batch adversarial loss: 0.663447\n",
      "epoch 178; iter: 0; batch classifier loss: 0.011064; batch adversarial loss: 0.654835\n",
      "epoch 179; iter: 0; batch classifier loss: 0.011059; batch adversarial loss: 0.656445\n",
      "epoch 180; iter: 0; batch classifier loss: 0.006883; batch adversarial loss: 0.684819\n",
      "epoch 181; iter: 0; batch classifier loss: 0.005844; batch adversarial loss: 0.668759\n",
      "epoch 182; iter: 0; batch classifier loss: 0.010299; batch adversarial loss: 0.652250\n",
      "epoch 183; iter: 0; batch classifier loss: 0.009332; batch adversarial loss: 0.636869\n",
      "epoch 184; iter: 0; batch classifier loss: 0.007956; batch adversarial loss: 0.649848\n",
      "epoch 185; iter: 0; batch classifier loss: 0.002967; batch adversarial loss: 0.655701\n",
      "epoch 186; iter: 0; batch classifier loss: 0.005528; batch adversarial loss: 0.670862\n",
      "epoch 187; iter: 0; batch classifier loss: 0.006163; batch adversarial loss: 0.661088\n",
      "epoch 188; iter: 0; batch classifier loss: 0.002716; batch adversarial loss: 0.660227\n",
      "epoch 189; iter: 0; batch classifier loss: 0.009835; batch adversarial loss: 0.666039\n",
      "epoch 190; iter: 0; batch classifier loss: 0.008421; batch adversarial loss: 0.651392\n",
      "epoch 191; iter: 0; batch classifier loss: 0.004264; batch adversarial loss: 0.666735\n",
      "epoch 192; iter: 0; batch classifier loss: 0.005115; batch adversarial loss: 0.667845\n",
      "epoch 193; iter: 0; batch classifier loss: 0.007830; batch adversarial loss: 0.671140\n",
      "epoch 194; iter: 0; batch classifier loss: 0.009400; batch adversarial loss: 0.668498\n",
      "epoch 195; iter: 0; batch classifier loss: 0.010101; batch adversarial loss: 0.642367\n",
      "epoch 196; iter: 0; batch classifier loss: 0.005413; batch adversarial loss: 0.610064\n",
      "epoch 197; iter: 0; batch classifier loss: 0.013482; batch adversarial loss: 0.693021\n",
      "epoch 198; iter: 0; batch classifier loss: 0.007402; batch adversarial loss: 0.686335\n",
      "epoch 199; iter: 0; batch classifier loss: 0.008318; batch adversarial loss: 0.620528\n",
      "X_df.isna().any().any() --  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 08:23:08.850346: W tensorflow/c/c_api.cc:304] Operation '{name:'4c969bd6-adee-11ee-9362-a9d23602d4a3/4c969bd6-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign' id:15310 op device:{requested: '', assigned: ''} def:{{{node 4c969bd6-adee-11ee-9362-a9d23602d4a3/4c969bd6-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](4c969bd6-adee-11ee-9362-a9d23602d4a3/4c969bd6-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1, 4c969bd6-adee-11ee-9362-a9d23602d4a3/4c969bd6-adee-11ee-9362-a9d23602d4a3/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.632057; batch adversarial loss: 0.698477\n",
      "epoch 1; iter: 0; batch classifier loss: 0.526374; batch adversarial loss: 0.691259\n",
      "epoch 2; iter: 0; batch classifier loss: 0.394444; batch adversarial loss: 0.716047\n",
      "epoch 3; iter: 0; batch classifier loss: 0.307671; batch adversarial loss: 0.663912\n",
      "epoch 4; iter: 0; batch classifier loss: 0.357819; batch adversarial loss: 0.757221\n",
      "epoch 5; iter: 0; batch classifier loss: 0.352706; batch adversarial loss: 0.759217\n",
      "epoch 6; iter: 0; batch classifier loss: 0.307334; batch adversarial loss: 0.692643\n",
      "epoch 7; iter: 0; batch classifier loss: 0.281844; batch adversarial loss: 0.687498\n",
      "epoch 8; iter: 0; batch classifier loss: 0.355205; batch adversarial loss: 0.743875\n",
      "epoch 9; iter: 0; batch classifier loss: 0.258748; batch adversarial loss: 0.706597\n",
      "epoch 10; iter: 0; batch classifier loss: 0.268020; batch adversarial loss: 0.673698\n",
      "epoch 11; iter: 0; batch classifier loss: 0.231270; batch adversarial loss: 0.753073\n",
      "epoch 12; iter: 0; batch classifier loss: 0.254531; batch adversarial loss: 0.738589\n",
      "epoch 13; iter: 0; batch classifier loss: 0.220059; batch adversarial loss: 0.719447\n",
      "epoch 14; iter: 0; batch classifier loss: 0.289268; batch adversarial loss: 0.724010\n",
      "epoch 15; iter: 0; batch classifier loss: 0.247416; batch adversarial loss: 0.723775\n",
      "epoch 16; iter: 0; batch classifier loss: 0.253217; batch adversarial loss: 0.692257\n",
      "epoch 17; iter: 0; batch classifier loss: 0.181033; batch adversarial loss: 0.678603\n",
      "epoch 18; iter: 0; batch classifier loss: 0.162020; batch adversarial loss: 0.710230\n",
      "epoch 19; iter: 0; batch classifier loss: 0.181339; batch adversarial loss: 0.692180\n",
      "epoch 20; iter: 0; batch classifier loss: 0.248349; batch adversarial loss: 0.699673\n",
      "epoch 21; iter: 0; batch classifier loss: 0.202529; batch adversarial loss: 0.726780\n",
      "epoch 22; iter: 0; batch classifier loss: 0.235662; batch adversarial loss: 0.730722\n",
      "epoch 23; iter: 0; batch classifier loss: 0.183361; batch adversarial loss: 0.752978\n",
      "epoch 24; iter: 0; batch classifier loss: 0.220408; batch adversarial loss: 0.762209\n",
      "epoch 25; iter: 0; batch classifier loss: 0.238249; batch adversarial loss: 0.793623\n",
      "epoch 26; iter: 0; batch classifier loss: 0.200765; batch adversarial loss: 0.733148\n",
      "epoch 27; iter: 0; batch classifier loss: 0.237000; batch adversarial loss: 0.738416\n",
      "epoch 28; iter: 0; batch classifier loss: 0.191905; batch adversarial loss: 0.756588\n",
      "epoch 29; iter: 0; batch classifier loss: 0.157549; batch adversarial loss: 0.694848\n",
      "epoch 30; iter: 0; batch classifier loss: 0.159665; batch adversarial loss: 0.709426\n",
      "epoch 31; iter: 0; batch classifier loss: 0.158638; batch adversarial loss: 0.666912\n",
      "epoch 32; iter: 0; batch classifier loss: 0.151302; batch adversarial loss: 0.677684\n",
      "epoch 33; iter: 0; batch classifier loss: 0.154142; batch adversarial loss: 0.713717\n",
      "epoch 34; iter: 0; batch classifier loss: 0.183992; batch adversarial loss: 0.742769\n",
      "epoch 35; iter: 0; batch classifier loss: 0.147399; batch adversarial loss: 0.692277\n",
      "epoch 36; iter: 0; batch classifier loss: 0.162381; batch adversarial loss: 0.761565\n",
      "epoch 37; iter: 0; batch classifier loss: 0.133421; batch adversarial loss: 0.700793\n",
      "epoch 38; iter: 0; batch classifier loss: 0.173511; batch adversarial loss: 0.738992\n",
      "epoch 39; iter: 0; batch classifier loss: 0.144565; batch adversarial loss: 0.708538\n",
      "epoch 40; iter: 0; batch classifier loss: 0.136975; batch adversarial loss: 0.690884\n",
      "epoch 41; iter: 0; batch classifier loss: 0.134597; batch adversarial loss: 0.704471\n",
      "epoch 42; iter: 0; batch classifier loss: 0.174238; batch adversarial loss: 0.715081\n",
      "epoch 43; iter: 0; batch classifier loss: 0.137546; batch adversarial loss: 0.717187\n",
      "epoch 44; iter: 0; batch classifier loss: 0.111108; batch adversarial loss: 0.692248\n",
      "epoch 45; iter: 0; batch classifier loss: 0.197271; batch adversarial loss: 0.743924\n",
      "epoch 46; iter: 0; batch classifier loss: 0.165687; batch adversarial loss: 0.705070\n",
      "epoch 47; iter: 0; batch classifier loss: 0.158201; batch adversarial loss: 0.714602\n",
      "epoch 48; iter: 0; batch classifier loss: 0.154662; batch adversarial loss: 0.725036\n",
      "epoch 49; iter: 0; batch classifier loss: 0.159391; batch adversarial loss: 0.720302\n",
      "epoch 50; iter: 0; batch classifier loss: 0.163991; batch adversarial loss: 0.703508\n",
      "epoch 51; iter: 0; batch classifier loss: 0.150209; batch adversarial loss: 0.716167\n",
      "epoch 52; iter: 0; batch classifier loss: 0.131707; batch adversarial loss: 0.697785\n",
      "epoch 53; iter: 0; batch classifier loss: 0.127500; batch adversarial loss: 0.711492\n",
      "epoch 54; iter: 0; batch classifier loss: 0.143391; batch adversarial loss: 0.681311\n",
      "epoch 55; iter: 0; batch classifier loss: 0.105329; batch adversarial loss: 0.714276\n",
      "epoch 56; iter: 0; batch classifier loss: 0.112878; batch adversarial loss: 0.699144\n",
      "epoch 57; iter: 0; batch classifier loss: 0.080676; batch adversarial loss: 0.682793\n",
      "epoch 58; iter: 0; batch classifier loss: 0.082221; batch adversarial loss: 0.677987\n",
      "epoch 59; iter: 0; batch classifier loss: 0.136781; batch adversarial loss: 0.713822\n",
      "epoch 60; iter: 0; batch classifier loss: 0.166240; batch adversarial loss: 0.744062\n",
      "epoch 61; iter: 0; batch classifier loss: 0.164678; batch adversarial loss: 0.710722\n",
      "epoch 62; iter: 0; batch classifier loss: 0.195743; batch adversarial loss: 0.724226\n",
      "epoch 63; iter: 0; batch classifier loss: 0.148849; batch adversarial loss: 0.733106\n",
      "epoch 64; iter: 0; batch classifier loss: 0.139407; batch adversarial loss: 0.746604\n",
      "epoch 65; iter: 0; batch classifier loss: 0.129752; batch adversarial loss: 0.707326\n",
      "epoch 66; iter: 0; batch classifier loss: 0.148890; batch adversarial loss: 0.724754\n",
      "epoch 67; iter: 0; batch classifier loss: 0.135292; batch adversarial loss: 0.716989\n",
      "epoch 68; iter: 0; batch classifier loss: 0.127416; batch adversarial loss: 0.708010\n",
      "epoch 69; iter: 0; batch classifier loss: 0.140745; batch adversarial loss: 0.736032\n",
      "epoch 70; iter: 0; batch classifier loss: 0.123337; batch adversarial loss: 0.708899\n",
      "epoch 71; iter: 0; batch classifier loss: 0.124264; batch adversarial loss: 0.703854\n",
      "epoch 72; iter: 0; batch classifier loss: 0.104102; batch adversarial loss: 0.697636\n",
      "epoch 73; iter: 0; batch classifier loss: 0.110645; batch adversarial loss: 0.699606\n",
      "epoch 74; iter: 0; batch classifier loss: 0.115624; batch adversarial loss: 0.708402\n",
      "epoch 75; iter: 0; batch classifier loss: 0.181624; batch adversarial loss: 0.754993\n",
      "epoch 76; iter: 0; batch classifier loss: 0.095429; batch adversarial loss: 0.681917\n",
      "epoch 77; iter: 0; batch classifier loss: 0.089859; batch adversarial loss: 0.705686\n",
      "epoch 78; iter: 0; batch classifier loss: 0.125156; batch adversarial loss: 0.714846\n",
      "epoch 79; iter: 0; batch classifier loss: 0.182920; batch adversarial loss: 0.731701\n",
      "epoch 80; iter: 0; batch classifier loss: 0.092267; batch adversarial loss: 0.693621\n",
      "epoch 81; iter: 0; batch classifier loss: 0.085492; batch adversarial loss: 0.676391\n",
      "epoch 82; iter: 0; batch classifier loss: 0.156814; batch adversarial loss: 0.756514\n",
      "epoch 83; iter: 0; batch classifier loss: 0.143435; batch adversarial loss: 0.719521\n",
      "epoch 84; iter: 0; batch classifier loss: 0.130616; batch adversarial loss: 0.713268\n",
      "epoch 85; iter: 0; batch classifier loss: 0.102387; batch adversarial loss: 0.705418\n",
      "epoch 86; iter: 0; batch classifier loss: 0.166848; batch adversarial loss: 0.719194\n",
      "epoch 87; iter: 0; batch classifier loss: 0.155789; batch adversarial loss: 0.729295\n",
      "epoch 88; iter: 0; batch classifier loss: 0.119994; batch adversarial loss: 0.707710\n",
      "epoch 89; iter: 0; batch classifier loss: 0.152048; batch adversarial loss: 0.730658\n",
      "epoch 90; iter: 0; batch classifier loss: 0.138827; batch adversarial loss: 0.684346\n",
      "epoch 91; iter: 0; batch classifier loss: 0.143611; batch adversarial loss: 0.711904\n",
      "epoch 92; iter: 0; batch classifier loss: 0.154752; batch adversarial loss: 0.744911\n",
      "epoch 93; iter: 0; batch classifier loss: 0.114255; batch adversarial loss: 0.700892\n",
      "epoch 94; iter: 0; batch classifier loss: 0.067197; batch adversarial loss: 0.711705\n",
      "epoch 95; iter: 0; batch classifier loss: 0.129236; batch adversarial loss: 0.699041\n",
      "epoch 96; iter: 0; batch classifier loss: 0.170808; batch adversarial loss: 0.766915\n",
      "epoch 97; iter: 0; batch classifier loss: 0.112442; batch adversarial loss: 0.721629\n",
      "epoch 98; iter: 0; batch classifier loss: 0.130850; batch adversarial loss: 0.713256\n",
      "epoch 99; iter: 0; batch classifier loss: 0.097140; batch adversarial loss: 0.689267\n",
      "epoch 100; iter: 0; batch classifier loss: 0.172453; batch adversarial loss: 0.746075\n",
      "epoch 101; iter: 0; batch classifier loss: 0.134209; batch adversarial loss: 0.724687\n",
      "epoch 102; iter: 0; batch classifier loss: 0.116321; batch adversarial loss: 0.711127\n",
      "epoch 103; iter: 0; batch classifier loss: 0.131725; batch adversarial loss: 0.714800\n",
      "epoch 104; iter: 0; batch classifier loss: 0.115279; batch adversarial loss: 0.709802\n",
      "epoch 105; iter: 0; batch classifier loss: 0.169573; batch adversarial loss: 0.733801\n",
      "epoch 106; iter: 0; batch classifier loss: 0.088154; batch adversarial loss: 0.678509\n",
      "epoch 107; iter: 0; batch classifier loss: 0.111594; batch adversarial loss: 0.716288\n",
      "epoch 108; iter: 0; batch classifier loss: 0.137019; batch adversarial loss: 0.724580\n",
      "epoch 109; iter: 0; batch classifier loss: 0.123406; batch adversarial loss: 0.712377\n",
      "epoch 110; iter: 0; batch classifier loss: 0.150447; batch adversarial loss: 0.721869\n",
      "epoch 111; iter: 0; batch classifier loss: 0.124295; batch adversarial loss: 0.685009\n",
      "epoch 112; iter: 0; batch classifier loss: 0.107617; batch adversarial loss: 0.694155\n",
      "epoch 113; iter: 0; batch classifier loss: 0.122605; batch adversarial loss: 0.709713\n",
      "epoch 114; iter: 0; batch classifier loss: 0.149816; batch adversarial loss: 0.731142\n",
      "epoch 115; iter: 0; batch classifier loss: 0.104115; batch adversarial loss: 0.707936\n",
      "epoch 116; iter: 0; batch classifier loss: 0.109764; batch adversarial loss: 0.724477\n",
      "epoch 117; iter: 0; batch classifier loss: 0.103023; batch adversarial loss: 0.703008\n",
      "epoch 118; iter: 0; batch classifier loss: 0.100258; batch adversarial loss: 0.692962\n",
      "epoch 119; iter: 0; batch classifier loss: 0.124624; batch adversarial loss: 0.698220\n",
      "epoch 120; iter: 0; batch classifier loss: 0.087670; batch adversarial loss: 0.705119\n",
      "epoch 121; iter: 0; batch classifier loss: 0.123456; batch adversarial loss: 0.691837\n",
      "epoch 122; iter: 0; batch classifier loss: 0.090165; batch adversarial loss: 0.689887\n",
      "epoch 123; iter: 0; batch classifier loss: 0.158681; batch adversarial loss: 0.737906\n",
      "epoch 124; iter: 0; batch classifier loss: 0.181433; batch adversarial loss: 0.730411\n",
      "epoch 125; iter: 0; batch classifier loss: 0.103176; batch adversarial loss: 0.697666\n",
      "epoch 126; iter: 0; batch classifier loss: 0.090642; batch adversarial loss: 0.680152\n",
      "epoch 127; iter: 0; batch classifier loss: 0.138956; batch adversarial loss: 0.714737\n",
      "epoch 128; iter: 0; batch classifier loss: 0.140242; batch adversarial loss: 0.707223\n",
      "epoch 129; iter: 0; batch classifier loss: 0.102475; batch adversarial loss: 0.698371\n",
      "epoch 130; iter: 0; batch classifier loss: 0.143597; batch adversarial loss: 0.688752\n",
      "epoch 131; iter: 0; batch classifier loss: 0.110606; batch adversarial loss: 0.683931\n",
      "epoch 132; iter: 0; batch classifier loss: 0.135633; batch adversarial loss: 0.701921\n",
      "epoch 133; iter: 0; batch classifier loss: 0.112911; batch adversarial loss: 0.682839\n",
      "epoch 134; iter: 0; batch classifier loss: 0.121741; batch adversarial loss: 0.718621\n",
      "epoch 135; iter: 0; batch classifier loss: 0.138275; batch adversarial loss: 0.692319\n",
      "epoch 136; iter: 0; batch classifier loss: 0.137693; batch adversarial loss: 0.730324\n",
      "epoch 137; iter: 0; batch classifier loss: 0.140276; batch adversarial loss: 0.718573\n",
      "epoch 138; iter: 0; batch classifier loss: 0.107662; batch adversarial loss: 0.693673\n",
      "epoch 139; iter: 0; batch classifier loss: 0.094018; batch adversarial loss: 0.683154\n",
      "epoch 140; iter: 0; batch classifier loss: 0.111571; batch adversarial loss: 0.691915\n",
      "epoch 141; iter: 0; batch classifier loss: 0.141277; batch adversarial loss: 0.715321\n",
      "epoch 142; iter: 0; batch classifier loss: 0.104400; batch adversarial loss: 0.686565\n",
      "epoch 143; iter: 0; batch classifier loss: 0.144457; batch adversarial loss: 0.721227\n",
      "epoch 144; iter: 0; batch classifier loss: 0.123580; batch adversarial loss: 0.720303\n",
      "epoch 145; iter: 0; batch classifier loss: 0.161572; batch adversarial loss: 0.699339\n",
      "epoch 146; iter: 0; batch classifier loss: 0.121366; batch adversarial loss: 0.715329\n",
      "epoch 147; iter: 0; batch classifier loss: 0.078720; batch adversarial loss: 0.701008\n",
      "epoch 148; iter: 0; batch classifier loss: 0.068052; batch adversarial loss: 0.690399\n",
      "epoch 149; iter: 0; batch classifier loss: 0.142448; batch adversarial loss: 0.730587\n",
      "epoch 150; iter: 0; batch classifier loss: 0.094371; batch adversarial loss: 0.698444\n",
      "epoch 151; iter: 0; batch classifier loss: 0.140372; batch adversarial loss: 0.735273\n",
      "epoch 152; iter: 0; batch classifier loss: 0.176626; batch adversarial loss: 0.705413\n",
      "epoch 153; iter: 0; batch classifier loss: 0.117542; batch adversarial loss: 0.697345\n",
      "epoch 154; iter: 0; batch classifier loss: 0.117893; batch adversarial loss: 0.719476\n",
      "epoch 155; iter: 0; batch classifier loss: 0.135203; batch adversarial loss: 0.705086\n",
      "epoch 156; iter: 0; batch classifier loss: 0.109854; batch adversarial loss: 0.723884\n",
      "epoch 157; iter: 0; batch classifier loss: 0.070980; batch adversarial loss: 0.674075\n",
      "epoch 158; iter: 0; batch classifier loss: 0.073432; batch adversarial loss: 0.678052\n",
      "epoch 159; iter: 0; batch classifier loss: 0.112810; batch adversarial loss: 0.710250\n",
      "epoch 160; iter: 0; batch classifier loss: 0.076909; batch adversarial loss: 0.696743\n",
      "epoch 161; iter: 0; batch classifier loss: 0.108006; batch adversarial loss: 0.692486\n",
      "epoch 162; iter: 0; batch classifier loss: 0.083159; batch adversarial loss: 0.708539\n",
      "epoch 163; iter: 0; batch classifier loss: 0.070153; batch adversarial loss: 0.705616\n",
      "epoch 164; iter: 0; batch classifier loss: 0.088555; batch adversarial loss: 0.692777\n",
      "epoch 165; iter: 0; batch classifier loss: 0.112862; batch adversarial loss: 0.695266\n",
      "epoch 166; iter: 0; batch classifier loss: 0.082554; batch adversarial loss: 0.658555\n",
      "epoch 167; iter: 0; batch classifier loss: 0.097411; batch adversarial loss: 0.671673\n",
      "epoch 168; iter: 0; batch classifier loss: 0.089304; batch adversarial loss: 0.671937\n",
      "epoch 169; iter: 0; batch classifier loss: 0.108115; batch adversarial loss: 0.701802\n",
      "epoch 170; iter: 0; batch classifier loss: 0.100608; batch adversarial loss: 0.672491\n",
      "epoch 171; iter: 0; batch classifier loss: 0.058143; batch adversarial loss: 0.660056\n",
      "epoch 172; iter: 0; batch classifier loss: 0.104314; batch adversarial loss: 0.687863\n",
      "epoch 173; iter: 0; batch classifier loss: 0.067073; batch adversarial loss: 0.675808\n",
      "epoch 174; iter: 0; batch classifier loss: 0.099302; batch adversarial loss: 0.691256\n",
      "epoch 175; iter: 0; batch classifier loss: 0.095810; batch adversarial loss: 0.682985\n",
      "epoch 176; iter: 0; batch classifier loss: 0.062362; batch adversarial loss: 0.680371\n",
      "epoch 177; iter: 0; batch classifier loss: 0.143766; batch adversarial loss: 0.703012\n",
      "epoch 178; iter: 0; batch classifier loss: 0.131090; batch adversarial loss: 0.687518\n",
      "epoch 179; iter: 0; batch classifier loss: 0.125262; batch adversarial loss: 0.656948\n",
      "epoch 180; iter: 0; batch classifier loss: 0.145712; batch adversarial loss: 0.699001\n",
      "epoch 181; iter: 0; batch classifier loss: 0.147378; batch adversarial loss: 0.712499\n",
      "epoch 182; iter: 0; batch classifier loss: 0.082452; batch adversarial loss: 0.682048\n",
      "epoch 183; iter: 0; batch classifier loss: 0.078876; batch adversarial loss: 0.666286\n",
      "epoch 184; iter: 0; batch classifier loss: 0.076739; batch adversarial loss: 0.705161\n",
      "epoch 185; iter: 0; batch classifier loss: 0.134741; batch adversarial loss: 0.703095\n",
      "epoch 186; iter: 0; batch classifier loss: 0.095829; batch adversarial loss: 0.675359\n",
      "epoch 187; iter: 0; batch classifier loss: 0.129005; batch adversarial loss: 0.705951\n",
      "epoch 188; iter: 0; batch classifier loss: 0.109009; batch adversarial loss: 0.687096\n",
      "epoch 189; iter: 0; batch classifier loss: 0.105063; batch adversarial loss: 0.676428\n",
      "epoch 190; iter: 0; batch classifier loss: 0.165589; batch adversarial loss: 0.723688\n",
      "epoch 191; iter: 0; batch classifier loss: 0.092135; batch adversarial loss: 0.687504\n",
      "epoch 192; iter: 0; batch classifier loss: 0.105835; batch adversarial loss: 0.720472\n",
      "epoch 193; iter: 0; batch classifier loss: 0.084475; batch adversarial loss: 0.696425\n",
      "epoch 194; iter: 0; batch classifier loss: 0.120600; batch adversarial loss: 0.686021\n",
      "epoch 195; iter: 0; batch classifier loss: 0.115537; batch adversarial loss: 0.690074\n",
      "epoch 196; iter: 0; batch classifier loss: 0.104439; batch adversarial loss: 0.682749\n",
      "epoch 197; iter: 0; batch classifier loss: 0.126777; batch adversarial loss: 0.668100\n",
      "epoch 198; iter: 0; batch classifier loss: 0.090583; batch adversarial loss: 0.693355\n",
      "epoch 199; iter: 0; batch classifier loss: 0.109822; batch adversarial loss: 0.692019\n",
      "X_df.isna().any().any() --  False\n"
     ]
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='StudentPerformancePortugueseDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d8270",
   "metadata": {},
   "source": [
    "### Experiment iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc40e12",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EXPERIMENT_SEEDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Configs for an experiment iteration\u001B[39;00m\n\u001B[1;32m      2\u001B[0m exp_iter_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m----> 3\u001B[0m experiment_seed \u001B[38;5;241m=\u001B[39m \u001B[43mEXPERIMENT_SEEDS\u001B[49m[exp_iter_num \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m      4\u001B[0m tuned_params_filenames \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      5\u001B[0m ]\n\u001B[1;32m      6\u001B[0m tuned_params_df_paths \u001B[38;5;241m=\u001B[39m [os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(ROOT_DIR, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiff_fairness_interventions_exp\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      7\u001B[0m                                       FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME, tuned_params_filename)\n\u001B[1;32m      8\u001B[0m                          \u001B[38;5;28;01mfor\u001B[39;00m tuned_params_filename \u001B[38;5;129;01min\u001B[39;00m tuned_params_filenames]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'EXPERIMENT_SEEDS' is not defined"
     ]
    }
   ],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 2\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "tuned_params_filenames = [\n",
    "]\n",
    "tuned_params_df_paths = [os.path.join(ROOT_DIR, 'results', 'diff_fairness_interventions_exp',\n",
    "                                      FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME, tuned_params_filename)\n",
    "                         for tuned_params_filename in tuned_params_filenames]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage\n",
    "models_params_for_tuning = get_folktables_employment_models_params_for_tuning(experiment_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be9a5b79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:56.249510Z",
     "start_time": "2024-01-04T20:53:56.233525Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 17:15:39 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 200,\n",
      " 'experiment_iteration': 'Exp_iter_2',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'intervention_param': 'True',\n",
      " 'model_init_seed': 200,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5645c6009d3b48d29619a238bc476502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 17:15:39 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-04 17:15:39 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([322, 293, 576, 300, 391, 343, 294, 558, 560, 439, 355, 440, 277,\n",
      "            492, 644, 639, 589, 259, 313, 129],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([322, 293, 576, 300, 391, 343, 294, 558, 560, 439, 355, 440, 277,\n",
      "            492, 644, 639, 589, 259, 313, 129],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 200, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c0277603884950a0c35cc9dcc3f539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7137564e8e0d4fcbbbfaaf03d73e170a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa30ab3dbbfe4acb8557a8c1e2d6556b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a36e32d08b44c1b01ae496881c6b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_eq_odds(data_loader=exp_iter_data_loader,\n",
    "                          experiment_seed=experiment_seed,\n",
    "                          test_set_fraction=TEST_SET_FRACTION,\n",
    "                          db_writer_func=db_writer_func,\n",
    "                          fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                          models_params_for_tuning=models_params_for_tuning,\n",
    "                          metrics_computation_config=metrics_computation_config,\n",
    "                          custom_table_fields_dct=custom_table_fields_dct,\n",
    "                          # with_tuning=True,\n",
    "                          with_tuning=False,\n",
    "                          tuned_params_df_paths=tuned_params_df_paths,\n",
    "                          save_results_dir_path=SAVE_RESULTS_DIR_PATH,\n",
    "                          dataset_name='StudentPerformancePortugueseDataset',\n",
    "                          postprocessor_name='ROC',\n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834a569",
   "metadata": {},
   "source": [
    "### Experiment iteration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d130fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 3\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "tuned_params_filenames = [\n",
    "]\n",
    "tuned_params_df_paths = [os.path.join(ROOT_DIR, 'results', 'diff_fairness_interventions_exp',\n",
    "                                      FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME, tuned_params_filename)\n",
    "                         for tuned_params_filename in tuned_params_filenames]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage\n",
    "models_params_for_tuning = get_folktables_employment_models_params_for_tuning(experiment_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab940edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:46.750905Z",
     "start_time": "2024-01-04T20:53:46.744795Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 19:29:01 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 300,\n",
      " 'experiment_iteration': 'Exp_iter_3',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'model_init_seed': 300,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48b0aa26f024ad1b1089190e134193d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 19:29:01 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-04 19:29:01 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([250, 438, 479, 326,  46, 565, 534, 382, 377, 457,  97, 388, 123,\n",
      "            156, 430, 466,  38, 474, 167, 524],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([250, 438, 479, 326,  46, 565, 534, 382, 377, 457,  97, 388, 123,\n",
      "            156, 430, 466,  38, 474, 167, 524],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 300, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eddcae8f6fc4651885a3144088ae9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43ba8d965cc436a91f10cc39b3d5e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94495e7bec64c9ebf4c05cdd52e605d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f7167dd20740a68f8f90158ec19560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_eq_odds(data_loader=exp_iter_data_loader,\n",
    "                          experiment_seed=experiment_seed,\n",
    "                          test_set_fraction=TEST_SET_FRACTION,\n",
    "                          db_writer_func=db_writer_func,\n",
    "                          fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                          models_params_for_tuning=models_params_for_tuning,\n",
    "                          metrics_computation_config=metrics_computation_config,\n",
    "                          custom_table_fields_dct=custom_table_fields_dct,\n",
    "                          # with_tuning=True,\n",
    "                          with_tuning=False,\n",
    "                          tuned_params_df_paths=tuned_params_df_paths,\n",
    "                          save_results_dir_path=SAVE_RESULTS_DIR_PATH,\n",
    "                          dataset_name='StudentPerformancePortugueseDataset',\n",
    "                          postprocessor_name='ROC',\n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9f5f7",
   "metadata": {},
   "source": [
    "### Experiment iteration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b363156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 4\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "tuned_params_filenames = [\n",
    "]\n",
    "tuned_params_df_paths = [os.path.join(ROOT_DIR, 'results', 'diff_fairness_interventions_exp',\n",
    "                                      FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME, tuned_params_filename)\n",
    "                         for tuned_params_filename in tuned_params_filenames]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage\n",
    "models_params_for_tuning = get_folktables_employment_models_params_for_tuning(experiment_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "048d1899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:37.675129Z",
     "start_time": "2024-01-04T20:53:37.670178Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 20:15:17 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 400,\n",
      " 'experiment_iteration': 'Exp_iter_4',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'intervention_param': 'True',\n",
      " 'model_init_seed': 400,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5b534568b14897b8686d22a9c79d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 20:15:17 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-04 20:15:17 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([331, 157, 559, 553, 580, 169, 561, 452, 180, 257, 160, 289, 197,\n",
      "             39, 290,  68,  56, 638,  54, 120],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([331, 157, 559, 553, 580, 169, 561, 452, 180, 257, 160, 289, 197,\n",
      "             39, 290,  68,  56, 638,  54, 120],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 400, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42164aebf0d74a5992f0ca8075639200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a128a0a62f40fb9bd0b88e41d55103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6300cf6db9cc4bcf95389b1f6e9b8108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e75feb8b5c4e00a1fc551544462dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_eq_odds(data_loader=exp_iter_data_loader,\n",
    "                          experiment_seed=experiment_seed,\n",
    "                          test_set_fraction=TEST_SET_FRACTION,\n",
    "                          db_writer_func=db_writer_func,\n",
    "                          fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                          models_params_for_tuning=models_params_for_tuning,\n",
    "                          metrics_computation_config=metrics_computation_config,\n",
    "                          custom_table_fields_dct=custom_table_fields_dct,\n",
    "                          # with_tuning=True,\n",
    "                          with_tuning=False,\n",
    "                          tuned_params_df_paths=tuned_params_df_paths,\n",
    "                          save_results_dir_path=SAVE_RESULTS_DIR_PATH,\n",
    "                          dataset_name='StudentPerformancePortugueseDataset',\n",
    "                          postprocessor_name='ROC',\n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6ac04",
   "metadata": {},
   "source": [
    "### Experiment iteration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13c30aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 5\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "tuned_params_filenames = [\n",
    "]\n",
    "tuned_params_df_paths = [os.path.join(ROOT_DIR, 'results', 'diff_fairness_interventions_exp',\n",
    "                                      FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME, tuned_params_filename)\n",
    "                         for tuned_params_filename in tuned_params_filenames]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage\n",
    "models_params_for_tuning = get_folktables_employment_models_params_for_tuning(experiment_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4077068c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:27.080554Z",
     "start_time": "2024-01-04T20:53:27.072313Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 03:55:49 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 500,\n",
      " 'experiment_iteration': 'Exp_iter_5',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'model_init_seed': 500,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8257cfd4f743e8887bbc74613694af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 03:55:49 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-05 03:55:49 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([ 92, 640, 589, 519, 377, 478, 298, 336, 149, 278, 343, 573, 365,\n",
      "            174, 171, 219, 469, 162, 567, 203],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([ 92, 640, 589, 519, 377, 478, 298, 336, 149, 278, 343, 573, 365,\n",
      "            174, 171, 219, 469, 162, 567, 203],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 500, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2d40e5f54c4d67bf9f1e66289ae23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63535df3c576469e9e855b70194bc572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15688b8597f74cdaadb60cc93599720c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3165f4884fae4f29829de3acc475dac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_eq_odds(data_loader=exp_iter_data_loader,\n",
    "                          experiment_seed=experiment_seed,\n",
    "                          test_set_fraction=TEST_SET_FRACTION,\n",
    "                          db_writer_func=db_writer_func,\n",
    "                          fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                          models_params_for_tuning=models_params_for_tuning,\n",
    "                          metrics_computation_config=metrics_computation_config,\n",
    "                          custom_table_fields_dct=custom_table_fields_dct,\n",
    "                          # with_tuning=True,\n",
    "                          with_tuning=False,\n",
    "                          tuned_params_df_paths=tuned_params_df_paths,\n",
    "                          save_results_dir_path=SAVE_RESULTS_DIR_PATH,\n",
    "                          dataset_name='StudentPerformancePortugueseDataset',\n",
    "                          postprocessor_name='ROC',\n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f1dfe",
   "metadata": {},
   "source": [
    "### Experiment iteration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e80e4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 6\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "tuned_params_filenames = [\n",
    "]\n",
    "tuned_params_df_paths = [os.path.join(ROOT_DIR, 'results', 'diff_fairness_interventions_exp',\n",
    "                                      FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME, tuned_params_filename)\n",
    "                         for tuned_params_filename in tuned_params_filenames]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage\n",
    "models_params_for_tuning = get_folktables_employment_models_params_for_tuning(experiment_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad31a7be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:16.632770Z",
     "start_time": "2024-01-04T20:53:16.629083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 04:49:43 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 600,\n",
      " 'experiment_iteration': 'Exp_iter_6',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'intervention_param': 'True',\n",
      " 'model_init_seed': 600,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2670abffe30e4a388a7a09306feb6673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 04:49:43 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-05 04:49:43 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([390, 397, 375, 507, 625, 369, 245, 428, 593, 102,  49, 621,  92,\n",
      "            158, 505, 612,  29, 452,  56, 145],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([390, 397, 375, 507, 625, 369, 245, 428, 593, 102,  49, 621,  92,\n",
      "            158, 505, 612,  29, 452,  56, 145],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 600, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f28080438649c0aa4a0f71f4b322c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511b20dacfe5420981d3cf173f60fc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40a989d937f483784b5138783479302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c5fe28a14a47118da3a7d485a77e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_eq_odds(data_loader=exp_iter_data_loader,\n",
    "                          experiment_seed=experiment_seed,\n",
    "                          test_set_fraction=TEST_SET_FRACTION,\n",
    "                          db_writer_func=db_writer_func,\n",
    "                          fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                          models_params_for_tuning=models_params_for_tuning,\n",
    "                          metrics_computation_config=metrics_computation_config,\n",
    "                          custom_table_fields_dct=custom_table_fields_dct,\n",
    "                          # with_tuning=True,\n",
    "                          with_tuning=False,\n",
    "                          tuned_params_df_paths=tuned_params_df_paths,\n",
    "                          save_results_dir_path=SAVE_RESULTS_DIR_PATH,\n",
    "                          dataset_name='StudentPerformancePortugueseDataset',\n",
    "                          postprocessor_name='ROC',\n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb640e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
