{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ded11654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5d85e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T10:55:54.052462Z",
     "start_time": "2024-01-06T10:55:54.038357Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall virny -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf024b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T10:56:09.679156Z",
     "start_time": "2024-01-06T10:56:09.668186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install using an HTTP link\n",
    "# !pip install git+https://github.com/DataResponsibly/Virny.git@feature/prepare_experiments_for_inprocessors\n",
    "\n",
    "# Install using an SSH link\n",
    "# !pip install git+ssh://git@github.com/DataResponsibly/Virny.git@feature/prepare_experiments_for_inprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39463327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:26.457257Z",
     "start_time": "2024-01-06T11:15:26.114625Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d83962",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:26.466361Z",
     "start_time": "2024-01-06T11:15:26.457627Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295621db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:26.478005Z",
     "start_time": "2024-01-06T11:15:26.467253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current location:  /home/denys_herasymuk/Research/NYU/Code/fairness-variance\n"
     ]
    }
   ],
   "source": [
    "cur_folder_name = os.getcwd().split('/')[-1]\n",
    "if cur_folder_name != \"fairness-variance\":\n",
    "    os.chdir(\"../../../..\")\n",
    "\n",
    "print('Current location: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d6dfa",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65442379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:31.734704Z",
     "start_time": "2024-01-06T11:15:28.036691Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:46:55.846896: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-08 14:46:55.906990: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-08 14:46:55.908072: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-08 14:46:56.870894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from virny.utils.custom_initializers import create_config_obj\n",
    "from virny.datasets import ACSIncomeDataset\n",
    "\n",
    "from configs.constants import TEST_SET_FRACTION, EXPERIMENT_SEEDS\n",
    "\n",
    "from source.experiment_interface import run_exp_iter_with_inprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb488976",
   "metadata": {},
   "source": [
    "## Define Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1f91c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:31.772286Z",
     "start_time": "2024-01-06T11:15:31.735883Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "EXPERIMENT_NAME = 'ADB_acs_income'\n",
    "DB_COLLECTION_NAME = 'one_repair_lvl_many_models'\n",
    "FAIRNESS_INTERVENTION_NAME = 'ADB'\n",
    "FAIR_INTERVENTION_PARAMS_LST = ['debiased_classifier']\n",
    "SAVE_RESULTS_DIR_PATH = os.path.join(ROOT_DIR, 'results', 'diff_fairness_interventions_exp',\n",
    "                                     FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME)\n",
    "\n",
    "config_yaml_path = os.path.join(ROOT_DIR, 'notebooks', 'diff_fairness_interventions_exp',\n",
    "                                FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME, 'folk_GA_2018_config.yaml')\n",
    "metrics_computation_config = create_config_obj(config_yaml_path=config_yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42b20f",
   "metadata": {},
   "source": [
    "## Define a db writer and custom fields to insert into your database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ed4e58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:31.813421Z",
     "start_time": "2024-01-06T11:15:31.771935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'fairness_variance'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./configs/secrets.env')\n",
    "os.getenv(\"DB_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2539023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:32.096974Z",
     "start_time": "2024-01-06T11:15:31.811395Z"
    }
   },
   "outputs": [],
   "source": [
    "from source.utils.db_functions import connect_to_mongodb\n",
    "\n",
    "client, collection_obj, db_writer_func = connect_to_mongodb(DB_COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9682b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:32.138747Z",
     "start_time": "2024-01-06T11:15:32.097343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current session uuid:  84eeb5f0-4ebe-4d9f-94ef-53ae302c2264\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "custom_table_fields_dct = {\n",
    "    # 'session_uuid': str(uuid.uuid4()),\n",
    "    'session_uuid': '84eeb5f0-4ebe-4d9f-94ef-53ae302c2264',\n",
    "}\n",
    "print('Current session uuid: ', custom_table_fields_dct['session_uuid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b32200",
   "metadata": {},
   "source": [
    "## Initialize custom objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7470042d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:33.528732Z",
     "start_time": "2024-01-06T11:15:33.475702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  SCHL COW MAR  OCCP POBP RELP SEX RAC1P  AGEP  WKHP\n0   23   7   3   230   36    0   1     1    55  55.0\n1   16   1   5  4110   13    2   2     1    20  35.0\n2   16   4   3  4130   51    0   2     1    59  30.0\n3   18   4   1  4020   13    0   1     2    43  40.0\n4   14   1   1  8300   20    1   2     2    33  20.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SCHL</th>\n      <th>COW</th>\n      <th>MAR</th>\n      <th>OCCP</th>\n      <th>POBP</th>\n      <th>RELP</th>\n      <th>SEX</th>\n      <th>RAC1P</th>\n      <th>AGEP</th>\n      <th>WKHP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23</td>\n      <td>7</td>\n      <td>3</td>\n      <td>230</td>\n      <td>36</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>55</td>\n      <td>55.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4110</td>\n      <td>13</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>20</td>\n      <td>35.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4130</td>\n      <td>51</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>59</td>\n      <td>30.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18</td>\n      <td>4</td>\n      <td>1</td>\n      <td>4020</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>43</td>\n      <td>40.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>14</td>\n      <td>1</td>\n      <td>1</td>\n      <td>8300</td>\n      <td>20</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>33</td>\n      <td>20.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = ACSIncomeDataset(state=['GA'], year=2018, with_nulls=False,\n",
    "                               subsample_size=15_000, subsample_seed=42)\n",
    "data_loader.X_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cce54a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:34.580537Z",
     "start_time": "2024-01-06T11:15:34.538952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(15000, 10)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.X_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c8f07",
   "metadata": {},
   "source": [
    "## Run experiment iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced15cb",
   "metadata": {},
   "source": [
    "### Experiment iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9302f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:37.135031Z",
     "start_time": "2024-01-06T11:15:37.105079Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 1\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26d52fce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T11:15:44.618835Z",
     "start_time": "2024-01-06T11:15:43.745040Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:47:01 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 100,\n",
      " 'experiment_iteration': 'Exp_iter_1',\n",
      " 'fair_intervention_params_lst': \"['debiased_classifier']\",\n",
      " 'model_init_seed': 100,\n",
      " 'session_uuid': '84eeb5f0-4ebe-4d9f-94ef-53ae302c2264'}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0774252fc3d84263b3f1952d6646e165"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:47:01 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  debiased_classifier\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__SCHL_1', 'cat__SCHL_10', 'cat__SCHL_11', 'cat__SCHL_12',\n",
      "       'cat__SCHL_13', 'cat__SCHL_14', 'cat__SCHL_15', 'cat__SCHL_16',\n",
      "       'cat__SCHL_17', 'cat__SCHL_18',\n",
      "       ...\n",
      "       'cat__RELP_3', 'cat__RELP_4', 'cat__RELP_5', 'cat__RELP_6',\n",
      "       'cat__RELP_7', 'cat__RELP_8', 'cat__RELP_9', 'num__AGEP', 'num__WKHP',\n",
      "       'SEX&RAC1P_binary'],\n",
      "      dtype='object', length=724)\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([10155, 11689, 12599, 12193,  8678,  8217,  4670, 12087,  5235,\n",
      "             4189,  7278, 10642,  5284,  7002, 14642, 10594,  7701,  8686,\n",
      "             8665,  6253],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([10155, 11689, 12599, 12193,  8678,  8217,  4670, 12087,  5235,\n",
      "             4189,  7278, 10642,  5284,  7002, 14642, 10594,  7701,  8686,\n",
      "             8665,  6253],\n",
      "           dtype='int64')\n",
      "Using AdversarialDebiasing postprocessor\n"
     ]
    },
    {
     "data": {
      "text/plain": "Analyze multiple models:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6afb71fcf4f547769fec45358ef2d8c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Classifiers testing by bootstrap:   0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b81e93a7c9ca4b759124ebae6b6a39d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/denys_herasymuk/Research/NYU/Code/fairness-variance/faact_venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/denys_herasymuk/Research/NYU/Code/fairness-variance/faact_venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2024-01-08 14:47:02.022905: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.706076; batch adversarial loss: 0.692046\n",
      "epoch 1; iter: 0; batch classifier loss: 0.528616; batch adversarial loss: 0.690824\n",
      "epoch 2; iter: 0; batch classifier loss: 0.437058; batch adversarial loss: 0.641205\n",
      "epoch 3; iter: 0; batch classifier loss: 0.374626; batch adversarial loss: 0.591401\n",
      "epoch 4; iter: 0; batch classifier loss: 0.381561; batch adversarial loss: 0.578531\n",
      "epoch 5; iter: 0; batch classifier loss: 0.362062; batch adversarial loss: 0.540871\n",
      "epoch 6; iter: 0; batch classifier loss: 0.253209; batch adversarial loss: 0.536458\n",
      "epoch 7; iter: 0; batch classifier loss: 0.249384; batch adversarial loss: 0.510654\n",
      "epoch 8; iter: 0; batch classifier loss: 0.222574; batch adversarial loss: 0.526174\n",
      "epoch 9; iter: 0; batch classifier loss: 0.231929; batch adversarial loss: 0.544187\n",
      "epoch 10; iter: 0; batch classifier loss: 0.269365; batch adversarial loss: 0.530938\n",
      "epoch 11; iter: 0; batch classifier loss: 0.269131; batch adversarial loss: 0.499622\n",
      "epoch 12; iter: 0; batch classifier loss: 0.209271; batch adversarial loss: 0.500838\n",
      "epoch 13; iter: 0; batch classifier loss: 0.232830; batch adversarial loss: 0.522906\n",
      "epoch 14; iter: 0; batch classifier loss: 0.200820; batch adversarial loss: 0.444583\n",
      "epoch 15; iter: 0; batch classifier loss: 0.175221; batch adversarial loss: 0.468809\n",
      "epoch 16; iter: 0; batch classifier loss: 0.195260; batch adversarial loss: 0.487876\n",
      "epoch 17; iter: 0; batch classifier loss: 0.108377; batch adversarial loss: 0.462686\n",
      "epoch 18; iter: 0; batch classifier loss: 0.154744; batch adversarial loss: 0.465643\n",
      "epoch 19; iter: 0; batch classifier loss: 0.114956; batch adversarial loss: 0.518928\n",
      "epoch 20; iter: 0; batch classifier loss: 0.185771; batch adversarial loss: 0.457848\n",
      "epoch 21; iter: 0; batch classifier loss: 0.114915; batch adversarial loss: 0.514133\n",
      "epoch 22; iter: 0; batch classifier loss: 0.135782; batch adversarial loss: 0.513373\n",
      "epoch 23; iter: 0; batch classifier loss: 0.134654; batch adversarial loss: 0.569677\n",
      "epoch 24; iter: 0; batch classifier loss: 0.175153; batch adversarial loss: 0.454520\n",
      "epoch 25; iter: 0; batch classifier loss: 0.180932; batch adversarial loss: 0.524089\n",
      "epoch 26; iter: 0; batch classifier loss: 0.194350; batch adversarial loss: 0.554577\n",
      "epoch 27; iter: 0; batch classifier loss: 0.175350; batch adversarial loss: 0.572550\n",
      "epoch 28; iter: 0; batch classifier loss: 0.203875; batch adversarial loss: 0.621340\n",
      "epoch 29; iter: 0; batch classifier loss: 0.220782; batch adversarial loss: 0.584977\n",
      "epoch 30; iter: 0; batch classifier loss: 0.192389; batch adversarial loss: 0.494302\n",
      "epoch 31; iter: 0; batch classifier loss: 0.166049; batch adversarial loss: 0.488495\n",
      "epoch 32; iter: 0; batch classifier loss: 0.181273; batch adversarial loss: 0.477612\n",
      "epoch 33; iter: 0; batch classifier loss: 0.197128; batch adversarial loss: 0.508084\n",
      "epoch 34; iter: 0; batch classifier loss: 0.233774; batch adversarial loss: 0.480641\n",
      "epoch 35; iter: 0; batch classifier loss: 0.171880; batch adversarial loss: 0.430034\n",
      "epoch 36; iter: 0; batch classifier loss: 0.272469; batch adversarial loss: 0.529935\n",
      "epoch 37; iter: 0; batch classifier loss: 0.227646; batch adversarial loss: 0.448445\n",
      "epoch 38; iter: 0; batch classifier loss: 0.163945; batch adversarial loss: 0.406804\n",
      "epoch 39; iter: 0; batch classifier loss: 0.079406; batch adversarial loss: 0.448137\n",
      "epoch 40; iter: 0; batch classifier loss: 0.091811; batch adversarial loss: 0.446052\n",
      "epoch 41; iter: 0; batch classifier loss: 0.067578; batch adversarial loss: 0.525227\n",
      "epoch 42; iter: 0; batch classifier loss: 0.059282; batch adversarial loss: 0.496629\n",
      "epoch 43; iter: 0; batch classifier loss: 0.104444; batch adversarial loss: 0.455580\n",
      "epoch 44; iter: 0; batch classifier loss: 0.119299; batch adversarial loss: 0.454710\n",
      "epoch 45; iter: 0; batch classifier loss: 0.076554; batch adversarial loss: 0.489845\n",
      "epoch 46; iter: 0; batch classifier loss: 0.086420; batch adversarial loss: 0.475786\n",
      "epoch 47; iter: 0; batch classifier loss: 0.067454; batch adversarial loss: 0.383989\n",
      "epoch 48; iter: 0; batch classifier loss: 0.099616; batch adversarial loss: 0.504962\n",
      "epoch 49; iter: 0; batch classifier loss: 0.129307; batch adversarial loss: 0.369170\n",
      "epoch 50; iter: 0; batch classifier loss: 0.074693; batch adversarial loss: 0.344991\n",
      "epoch 51; iter: 0; batch classifier loss: 0.073794; batch adversarial loss: 0.533122\n",
      "epoch 52; iter: 0; batch classifier loss: 0.101932; batch adversarial loss: 0.551938\n",
      "epoch 53; iter: 0; batch classifier loss: 0.079983; batch adversarial loss: 0.460136\n",
      "epoch 54; iter: 0; batch classifier loss: 0.074297; batch adversarial loss: 0.473660\n",
      "epoch 55; iter: 0; batch classifier loss: 0.119616; batch adversarial loss: 0.417397\n",
      "epoch 56; iter: 0; batch classifier loss: 0.096468; batch adversarial loss: 0.344682\n",
      "epoch 57; iter: 0; batch classifier loss: 0.112178; batch adversarial loss: 0.483005\n",
      "epoch 58; iter: 0; batch classifier loss: 0.097981; batch adversarial loss: 0.495853\n",
      "epoch 59; iter: 0; batch classifier loss: 0.082937; batch adversarial loss: 0.485810\n",
      "epoch 60; iter: 0; batch classifier loss: 0.061327; batch adversarial loss: 0.522080\n",
      "epoch 61; iter: 0; batch classifier loss: 0.090307; batch adversarial loss: 0.448743\n",
      "epoch 62; iter: 0; batch classifier loss: 0.047843; batch adversarial loss: 0.452977\n",
      "epoch 63; iter: 0; batch classifier loss: 0.065588; batch adversarial loss: 0.387373\n",
      "epoch 64; iter: 0; batch classifier loss: 0.098521; batch adversarial loss: 0.422866\n",
      "epoch 65; iter: 0; batch classifier loss: 0.058718; batch adversarial loss: 0.489143\n",
      "epoch 66; iter: 0; batch classifier loss: 0.089657; batch adversarial loss: 0.364338\n",
      "epoch 67; iter: 0; batch classifier loss: 0.069073; batch adversarial loss: 0.508808\n",
      "epoch 68; iter: 0; batch classifier loss: 0.079268; batch adversarial loss: 0.435710\n",
      "epoch 69; iter: 0; batch classifier loss: 0.075794; batch adversarial loss: 0.451353\n",
      "epoch 70; iter: 0; batch classifier loss: 0.103014; batch adversarial loss: 0.524944\n",
      "epoch 71; iter: 0; batch classifier loss: 0.079265; batch adversarial loss: 0.458990\n",
      "epoch 72; iter: 0; batch classifier loss: 0.054589; batch adversarial loss: 0.416570\n",
      "epoch 73; iter: 0; batch classifier loss: 0.067984; batch adversarial loss: 0.571959\n",
      "epoch 74; iter: 0; batch classifier loss: 0.075297; batch adversarial loss: 0.431452\n",
      "epoch 75; iter: 0; batch classifier loss: 0.071271; batch adversarial loss: 0.468172\n",
      "epoch 76; iter: 0; batch classifier loss: 0.070982; batch adversarial loss: 0.451261\n",
      "epoch 77; iter: 0; batch classifier loss: 0.076285; batch adversarial loss: 0.433854\n",
      "epoch 78; iter: 0; batch classifier loss: 0.062035; batch adversarial loss: 0.504788\n",
      "epoch 79; iter: 0; batch classifier loss: 0.075761; batch adversarial loss: 0.467174\n",
      "epoch 80; iter: 0; batch classifier loss: 0.098143; batch adversarial loss: 0.505237\n",
      "epoch 81; iter: 0; batch classifier loss: 0.084273; batch adversarial loss: 0.357909\n",
      "epoch 82; iter: 0; batch classifier loss: 0.071487; batch adversarial loss: 0.473864\n",
      "epoch 83; iter: 0; batch classifier loss: 0.121158; batch adversarial loss: 0.486395\n",
      "epoch 84; iter: 0; batch classifier loss: 0.045600; batch adversarial loss: 0.409071\n",
      "epoch 85; iter: 0; batch classifier loss: 0.065818; batch adversarial loss: 0.503372\n",
      "epoch 86; iter: 0; batch classifier loss: 0.082517; batch adversarial loss: 0.587905\n",
      "epoch 87; iter: 0; batch classifier loss: 0.111101; batch adversarial loss: 0.466063\n",
      "epoch 88; iter: 0; batch classifier loss: 0.055440; batch adversarial loss: 0.564533\n",
      "epoch 89; iter: 0; batch classifier loss: 0.089993; batch adversarial loss: 0.390280\n",
      "epoch 90; iter: 0; batch classifier loss: 0.082860; batch adversarial loss: 0.463198\n",
      "epoch 91; iter: 0; batch classifier loss: 0.047112; batch adversarial loss: 0.537224\n",
      "epoch 92; iter: 0; batch classifier loss: 0.101391; batch adversarial loss: 0.538014\n",
      "epoch 93; iter: 0; batch classifier loss: 0.131220; batch adversarial loss: 0.459360\n",
      "epoch 94; iter: 0; batch classifier loss: 0.109573; batch adversarial loss: 0.465781\n",
      "epoch 95; iter: 0; batch classifier loss: 0.054340; batch adversarial loss: 0.397680\n",
      "epoch 96; iter: 0; batch classifier loss: 0.046966; batch adversarial loss: 0.495775\n",
      "epoch 97; iter: 0; batch classifier loss: 0.062126; batch adversarial loss: 0.498909\n",
      "epoch 98; iter: 0; batch classifier loss: 0.062363; batch adversarial loss: 0.431311\n",
      "epoch 99; iter: 0; batch classifier loss: 0.102993; batch adversarial loss: 0.376494\n",
      "epoch 100; iter: 0; batch classifier loss: 0.102044; batch adversarial loss: 0.357074\n",
      "epoch 101; iter: 0; batch classifier loss: 0.054333; batch adversarial loss: 0.467210\n",
      "epoch 102; iter: 0; batch classifier loss: 0.054543; batch adversarial loss: 0.427762\n",
      "epoch 103; iter: 0; batch classifier loss: 0.034367; batch adversarial loss: 0.450090\n",
      "epoch 104; iter: 0; batch classifier loss: 0.065666; batch adversarial loss: 0.440686\n",
      "epoch 105; iter: 0; batch classifier loss: 0.081424; batch adversarial loss: 0.314604\n",
      "epoch 106; iter: 0; batch classifier loss: 0.067678; batch adversarial loss: 0.510275\n",
      "epoch 107; iter: 0; batch classifier loss: 0.063083; batch adversarial loss: 0.474941\n",
      "epoch 108; iter: 0; batch classifier loss: 0.019212; batch adversarial loss: 0.407522\n",
      "epoch 109; iter: 0; batch classifier loss: 0.034522; batch adversarial loss: 0.409571\n",
      "epoch 110; iter: 0; batch classifier loss: 0.058404; batch adversarial loss: 0.502819\n",
      "epoch 111; iter: 0; batch classifier loss: 0.040291; batch adversarial loss: 0.444476\n",
      "epoch 112; iter: 0; batch classifier loss: 0.042527; batch adversarial loss: 0.447824\n",
      "epoch 113; iter: 0; batch classifier loss: 0.053517; batch adversarial loss: 0.419865\n",
      "epoch 114; iter: 0; batch classifier loss: 0.054878; batch adversarial loss: 0.463392\n",
      "epoch 115; iter: 0; batch classifier loss: 0.053708; batch adversarial loss: 0.508642\n",
      "epoch 116; iter: 0; batch classifier loss: 0.035963; batch adversarial loss: 0.455745\n",
      "epoch 117; iter: 0; batch classifier loss: 0.072132; batch adversarial loss: 0.499220\n",
      "epoch 118; iter: 0; batch classifier loss: 0.043367; batch adversarial loss: 0.420356\n",
      "epoch 119; iter: 0; batch classifier loss: 0.048310; batch adversarial loss: 0.492525\n",
      "epoch 120; iter: 0; batch classifier loss: 0.074792; batch adversarial loss: 0.405318\n",
      "epoch 121; iter: 0; batch classifier loss: 0.050102; batch adversarial loss: 0.584632\n",
      "epoch 122; iter: 0; batch classifier loss: 0.066402; batch adversarial loss: 0.514826\n",
      "epoch 123; iter: 0; batch classifier loss: 0.048277; batch adversarial loss: 0.436852\n",
      "epoch 124; iter: 0; batch classifier loss: 0.040002; batch adversarial loss: 0.377582\n",
      "epoch 125; iter: 0; batch classifier loss: 0.017416; batch adversarial loss: 0.497774\n",
      "epoch 126; iter: 0; batch classifier loss: 0.071384; batch adversarial loss: 0.499982\n",
      "epoch 127; iter: 0; batch classifier loss: 0.037890; batch adversarial loss: 0.466903\n",
      "epoch 128; iter: 0; batch classifier loss: 0.045670; batch adversarial loss: 0.433337\n",
      "epoch 129; iter: 0; batch classifier loss: 0.043349; batch adversarial loss: 0.491828\n",
      "epoch 130; iter: 0; batch classifier loss: 0.017410; batch adversarial loss: 0.436271\n",
      "epoch 131; iter: 0; batch classifier loss: 0.050517; batch adversarial loss: 0.450991\n",
      "epoch 132; iter: 0; batch classifier loss: 0.047536; batch adversarial loss: 0.512238\n",
      "epoch 133; iter: 0; batch classifier loss: 0.013396; batch adversarial loss: 0.419450\n",
      "epoch 134; iter: 0; batch classifier loss: 0.069495; batch adversarial loss: 0.453873\n",
      "epoch 135; iter: 0; batch classifier loss: 0.049653; batch adversarial loss: 0.442644\n",
      "epoch 136; iter: 0; batch classifier loss: 0.039447; batch adversarial loss: 0.452397\n",
      "epoch 137; iter: 0; batch classifier loss: 0.046000; batch adversarial loss: 0.442586\n",
      "epoch 138; iter: 0; batch classifier loss: 0.029759; batch adversarial loss: 0.361690\n",
      "epoch 139; iter: 0; batch classifier loss: 0.037082; batch adversarial loss: 0.530520\n",
      "epoch 140; iter: 0; batch classifier loss: 0.061619; batch adversarial loss: 0.476281\n",
      "epoch 141; iter: 0; batch classifier loss: 0.027611; batch adversarial loss: 0.463160\n",
      "epoch 142; iter: 0; batch classifier loss: 0.048353; batch adversarial loss: 0.430123\n",
      "epoch 143; iter: 0; batch classifier loss: 0.023072; batch adversarial loss: 0.542561\n",
      "epoch 144; iter: 0; batch classifier loss: 0.014643; batch adversarial loss: 0.402583\n",
      "epoch 145; iter: 0; batch classifier loss: 0.040828; batch adversarial loss: 0.458558\n",
      "epoch 146; iter: 0; batch classifier loss: 0.052981; batch adversarial loss: 0.499904\n",
      "epoch 147; iter: 0; batch classifier loss: 0.052879; batch adversarial loss: 0.468605\n",
      "epoch 148; iter: 0; batch classifier loss: 0.054970; batch adversarial loss: 0.437735\n",
      "epoch 149; iter: 0; batch classifier loss: 0.013094; batch adversarial loss: 0.392392\n",
      "epoch 150; iter: 0; batch classifier loss: 0.054930; batch adversarial loss: 0.392228\n",
      "epoch 151; iter: 0; batch classifier loss: 0.033070; batch adversarial loss: 0.429573\n",
      "epoch 152; iter: 0; batch classifier loss: 0.013686; batch adversarial loss: 0.480302\n",
      "epoch 153; iter: 0; batch classifier loss: 0.025517; batch adversarial loss: 0.474301\n",
      "epoch 154; iter: 0; batch classifier loss: 0.026941; batch adversarial loss: 0.397288\n",
      "epoch 155; iter: 0; batch classifier loss: 0.021937; batch adversarial loss: 0.382238\n",
      "epoch 156; iter: 0; batch classifier loss: 0.024708; batch adversarial loss: 0.386828\n",
      "epoch 157; iter: 0; batch classifier loss: 0.029149; batch adversarial loss: 0.362087\n",
      "epoch 158; iter: 0; batch classifier loss: 0.032100; batch adversarial loss: 0.514566\n",
      "epoch 159; iter: 0; batch classifier loss: 0.026255; batch adversarial loss: 0.514117\n",
      "epoch 160; iter: 0; batch classifier loss: 0.030920; batch adversarial loss: 0.367334\n",
      "epoch 161; iter: 0; batch classifier loss: 0.015507; batch adversarial loss: 0.385345\n",
      "epoch 162; iter: 0; batch classifier loss: 0.041188; batch adversarial loss: 0.557631\n",
      "epoch 163; iter: 0; batch classifier loss: 0.038723; batch adversarial loss: 0.467234\n",
      "epoch 164; iter: 0; batch classifier loss: 0.024678; batch adversarial loss: 0.458803\n",
      "epoch 165; iter: 0; batch classifier loss: 0.020329; batch adversarial loss: 0.389683\n",
      "epoch 166; iter: 0; batch classifier loss: 0.036762; batch adversarial loss: 0.340316\n",
      "epoch 167; iter: 0; batch classifier loss: 0.030933; batch adversarial loss: 0.488713\n",
      "epoch 168; iter: 0; batch classifier loss: 0.017171; batch adversarial loss: 0.493203\n",
      "epoch 169; iter: 0; batch classifier loss: 0.040985; batch adversarial loss: 0.462638\n",
      "epoch 170; iter: 0; batch classifier loss: 0.028517; batch adversarial loss: 0.418939\n",
      "epoch 171; iter: 0; batch classifier loss: 0.026486; batch adversarial loss: 0.536990\n",
      "epoch 172; iter: 0; batch classifier loss: 0.020521; batch adversarial loss: 0.486689\n",
      "epoch 173; iter: 0; batch classifier loss: 0.030953; batch adversarial loss: 0.454736\n",
      "epoch 174; iter: 0; batch classifier loss: 0.006511; batch adversarial loss: 0.437390\n",
      "epoch 175; iter: 0; batch classifier loss: 0.016025; batch adversarial loss: 0.431013\n",
      "epoch 176; iter: 0; batch classifier loss: 0.023440; batch adversarial loss: 0.506947\n",
      "epoch 177; iter: 0; batch classifier loss: 0.049653; batch adversarial loss: 0.402723\n",
      "epoch 178; iter: 0; batch classifier loss: 0.016105; batch adversarial loss: 0.441069\n",
      "epoch 179; iter: 0; batch classifier loss: 0.010429; batch adversarial loss: 0.496293\n",
      "epoch 180; iter: 0; batch classifier loss: 0.021091; batch adversarial loss: 0.415183\n",
      "epoch 181; iter: 0; batch classifier loss: 0.021063; batch adversarial loss: 0.424210\n",
      "epoch 182; iter: 0; batch classifier loss: 0.029554; batch adversarial loss: 0.479177\n",
      "epoch 183; iter: 0; batch classifier loss: 0.011231; batch adversarial loss: 0.444569\n",
      "epoch 184; iter: 0; batch classifier loss: 0.027491; batch adversarial loss: 0.465693\n",
      "epoch 185; iter: 0; batch classifier loss: 0.033915; batch adversarial loss: 0.559489\n",
      "epoch 186; iter: 0; batch classifier loss: 0.023120; batch adversarial loss: 0.453808\n",
      "epoch 187; iter: 0; batch classifier loss: 0.012470; batch adversarial loss: 0.402795\n",
      "epoch 188; iter: 0; batch classifier loss: 0.028664; batch adversarial loss: 0.427348\n",
      "epoch 189; iter: 0; batch classifier loss: 0.041508; batch adversarial loss: 0.460209\n",
      "epoch 190; iter: 0; batch classifier loss: 0.026672; batch adversarial loss: 0.323445\n",
      "epoch 191; iter: 0; batch classifier loss: 0.039655; batch adversarial loss: 0.390811\n",
      "epoch 192; iter: 0; batch classifier loss: 0.015852; batch adversarial loss: 0.466843\n",
      "epoch 193; iter: 0; batch classifier loss: 0.035564; batch adversarial loss: 0.504015\n",
      "epoch 194; iter: 0; batch classifier loss: 0.008910; batch adversarial loss: 0.485968\n",
      "epoch 195; iter: 0; batch classifier loss: 0.056930; batch adversarial loss: 0.580673\n",
      "epoch 196; iter: 0; batch classifier loss: 0.016108; batch adversarial loss: 0.370200\n",
      "epoch 197; iter: 0; batch classifier loss: 0.045960; batch adversarial loss: 0.446720\n",
      "epoch 198; iter: 0; batch classifier loss: 0.041652; batch adversarial loss: 0.417590\n",
      "epoch 199; iter: 0; batch classifier loss: 0.012187; batch adversarial loss: 0.526439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:47:38.105341: W tensorflow/c/c_api.cc:304] Operation '{name:'04a441b2-ae24-11ee-be98-ef9b34f2853b/04a441b2-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:784 op device:{requested: '', assigned: ''} def:{{{node 04a441b2-ae24-11ee-be98-ef9b34f2853b/04a441b2-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a441b2-ae24-11ee-be98-ef9b34f2853b/04a441b2-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a441b2-ae24-11ee-be98-ef9b34f2853b/04a441b2-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.706066; batch adversarial loss: 0.736104\n",
      "epoch 1; iter: 0; batch classifier loss: 0.530403; batch adversarial loss: 0.678148\n",
      "epoch 2; iter: 0; batch classifier loss: 0.459406; batch adversarial loss: 0.631632\n",
      "epoch 3; iter: 0; batch classifier loss: 0.410012; batch adversarial loss: 0.603996\n",
      "epoch 4; iter: 0; batch classifier loss: 0.406459; batch adversarial loss: 0.622275\n",
      "epoch 5; iter: 0; batch classifier loss: 0.403324; batch adversarial loss: 0.592824\n",
      "epoch 6; iter: 0; batch classifier loss: 0.326480; batch adversarial loss: 0.557714\n",
      "epoch 7; iter: 0; batch classifier loss: 0.264814; batch adversarial loss: 0.563979\n",
      "epoch 8; iter: 0; batch classifier loss: 0.309007; batch adversarial loss: 0.561000\n",
      "epoch 9; iter: 0; batch classifier loss: 0.337536; batch adversarial loss: 0.511946\n",
      "epoch 10; iter: 0; batch classifier loss: 0.358785; batch adversarial loss: 0.497081\n",
      "epoch 11; iter: 0; batch classifier loss: 0.326900; batch adversarial loss: 0.521918\n",
      "epoch 12; iter: 0; batch classifier loss: 0.282505; batch adversarial loss: 0.507427\n",
      "epoch 13; iter: 0; batch classifier loss: 0.319482; batch adversarial loss: 0.490685\n",
      "epoch 14; iter: 0; batch classifier loss: 0.297245; batch adversarial loss: 0.530186\n",
      "epoch 15; iter: 0; batch classifier loss: 0.348114; batch adversarial loss: 0.492884\n",
      "epoch 16; iter: 0; batch classifier loss: 0.315601; batch adversarial loss: 0.543774\n",
      "epoch 17; iter: 0; batch classifier loss: 0.353514; batch adversarial loss: 0.479612\n",
      "epoch 18; iter: 0; batch classifier loss: 0.284691; batch adversarial loss: 0.557426\n",
      "epoch 19; iter: 0; batch classifier loss: 0.321636; batch adversarial loss: 0.461743\n",
      "epoch 20; iter: 0; batch classifier loss: 0.313884; batch adversarial loss: 0.535758\n",
      "epoch 21; iter: 0; batch classifier loss: 0.354032; batch adversarial loss: 0.452499\n",
      "epoch 22; iter: 0; batch classifier loss: 0.307755; batch adversarial loss: 0.462012\n",
      "epoch 23; iter: 0; batch classifier loss: 0.339093; batch adversarial loss: 0.481600\n",
      "epoch 24; iter: 0; batch classifier loss: 0.280347; batch adversarial loss: 0.487124\n",
      "epoch 25; iter: 0; batch classifier loss: 0.349690; batch adversarial loss: 0.456283\n",
      "epoch 26; iter: 0; batch classifier loss: 0.296061; batch adversarial loss: 0.497201\n",
      "epoch 27; iter: 0; batch classifier loss: 0.333900; batch adversarial loss: 0.465661\n",
      "epoch 28; iter: 0; batch classifier loss: 0.244060; batch adversarial loss: 0.450193\n",
      "epoch 29; iter: 0; batch classifier loss: 0.281684; batch adversarial loss: 0.416492\n",
      "epoch 30; iter: 0; batch classifier loss: 0.234843; batch adversarial loss: 0.482024\n",
      "epoch 31; iter: 0; batch classifier loss: 0.302185; batch adversarial loss: 0.407197\n",
      "epoch 32; iter: 0; batch classifier loss: 0.187466; batch adversarial loss: 0.474231\n",
      "epoch 33; iter: 0; batch classifier loss: 0.230678; batch adversarial loss: 0.523563\n",
      "epoch 34; iter: 0; batch classifier loss: 0.241098; batch adversarial loss: 0.496128\n",
      "epoch 35; iter: 0; batch classifier loss: 0.299920; batch adversarial loss: 0.424774\n",
      "epoch 36; iter: 0; batch classifier loss: 0.261082; batch adversarial loss: 0.428908\n",
      "epoch 37; iter: 0; batch classifier loss: 0.260111; batch adversarial loss: 0.474318\n",
      "epoch 38; iter: 0; batch classifier loss: 0.284937; batch adversarial loss: 0.372036\n",
      "epoch 39; iter: 0; batch classifier loss: 0.288252; batch adversarial loss: 0.389466\n",
      "epoch 40; iter: 0; batch classifier loss: 0.252943; batch adversarial loss: 0.422020\n",
      "epoch 41; iter: 0; batch classifier loss: 0.203067; batch adversarial loss: 0.377419\n",
      "epoch 42; iter: 0; batch classifier loss: 0.247434; batch adversarial loss: 0.503243\n",
      "epoch 43; iter: 0; batch classifier loss: 0.251894; batch adversarial loss: 0.407028\n",
      "epoch 44; iter: 0; batch classifier loss: 0.323468; batch adversarial loss: 0.434153\n",
      "epoch 45; iter: 0; batch classifier loss: 0.277021; batch adversarial loss: 0.421189\n",
      "epoch 46; iter: 0; batch classifier loss: 0.304496; batch adversarial loss: 0.501504\n",
      "epoch 47; iter: 0; batch classifier loss: 0.246984; batch adversarial loss: 0.447492\n",
      "epoch 48; iter: 0; batch classifier loss: 0.231022; batch adversarial loss: 0.519735\n",
      "epoch 49; iter: 0; batch classifier loss: 0.179348; batch adversarial loss: 0.410107\n",
      "epoch 50; iter: 0; batch classifier loss: 0.257322; batch adversarial loss: 0.496071\n",
      "epoch 51; iter: 0; batch classifier loss: 0.267536; batch adversarial loss: 0.423316\n",
      "epoch 52; iter: 0; batch classifier loss: 0.341285; batch adversarial loss: 0.459359\n",
      "epoch 53; iter: 0; batch classifier loss: 0.095520; batch adversarial loss: 0.458750\n",
      "epoch 54; iter: 0; batch classifier loss: 0.116211; batch adversarial loss: 0.434297\n",
      "epoch 55; iter: 0; batch classifier loss: 0.125691; batch adversarial loss: 0.351374\n",
      "epoch 56; iter: 0; batch classifier loss: 0.099187; batch adversarial loss: 0.398515\n",
      "epoch 57; iter: 0; batch classifier loss: 0.078252; batch adversarial loss: 0.323342\n",
      "epoch 58; iter: 0; batch classifier loss: 0.138991; batch adversarial loss: 0.535808\n",
      "epoch 59; iter: 0; batch classifier loss: 0.090362; batch adversarial loss: 0.529794\n",
      "epoch 60; iter: 0; batch classifier loss: 0.081609; batch adversarial loss: 0.375216\n",
      "epoch 61; iter: 0; batch classifier loss: 0.114897; batch adversarial loss: 0.478639\n",
      "epoch 62; iter: 0; batch classifier loss: 0.108687; batch adversarial loss: 0.408623\n",
      "epoch 63; iter: 0; batch classifier loss: 0.104107; batch adversarial loss: 0.481544\n",
      "epoch 64; iter: 0; batch classifier loss: 0.044141; batch adversarial loss: 0.368196\n",
      "epoch 65; iter: 0; batch classifier loss: 0.082003; batch adversarial loss: 0.465862\n",
      "epoch 66; iter: 0; batch classifier loss: 0.105736; batch adversarial loss: 0.443595\n",
      "epoch 67; iter: 0; batch classifier loss: 0.073735; batch adversarial loss: 0.489333\n",
      "epoch 68; iter: 0; batch classifier loss: 0.060975; batch adversarial loss: 0.436080\n",
      "epoch 69; iter: 0; batch classifier loss: 0.073324; batch adversarial loss: 0.429296\n",
      "epoch 70; iter: 0; batch classifier loss: 0.049926; batch adversarial loss: 0.365259\n",
      "epoch 71; iter: 0; batch classifier loss: 0.083830; batch adversarial loss: 0.494927\n",
      "epoch 72; iter: 0; batch classifier loss: 0.074709; batch adversarial loss: 0.461633\n",
      "epoch 73; iter: 0; batch classifier loss: 0.111192; batch adversarial loss: 0.433871\n",
      "epoch 74; iter: 0; batch classifier loss: 0.039334; batch adversarial loss: 0.515788\n",
      "epoch 75; iter: 0; batch classifier loss: 0.071055; batch adversarial loss: 0.406289\n",
      "epoch 76; iter: 0; batch classifier loss: 0.082684; batch adversarial loss: 0.357580\n",
      "epoch 77; iter: 0; batch classifier loss: 0.067315; batch adversarial loss: 0.511623\n",
      "epoch 78; iter: 0; batch classifier loss: 0.069397; batch adversarial loss: 0.385906\n",
      "epoch 79; iter: 0; batch classifier loss: 0.075082; batch adversarial loss: 0.456910\n",
      "epoch 80; iter: 0; batch classifier loss: 0.048823; batch adversarial loss: 0.406157\n",
      "epoch 81; iter: 0; batch classifier loss: 0.081523; batch adversarial loss: 0.411017\n",
      "epoch 82; iter: 0; batch classifier loss: 0.091294; batch adversarial loss: 0.499160\n",
      "epoch 83; iter: 0; batch classifier loss: 0.064026; batch adversarial loss: 0.350682\n",
      "epoch 84; iter: 0; batch classifier loss: 0.042356; batch adversarial loss: 0.517212\n",
      "epoch 85; iter: 0; batch classifier loss: 0.047720; batch adversarial loss: 0.350081\n",
      "epoch 86; iter: 0; batch classifier loss: 0.035308; batch adversarial loss: 0.443477\n",
      "epoch 87; iter: 0; batch classifier loss: 0.076340; batch adversarial loss: 0.374428\n",
      "epoch 88; iter: 0; batch classifier loss: 0.040681; batch adversarial loss: 0.407967\n",
      "epoch 89; iter: 0; batch classifier loss: 0.102669; batch adversarial loss: 0.389595\n",
      "epoch 90; iter: 0; batch classifier loss: 0.099464; batch adversarial loss: 0.437800\n",
      "epoch 91; iter: 0; batch classifier loss: 0.056865; batch adversarial loss: 0.437627\n",
      "epoch 92; iter: 0; batch classifier loss: 0.041417; batch adversarial loss: 0.460328\n",
      "epoch 93; iter: 0; batch classifier loss: 0.036856; batch adversarial loss: 0.436436\n",
      "epoch 94; iter: 0; batch classifier loss: 0.021777; batch adversarial loss: 0.363280\n",
      "epoch 95; iter: 0; batch classifier loss: 0.105770; batch adversarial loss: 0.378147\n",
      "epoch 96; iter: 0; batch classifier loss: 0.056142; batch adversarial loss: 0.399404\n",
      "epoch 97; iter: 0; batch classifier loss: 0.037050; batch adversarial loss: 0.386856\n",
      "epoch 98; iter: 0; batch classifier loss: 0.067514; batch adversarial loss: 0.414906\n",
      "epoch 99; iter: 0; batch classifier loss: 0.097396; batch adversarial loss: 0.436210\n",
      "epoch 100; iter: 0; batch classifier loss: 0.066562; batch adversarial loss: 0.466013\n",
      "epoch 101; iter: 0; batch classifier loss: 0.042754; batch adversarial loss: 0.375284\n",
      "epoch 102; iter: 0; batch classifier loss: 0.046262; batch adversarial loss: 0.387627\n",
      "epoch 103; iter: 0; batch classifier loss: 0.060422; batch adversarial loss: 0.400367\n",
      "epoch 104; iter: 0; batch classifier loss: 0.047418; batch adversarial loss: 0.438297\n",
      "epoch 105; iter: 0; batch classifier loss: 0.084870; batch adversarial loss: 0.451563\n",
      "epoch 106; iter: 0; batch classifier loss: 0.083724; batch adversarial loss: 0.437510\n",
      "epoch 107; iter: 0; batch classifier loss: 0.024412; batch adversarial loss: 0.500079\n",
      "epoch 108; iter: 0; batch classifier loss: 0.077510; batch adversarial loss: 0.362601\n",
      "epoch 109; iter: 0; batch classifier loss: 0.072262; batch adversarial loss: 0.485094\n",
      "epoch 110; iter: 0; batch classifier loss: 0.028390; batch adversarial loss: 0.414303\n",
      "epoch 111; iter: 0; batch classifier loss: 0.066319; batch adversarial loss: 0.512818\n",
      "epoch 112; iter: 0; batch classifier loss: 0.047636; batch adversarial loss: 0.424238\n",
      "epoch 113; iter: 0; batch classifier loss: 0.041746; batch adversarial loss: 0.410702\n",
      "epoch 114; iter: 0; batch classifier loss: 0.032933; batch adversarial loss: 0.448275\n",
      "epoch 115; iter: 0; batch classifier loss: 0.046158; batch adversarial loss: 0.529532\n",
      "epoch 116; iter: 0; batch classifier loss: 0.065540; batch adversarial loss: 0.429329\n",
      "epoch 117; iter: 0; batch classifier loss: 0.055839; batch adversarial loss: 0.436495\n",
      "epoch 118; iter: 0; batch classifier loss: 0.044993; batch adversarial loss: 0.402658\n",
      "epoch 119; iter: 0; batch classifier loss: 0.073519; batch adversarial loss: 0.378357\n",
      "epoch 120; iter: 0; batch classifier loss: 0.068609; batch adversarial loss: 0.464458\n",
      "epoch 121; iter: 0; batch classifier loss: 0.050895; batch adversarial loss: 0.518541\n",
      "epoch 122; iter: 0; batch classifier loss: 0.059184; batch adversarial loss: 0.345951\n",
      "epoch 123; iter: 0; batch classifier loss: 0.057807; batch adversarial loss: 0.421292\n",
      "epoch 124; iter: 0; batch classifier loss: 0.041276; batch adversarial loss: 0.383915\n",
      "epoch 125; iter: 0; batch classifier loss: 0.049520; batch adversarial loss: 0.421313\n",
      "epoch 126; iter: 0; batch classifier loss: 0.027136; batch adversarial loss: 0.363910\n",
      "epoch 127; iter: 0; batch classifier loss: 0.040973; batch adversarial loss: 0.449873\n",
      "epoch 128; iter: 0; batch classifier loss: 0.032641; batch adversarial loss: 0.476491\n",
      "epoch 129; iter: 0; batch classifier loss: 0.039640; batch adversarial loss: 0.436265\n",
      "epoch 130; iter: 0; batch classifier loss: 0.083309; batch adversarial loss: 0.409344\n",
      "epoch 131; iter: 0; batch classifier loss: 0.066746; batch adversarial loss: 0.464579\n",
      "epoch 132; iter: 0; batch classifier loss: 0.066838; batch adversarial loss: 0.375523\n",
      "epoch 133; iter: 0; batch classifier loss: 0.060657; batch adversarial loss: 0.382825\n",
      "epoch 134; iter: 0; batch classifier loss: 0.039980; batch adversarial loss: 0.450408\n",
      "epoch 135; iter: 0; batch classifier loss: 0.059749; batch adversarial loss: 0.403990\n",
      "epoch 136; iter: 0; batch classifier loss: 0.054896; batch adversarial loss: 0.450985\n",
      "epoch 137; iter: 0; batch classifier loss: 0.039119; batch adversarial loss: 0.574739\n",
      "epoch 138; iter: 0; batch classifier loss: 0.065396; batch adversarial loss: 0.398369\n",
      "epoch 139; iter: 0; batch classifier loss: 0.033868; batch adversarial loss: 0.476669\n",
      "epoch 140; iter: 0; batch classifier loss: 0.028837; batch adversarial loss: 0.481624\n",
      "epoch 141; iter: 0; batch classifier loss: 0.049655; batch adversarial loss: 0.499269\n",
      "epoch 142; iter: 0; batch classifier loss: 0.049504; batch adversarial loss: 0.316459\n",
      "epoch 143; iter: 0; batch classifier loss: 0.070289; batch adversarial loss: 0.535042\n",
      "epoch 144; iter: 0; batch classifier loss: 0.039829; batch adversarial loss: 0.366865\n",
      "epoch 145; iter: 0; batch classifier loss: 0.046085; batch adversarial loss: 0.375202\n",
      "epoch 146; iter: 0; batch classifier loss: 0.023581; batch adversarial loss: 0.446702\n",
      "epoch 147; iter: 0; batch classifier loss: 0.044651; batch adversarial loss: 0.384226\n",
      "epoch 148; iter: 0; batch classifier loss: 0.049078; batch adversarial loss: 0.387665\n",
      "epoch 149; iter: 0; batch classifier loss: 0.039845; batch adversarial loss: 0.441024\n",
      "epoch 150; iter: 0; batch classifier loss: 0.040127; batch adversarial loss: 0.414215\n",
      "epoch 151; iter: 0; batch classifier loss: 0.038650; batch adversarial loss: 0.382847\n",
      "epoch 152; iter: 0; batch classifier loss: 0.031766; batch adversarial loss: 0.458061\n",
      "epoch 153; iter: 0; batch classifier loss: 0.022421; batch adversarial loss: 0.437063\n",
      "epoch 154; iter: 0; batch classifier loss: 0.039472; batch adversarial loss: 0.480207\n",
      "epoch 155; iter: 0; batch classifier loss: 0.034906; batch adversarial loss: 0.514492\n",
      "epoch 156; iter: 0; batch classifier loss: 0.027295; batch adversarial loss: 0.377841\n",
      "epoch 157; iter: 0; batch classifier loss: 0.028500; batch adversarial loss: 0.396452\n",
      "epoch 158; iter: 0; batch classifier loss: 0.034823; batch adversarial loss: 0.453429\n",
      "epoch 159; iter: 0; batch classifier loss: 0.033487; batch adversarial loss: 0.445524\n",
      "epoch 160; iter: 0; batch classifier loss: 0.044612; batch adversarial loss: 0.459001\n",
      "epoch 161; iter: 0; batch classifier loss: 0.017963; batch adversarial loss: 0.343711\n",
      "epoch 162; iter: 0; batch classifier loss: 0.052142; batch adversarial loss: 0.403664\n",
      "epoch 163; iter: 0; batch classifier loss: 0.037099; batch adversarial loss: 0.421914\n",
      "epoch 164; iter: 0; batch classifier loss: 0.032402; batch adversarial loss: 0.472692\n",
      "epoch 165; iter: 0; batch classifier loss: 0.029637; batch adversarial loss: 0.418011\n",
      "epoch 166; iter: 0; batch classifier loss: 0.024944; batch adversarial loss: 0.452449\n",
      "epoch 167; iter: 0; batch classifier loss: 0.017184; batch adversarial loss: 0.458858\n",
      "epoch 168; iter: 0; batch classifier loss: 0.027762; batch adversarial loss: 0.402869\n",
      "epoch 169; iter: 0; batch classifier loss: 0.016027; batch adversarial loss: 0.471132\n",
      "epoch 170; iter: 0; batch classifier loss: 0.020083; batch adversarial loss: 0.480702\n",
      "epoch 171; iter: 0; batch classifier loss: 0.014721; batch adversarial loss: 0.471981\n",
      "epoch 172; iter: 0; batch classifier loss: 0.034698; batch adversarial loss: 0.427563\n",
      "epoch 173; iter: 0; batch classifier loss: 0.026398; batch adversarial loss: 0.372757\n",
      "epoch 174; iter: 0; batch classifier loss: 0.016702; batch adversarial loss: 0.371487\n",
      "epoch 175; iter: 0; batch classifier loss: 0.039552; batch adversarial loss: 0.394009\n",
      "epoch 176; iter: 0; batch classifier loss: 0.031724; batch adversarial loss: 0.377999\n",
      "epoch 177; iter: 0; batch classifier loss: 0.029643; batch adversarial loss: 0.450269\n",
      "epoch 178; iter: 0; batch classifier loss: 0.025091; batch adversarial loss: 0.445277\n",
      "epoch 179; iter: 0; batch classifier loss: 0.026109; batch adversarial loss: 0.446072\n",
      "epoch 180; iter: 0; batch classifier loss: 0.049046; batch adversarial loss: 0.375977\n",
      "epoch 181; iter: 0; batch classifier loss: 0.025078; batch adversarial loss: 0.362209\n",
      "epoch 182; iter: 0; batch classifier loss: 0.031550; batch adversarial loss: 0.392815\n",
      "epoch 183; iter: 0; batch classifier loss: 0.027878; batch adversarial loss: 0.382241\n",
      "epoch 184; iter: 0; batch classifier loss: 0.027336; batch adversarial loss: 0.445629\n",
      "epoch 185; iter: 0; batch classifier loss: 0.022268; batch adversarial loss: 0.451700\n",
      "epoch 186; iter: 0; batch classifier loss: 0.025109; batch adversarial loss: 0.451593\n",
      "epoch 187; iter: 0; batch classifier loss: 0.015620; batch adversarial loss: 0.427051\n",
      "epoch 188; iter: 0; batch classifier loss: 0.013901; batch adversarial loss: 0.616939\n",
      "epoch 189; iter: 0; batch classifier loss: 0.016848; batch adversarial loss: 0.451149\n",
      "epoch 190; iter: 0; batch classifier loss: 0.016180; batch adversarial loss: 0.478983\n",
      "epoch 191; iter: 0; batch classifier loss: 0.034234; batch adversarial loss: 0.362182\n",
      "epoch 192; iter: 0; batch classifier loss: 0.039279; batch adversarial loss: 0.432742\n",
      "epoch 193; iter: 0; batch classifier loss: 0.021960; batch adversarial loss: 0.409336\n",
      "epoch 194; iter: 0; batch classifier loss: 0.023780; batch adversarial loss: 0.408788\n",
      "epoch 195; iter: 0; batch classifier loss: 0.013846; batch adversarial loss: 0.398356\n",
      "epoch 196; iter: 0; batch classifier loss: 0.020329; batch adversarial loss: 0.364331\n",
      "epoch 197; iter: 0; batch classifier loss: 0.007561; batch adversarial loss: 0.454264\n",
      "epoch 198; iter: 0; batch classifier loss: 0.007466; batch adversarial loss: 0.421940\n",
      "epoch 199; iter: 0; batch classifier loss: 0.035742; batch adversarial loss: 0.398075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:48:13.265192: W tensorflow/c/c_api.cc:304] Operation '{name:'04a4468a-ae24-11ee-be98-ef9b34f2853b/04a4468a-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:1591 op device:{requested: '', assigned: ''} def:{{{node 04a4468a-ae24-11ee-be98-ef9b34f2853b/04a4468a-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a4468a-ae24-11ee-be98-ef9b34f2853b/04a4468a-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a4468a-ae24-11ee-be98-ef9b34f2853b/04a4468a-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.688129; batch adversarial loss: 0.582458\n",
      "epoch 1; iter: 0; batch classifier loss: 0.418286; batch adversarial loss: 0.582047\n",
      "epoch 2; iter: 0; batch classifier loss: 0.525055; batch adversarial loss: 0.583817\n",
      "epoch 3; iter: 0; batch classifier loss: 0.428255; batch adversarial loss: 0.520912\n",
      "epoch 4; iter: 0; batch classifier loss: 0.349640; batch adversarial loss: 0.566984\n",
      "epoch 5; iter: 0; batch classifier loss: 0.340495; batch adversarial loss: 0.611152\n",
      "epoch 6; iter: 0; batch classifier loss: 0.408788; batch adversarial loss: 0.555860\n",
      "epoch 7; iter: 0; batch classifier loss: 0.336668; batch adversarial loss: 0.536746\n",
      "epoch 8; iter: 0; batch classifier loss: 0.377252; batch adversarial loss: 0.547035\n",
      "epoch 9; iter: 0; batch classifier loss: 0.416875; batch adversarial loss: 0.571816\n",
      "epoch 10; iter: 0; batch classifier loss: 0.458405; batch adversarial loss: 0.447826\n",
      "epoch 11; iter: 0; batch classifier loss: 0.361180; batch adversarial loss: 0.517236\n",
      "epoch 12; iter: 0; batch classifier loss: 0.285615; batch adversarial loss: 0.506002\n",
      "epoch 13; iter: 0; batch classifier loss: 0.346196; batch adversarial loss: 0.575380\n",
      "epoch 14; iter: 0; batch classifier loss: 0.283920; batch adversarial loss: 0.515648\n",
      "epoch 15; iter: 0; batch classifier loss: 0.200360; batch adversarial loss: 0.501168\n",
      "epoch 16; iter: 0; batch classifier loss: 0.243708; batch adversarial loss: 0.432575\n",
      "epoch 17; iter: 0; batch classifier loss: 0.178450; batch adversarial loss: 0.420903\n",
      "epoch 18; iter: 0; batch classifier loss: 0.254595; batch adversarial loss: 0.438466\n",
      "epoch 19; iter: 0; batch classifier loss: 0.257026; batch adversarial loss: 0.496501\n",
      "epoch 20; iter: 0; batch classifier loss: 0.178530; batch adversarial loss: 0.502721\n",
      "epoch 21; iter: 0; batch classifier loss: 0.232064; batch adversarial loss: 0.468430\n",
      "epoch 22; iter: 0; batch classifier loss: 0.189602; batch adversarial loss: 0.435759\n",
      "epoch 23; iter: 0; batch classifier loss: 0.213089; batch adversarial loss: 0.513310\n",
      "epoch 24; iter: 0; batch classifier loss: 0.195573; batch adversarial loss: 0.461283\n",
      "epoch 25; iter: 0; batch classifier loss: 0.202941; batch adversarial loss: 0.458565\n",
      "epoch 26; iter: 0; batch classifier loss: 0.216715; batch adversarial loss: 0.509816\n",
      "epoch 27; iter: 0; batch classifier loss: 0.226038; batch adversarial loss: 0.503132\n",
      "epoch 28; iter: 0; batch classifier loss: 0.185449; batch adversarial loss: 0.474548\n",
      "epoch 29; iter: 0; batch classifier loss: 0.197551; batch adversarial loss: 0.513540\n",
      "epoch 30; iter: 0; batch classifier loss: 0.198968; batch adversarial loss: 0.444631\n",
      "epoch 31; iter: 0; batch classifier loss: 0.178431; batch adversarial loss: 0.418701\n",
      "epoch 32; iter: 0; batch classifier loss: 0.165924; batch adversarial loss: 0.471863\n",
      "epoch 33; iter: 0; batch classifier loss: 0.200710; batch adversarial loss: 0.389130\n",
      "epoch 34; iter: 0; batch classifier loss: 0.215499; batch adversarial loss: 0.398050\n",
      "epoch 35; iter: 0; batch classifier loss: 0.148965; batch adversarial loss: 0.436479\n",
      "epoch 36; iter: 0; batch classifier loss: 0.196180; batch adversarial loss: 0.435999\n",
      "epoch 37; iter: 0; batch classifier loss: 0.219984; batch adversarial loss: 0.414746\n",
      "epoch 38; iter: 0; batch classifier loss: 0.253643; batch adversarial loss: 0.502879\n",
      "epoch 39; iter: 0; batch classifier loss: 0.148975; batch adversarial loss: 0.460152\n",
      "epoch 40; iter: 0; batch classifier loss: 0.182249; batch adversarial loss: 0.430215\n",
      "epoch 41; iter: 0; batch classifier loss: 0.212471; batch adversarial loss: 0.516455\n",
      "epoch 42; iter: 0; batch classifier loss: 0.204900; batch adversarial loss: 0.486186\n",
      "epoch 43; iter: 0; batch classifier loss: 0.273834; batch adversarial loss: 0.370007\n",
      "epoch 44; iter: 0; batch classifier loss: 0.184662; batch adversarial loss: 0.446620\n",
      "epoch 45; iter: 0; batch classifier loss: 0.211863; batch adversarial loss: 0.532505\n",
      "epoch 46; iter: 0; batch classifier loss: 0.246754; batch adversarial loss: 0.378046\n",
      "epoch 47; iter: 0; batch classifier loss: 0.207984; batch adversarial loss: 0.432995\n",
      "epoch 48; iter: 0; batch classifier loss: 0.217221; batch adversarial loss: 0.504403\n",
      "epoch 49; iter: 0; batch classifier loss: 0.199260; batch adversarial loss: 0.494291\n",
      "epoch 50; iter: 0; batch classifier loss: 0.225734; batch adversarial loss: 0.469332\n",
      "epoch 51; iter: 0; batch classifier loss: 0.280469; batch adversarial loss: 0.450357\n",
      "epoch 52; iter: 0; batch classifier loss: 0.256328; batch adversarial loss: 0.400179\n",
      "epoch 53; iter: 0; batch classifier loss: 0.235736; batch adversarial loss: 0.472313\n",
      "epoch 54; iter: 0; batch classifier loss: 0.268841; batch adversarial loss: 0.423809\n",
      "epoch 55; iter: 0; batch classifier loss: 0.215488; batch adversarial loss: 0.494381\n",
      "epoch 56; iter: 0; batch classifier loss: 0.156838; batch adversarial loss: 0.493873\n",
      "epoch 57; iter: 0; batch classifier loss: 0.067571; batch adversarial loss: 0.446264\n",
      "epoch 58; iter: 0; batch classifier loss: 0.103650; batch adversarial loss: 0.479958\n",
      "epoch 59; iter: 0; batch classifier loss: 0.080905; batch adversarial loss: 0.476880\n",
      "epoch 60; iter: 0; batch classifier loss: 0.087581; batch adversarial loss: 0.438387\n",
      "epoch 61; iter: 0; batch classifier loss: 0.057121; batch adversarial loss: 0.527533\n",
      "epoch 62; iter: 0; batch classifier loss: 0.069701; batch adversarial loss: 0.451979\n",
      "epoch 63; iter: 0; batch classifier loss: 0.092830; batch adversarial loss: 0.408839\n",
      "epoch 64; iter: 0; batch classifier loss: 0.179970; batch adversarial loss: 0.439693\n",
      "epoch 65; iter: 0; batch classifier loss: 0.190992; batch adversarial loss: 0.491628\n",
      "epoch 66; iter: 0; batch classifier loss: 0.146571; batch adversarial loss: 0.574472\n",
      "epoch 67; iter: 0; batch classifier loss: 0.124498; batch adversarial loss: 0.413266\n",
      "epoch 68; iter: 0; batch classifier loss: 0.170022; batch adversarial loss: 0.442749\n",
      "epoch 69; iter: 0; batch classifier loss: 0.124150; batch adversarial loss: 0.469246\n",
      "epoch 70; iter: 0; batch classifier loss: 0.086658; batch adversarial loss: 0.446183\n",
      "epoch 71; iter: 0; batch classifier loss: 0.097854; batch adversarial loss: 0.510166\n",
      "epoch 72; iter: 0; batch classifier loss: 0.098005; batch adversarial loss: 0.506230\n",
      "epoch 73; iter: 0; batch classifier loss: 0.103036; batch adversarial loss: 0.476823\n",
      "epoch 74; iter: 0; batch classifier loss: 0.119643; batch adversarial loss: 0.513143\n",
      "epoch 75; iter: 0; batch classifier loss: 0.154620; batch adversarial loss: 0.419525\n",
      "epoch 76; iter: 0; batch classifier loss: 0.085910; batch adversarial loss: 0.417197\n",
      "epoch 77; iter: 0; batch classifier loss: 0.090060; batch adversarial loss: 0.433126\n",
      "epoch 78; iter: 0; batch classifier loss: 0.108649; batch adversarial loss: 0.491992\n",
      "epoch 79; iter: 0; batch classifier loss: 0.072665; batch adversarial loss: 0.427414\n",
      "epoch 80; iter: 0; batch classifier loss: 0.078432; batch adversarial loss: 0.416684\n",
      "epoch 81; iter: 0; batch classifier loss: 0.081628; batch adversarial loss: 0.618467\n",
      "epoch 82; iter: 0; batch classifier loss: 0.081460; batch adversarial loss: 0.451962\n",
      "epoch 83; iter: 0; batch classifier loss: 0.083842; batch adversarial loss: 0.409290\n",
      "epoch 84; iter: 0; batch classifier loss: 0.065025; batch adversarial loss: 0.527774\n",
      "epoch 85; iter: 0; batch classifier loss: 0.076811; batch adversarial loss: 0.395808\n",
      "epoch 86; iter: 0; batch classifier loss: 0.080542; batch adversarial loss: 0.426802\n",
      "epoch 87; iter: 0; batch classifier loss: 0.035755; batch adversarial loss: 0.457259\n",
      "epoch 88; iter: 0; batch classifier loss: 0.075089; batch adversarial loss: 0.522649\n",
      "epoch 89; iter: 0; batch classifier loss: 0.073160; batch adversarial loss: 0.524856\n",
      "epoch 90; iter: 0; batch classifier loss: 0.075504; batch adversarial loss: 0.454985\n",
      "epoch 91; iter: 0; batch classifier loss: 0.082712; batch adversarial loss: 0.468551\n",
      "epoch 92; iter: 0; batch classifier loss: 0.081159; batch adversarial loss: 0.424298\n",
      "epoch 93; iter: 0; batch classifier loss: 0.105341; batch adversarial loss: 0.483678\n",
      "epoch 94; iter: 0; batch classifier loss: 0.103236; batch adversarial loss: 0.413750\n",
      "epoch 95; iter: 0; batch classifier loss: 0.056362; batch adversarial loss: 0.477983\n",
      "epoch 96; iter: 0; batch classifier loss: 0.096910; batch adversarial loss: 0.419712\n",
      "epoch 97; iter: 0; batch classifier loss: 0.088785; batch adversarial loss: 0.338753\n",
      "epoch 98; iter: 0; batch classifier loss: 0.056480; batch adversarial loss: 0.493365\n",
      "epoch 99; iter: 0; batch classifier loss: 0.078842; batch adversarial loss: 0.472535\n",
      "epoch 100; iter: 0; batch classifier loss: 0.045361; batch adversarial loss: 0.404357\n",
      "epoch 101; iter: 0; batch classifier loss: 0.062635; batch adversarial loss: 0.445790\n",
      "epoch 102; iter: 0; batch classifier loss: 0.078453; batch adversarial loss: 0.472064\n",
      "epoch 103; iter: 0; batch classifier loss: 0.038869; batch adversarial loss: 0.438187\n",
      "epoch 104; iter: 0; batch classifier loss: 0.047181; batch adversarial loss: 0.479073\n",
      "epoch 105; iter: 0; batch classifier loss: 0.052053; batch adversarial loss: 0.491621\n",
      "epoch 106; iter: 0; batch classifier loss: 0.031616; batch adversarial loss: 0.548412\n",
      "epoch 107; iter: 0; batch classifier loss: 0.058051; batch adversarial loss: 0.448620\n",
      "epoch 108; iter: 0; batch classifier loss: 0.058622; batch adversarial loss: 0.465527\n",
      "epoch 109; iter: 0; batch classifier loss: 0.026510; batch adversarial loss: 0.452133\n",
      "epoch 110; iter: 0; batch classifier loss: 0.038677; batch adversarial loss: 0.402920\n",
      "epoch 111; iter: 0; batch classifier loss: 0.019532; batch adversarial loss: 0.465544\n",
      "epoch 112; iter: 0; batch classifier loss: 0.056462; batch adversarial loss: 0.441909\n",
      "epoch 113; iter: 0; batch classifier loss: 0.072204; batch adversarial loss: 0.602850\n",
      "epoch 114; iter: 0; batch classifier loss: 0.029742; batch adversarial loss: 0.479939\n",
      "epoch 115; iter: 0; batch classifier loss: 0.050471; batch adversarial loss: 0.433319\n",
      "epoch 116; iter: 0; batch classifier loss: 0.060567; batch adversarial loss: 0.398176\n",
      "epoch 117; iter: 0; batch classifier loss: 0.047180; batch adversarial loss: 0.439185\n",
      "epoch 118; iter: 0; batch classifier loss: 0.029035; batch adversarial loss: 0.418488\n",
      "epoch 119; iter: 0; batch classifier loss: 0.033030; batch adversarial loss: 0.450658\n",
      "epoch 120; iter: 0; batch classifier loss: 0.051230; batch adversarial loss: 0.409980\n",
      "epoch 121; iter: 0; batch classifier loss: 0.034502; batch adversarial loss: 0.483479\n",
      "epoch 122; iter: 0; batch classifier loss: 0.066685; batch adversarial loss: 0.415055\n",
      "epoch 123; iter: 0; batch classifier loss: 0.036303; batch adversarial loss: 0.464543\n",
      "epoch 124; iter: 0; batch classifier loss: 0.052519; batch adversarial loss: 0.493312\n",
      "epoch 125; iter: 0; batch classifier loss: 0.030684; batch adversarial loss: 0.400999\n",
      "epoch 126; iter: 0; batch classifier loss: 0.023634; batch adversarial loss: 0.445230\n",
      "epoch 127; iter: 0; batch classifier loss: 0.032854; batch adversarial loss: 0.453296\n",
      "epoch 128; iter: 0; batch classifier loss: 0.026536; batch adversarial loss: 0.507458\n",
      "epoch 129; iter: 0; batch classifier loss: 0.029507; batch adversarial loss: 0.440097\n",
      "epoch 130; iter: 0; batch classifier loss: 0.037143; batch adversarial loss: 0.370093\n",
      "epoch 131; iter: 0; batch classifier loss: 0.036277; batch adversarial loss: 0.417342\n",
      "epoch 132; iter: 0; batch classifier loss: 0.029931; batch adversarial loss: 0.464351\n",
      "epoch 133; iter: 0; batch classifier loss: 0.032374; batch adversarial loss: 0.545062\n",
      "epoch 134; iter: 0; batch classifier loss: 0.015927; batch adversarial loss: 0.349027\n",
      "epoch 135; iter: 0; batch classifier loss: 0.016774; batch adversarial loss: 0.495524\n",
      "epoch 136; iter: 0; batch classifier loss: 0.030935; batch adversarial loss: 0.449530\n",
      "epoch 137; iter: 0; batch classifier loss: 0.038979; batch adversarial loss: 0.456351\n",
      "epoch 138; iter: 0; batch classifier loss: 0.013323; batch adversarial loss: 0.456658\n",
      "epoch 139; iter: 0; batch classifier loss: 0.030194; batch adversarial loss: 0.510743\n",
      "epoch 140; iter: 0; batch classifier loss: 0.021454; batch adversarial loss: 0.356752\n",
      "epoch 141; iter: 0; batch classifier loss: 0.028846; batch adversarial loss: 0.352905\n",
      "epoch 142; iter: 0; batch classifier loss: 0.023706; batch adversarial loss: 0.536787\n",
      "epoch 143; iter: 0; batch classifier loss: 0.019686; batch adversarial loss: 0.470078\n",
      "epoch 144; iter: 0; batch classifier loss: 0.041820; batch adversarial loss: 0.487558\n",
      "epoch 145; iter: 0; batch classifier loss: 0.011119; batch adversarial loss: 0.483874\n",
      "epoch 146; iter: 0; batch classifier loss: 0.030574; batch adversarial loss: 0.457222\n",
      "epoch 147; iter: 0; batch classifier loss: 0.024936; batch adversarial loss: 0.558170\n",
      "epoch 148; iter: 0; batch classifier loss: 0.022186; batch adversarial loss: 0.372945\n",
      "epoch 149; iter: 0; batch classifier loss: 0.027140; batch adversarial loss: 0.432468\n",
      "epoch 150; iter: 0; batch classifier loss: 0.023059; batch adversarial loss: 0.319447\n",
      "epoch 151; iter: 0; batch classifier loss: 0.041326; batch adversarial loss: 0.445123\n",
      "epoch 152; iter: 0; batch classifier loss: 0.013528; batch adversarial loss: 0.437946\n",
      "epoch 153; iter: 0; batch classifier loss: 0.017250; batch adversarial loss: 0.482155\n",
      "epoch 154; iter: 0; batch classifier loss: 0.012327; batch adversarial loss: 0.448138\n",
      "epoch 155; iter: 0; batch classifier loss: 0.026185; batch adversarial loss: 0.413030\n",
      "epoch 156; iter: 0; batch classifier loss: 0.040082; batch adversarial loss: 0.496351\n",
      "epoch 157; iter: 0; batch classifier loss: 0.021876; batch adversarial loss: 0.453756\n",
      "epoch 158; iter: 0; batch classifier loss: 0.029481; batch adversarial loss: 0.465553\n",
      "epoch 159; iter: 0; batch classifier loss: 0.004143; batch adversarial loss: 0.414448\n",
      "epoch 160; iter: 0; batch classifier loss: 0.013102; batch adversarial loss: 0.464395\n",
      "epoch 161; iter: 0; batch classifier loss: 0.008841; batch adversarial loss: 0.449652\n",
      "epoch 162; iter: 0; batch classifier loss: 0.009041; batch adversarial loss: 0.577473\n",
      "epoch 163; iter: 0; batch classifier loss: 0.012078; batch adversarial loss: 0.431391\n",
      "epoch 164; iter: 0; batch classifier loss: 0.005280; batch adversarial loss: 0.348936\n",
      "epoch 165; iter: 0; batch classifier loss: 0.022938; batch adversarial loss: 0.531446\n",
      "epoch 166; iter: 0; batch classifier loss: 0.007558; batch adversarial loss: 0.368732\n",
      "epoch 167; iter: 0; batch classifier loss: 0.007948; batch adversarial loss: 0.418823\n",
      "epoch 168; iter: 0; batch classifier loss: 0.011378; batch adversarial loss: 0.462137\n",
      "epoch 169; iter: 0; batch classifier loss: 0.012475; batch adversarial loss: 0.522107\n",
      "epoch 170; iter: 0; batch classifier loss: 0.089205; batch adversarial loss: 0.425102\n",
      "epoch 171; iter: 0; batch classifier loss: 0.013193; batch adversarial loss: 0.438883\n",
      "epoch 172; iter: 0; batch classifier loss: 0.019147; batch adversarial loss: 0.464129\n",
      "epoch 173; iter: 0; batch classifier loss: 0.016692; batch adversarial loss: 0.464161\n",
      "epoch 174; iter: 0; batch classifier loss: 0.014131; batch adversarial loss: 0.416692\n",
      "epoch 175; iter: 0; batch classifier loss: 0.019746; batch adversarial loss: 0.454887\n",
      "epoch 176; iter: 0; batch classifier loss: 0.003722; batch adversarial loss: 0.416714\n",
      "epoch 177; iter: 0; batch classifier loss: 0.009313; batch adversarial loss: 0.439547\n",
      "epoch 178; iter: 0; batch classifier loss: 0.013868; batch adversarial loss: 0.486130\n",
      "epoch 179; iter: 0; batch classifier loss: 0.022029; batch adversarial loss: 0.498505\n",
      "epoch 180; iter: 0; batch classifier loss: 0.008864; batch adversarial loss: 0.420729\n",
      "epoch 181; iter: 0; batch classifier loss: 0.038184; batch adversarial loss: 0.417415\n",
      "epoch 182; iter: 0; batch classifier loss: 0.031390; batch adversarial loss: 0.408334\n",
      "epoch 183; iter: 0; batch classifier loss: 0.026545; batch adversarial loss: 0.569768\n",
      "epoch 184; iter: 0; batch classifier loss: 0.006229; batch adversarial loss: 0.342425\n",
      "epoch 185; iter: 0; batch classifier loss: 0.011448; batch adversarial loss: 0.482688\n",
      "epoch 186; iter: 0; batch classifier loss: 0.023002; batch adversarial loss: 0.445266\n",
      "epoch 187; iter: 0; batch classifier loss: 0.004066; batch adversarial loss: 0.428464\n",
      "epoch 188; iter: 0; batch classifier loss: 0.003665; batch adversarial loss: 0.430902\n",
      "epoch 189; iter: 0; batch classifier loss: 0.027354; batch adversarial loss: 0.399141\n",
      "epoch 190; iter: 0; batch classifier loss: 0.005157; batch adversarial loss: 0.474395\n",
      "epoch 191; iter: 0; batch classifier loss: 0.018864; batch adversarial loss: 0.400792\n",
      "epoch 192; iter: 0; batch classifier loss: 0.050361; batch adversarial loss: 0.441677\n",
      "epoch 193; iter: 0; batch classifier loss: 0.012426; batch adversarial loss: 0.475218\n",
      "epoch 194; iter: 0; batch classifier loss: 0.006026; batch adversarial loss: 0.468294\n",
      "epoch 195; iter: 0; batch classifier loss: 0.008955; batch adversarial loss: 0.498031\n",
      "epoch 196; iter: 0; batch classifier loss: 0.012578; batch adversarial loss: 0.443660\n",
      "epoch 197; iter: 0; batch classifier loss: 0.005842; batch adversarial loss: 0.529499\n",
      "epoch 198; iter: 0; batch classifier loss: 0.009330; batch adversarial loss: 0.451309\n",
      "epoch 199; iter: 0; batch classifier loss: 0.053960; batch adversarial loss: 0.396820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:48:49.999533: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44784-ae24-11ee-be98-ef9b34f2853b/04a44784-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:2398 op device:{requested: '', assigned: ''} def:{{{node 04a44784-ae24-11ee-be98-ef9b34f2853b/04a44784-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44784-ae24-11ee-be98-ef9b34f2853b/04a44784-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44784-ae24-11ee-be98-ef9b34f2853b/04a44784-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.709423; batch adversarial loss: 0.826485\n",
      "epoch 1; iter: 0; batch classifier loss: 0.589269; batch adversarial loss: 0.774333\n",
      "epoch 2; iter: 0; batch classifier loss: 0.757826; batch adversarial loss: 0.757554\n",
      "epoch 3; iter: 0; batch classifier loss: 0.651808; batch adversarial loss: 0.683050\n",
      "epoch 4; iter: 0; batch classifier loss: 0.559073; batch adversarial loss: 0.616526\n",
      "epoch 5; iter: 0; batch classifier loss: 0.383718; batch adversarial loss: 0.587390\n",
      "epoch 6; iter: 0; batch classifier loss: 0.361682; batch adversarial loss: 0.608442\n",
      "epoch 7; iter: 0; batch classifier loss: 0.300812; batch adversarial loss: 0.533616\n",
      "epoch 8; iter: 0; batch classifier loss: 0.428041; batch adversarial loss: 0.522132\n",
      "epoch 9; iter: 0; batch classifier loss: 0.334713; batch adversarial loss: 0.554177\n",
      "epoch 10; iter: 0; batch classifier loss: 0.337976; batch adversarial loss: 0.478305\n",
      "epoch 11; iter: 0; batch classifier loss: 0.403465; batch adversarial loss: 0.537060\n",
      "epoch 12; iter: 0; batch classifier loss: 0.316303; batch adversarial loss: 0.511609\n",
      "epoch 13; iter: 0; batch classifier loss: 0.331920; batch adversarial loss: 0.503228\n",
      "epoch 14; iter: 0; batch classifier loss: 0.314666; batch adversarial loss: 0.467400\n",
      "epoch 15; iter: 0; batch classifier loss: 0.338908; batch adversarial loss: 0.471192\n",
      "epoch 16; iter: 0; batch classifier loss: 0.334456; batch adversarial loss: 0.463481\n",
      "epoch 17; iter: 0; batch classifier loss: 0.237582; batch adversarial loss: 0.487386\n",
      "epoch 18; iter: 0; batch classifier loss: 0.243639; batch adversarial loss: 0.559268\n",
      "epoch 19; iter: 0; batch classifier loss: 0.223562; batch adversarial loss: 0.497359\n",
      "epoch 20; iter: 0; batch classifier loss: 0.289271; batch adversarial loss: 0.560765\n",
      "epoch 21; iter: 0; batch classifier loss: 0.308990; batch adversarial loss: 0.482442\n",
      "epoch 22; iter: 0; batch classifier loss: 0.294239; batch adversarial loss: 0.508942\n",
      "epoch 23; iter: 0; batch classifier loss: 0.245820; batch adversarial loss: 0.469365\n",
      "epoch 24; iter: 0; batch classifier loss: 0.199940; batch adversarial loss: 0.491858\n",
      "epoch 25; iter: 0; batch classifier loss: 0.237852; batch adversarial loss: 0.483418\n",
      "epoch 26; iter: 0; batch classifier loss: 0.248950; batch adversarial loss: 0.494269\n",
      "epoch 27; iter: 0; batch classifier loss: 0.215890; batch adversarial loss: 0.493594\n",
      "epoch 28; iter: 0; batch classifier loss: 0.219729; batch adversarial loss: 0.477457\n",
      "epoch 29; iter: 0; batch classifier loss: 0.219291; batch adversarial loss: 0.576004\n",
      "epoch 30; iter: 0; batch classifier loss: 0.205509; batch adversarial loss: 0.407661\n",
      "epoch 31; iter: 0; batch classifier loss: 0.229055; batch adversarial loss: 0.420588\n",
      "epoch 32; iter: 0; batch classifier loss: 0.231041; batch adversarial loss: 0.436854\n",
      "epoch 33; iter: 0; batch classifier loss: 0.188074; batch adversarial loss: 0.454388\n",
      "epoch 34; iter: 0; batch classifier loss: 0.252571; batch adversarial loss: 0.415374\n",
      "epoch 35; iter: 0; batch classifier loss: 0.175120; batch adversarial loss: 0.415148\n",
      "epoch 36; iter: 0; batch classifier loss: 0.156770; batch adversarial loss: 0.482264\n",
      "epoch 37; iter: 0; batch classifier loss: 0.153350; batch adversarial loss: 0.431682\n",
      "epoch 38; iter: 0; batch classifier loss: 0.211564; batch adversarial loss: 0.467146\n",
      "epoch 39; iter: 0; batch classifier loss: 0.260667; batch adversarial loss: 0.425384\n",
      "epoch 40; iter: 0; batch classifier loss: 0.224133; batch adversarial loss: 0.424066\n",
      "epoch 41; iter: 0; batch classifier loss: 0.186538; batch adversarial loss: 0.481687\n",
      "epoch 42; iter: 0; batch classifier loss: 0.217674; batch adversarial loss: 0.570059\n",
      "epoch 43; iter: 0; batch classifier loss: 0.204773; batch adversarial loss: 0.480069\n",
      "epoch 44; iter: 0; batch classifier loss: 0.287237; batch adversarial loss: 0.476578\n",
      "epoch 45; iter: 0; batch classifier loss: 0.318581; batch adversarial loss: 0.362229\n",
      "epoch 46; iter: 0; batch classifier loss: 0.227443; batch adversarial loss: 0.465089\n",
      "epoch 47; iter: 0; batch classifier loss: 0.194799; batch adversarial loss: 0.446107\n",
      "epoch 48; iter: 0; batch classifier loss: 0.306833; batch adversarial loss: 0.507393\n",
      "epoch 49; iter: 0; batch classifier loss: 0.202465; batch adversarial loss: 0.537706\n",
      "epoch 50; iter: 0; batch classifier loss: 0.223188; batch adversarial loss: 0.423028\n",
      "epoch 51; iter: 0; batch classifier loss: 0.225845; batch adversarial loss: 0.447576\n",
      "epoch 52; iter: 0; batch classifier loss: 0.155381; batch adversarial loss: 0.486634\n",
      "epoch 53; iter: 0; batch classifier loss: 0.203742; batch adversarial loss: 0.445566\n",
      "epoch 54; iter: 0; batch classifier loss: 0.250815; batch adversarial loss: 0.460954\n",
      "epoch 55; iter: 0; batch classifier loss: 0.182396; batch adversarial loss: 0.564597\n",
      "epoch 56; iter: 0; batch classifier loss: 0.178321; batch adversarial loss: 0.466417\n",
      "epoch 57; iter: 0; batch classifier loss: 0.135660; batch adversarial loss: 0.492085\n",
      "epoch 58; iter: 0; batch classifier loss: 0.227268; batch adversarial loss: 0.379017\n",
      "epoch 59; iter: 0; batch classifier loss: 0.185556; batch adversarial loss: 0.459743\n",
      "epoch 60; iter: 0; batch classifier loss: 0.153740; batch adversarial loss: 0.510053\n",
      "epoch 61; iter: 0; batch classifier loss: 0.155630; batch adversarial loss: 0.541576\n",
      "epoch 62; iter: 0; batch classifier loss: 0.116137; batch adversarial loss: 0.521728\n",
      "epoch 63; iter: 0; batch classifier loss: 0.178817; batch adversarial loss: 0.517786\n",
      "epoch 64; iter: 0; batch classifier loss: 0.213310; batch adversarial loss: 0.396766\n",
      "epoch 65; iter: 0; batch classifier loss: 0.174103; batch adversarial loss: 0.519237\n",
      "epoch 66; iter: 0; batch classifier loss: 0.168989; batch adversarial loss: 0.469374\n",
      "epoch 67; iter: 0; batch classifier loss: 0.171487; batch adversarial loss: 0.438444\n",
      "epoch 68; iter: 0; batch classifier loss: 0.227675; batch adversarial loss: 0.382676\n",
      "epoch 69; iter: 0; batch classifier loss: 0.129285; batch adversarial loss: 0.448061\n",
      "epoch 70; iter: 0; batch classifier loss: 0.153188; batch adversarial loss: 0.513892\n",
      "epoch 71; iter: 0; batch classifier loss: 0.189201; batch adversarial loss: 0.483813\n",
      "epoch 72; iter: 0; batch classifier loss: 0.143782; batch adversarial loss: 0.399117\n",
      "epoch 73; iter: 0; batch classifier loss: 0.194640; batch adversarial loss: 0.492915\n",
      "epoch 74; iter: 0; batch classifier loss: 0.222503; batch adversarial loss: 0.506290\n",
      "epoch 75; iter: 0; batch classifier loss: 0.192905; batch adversarial loss: 0.444399\n",
      "epoch 76; iter: 0; batch classifier loss: 0.185618; batch adversarial loss: 0.465811\n",
      "epoch 77; iter: 0; batch classifier loss: 0.102490; batch adversarial loss: 0.387088\n",
      "epoch 78; iter: 0; batch classifier loss: 0.260674; batch adversarial loss: 0.397151\n",
      "epoch 79; iter: 0; batch classifier loss: 0.162815; batch adversarial loss: 0.474389\n",
      "epoch 80; iter: 0; batch classifier loss: 0.195085; batch adversarial loss: 0.410728\n",
      "epoch 81; iter: 0; batch classifier loss: 0.132135; batch adversarial loss: 0.481101\n",
      "epoch 82; iter: 0; batch classifier loss: 0.157003; batch adversarial loss: 0.405431\n",
      "epoch 83; iter: 0; batch classifier loss: 0.114738; batch adversarial loss: 0.472957\n",
      "epoch 84; iter: 0; batch classifier loss: 0.124425; batch adversarial loss: 0.469340\n",
      "epoch 85; iter: 0; batch classifier loss: 0.133828; batch adversarial loss: 0.459536\n",
      "epoch 86; iter: 0; batch classifier loss: 0.127560; batch adversarial loss: 0.468921\n",
      "epoch 87; iter: 0; batch classifier loss: 0.106405; batch adversarial loss: 0.604421\n",
      "epoch 88; iter: 0; batch classifier loss: 0.144443; batch adversarial loss: 0.520639\n",
      "epoch 89; iter: 0; batch classifier loss: 0.117262; batch adversarial loss: 0.506146\n",
      "epoch 90; iter: 0; batch classifier loss: 0.087750; batch adversarial loss: 0.443731\n",
      "epoch 91; iter: 0; batch classifier loss: 0.113289; batch adversarial loss: 0.463015\n",
      "epoch 92; iter: 0; batch classifier loss: 0.128436; batch adversarial loss: 0.406349\n",
      "epoch 93; iter: 0; batch classifier loss: 0.095744; batch adversarial loss: 0.532681\n",
      "epoch 94; iter: 0; batch classifier loss: 0.072927; batch adversarial loss: 0.510598\n",
      "epoch 95; iter: 0; batch classifier loss: 0.050717; batch adversarial loss: 0.512368\n",
      "epoch 96; iter: 0; batch classifier loss: 0.103528; batch adversarial loss: 0.485659\n",
      "epoch 97; iter: 0; batch classifier loss: 0.055867; batch adversarial loss: 0.489014\n",
      "epoch 98; iter: 0; batch classifier loss: 0.119304; batch adversarial loss: 0.409256\n",
      "epoch 99; iter: 0; batch classifier loss: 0.058918; batch adversarial loss: 0.356944\n",
      "epoch 100; iter: 0; batch classifier loss: 0.078266; batch adversarial loss: 0.489582\n",
      "epoch 101; iter: 0; batch classifier loss: 0.100184; batch adversarial loss: 0.437631\n",
      "epoch 102; iter: 0; batch classifier loss: 0.037179; batch adversarial loss: 0.432486\n",
      "epoch 103; iter: 0; batch classifier loss: 0.076005; batch adversarial loss: 0.420993\n",
      "epoch 104; iter: 0; batch classifier loss: 0.023383; batch adversarial loss: 0.489630\n",
      "epoch 105; iter: 0; batch classifier loss: 0.057989; batch adversarial loss: 0.427848\n",
      "epoch 106; iter: 0; batch classifier loss: 0.057579; batch adversarial loss: 0.446779\n",
      "epoch 107; iter: 0; batch classifier loss: 0.016594; batch adversarial loss: 0.571494\n",
      "epoch 108; iter: 0; batch classifier loss: 0.043061; batch adversarial loss: 0.435629\n",
      "epoch 109; iter: 0; batch classifier loss: 0.041660; batch adversarial loss: 0.449064\n",
      "epoch 110; iter: 0; batch classifier loss: 0.052646; batch adversarial loss: 0.429082\n",
      "epoch 111; iter: 0; batch classifier loss: 0.064466; batch adversarial loss: 0.544246\n",
      "epoch 112; iter: 0; batch classifier loss: 0.019509; batch adversarial loss: 0.414706\n",
      "epoch 113; iter: 0; batch classifier loss: 0.053451; batch adversarial loss: 0.438979\n",
      "epoch 114; iter: 0; batch classifier loss: 0.041520; batch adversarial loss: 0.450136\n",
      "epoch 115; iter: 0; batch classifier loss: 0.039195; batch adversarial loss: 0.526165\n",
      "epoch 116; iter: 0; batch classifier loss: 0.046723; batch adversarial loss: 0.487179\n",
      "epoch 117; iter: 0; batch classifier loss: 0.028024; batch adversarial loss: 0.413543\n",
      "epoch 118; iter: 0; batch classifier loss: 0.043546; batch adversarial loss: 0.430986\n",
      "epoch 119; iter: 0; batch classifier loss: 0.042654; batch adversarial loss: 0.482377\n",
      "epoch 120; iter: 0; batch classifier loss: 0.038959; batch adversarial loss: 0.380076\n",
      "epoch 121; iter: 0; batch classifier loss: 0.046653; batch adversarial loss: 0.495118\n",
      "epoch 122; iter: 0; batch classifier loss: 0.015794; batch adversarial loss: 0.513909\n",
      "epoch 123; iter: 0; batch classifier loss: 0.023094; batch adversarial loss: 0.336286\n",
      "epoch 124; iter: 0; batch classifier loss: 0.063677; batch adversarial loss: 0.488526\n",
      "epoch 125; iter: 0; batch classifier loss: 0.023231; batch adversarial loss: 0.407590\n",
      "epoch 126; iter: 0; batch classifier loss: 0.051555; batch adversarial loss: 0.499945\n",
      "epoch 127; iter: 0; batch classifier loss: 0.025814; batch adversarial loss: 0.477885\n",
      "epoch 128; iter: 0; batch classifier loss: 0.016976; batch adversarial loss: 0.424454\n",
      "epoch 129; iter: 0; batch classifier loss: 0.050417; batch adversarial loss: 0.436375\n",
      "epoch 130; iter: 0; batch classifier loss: 0.017588; batch adversarial loss: 0.497257\n",
      "epoch 131; iter: 0; batch classifier loss: 0.035290; batch adversarial loss: 0.445353\n",
      "epoch 132; iter: 0; batch classifier loss: 0.025357; batch adversarial loss: 0.495507\n",
      "epoch 133; iter: 0; batch classifier loss: 0.023691; batch adversarial loss: 0.398855\n",
      "epoch 134; iter: 0; batch classifier loss: 0.032709; batch adversarial loss: 0.477906\n",
      "epoch 135; iter: 0; batch classifier loss: 0.012807; batch adversarial loss: 0.425623\n",
      "epoch 136; iter: 0; batch classifier loss: 0.030065; batch adversarial loss: 0.464185\n",
      "epoch 137; iter: 0; batch classifier loss: 0.041319; batch adversarial loss: 0.495098\n",
      "epoch 138; iter: 0; batch classifier loss: 0.036601; batch adversarial loss: 0.409268\n",
      "epoch 139; iter: 0; batch classifier loss: 0.009201; batch adversarial loss: 0.415366\n",
      "epoch 140; iter: 0; batch classifier loss: 0.010369; batch adversarial loss: 0.548791\n",
      "epoch 141; iter: 0; batch classifier loss: 0.018108; batch adversarial loss: 0.535071\n",
      "epoch 142; iter: 0; batch classifier loss: 0.036020; batch adversarial loss: 0.589606\n",
      "epoch 143; iter: 0; batch classifier loss: 0.015933; batch adversarial loss: 0.460440\n",
      "epoch 144; iter: 0; batch classifier loss: 0.012548; batch adversarial loss: 0.467244\n",
      "epoch 145; iter: 0; batch classifier loss: 0.013664; batch adversarial loss: 0.545864\n",
      "epoch 146; iter: 0; batch classifier loss: 0.026615; batch adversarial loss: 0.433055\n",
      "epoch 147; iter: 0; batch classifier loss: 0.026677; batch adversarial loss: 0.394158\n",
      "epoch 148; iter: 0; batch classifier loss: 0.015726; batch adversarial loss: 0.499501\n",
      "epoch 149; iter: 0; batch classifier loss: 0.013428; batch adversarial loss: 0.500811\n",
      "epoch 150; iter: 0; batch classifier loss: 0.023914; batch adversarial loss: 0.507878\n",
      "epoch 151; iter: 0; batch classifier loss: 0.023069; batch adversarial loss: 0.444022\n",
      "epoch 152; iter: 0; batch classifier loss: 0.015060; batch adversarial loss: 0.406906\n",
      "epoch 153; iter: 0; batch classifier loss: 0.021294; batch adversarial loss: 0.431451\n",
      "epoch 154; iter: 0; batch classifier loss: 0.029299; batch adversarial loss: 0.384312\n",
      "epoch 155; iter: 0; batch classifier loss: 0.021011; batch adversarial loss: 0.425266\n",
      "epoch 156; iter: 0; batch classifier loss: 0.010244; batch adversarial loss: 0.462371\n",
      "epoch 157; iter: 0; batch classifier loss: 0.013574; batch adversarial loss: 0.496028\n",
      "epoch 158; iter: 0; batch classifier loss: 0.016454; batch adversarial loss: 0.442006\n",
      "epoch 159; iter: 0; batch classifier loss: 0.011610; batch adversarial loss: 0.426134\n",
      "epoch 160; iter: 0; batch classifier loss: 0.005939; batch adversarial loss: 0.502678\n",
      "epoch 161; iter: 0; batch classifier loss: 0.007833; batch adversarial loss: 0.470883\n",
      "epoch 162; iter: 0; batch classifier loss: 0.020268; batch adversarial loss: 0.378876\n",
      "epoch 163; iter: 0; batch classifier loss: 0.024588; batch adversarial loss: 0.436137\n",
      "epoch 164; iter: 0; batch classifier loss: 0.023924; batch adversarial loss: 0.466352\n",
      "epoch 165; iter: 0; batch classifier loss: 0.030063; batch adversarial loss: 0.400956\n",
      "epoch 166; iter: 0; batch classifier loss: 0.022583; batch adversarial loss: 0.604907\n",
      "epoch 167; iter: 0; batch classifier loss: 0.008819; batch adversarial loss: 0.359732\n",
      "epoch 168; iter: 0; batch classifier loss: 0.005808; batch adversarial loss: 0.514479\n",
      "epoch 169; iter: 0; batch classifier loss: 0.026108; batch adversarial loss: 0.435591\n",
      "epoch 170; iter: 0; batch classifier loss: 0.022649; batch adversarial loss: 0.438637\n",
      "epoch 171; iter: 0; batch classifier loss: 0.025742; batch adversarial loss: 0.494849\n",
      "epoch 172; iter: 0; batch classifier loss: 0.024279; batch adversarial loss: 0.371164\n",
      "epoch 173; iter: 0; batch classifier loss: 0.011350; batch adversarial loss: 0.457860\n",
      "epoch 174; iter: 0; batch classifier loss: 0.007689; batch adversarial loss: 0.433103\n",
      "epoch 175; iter: 0; batch classifier loss: 0.013153; batch adversarial loss: 0.531247\n",
      "epoch 176; iter: 0; batch classifier loss: 0.007330; batch adversarial loss: 0.419843\n",
      "epoch 177; iter: 0; batch classifier loss: 0.012984; batch adversarial loss: 0.491824\n",
      "epoch 178; iter: 0; batch classifier loss: 0.013910; batch adversarial loss: 0.441699\n",
      "epoch 179; iter: 0; batch classifier loss: 0.012498; batch adversarial loss: 0.515906\n",
      "epoch 180; iter: 0; batch classifier loss: 0.031273; batch adversarial loss: 0.458018\n",
      "epoch 181; iter: 0; batch classifier loss: 0.039369; batch adversarial loss: 0.432972\n",
      "epoch 182; iter: 0; batch classifier loss: 0.034488; batch adversarial loss: 0.494271\n",
      "epoch 183; iter: 0; batch classifier loss: 0.028760; batch adversarial loss: 0.477630\n",
      "epoch 184; iter: 0; batch classifier loss: 0.003358; batch adversarial loss: 0.410230\n",
      "epoch 185; iter: 0; batch classifier loss: 0.008431; batch adversarial loss: 0.415887\n",
      "epoch 186; iter: 0; batch classifier loss: 0.012250; batch adversarial loss: 0.528063\n",
      "epoch 187; iter: 0; batch classifier loss: 0.018286; batch adversarial loss: 0.525953\n",
      "epoch 188; iter: 0; batch classifier loss: 0.012842; batch adversarial loss: 0.436430\n",
      "epoch 189; iter: 0; batch classifier loss: 0.015511; batch adversarial loss: 0.486868\n",
      "epoch 190; iter: 0; batch classifier loss: 0.016325; batch adversarial loss: 0.362743\n",
      "epoch 191; iter: 0; batch classifier loss: 0.025971; batch adversarial loss: 0.450625\n",
      "epoch 192; iter: 0; batch classifier loss: 0.002842; batch adversarial loss: 0.520413\n",
      "epoch 193; iter: 0; batch classifier loss: 0.014467; batch adversarial loss: 0.450268\n",
      "epoch 194; iter: 0; batch classifier loss: 0.011491; batch adversarial loss: 0.497005\n",
      "epoch 195; iter: 0; batch classifier loss: 0.012869; batch adversarial loss: 0.466075\n",
      "epoch 196; iter: 0; batch classifier loss: 0.046112; batch adversarial loss: 0.474331\n",
      "epoch 197; iter: 0; batch classifier loss: 0.001744; batch adversarial loss: 0.376615\n",
      "epoch 198; iter: 0; batch classifier loss: 0.002974; batch adversarial loss: 0.495768\n",
      "epoch 199; iter: 0; batch classifier loss: 0.006627; batch adversarial loss: 0.450932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:49:25.594013: W tensorflow/c/c_api.cc:304] Operation '{name:'04a4482e-ae24-11ee-be98-ef9b34f2853b/04a4482e-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:3205 op device:{requested: '', assigned: ''} def:{{{node 04a4482e-ae24-11ee-be98-ef9b34f2853b/04a4482e-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a4482e-ae24-11ee-be98-ef9b34f2853b/04a4482e-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a4482e-ae24-11ee-be98-ef9b34f2853b/04a4482e-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.683073; batch adversarial loss: 0.620856\n",
      "epoch 1; iter: 0; batch classifier loss: 0.440653; batch adversarial loss: 0.621911\n",
      "epoch 2; iter: 0; batch classifier loss: 0.365344; batch adversarial loss: 0.600834\n",
      "epoch 3; iter: 0; batch classifier loss: 0.300769; batch adversarial loss: 0.601682\n",
      "epoch 4; iter: 0; batch classifier loss: 0.272899; batch adversarial loss: 0.534356\n",
      "epoch 5; iter: 0; batch classifier loss: 0.387933; batch adversarial loss: 0.513569\n",
      "epoch 6; iter: 0; batch classifier loss: 0.376173; batch adversarial loss: 0.530133\n",
      "epoch 7; iter: 0; batch classifier loss: 0.369582; batch adversarial loss: 0.561520\n",
      "epoch 8; iter: 0; batch classifier loss: 0.275634; batch adversarial loss: 0.532561\n",
      "epoch 9; iter: 0; batch classifier loss: 0.282988; batch adversarial loss: 0.506175\n",
      "epoch 10; iter: 0; batch classifier loss: 0.250687; batch adversarial loss: 0.404790\n",
      "epoch 11; iter: 0; batch classifier loss: 0.310366; batch adversarial loss: 0.531628\n",
      "epoch 12; iter: 0; batch classifier loss: 0.267975; batch adversarial loss: 0.560586\n",
      "epoch 13; iter: 0; batch classifier loss: 0.340264; batch adversarial loss: 0.544327\n",
      "epoch 14; iter: 0; batch classifier loss: 0.312451; batch adversarial loss: 0.493957\n",
      "epoch 15; iter: 0; batch classifier loss: 0.489029; batch adversarial loss: 0.491952\n",
      "epoch 16; iter: 0; batch classifier loss: 0.488049; batch adversarial loss: 0.555351\n",
      "epoch 17; iter: 0; batch classifier loss: 0.559185; batch adversarial loss: 0.456053\n",
      "epoch 18; iter: 0; batch classifier loss: 0.281974; batch adversarial loss: 0.509155\n",
      "epoch 19; iter: 0; batch classifier loss: 0.237257; batch adversarial loss: 0.485235\n",
      "epoch 20; iter: 0; batch classifier loss: 0.170045; batch adversarial loss: 0.451948\n",
      "epoch 21; iter: 0; batch classifier loss: 0.208757; batch adversarial loss: 0.488383\n",
      "epoch 22; iter: 0; batch classifier loss: 0.176183; batch adversarial loss: 0.451187\n",
      "epoch 23; iter: 0; batch classifier loss: 0.195614; batch adversarial loss: 0.419134\n",
      "epoch 24; iter: 0; batch classifier loss: 0.246046; batch adversarial loss: 0.445964\n",
      "epoch 25; iter: 0; batch classifier loss: 0.174144; batch adversarial loss: 0.449422\n",
      "epoch 26; iter: 0; batch classifier loss: 0.182457; batch adversarial loss: 0.482860\n",
      "epoch 27; iter: 0; batch classifier loss: 0.198129; batch adversarial loss: 0.478548\n",
      "epoch 28; iter: 0; batch classifier loss: 0.212512; batch adversarial loss: 0.394042\n",
      "epoch 29; iter: 0; batch classifier loss: 0.183227; batch adversarial loss: 0.388321\n",
      "epoch 30; iter: 0; batch classifier loss: 0.174824; batch adversarial loss: 0.420980\n",
      "epoch 31; iter: 0; batch classifier loss: 0.156038; batch adversarial loss: 0.456845\n",
      "epoch 32; iter: 0; batch classifier loss: 0.223948; batch adversarial loss: 0.470265\n",
      "epoch 33; iter: 0; batch classifier loss: 0.105976; batch adversarial loss: 0.556003\n",
      "epoch 34; iter: 0; batch classifier loss: 0.142961; batch adversarial loss: 0.437921\n",
      "epoch 35; iter: 0; batch classifier loss: 0.196068; batch adversarial loss: 0.411923\n",
      "epoch 36; iter: 0; batch classifier loss: 0.141115; batch adversarial loss: 0.470426\n",
      "epoch 37; iter: 0; batch classifier loss: 0.165741; batch adversarial loss: 0.353729\n",
      "epoch 38; iter: 0; batch classifier loss: 0.152037; batch adversarial loss: 0.526223\n",
      "epoch 39; iter: 0; batch classifier loss: 0.099027; batch adversarial loss: 0.464474\n",
      "epoch 40; iter: 0; batch classifier loss: 0.110921; batch adversarial loss: 0.452975\n",
      "epoch 41; iter: 0; batch classifier loss: 0.089708; batch adversarial loss: 0.475388\n",
      "epoch 42; iter: 0; batch classifier loss: 0.117443; batch adversarial loss: 0.463111\n",
      "epoch 43; iter: 0; batch classifier loss: 0.081043; batch adversarial loss: 0.560608\n",
      "epoch 44; iter: 0; batch classifier loss: 0.120436; batch adversarial loss: 0.472981\n",
      "epoch 45; iter: 0; batch classifier loss: 0.098060; batch adversarial loss: 0.487844\n",
      "epoch 46; iter: 0; batch classifier loss: 0.127252; batch adversarial loss: 0.368227\n",
      "epoch 47; iter: 0; batch classifier loss: 0.155660; batch adversarial loss: 0.397125\n",
      "epoch 48; iter: 0; batch classifier loss: 0.104707; batch adversarial loss: 0.495177\n",
      "epoch 49; iter: 0; batch classifier loss: 0.094743; batch adversarial loss: 0.437039\n",
      "epoch 50; iter: 0; batch classifier loss: 0.128758; batch adversarial loss: 0.367638\n",
      "epoch 51; iter: 0; batch classifier loss: 0.070209; batch adversarial loss: 0.419098\n",
      "epoch 52; iter: 0; batch classifier loss: 0.121524; batch adversarial loss: 0.511020\n",
      "epoch 53; iter: 0; batch classifier loss: 0.109622; batch adversarial loss: 0.444450\n",
      "epoch 54; iter: 0; batch classifier loss: 0.133209; batch adversarial loss: 0.376170\n",
      "epoch 55; iter: 0; batch classifier loss: 0.119858; batch adversarial loss: 0.415048\n",
      "epoch 56; iter: 0; batch classifier loss: 0.084955; batch adversarial loss: 0.505013\n",
      "epoch 57; iter: 0; batch classifier loss: 0.079445; batch adversarial loss: 0.437708\n",
      "epoch 58; iter: 0; batch classifier loss: 0.173282; batch adversarial loss: 0.451169\n",
      "epoch 59; iter: 0; batch classifier loss: 0.167735; batch adversarial loss: 0.457671\n",
      "epoch 60; iter: 0; batch classifier loss: 0.118800; batch adversarial loss: 0.457323\n",
      "epoch 61; iter: 0; batch classifier loss: 0.104539; batch adversarial loss: 0.457290\n",
      "epoch 62; iter: 0; batch classifier loss: 0.104589; batch adversarial loss: 0.386224\n",
      "epoch 63; iter: 0; batch classifier loss: 0.178848; batch adversarial loss: 0.489418\n",
      "epoch 64; iter: 0; batch classifier loss: 0.118512; batch adversarial loss: 0.493791\n",
      "epoch 65; iter: 0; batch classifier loss: 0.118857; batch adversarial loss: 0.426787\n",
      "epoch 66; iter: 0; batch classifier loss: 0.088094; batch adversarial loss: 0.422159\n",
      "epoch 67; iter: 0; batch classifier loss: 0.094197; batch adversarial loss: 0.431979\n",
      "epoch 68; iter: 0; batch classifier loss: 0.094664; batch adversarial loss: 0.475427\n",
      "epoch 69; iter: 0; batch classifier loss: 0.085103; batch adversarial loss: 0.465173\n",
      "epoch 70; iter: 0; batch classifier loss: 0.144985; batch adversarial loss: 0.411650\n",
      "epoch 71; iter: 0; batch classifier loss: 0.137528; batch adversarial loss: 0.399713\n",
      "epoch 72; iter: 0; batch classifier loss: 0.100343; batch adversarial loss: 0.345952\n",
      "epoch 73; iter: 0; batch classifier loss: 0.114244; batch adversarial loss: 0.513249\n",
      "epoch 74; iter: 0; batch classifier loss: 0.079822; batch adversarial loss: 0.488730\n",
      "epoch 75; iter: 0; batch classifier loss: 0.163532; batch adversarial loss: 0.430517\n",
      "epoch 76; iter: 0; batch classifier loss: 0.106552; batch adversarial loss: 0.413256\n",
      "epoch 77; iter: 0; batch classifier loss: 0.108928; batch adversarial loss: 0.469671\n",
      "epoch 78; iter: 0; batch classifier loss: 0.072789; batch adversarial loss: 0.419774\n",
      "epoch 79; iter: 0; batch classifier loss: 0.080087; batch adversarial loss: 0.451439\n",
      "epoch 80; iter: 0; batch classifier loss: 0.133886; batch adversarial loss: 0.384145\n",
      "epoch 81; iter: 0; batch classifier loss: 0.108813; batch adversarial loss: 0.489638\n",
      "epoch 82; iter: 0; batch classifier loss: 0.127258; batch adversarial loss: 0.392087\n",
      "epoch 83; iter: 0; batch classifier loss: 0.086930; batch adversarial loss: 0.473320\n",
      "epoch 84; iter: 0; batch classifier loss: 0.052324; batch adversarial loss: 0.472512\n",
      "epoch 85; iter: 0; batch classifier loss: 0.095781; batch adversarial loss: 0.449457\n",
      "epoch 86; iter: 0; batch classifier loss: 0.089477; batch adversarial loss: 0.437829\n",
      "epoch 87; iter: 0; batch classifier loss: 0.079782; batch adversarial loss: 0.453353\n",
      "epoch 88; iter: 0; batch classifier loss: 0.143531; batch adversarial loss: 0.466258\n",
      "epoch 89; iter: 0; batch classifier loss: 0.069261; batch adversarial loss: 0.354200\n",
      "epoch 90; iter: 0; batch classifier loss: 0.052139; batch adversarial loss: 0.470479\n",
      "epoch 91; iter: 0; batch classifier loss: 0.062476; batch adversarial loss: 0.541695\n",
      "epoch 92; iter: 0; batch classifier loss: 0.082710; batch adversarial loss: 0.480140\n",
      "epoch 93; iter: 0; batch classifier loss: 0.041896; batch adversarial loss: 0.473447\n",
      "epoch 94; iter: 0; batch classifier loss: 0.075548; batch adversarial loss: 0.447946\n",
      "epoch 95; iter: 0; batch classifier loss: 0.058477; batch adversarial loss: 0.426762\n",
      "epoch 96; iter: 0; batch classifier loss: 0.065355; batch adversarial loss: 0.453171\n",
      "epoch 97; iter: 0; batch classifier loss: 0.046726; batch adversarial loss: 0.335832\n",
      "epoch 98; iter: 0; batch classifier loss: 0.053453; batch adversarial loss: 0.476380\n",
      "epoch 99; iter: 0; batch classifier loss: 0.073238; batch adversarial loss: 0.586781\n",
      "epoch 100; iter: 0; batch classifier loss: 0.035395; batch adversarial loss: 0.427228\n",
      "epoch 101; iter: 0; batch classifier loss: 0.042540; batch adversarial loss: 0.432745\n",
      "epoch 102; iter: 0; batch classifier loss: 0.071360; batch adversarial loss: 0.362941\n",
      "epoch 103; iter: 0; batch classifier loss: 0.093896; batch adversarial loss: 0.463941\n",
      "epoch 104; iter: 0; batch classifier loss: 0.057524; batch adversarial loss: 0.437521\n",
      "epoch 105; iter: 0; batch classifier loss: 0.051401; batch adversarial loss: 0.405707\n",
      "epoch 106; iter: 0; batch classifier loss: 0.054419; batch adversarial loss: 0.473556\n",
      "epoch 107; iter: 0; batch classifier loss: 0.065696; batch adversarial loss: 0.484608\n",
      "epoch 108; iter: 0; batch classifier loss: 0.038128; batch adversarial loss: 0.450714\n",
      "epoch 109; iter: 0; batch classifier loss: 0.050737; batch adversarial loss: 0.470143\n",
      "epoch 110; iter: 0; batch classifier loss: 0.064994; batch adversarial loss: 0.515462\n",
      "epoch 111; iter: 0; batch classifier loss: 0.064557; batch adversarial loss: 0.451665\n",
      "epoch 112; iter: 0; batch classifier loss: 0.038048; batch adversarial loss: 0.495670\n",
      "epoch 113; iter: 0; batch classifier loss: 0.069824; batch adversarial loss: 0.392696\n",
      "epoch 114; iter: 0; batch classifier loss: 0.035725; batch adversarial loss: 0.484216\n",
      "epoch 115; iter: 0; batch classifier loss: 0.044866; batch adversarial loss: 0.430529\n",
      "epoch 116; iter: 0; batch classifier loss: 0.073175; batch adversarial loss: 0.408202\n",
      "epoch 117; iter: 0; batch classifier loss: 0.107425; batch adversarial loss: 0.540736\n",
      "epoch 118; iter: 0; batch classifier loss: 0.043385; batch adversarial loss: 0.383689\n",
      "epoch 119; iter: 0; batch classifier loss: 0.049156; batch adversarial loss: 0.482229\n",
      "epoch 120; iter: 0; batch classifier loss: 0.043849; batch adversarial loss: 0.469089\n",
      "epoch 121; iter: 0; batch classifier loss: 0.058626; batch adversarial loss: 0.445064\n",
      "epoch 122; iter: 0; batch classifier loss: 0.015754; batch adversarial loss: 0.330612\n",
      "epoch 123; iter: 0; batch classifier loss: 0.045682; batch adversarial loss: 0.485688\n",
      "epoch 124; iter: 0; batch classifier loss: 0.033378; batch adversarial loss: 0.408941\n",
      "epoch 125; iter: 0; batch classifier loss: 0.061630; batch adversarial loss: 0.451863\n",
      "epoch 126; iter: 0; batch classifier loss: 0.019002; batch adversarial loss: 0.464778\n",
      "epoch 127; iter: 0; batch classifier loss: 0.026591; batch adversarial loss: 0.464596\n",
      "epoch 128; iter: 0; batch classifier loss: 0.028328; batch adversarial loss: 0.404924\n",
      "epoch 129; iter: 0; batch classifier loss: 0.041040; batch adversarial loss: 0.422667\n",
      "epoch 130; iter: 0; batch classifier loss: 0.021741; batch adversarial loss: 0.410978\n",
      "epoch 131; iter: 0; batch classifier loss: 0.029504; batch adversarial loss: 0.448252\n",
      "epoch 132; iter: 0; batch classifier loss: 0.026199; batch adversarial loss: 0.412464\n",
      "epoch 133; iter: 0; batch classifier loss: 0.039348; batch adversarial loss: 0.566884\n",
      "epoch 134; iter: 0; batch classifier loss: 0.051199; batch adversarial loss: 0.374595\n",
      "epoch 135; iter: 0; batch classifier loss: 0.029727; batch adversarial loss: 0.502335\n",
      "epoch 136; iter: 0; batch classifier loss: 0.086487; batch adversarial loss: 0.358220\n",
      "epoch 137; iter: 0; batch classifier loss: 0.060143; batch adversarial loss: 0.515063\n",
      "epoch 138; iter: 0; batch classifier loss: 0.053990; batch adversarial loss: 0.432021\n",
      "epoch 139; iter: 0; batch classifier loss: 0.022910; batch adversarial loss: 0.428206\n",
      "epoch 140; iter: 0; batch classifier loss: 0.028811; batch adversarial loss: 0.459213\n",
      "epoch 141; iter: 0; batch classifier loss: 0.052790; batch adversarial loss: 0.467816\n",
      "epoch 142; iter: 0; batch classifier loss: 0.042714; batch adversarial loss: 0.384554\n",
      "epoch 143; iter: 0; batch classifier loss: 0.043146; batch adversarial loss: 0.339316\n",
      "epoch 144; iter: 0; batch classifier loss: 0.024966; batch adversarial loss: 0.508985\n",
      "epoch 145; iter: 0; batch classifier loss: 0.020702; batch adversarial loss: 0.497956\n",
      "epoch 146; iter: 0; batch classifier loss: 0.026609; batch adversarial loss: 0.396169\n",
      "epoch 147; iter: 0; batch classifier loss: 0.042491; batch adversarial loss: 0.516891\n",
      "epoch 148; iter: 0; batch classifier loss: 0.015826; batch adversarial loss: 0.594180\n",
      "epoch 149; iter: 0; batch classifier loss: 0.040416; batch adversarial loss: 0.530106\n",
      "epoch 150; iter: 0; batch classifier loss: 0.020989; batch adversarial loss: 0.460259\n",
      "epoch 151; iter: 0; batch classifier loss: 0.008401; batch adversarial loss: 0.446931\n",
      "epoch 152; iter: 0; batch classifier loss: 0.014468; batch adversarial loss: 0.462676\n",
      "epoch 153; iter: 0; batch classifier loss: 0.028570; batch adversarial loss: 0.427720\n",
      "epoch 154; iter: 0; batch classifier loss: 0.067858; batch adversarial loss: 0.356513\n",
      "epoch 155; iter: 0; batch classifier loss: 0.037252; batch adversarial loss: 0.426754\n",
      "epoch 156; iter: 0; batch classifier loss: 0.058051; batch adversarial loss: 0.408194\n",
      "epoch 157; iter: 0; batch classifier loss: 0.024619; batch adversarial loss: 0.531720\n",
      "epoch 158; iter: 0; batch classifier loss: 0.044226; batch adversarial loss: 0.399916\n",
      "epoch 159; iter: 0; batch classifier loss: 0.024355; batch adversarial loss: 0.398681\n",
      "epoch 160; iter: 0; batch classifier loss: 0.020534; batch adversarial loss: 0.469875\n",
      "epoch 161; iter: 0; batch classifier loss: 0.043514; batch adversarial loss: 0.437727\n",
      "epoch 162; iter: 0; batch classifier loss: 0.016105; batch adversarial loss: 0.424770\n",
      "epoch 163; iter: 0; batch classifier loss: 0.024601; batch adversarial loss: 0.405221\n",
      "epoch 164; iter: 0; batch classifier loss: 0.023990; batch adversarial loss: 0.350152\n",
      "epoch 165; iter: 0; batch classifier loss: 0.040421; batch adversarial loss: 0.382282\n",
      "epoch 166; iter: 0; batch classifier loss: 0.045091; batch adversarial loss: 0.472792\n",
      "epoch 167; iter: 0; batch classifier loss: 0.046813; batch adversarial loss: 0.379816\n",
      "epoch 168; iter: 0; batch classifier loss: 0.026784; batch adversarial loss: 0.394039\n",
      "epoch 169; iter: 0; batch classifier loss: 0.021587; batch adversarial loss: 0.414904\n",
      "epoch 170; iter: 0; batch classifier loss: 0.014293; batch adversarial loss: 0.413440\n",
      "epoch 171; iter: 0; batch classifier loss: 0.013882; batch adversarial loss: 0.478051\n",
      "epoch 172; iter: 0; batch classifier loss: 0.017514; batch adversarial loss: 0.448272\n",
      "epoch 173; iter: 0; batch classifier loss: 0.025829; batch adversarial loss: 0.448238\n",
      "epoch 174; iter: 0; batch classifier loss: 0.065883; batch adversarial loss: 0.425081\n",
      "epoch 175; iter: 0; batch classifier loss: 0.040838; batch adversarial loss: 0.481297\n",
      "epoch 176; iter: 0; batch classifier loss: 0.018787; batch adversarial loss: 0.461707\n",
      "epoch 177; iter: 0; batch classifier loss: 0.051299; batch adversarial loss: 0.391626\n",
      "epoch 178; iter: 0; batch classifier loss: 0.021155; batch adversarial loss: 0.433748\n",
      "epoch 179; iter: 0; batch classifier loss: 0.028798; batch adversarial loss: 0.413467\n",
      "epoch 180; iter: 0; batch classifier loss: 0.019515; batch adversarial loss: 0.445502\n",
      "epoch 181; iter: 0; batch classifier loss: 0.012136; batch adversarial loss: 0.463219\n",
      "epoch 182; iter: 0; batch classifier loss: 0.021031; batch adversarial loss: 0.507201\n",
      "epoch 183; iter: 0; batch classifier loss: 0.012701; batch adversarial loss: 0.405407\n",
      "epoch 184; iter: 0; batch classifier loss: 0.036952; batch adversarial loss: 0.458699\n",
      "epoch 185; iter: 0; batch classifier loss: 0.013958; batch adversarial loss: 0.426156\n",
      "epoch 186; iter: 0; batch classifier loss: 0.022732; batch adversarial loss: 0.356023\n",
      "epoch 187; iter: 0; batch classifier loss: 0.017771; batch adversarial loss: 0.448996\n",
      "epoch 188; iter: 0; batch classifier loss: 0.058177; batch adversarial loss: 0.411220\n",
      "epoch 189; iter: 0; batch classifier loss: 0.014598; batch adversarial loss: 0.374938\n",
      "epoch 190; iter: 0; batch classifier loss: 0.023809; batch adversarial loss: 0.396063\n",
      "epoch 191; iter: 0; batch classifier loss: 0.017848; batch adversarial loss: 0.529327\n",
      "epoch 192; iter: 0; batch classifier loss: 0.005764; batch adversarial loss: 0.395014\n",
      "epoch 193; iter: 0; batch classifier loss: 0.005059; batch adversarial loss: 0.442585\n",
      "epoch 194; iter: 0; batch classifier loss: 0.016724; batch adversarial loss: 0.405878\n",
      "epoch 195; iter: 0; batch classifier loss: 0.024334; batch adversarial loss: 0.359202\n",
      "epoch 196; iter: 0; batch classifier loss: 0.006897; batch adversarial loss: 0.479158\n",
      "epoch 197; iter: 0; batch classifier loss: 0.007722; batch adversarial loss: 0.473242\n",
      "epoch 198; iter: 0; batch classifier loss: 0.028269; batch adversarial loss: 0.322968\n",
      "epoch 199; iter: 0; batch classifier loss: 0.031228; batch adversarial loss: 0.561645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:50:02.788655: W tensorflow/c/c_api.cc:304] Operation '{name:'04a448ba-ae24-11ee-be98-ef9b34f2853b/04a448ba-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:4012 op device:{requested: '', assigned: ''} def:{{{node 04a448ba-ae24-11ee-be98-ef9b34f2853b/04a448ba-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a448ba-ae24-11ee-be98-ef9b34f2853b/04a448ba-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a448ba-ae24-11ee-be98-ef9b34f2853b/04a448ba-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.688715; batch adversarial loss: 0.983039\n",
      "epoch 1; iter: 0; batch classifier loss: 0.628121; batch adversarial loss: 1.100539\n",
      "epoch 2; iter: 0; batch classifier loss: 0.900819; batch adversarial loss: 1.126426\n",
      "epoch 3; iter: 0; batch classifier loss: 1.017392; batch adversarial loss: 1.014833\n",
      "epoch 4; iter: 0; batch classifier loss: 0.997079; batch adversarial loss: 0.934127\n",
      "epoch 5; iter: 0; batch classifier loss: 0.972972; batch adversarial loss: 0.840362\n",
      "epoch 6; iter: 0; batch classifier loss: 1.035811; batch adversarial loss: 0.765038\n",
      "epoch 7; iter: 0; batch classifier loss: 0.989932; batch adversarial loss: 0.694059\n",
      "epoch 8; iter: 0; batch classifier loss: 1.016223; batch adversarial loss: 0.648634\n",
      "epoch 9; iter: 0; batch classifier loss: 0.816278; batch adversarial loss: 0.583646\n",
      "epoch 10; iter: 0; batch classifier loss: 0.814806; batch adversarial loss: 0.581283\n",
      "epoch 11; iter: 0; batch classifier loss: 0.388503; batch adversarial loss: 0.534007\n",
      "epoch 12; iter: 0; batch classifier loss: 0.460209; batch adversarial loss: 0.522313\n",
      "epoch 13; iter: 0; batch classifier loss: 0.363631; batch adversarial loss: 0.483172\n",
      "epoch 14; iter: 0; batch classifier loss: 0.334066; batch adversarial loss: 0.538998\n",
      "epoch 15; iter: 0; batch classifier loss: 0.330832; batch adversarial loss: 0.503753\n",
      "epoch 16; iter: 0; batch classifier loss: 0.335103; batch adversarial loss: 0.460181\n",
      "epoch 17; iter: 0; batch classifier loss: 0.217084; batch adversarial loss: 0.449338\n",
      "epoch 18; iter: 0; batch classifier loss: 0.222560; batch adversarial loss: 0.452156\n",
      "epoch 19; iter: 0; batch classifier loss: 0.253071; batch adversarial loss: 0.532440\n",
      "epoch 20; iter: 0; batch classifier loss: 0.192251; batch adversarial loss: 0.515274\n",
      "epoch 21; iter: 0; batch classifier loss: 0.195617; batch adversarial loss: 0.457270\n",
      "epoch 22; iter: 0; batch classifier loss: 0.116874; batch adversarial loss: 0.416551\n",
      "epoch 23; iter: 0; batch classifier loss: 0.110665; batch adversarial loss: 0.418451\n",
      "epoch 24; iter: 0; batch classifier loss: 0.130526; batch adversarial loss: 0.442453\n",
      "epoch 25; iter: 0; batch classifier loss: 0.148387; batch adversarial loss: 0.546933\n",
      "epoch 26; iter: 0; batch classifier loss: 0.121442; batch adversarial loss: 0.433987\n",
      "epoch 27; iter: 0; batch classifier loss: 0.104637; batch adversarial loss: 0.486104\n",
      "epoch 28; iter: 0; batch classifier loss: 0.091559; batch adversarial loss: 0.451715\n",
      "epoch 29; iter: 0; batch classifier loss: 0.084214; batch adversarial loss: 0.447392\n",
      "epoch 30; iter: 0; batch classifier loss: 0.066105; batch adversarial loss: 0.413685\n",
      "epoch 31; iter: 0; batch classifier loss: 0.068282; batch adversarial loss: 0.391311\n",
      "epoch 32; iter: 0; batch classifier loss: 0.114118; batch adversarial loss: 0.363025\n",
      "epoch 33; iter: 0; batch classifier loss: 0.084376; batch adversarial loss: 0.496647\n",
      "epoch 34; iter: 0; batch classifier loss: 0.109469; batch adversarial loss: 0.473723\n",
      "epoch 35; iter: 0; batch classifier loss: 0.130510; batch adversarial loss: 0.433919\n",
      "epoch 36; iter: 0; batch classifier loss: 0.102940; batch adversarial loss: 0.484450\n",
      "epoch 37; iter: 0; batch classifier loss: 0.109572; batch adversarial loss: 0.418393\n",
      "epoch 38; iter: 0; batch classifier loss: 0.059485; batch adversarial loss: 0.538812\n",
      "epoch 39; iter: 0; batch classifier loss: 0.107677; batch adversarial loss: 0.465860\n",
      "epoch 40; iter: 0; batch classifier loss: 0.131476; batch adversarial loss: 0.472353\n",
      "epoch 41; iter: 0; batch classifier loss: 0.057669; batch adversarial loss: 0.450761\n",
      "epoch 42; iter: 0; batch classifier loss: 0.082112; batch adversarial loss: 0.501039\n",
      "epoch 43; iter: 0; batch classifier loss: 0.063107; batch adversarial loss: 0.426068\n",
      "epoch 44; iter: 0; batch classifier loss: 0.101416; batch adversarial loss: 0.474336\n",
      "epoch 45; iter: 0; batch classifier loss: 0.090052; batch adversarial loss: 0.427132\n",
      "epoch 46; iter: 0; batch classifier loss: 0.081061; batch adversarial loss: 0.443573\n",
      "epoch 47; iter: 0; batch classifier loss: 0.100561; batch adversarial loss: 0.462411\n",
      "epoch 48; iter: 0; batch classifier loss: 0.082665; batch adversarial loss: 0.528099\n",
      "epoch 49; iter: 0; batch classifier loss: 0.080586; batch adversarial loss: 0.463851\n",
      "epoch 50; iter: 0; batch classifier loss: 0.074351; batch adversarial loss: 0.467955\n",
      "epoch 51; iter: 0; batch classifier loss: 0.065128; batch adversarial loss: 0.363463\n",
      "epoch 52; iter: 0; batch classifier loss: 0.065074; batch adversarial loss: 0.505935\n",
      "epoch 53; iter: 0; batch classifier loss: 0.062227; batch adversarial loss: 0.450037\n",
      "epoch 54; iter: 0; batch classifier loss: 0.082096; batch adversarial loss: 0.483712\n",
      "epoch 55; iter: 0; batch classifier loss: 0.098301; batch adversarial loss: 0.472368\n",
      "epoch 56; iter: 0; batch classifier loss: 0.066021; batch adversarial loss: 0.470020\n",
      "epoch 57; iter: 0; batch classifier loss: 0.061266; batch adversarial loss: 0.469520\n",
      "epoch 58; iter: 0; batch classifier loss: 0.091018; batch adversarial loss: 0.460826\n",
      "epoch 59; iter: 0; batch classifier loss: 0.088777; batch adversarial loss: 0.368837\n",
      "epoch 60; iter: 0; batch classifier loss: 0.072266; batch adversarial loss: 0.417231\n",
      "epoch 61; iter: 0; batch classifier loss: 0.088821; batch adversarial loss: 0.371266\n",
      "epoch 62; iter: 0; batch classifier loss: 0.084045; batch adversarial loss: 0.388032\n",
      "epoch 63; iter: 0; batch classifier loss: 0.099800; batch adversarial loss: 0.480330\n",
      "epoch 64; iter: 0; batch classifier loss: 0.070695; batch adversarial loss: 0.464629\n",
      "epoch 65; iter: 0; batch classifier loss: 0.050034; batch adversarial loss: 0.461426\n",
      "epoch 66; iter: 0; batch classifier loss: 0.091146; batch adversarial loss: 0.408800\n",
      "epoch 67; iter: 0; batch classifier loss: 0.106377; batch adversarial loss: 0.504875\n",
      "epoch 68; iter: 0; batch classifier loss: 0.043170; batch adversarial loss: 0.464844\n",
      "epoch 69; iter: 0; batch classifier loss: 0.072073; batch adversarial loss: 0.433548\n",
      "epoch 70; iter: 0; batch classifier loss: 0.052125; batch adversarial loss: 0.480371\n",
      "epoch 71; iter: 0; batch classifier loss: 0.057055; batch adversarial loss: 0.423309\n",
      "epoch 72; iter: 0; batch classifier loss: 0.088492; batch adversarial loss: 0.363886\n",
      "epoch 73; iter: 0; batch classifier loss: 0.047265; batch adversarial loss: 0.382111\n",
      "epoch 74; iter: 0; batch classifier loss: 0.040557; batch adversarial loss: 0.378222\n",
      "epoch 75; iter: 0; batch classifier loss: 0.042162; batch adversarial loss: 0.436483\n",
      "epoch 76; iter: 0; batch classifier loss: 0.047782; batch adversarial loss: 0.469712\n",
      "epoch 77; iter: 0; batch classifier loss: 0.045663; batch adversarial loss: 0.431864\n",
      "epoch 78; iter: 0; batch classifier loss: 0.059991; batch adversarial loss: 0.420678\n",
      "epoch 79; iter: 0; batch classifier loss: 0.064905; batch adversarial loss: 0.493314\n",
      "epoch 80; iter: 0; batch classifier loss: 0.045461; batch adversarial loss: 0.376153\n",
      "epoch 81; iter: 0; batch classifier loss: 0.050206; batch adversarial loss: 0.409281\n",
      "epoch 82; iter: 0; batch classifier loss: 0.046113; batch adversarial loss: 0.467115\n",
      "epoch 83; iter: 0; batch classifier loss: 0.035233; batch adversarial loss: 0.469703\n",
      "epoch 84; iter: 0; batch classifier loss: 0.029775; batch adversarial loss: 0.505177\n",
      "epoch 85; iter: 0; batch classifier loss: 0.042924; batch adversarial loss: 0.485360\n",
      "epoch 86; iter: 0; batch classifier loss: 0.021504; batch adversarial loss: 0.443047\n",
      "epoch 87; iter: 0; batch classifier loss: 0.047788; batch adversarial loss: 0.547391\n",
      "epoch 88; iter: 0; batch classifier loss: 0.042897; batch adversarial loss: 0.348584\n",
      "epoch 89; iter: 0; batch classifier loss: 0.030057; batch adversarial loss: 0.445219\n",
      "epoch 90; iter: 0; batch classifier loss: 0.048613; batch adversarial loss: 0.498932\n",
      "epoch 91; iter: 0; batch classifier loss: 0.028670; batch adversarial loss: 0.392131\n",
      "epoch 92; iter: 0; batch classifier loss: 0.024187; batch adversarial loss: 0.528975\n",
      "epoch 93; iter: 0; batch classifier loss: 0.032129; batch adversarial loss: 0.476735\n",
      "epoch 94; iter: 0; batch classifier loss: 0.033969; batch adversarial loss: 0.491378\n",
      "epoch 95; iter: 0; batch classifier loss: 0.033368; batch adversarial loss: 0.406544\n",
      "epoch 96; iter: 0; batch classifier loss: 0.035445; batch adversarial loss: 0.499140\n",
      "epoch 97; iter: 0; batch classifier loss: 0.063905; batch adversarial loss: 0.452456\n",
      "epoch 98; iter: 0; batch classifier loss: 0.056222; batch adversarial loss: 0.370576\n",
      "epoch 99; iter: 0; batch classifier loss: 0.046255; batch adversarial loss: 0.461843\n",
      "epoch 100; iter: 0; batch classifier loss: 0.019374; batch adversarial loss: 0.526640\n",
      "epoch 101; iter: 0; batch classifier loss: 0.037838; batch adversarial loss: 0.428831\n",
      "epoch 102; iter: 0; batch classifier loss: 0.029502; batch adversarial loss: 0.517934\n",
      "epoch 103; iter: 0; batch classifier loss: 0.036693; batch adversarial loss: 0.479803\n",
      "epoch 104; iter: 0; batch classifier loss: 0.033586; batch adversarial loss: 0.462126\n",
      "epoch 105; iter: 0; batch classifier loss: 0.022528; batch adversarial loss: 0.480492\n",
      "epoch 106; iter: 0; batch classifier loss: 0.051269; batch adversarial loss: 0.390230\n",
      "epoch 107; iter: 0; batch classifier loss: 0.025912; batch adversarial loss: 0.428924\n",
      "epoch 108; iter: 0; batch classifier loss: 0.042159; batch adversarial loss: 0.443559\n",
      "epoch 109; iter: 0; batch classifier loss: 0.021290; batch adversarial loss: 0.487578\n",
      "epoch 110; iter: 0; batch classifier loss: 0.023505; batch adversarial loss: 0.492512\n",
      "epoch 111; iter: 0; batch classifier loss: 0.017535; batch adversarial loss: 0.410937\n",
      "epoch 112; iter: 0; batch classifier loss: 0.025621; batch adversarial loss: 0.450347\n",
      "epoch 113; iter: 0; batch classifier loss: 0.038833; batch adversarial loss: 0.396705\n",
      "epoch 114; iter: 0; batch classifier loss: 0.033631; batch adversarial loss: 0.478048\n",
      "epoch 115; iter: 0; batch classifier loss: 0.033618; batch adversarial loss: 0.445996\n",
      "epoch 116; iter: 0; batch classifier loss: 0.021869; batch adversarial loss: 0.450988\n",
      "epoch 117; iter: 0; batch classifier loss: 0.025700; batch adversarial loss: 0.489628\n",
      "epoch 118; iter: 0; batch classifier loss: 0.052874; batch adversarial loss: 0.503270\n",
      "epoch 119; iter: 0; batch classifier loss: 0.044407; batch adversarial loss: 0.487963\n",
      "epoch 120; iter: 0; batch classifier loss: 0.039415; batch adversarial loss: 0.468673\n",
      "epoch 121; iter: 0; batch classifier loss: 0.042812; batch adversarial loss: 0.489505\n",
      "epoch 122; iter: 0; batch classifier loss: 0.038994; batch adversarial loss: 0.412301\n",
      "epoch 123; iter: 0; batch classifier loss: 0.036888; batch adversarial loss: 0.312297\n",
      "epoch 124; iter: 0; batch classifier loss: 0.036261; batch adversarial loss: 0.461313\n",
      "epoch 125; iter: 0; batch classifier loss: 0.038236; batch adversarial loss: 0.519647\n",
      "epoch 126; iter: 0; batch classifier loss: 0.039189; batch adversarial loss: 0.494202\n",
      "epoch 127; iter: 0; batch classifier loss: 0.035679; batch adversarial loss: 0.501105\n",
      "epoch 128; iter: 0; batch classifier loss: 0.010645; batch adversarial loss: 0.499304\n",
      "epoch 129; iter: 0; batch classifier loss: 0.018803; batch adversarial loss: 0.537932\n",
      "epoch 130; iter: 0; batch classifier loss: 0.061528; batch adversarial loss: 0.496497\n",
      "epoch 131; iter: 0; batch classifier loss: 0.009915; batch adversarial loss: 0.458411\n",
      "epoch 132; iter: 0; batch classifier loss: 0.034905; batch adversarial loss: 0.429200\n",
      "epoch 133; iter: 0; batch classifier loss: 0.013308; batch adversarial loss: 0.538931\n",
      "epoch 134; iter: 0; batch classifier loss: 0.020050; batch adversarial loss: 0.378343\n",
      "epoch 135; iter: 0; batch classifier loss: 0.036235; batch adversarial loss: 0.495071\n",
      "epoch 136; iter: 0; batch classifier loss: 0.033578; batch adversarial loss: 0.513097\n",
      "epoch 137; iter: 0; batch classifier loss: 0.042364; batch adversarial loss: 0.465550\n",
      "epoch 138; iter: 0; batch classifier loss: 0.021714; batch adversarial loss: 0.450820\n",
      "epoch 139; iter: 0; batch classifier loss: 0.014111; batch adversarial loss: 0.409608\n",
      "epoch 140; iter: 0; batch classifier loss: 0.017977; batch adversarial loss: 0.388635\n",
      "epoch 141; iter: 0; batch classifier loss: 0.023161; batch adversarial loss: 0.446668\n",
      "epoch 142; iter: 0; batch classifier loss: 0.048083; batch adversarial loss: 0.429200\n",
      "epoch 143; iter: 0; batch classifier loss: 0.025612; batch adversarial loss: 0.396818\n",
      "epoch 144; iter: 0; batch classifier loss: 0.032844; batch adversarial loss: 0.315482\n",
      "epoch 145; iter: 0; batch classifier loss: 0.024941; batch adversarial loss: 0.376494\n",
      "epoch 146; iter: 0; batch classifier loss: 0.041394; batch adversarial loss: 0.386173\n",
      "epoch 147; iter: 0; batch classifier loss: 0.024354; batch adversarial loss: 0.495191\n",
      "epoch 148; iter: 0; batch classifier loss: 0.014848; batch adversarial loss: 0.530843\n",
      "epoch 149; iter: 0; batch classifier loss: 0.033395; batch adversarial loss: 0.405986\n",
      "epoch 150; iter: 0; batch classifier loss: 0.053988; batch adversarial loss: 0.362676\n",
      "epoch 151; iter: 0; batch classifier loss: 0.024418; batch adversarial loss: 0.431975\n",
      "epoch 152; iter: 0; batch classifier loss: 0.030281; batch adversarial loss: 0.427211\n",
      "epoch 153; iter: 0; batch classifier loss: 0.020341; batch adversarial loss: 0.485753\n",
      "epoch 154; iter: 0; batch classifier loss: 0.040641; batch adversarial loss: 0.496311\n",
      "epoch 155; iter: 0; batch classifier loss: 0.024292; batch adversarial loss: 0.391502\n",
      "epoch 156; iter: 0; batch classifier loss: 0.030846; batch adversarial loss: 0.431918\n",
      "epoch 157; iter: 0; batch classifier loss: 0.019141; batch adversarial loss: 0.478216\n",
      "epoch 158; iter: 0; batch classifier loss: 0.012302; batch adversarial loss: 0.474453\n",
      "epoch 159; iter: 0; batch classifier loss: 0.026862; batch adversarial loss: 0.497809\n",
      "epoch 160; iter: 0; batch classifier loss: 0.017936; batch adversarial loss: 0.467878\n",
      "epoch 161; iter: 0; batch classifier loss: 0.010025; batch adversarial loss: 0.482063\n",
      "epoch 162; iter: 0; batch classifier loss: 0.019128; batch adversarial loss: 0.359276\n",
      "epoch 163; iter: 0; batch classifier loss: 0.006330; batch adversarial loss: 0.433474\n",
      "epoch 164; iter: 0; batch classifier loss: 0.029055; batch adversarial loss: 0.578649\n",
      "epoch 165; iter: 0; batch classifier loss: 0.020343; batch adversarial loss: 0.478374\n",
      "epoch 166; iter: 0; batch classifier loss: 0.024393; batch adversarial loss: 0.476705\n",
      "epoch 167; iter: 0; batch classifier loss: 0.022079; batch adversarial loss: 0.578530\n",
      "epoch 168; iter: 0; batch classifier loss: 0.017073; batch adversarial loss: 0.552534\n",
      "epoch 169; iter: 0; batch classifier loss: 0.013898; batch adversarial loss: 0.532417\n",
      "epoch 170; iter: 0; batch classifier loss: 0.012398; batch adversarial loss: 0.422551\n",
      "epoch 171; iter: 0; batch classifier loss: 0.012326; batch adversarial loss: 0.436369\n",
      "epoch 172; iter: 0; batch classifier loss: 0.031451; batch adversarial loss: 0.428115\n",
      "epoch 173; iter: 0; batch classifier loss: 0.020282; batch adversarial loss: 0.416464\n",
      "epoch 174; iter: 0; batch classifier loss: 0.021471; batch adversarial loss: 0.379303\n",
      "epoch 175; iter: 0; batch classifier loss: 0.009096; batch adversarial loss: 0.368390\n",
      "epoch 176; iter: 0; batch classifier loss: 0.041444; batch adversarial loss: 0.411759\n",
      "epoch 177; iter: 0; batch classifier loss: 0.008289; batch adversarial loss: 0.522269\n",
      "epoch 178; iter: 0; batch classifier loss: 0.013008; batch adversarial loss: 0.357271\n",
      "epoch 179; iter: 0; batch classifier loss: 0.010976; batch adversarial loss: 0.453636\n",
      "epoch 180; iter: 0; batch classifier loss: 0.011228; batch adversarial loss: 0.610253\n",
      "epoch 181; iter: 0; batch classifier loss: 0.032438; batch adversarial loss: 0.416747\n",
      "epoch 182; iter: 0; batch classifier loss: 0.004702; batch adversarial loss: 0.519912\n",
      "epoch 183; iter: 0; batch classifier loss: 0.007755; batch adversarial loss: 0.428016\n",
      "epoch 184; iter: 0; batch classifier loss: 0.004324; batch adversarial loss: 0.480400\n",
      "epoch 185; iter: 0; batch classifier loss: 0.021910; batch adversarial loss: 0.397073\n",
      "epoch 186; iter: 0; batch classifier loss: 0.016745; batch adversarial loss: 0.464728\n",
      "epoch 187; iter: 0; batch classifier loss: 0.004339; batch adversarial loss: 0.428473\n",
      "epoch 188; iter: 0; batch classifier loss: 0.027548; batch adversarial loss: 0.390911\n",
      "epoch 189; iter: 0; batch classifier loss: 0.027850; batch adversarial loss: 0.499076\n",
      "epoch 190; iter: 0; batch classifier loss: 0.011445; batch adversarial loss: 0.405312\n",
      "epoch 191; iter: 0; batch classifier loss: 0.013675; batch adversarial loss: 0.470618\n",
      "epoch 192; iter: 0; batch classifier loss: 0.028118; batch adversarial loss: 0.435966\n",
      "epoch 193; iter: 0; batch classifier loss: 0.002724; batch adversarial loss: 0.477006\n",
      "epoch 194; iter: 0; batch classifier loss: 0.001283; batch adversarial loss: 0.388005\n",
      "epoch 195; iter: 0; batch classifier loss: 0.015523; batch adversarial loss: 0.534416\n",
      "epoch 196; iter: 0; batch classifier loss: 0.025028; batch adversarial loss: 0.375557\n",
      "epoch 197; iter: 0; batch classifier loss: 0.020406; batch adversarial loss: 0.425316\n",
      "epoch 198; iter: 0; batch classifier loss: 0.003503; batch adversarial loss: 0.424090\n",
      "epoch 199; iter: 0; batch classifier loss: 0.013261; batch adversarial loss: 0.421812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:50:37.757817: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44946-ae24-11ee-be98-ef9b34f2853b/04a44946-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:4819 op device:{requested: '', assigned: ''} def:{{{node 04a44946-ae24-11ee-be98-ef9b34f2853b/04a44946-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44946-ae24-11ee-be98-ef9b34f2853b/04a44946-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44946-ae24-11ee-be98-ef9b34f2853b/04a44946-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.709666; batch adversarial loss: 0.652322\n",
      "epoch 1; iter: 0; batch classifier loss: 0.487913; batch adversarial loss: 0.615428\n",
      "epoch 2; iter: 0; batch classifier loss: 0.422762; batch adversarial loss: 0.597809\n",
      "epoch 3; iter: 0; batch classifier loss: 0.330662; batch adversarial loss: 0.559265\n",
      "epoch 4; iter: 0; batch classifier loss: 0.358536; batch adversarial loss: 0.557136\n",
      "epoch 5; iter: 0; batch classifier loss: 0.270453; batch adversarial loss: 0.543832\n",
      "epoch 6; iter: 0; batch classifier loss: 0.302951; batch adversarial loss: 0.535884\n",
      "epoch 7; iter: 0; batch classifier loss: 0.275212; batch adversarial loss: 0.536102\n",
      "epoch 8; iter: 0; batch classifier loss: 0.245850; batch adversarial loss: 0.529815\n",
      "epoch 9; iter: 0; batch classifier loss: 0.278092; batch adversarial loss: 0.485863\n",
      "epoch 10; iter: 0; batch classifier loss: 0.249208; batch adversarial loss: 0.478156\n",
      "epoch 11; iter: 0; batch classifier loss: 0.244090; batch adversarial loss: 0.492250\n",
      "epoch 12; iter: 0; batch classifier loss: 0.262370; batch adversarial loss: 0.515203\n",
      "epoch 13; iter: 0; batch classifier loss: 0.189176; batch adversarial loss: 0.515853\n",
      "epoch 14; iter: 0; batch classifier loss: 0.166411; batch adversarial loss: 0.513879\n",
      "epoch 15; iter: 0; batch classifier loss: 0.197116; batch adversarial loss: 0.489587\n",
      "epoch 16; iter: 0; batch classifier loss: 0.145804; batch adversarial loss: 0.510840\n",
      "epoch 17; iter: 0; batch classifier loss: 0.152704; batch adversarial loss: 0.496275\n",
      "epoch 18; iter: 0; batch classifier loss: 0.185895; batch adversarial loss: 0.459035\n",
      "epoch 19; iter: 0; batch classifier loss: 0.174332; batch adversarial loss: 0.596207\n",
      "epoch 20; iter: 0; batch classifier loss: 0.216591; batch adversarial loss: 0.559024\n",
      "epoch 21; iter: 0; batch classifier loss: 0.178666; batch adversarial loss: 0.490923\n",
      "epoch 22; iter: 0; batch classifier loss: 0.156121; batch adversarial loss: 0.502865\n",
      "epoch 23; iter: 0; batch classifier loss: 0.247914; batch adversarial loss: 0.458660\n",
      "epoch 24; iter: 0; batch classifier loss: 0.179379; batch adversarial loss: 0.421031\n",
      "epoch 25; iter: 0; batch classifier loss: 0.166019; batch adversarial loss: 0.526506\n",
      "epoch 26; iter: 0; batch classifier loss: 0.223056; batch adversarial loss: 0.562778\n",
      "epoch 27; iter: 0; batch classifier loss: 0.264533; batch adversarial loss: 0.525459\n",
      "epoch 28; iter: 0; batch classifier loss: 0.199363; batch adversarial loss: 0.473006\n",
      "epoch 29; iter: 0; batch classifier loss: 0.307068; batch adversarial loss: 0.470549\n",
      "epoch 30; iter: 0; batch classifier loss: 0.227435; batch adversarial loss: 0.418496\n",
      "epoch 31; iter: 0; batch classifier loss: 0.220575; batch adversarial loss: 0.521048\n",
      "epoch 32; iter: 0; batch classifier loss: 0.190916; batch adversarial loss: 0.505120\n",
      "epoch 33; iter: 0; batch classifier loss: 0.142762; batch adversarial loss: 0.507536\n",
      "epoch 34; iter: 0; batch classifier loss: 0.128263; batch adversarial loss: 0.456495\n",
      "epoch 35; iter: 0; batch classifier loss: 0.103735; batch adversarial loss: 0.422918\n",
      "epoch 36; iter: 0; batch classifier loss: 0.130241; batch adversarial loss: 0.497547\n",
      "epoch 37; iter: 0; batch classifier loss: 0.155589; batch adversarial loss: 0.385772\n",
      "epoch 38; iter: 0; batch classifier loss: 0.149023; batch adversarial loss: 0.484459\n",
      "epoch 39; iter: 0; batch classifier loss: 0.076214; batch adversarial loss: 0.380640\n",
      "epoch 40; iter: 0; batch classifier loss: 0.142496; batch adversarial loss: 0.472683\n",
      "epoch 41; iter: 0; batch classifier loss: 0.122125; batch adversarial loss: 0.466511\n",
      "epoch 42; iter: 0; batch classifier loss: 0.061334; batch adversarial loss: 0.483487\n",
      "epoch 43; iter: 0; batch classifier loss: 0.065287; batch adversarial loss: 0.483821\n",
      "epoch 44; iter: 0; batch classifier loss: 0.108543; batch adversarial loss: 0.459815\n",
      "epoch 45; iter: 0; batch classifier loss: 0.081619; batch adversarial loss: 0.398461\n",
      "epoch 46; iter: 0; batch classifier loss: 0.096150; batch adversarial loss: 0.394131\n",
      "epoch 47; iter: 0; batch classifier loss: 0.128536; batch adversarial loss: 0.443975\n",
      "epoch 48; iter: 0; batch classifier loss: 0.076975; batch adversarial loss: 0.438589\n",
      "epoch 49; iter: 0; batch classifier loss: 0.075244; batch adversarial loss: 0.450547\n",
      "epoch 50; iter: 0; batch classifier loss: 0.150313; batch adversarial loss: 0.430297\n",
      "epoch 51; iter: 0; batch classifier loss: 0.060679; batch adversarial loss: 0.510152\n",
      "epoch 52; iter: 0; batch classifier loss: 0.082917; batch adversarial loss: 0.431621\n",
      "epoch 53; iter: 0; batch classifier loss: 0.077353; batch adversarial loss: 0.415035\n",
      "epoch 54; iter: 0; batch classifier loss: 0.071700; batch adversarial loss: 0.401364\n",
      "epoch 55; iter: 0; batch classifier loss: 0.094778; batch adversarial loss: 0.419842\n",
      "epoch 56; iter: 0; batch classifier loss: 0.079538; batch adversarial loss: 0.411874\n",
      "epoch 57; iter: 0; batch classifier loss: 0.082645; batch adversarial loss: 0.392096\n",
      "epoch 58; iter: 0; batch classifier loss: 0.111331; batch adversarial loss: 0.598300\n",
      "epoch 59; iter: 0; batch classifier loss: 0.111180; batch adversarial loss: 0.487187\n",
      "epoch 60; iter: 0; batch classifier loss: 0.080592; batch adversarial loss: 0.440918\n",
      "epoch 61; iter: 0; batch classifier loss: 0.100305; batch adversarial loss: 0.518265\n",
      "epoch 62; iter: 0; batch classifier loss: 0.092781; batch adversarial loss: 0.324230\n",
      "epoch 63; iter: 0; batch classifier loss: 0.059969; batch adversarial loss: 0.580220\n",
      "epoch 64; iter: 0; batch classifier loss: 0.059917; batch adversarial loss: 0.413839\n",
      "epoch 65; iter: 0; batch classifier loss: 0.073846; batch adversarial loss: 0.436463\n",
      "epoch 66; iter: 0; batch classifier loss: 0.103777; batch adversarial loss: 0.329412\n",
      "epoch 67; iter: 0; batch classifier loss: 0.081255; batch adversarial loss: 0.465377\n",
      "epoch 68; iter: 0; batch classifier loss: 0.081365; batch adversarial loss: 0.504300\n",
      "epoch 69; iter: 0; batch classifier loss: 0.082400; batch adversarial loss: 0.431333\n",
      "epoch 70; iter: 0; batch classifier loss: 0.088481; batch adversarial loss: 0.464038\n",
      "epoch 71; iter: 0; batch classifier loss: 0.051159; batch adversarial loss: 0.516132\n",
      "epoch 72; iter: 0; batch classifier loss: 0.076971; batch adversarial loss: 0.464877\n",
      "epoch 73; iter: 0; batch classifier loss: 0.100155; batch adversarial loss: 0.371750\n",
      "epoch 74; iter: 0; batch classifier loss: 0.113045; batch adversarial loss: 0.443041\n",
      "epoch 75; iter: 0; batch classifier loss: 0.066771; batch adversarial loss: 0.558190\n",
      "epoch 76; iter: 0; batch classifier loss: 0.071340; batch adversarial loss: 0.529928\n",
      "epoch 77; iter: 0; batch classifier loss: 0.119604; batch adversarial loss: 0.411974\n",
      "epoch 78; iter: 0; batch classifier loss: 0.052242; batch adversarial loss: 0.407726\n",
      "epoch 79; iter: 0; batch classifier loss: 0.107007; batch adversarial loss: 0.414144\n",
      "epoch 80; iter: 0; batch classifier loss: 0.059612; batch adversarial loss: 0.505051\n",
      "epoch 81; iter: 0; batch classifier loss: 0.100828; batch adversarial loss: 0.428108\n",
      "epoch 82; iter: 0; batch classifier loss: 0.059424; batch adversarial loss: 0.448603\n",
      "epoch 83; iter: 0; batch classifier loss: 0.078222; batch adversarial loss: 0.431681\n",
      "epoch 84; iter: 0; batch classifier loss: 0.082916; batch adversarial loss: 0.498300\n",
      "epoch 85; iter: 0; batch classifier loss: 0.083985; batch adversarial loss: 0.511922\n",
      "epoch 86; iter: 0; batch classifier loss: 0.055667; batch adversarial loss: 0.430737\n",
      "epoch 87; iter: 0; batch classifier loss: 0.084427; batch adversarial loss: 0.463341\n",
      "epoch 88; iter: 0; batch classifier loss: 0.073853; batch adversarial loss: 0.476415\n",
      "epoch 89; iter: 0; batch classifier loss: 0.073631; batch adversarial loss: 0.448659\n",
      "epoch 90; iter: 0; batch classifier loss: 0.080064; batch adversarial loss: 0.366200\n",
      "epoch 91; iter: 0; batch classifier loss: 0.079616; batch adversarial loss: 0.489144\n",
      "epoch 92; iter: 0; batch classifier loss: 0.082434; batch adversarial loss: 0.467308\n",
      "epoch 93; iter: 0; batch classifier loss: 0.075743; batch adversarial loss: 0.517550\n",
      "epoch 94; iter: 0; batch classifier loss: 0.076254; batch adversarial loss: 0.411008\n",
      "epoch 95; iter: 0; batch classifier loss: 0.060096; batch adversarial loss: 0.367234\n",
      "epoch 96; iter: 0; batch classifier loss: 0.133625; batch adversarial loss: 0.442900\n",
      "epoch 97; iter: 0; batch classifier loss: 0.075073; batch adversarial loss: 0.452766\n",
      "epoch 98; iter: 0; batch classifier loss: 0.043442; batch adversarial loss: 0.369900\n",
      "epoch 99; iter: 0; batch classifier loss: 0.075037; batch adversarial loss: 0.585151\n",
      "epoch 100; iter: 0; batch classifier loss: 0.043710; batch adversarial loss: 0.461248\n",
      "epoch 101; iter: 0; batch classifier loss: 0.067134; batch adversarial loss: 0.485591\n",
      "epoch 102; iter: 0; batch classifier loss: 0.027281; batch adversarial loss: 0.524919\n",
      "epoch 103; iter: 0; batch classifier loss: 0.048011; batch adversarial loss: 0.542083\n",
      "epoch 104; iter: 0; batch classifier loss: 0.044805; batch adversarial loss: 0.423011\n",
      "epoch 105; iter: 0; batch classifier loss: 0.055350; batch adversarial loss: 0.497771\n",
      "epoch 106; iter: 0; batch classifier loss: 0.032033; batch adversarial loss: 0.529720\n",
      "epoch 107; iter: 0; batch classifier loss: 0.035209; batch adversarial loss: 0.502558\n",
      "epoch 108; iter: 0; batch classifier loss: 0.067646; batch adversarial loss: 0.468725\n",
      "epoch 109; iter: 0; batch classifier loss: 0.029181; batch adversarial loss: 0.473795\n",
      "epoch 110; iter: 0; batch classifier loss: 0.034821; batch adversarial loss: 0.438891\n",
      "epoch 111; iter: 0; batch classifier loss: 0.044647; batch adversarial loss: 0.451927\n",
      "epoch 112; iter: 0; batch classifier loss: 0.033905; batch adversarial loss: 0.475064\n",
      "epoch 113; iter: 0; batch classifier loss: 0.067229; batch adversarial loss: 0.414826\n",
      "epoch 114; iter: 0; batch classifier loss: 0.095853; batch adversarial loss: 0.456774\n",
      "epoch 115; iter: 0; batch classifier loss: 0.053253; batch adversarial loss: 0.474489\n",
      "epoch 116; iter: 0; batch classifier loss: 0.046204; batch adversarial loss: 0.421206\n",
      "epoch 117; iter: 0; batch classifier loss: 0.032877; batch adversarial loss: 0.526524\n",
      "epoch 118; iter: 0; batch classifier loss: 0.053690; batch adversarial loss: 0.410245\n",
      "epoch 119; iter: 0; batch classifier loss: 0.042836; batch adversarial loss: 0.477704\n",
      "epoch 120; iter: 0; batch classifier loss: 0.065318; batch adversarial loss: 0.450585\n",
      "epoch 121; iter: 0; batch classifier loss: 0.043782; batch adversarial loss: 0.502895\n",
      "epoch 122; iter: 0; batch classifier loss: 0.044551; batch adversarial loss: 0.406207\n",
      "epoch 123; iter: 0; batch classifier loss: 0.058272; batch adversarial loss: 0.432001\n",
      "epoch 124; iter: 0; batch classifier loss: 0.044786; batch adversarial loss: 0.509728\n",
      "epoch 125; iter: 0; batch classifier loss: 0.036823; batch adversarial loss: 0.452620\n",
      "epoch 126; iter: 0; batch classifier loss: 0.049650; batch adversarial loss: 0.404466\n",
      "epoch 127; iter: 0; batch classifier loss: 0.048545; batch adversarial loss: 0.457919\n",
      "epoch 128; iter: 0; batch classifier loss: 0.018986; batch adversarial loss: 0.481931\n",
      "epoch 129; iter: 0; batch classifier loss: 0.063080; batch adversarial loss: 0.383992\n",
      "epoch 130; iter: 0; batch classifier loss: 0.046148; batch adversarial loss: 0.452107\n",
      "epoch 131; iter: 0; batch classifier loss: 0.057933; batch adversarial loss: 0.404293\n",
      "epoch 132; iter: 0; batch classifier loss: 0.022197; batch adversarial loss: 0.435154\n",
      "epoch 133; iter: 0; batch classifier loss: 0.023818; batch adversarial loss: 0.395802\n",
      "epoch 134; iter: 0; batch classifier loss: 0.060273; batch adversarial loss: 0.410372\n",
      "epoch 135; iter: 0; batch classifier loss: 0.044178; batch adversarial loss: 0.493589\n",
      "epoch 136; iter: 0; batch classifier loss: 0.047651; batch adversarial loss: 0.455350\n",
      "epoch 137; iter: 0; batch classifier loss: 0.047731; batch adversarial loss: 0.450876\n",
      "epoch 138; iter: 0; batch classifier loss: 0.041045; batch adversarial loss: 0.480120\n",
      "epoch 139; iter: 0; batch classifier loss: 0.040701; batch adversarial loss: 0.414082\n",
      "epoch 140; iter: 0; batch classifier loss: 0.027044; batch adversarial loss: 0.487928\n",
      "epoch 141; iter: 0; batch classifier loss: 0.036063; batch adversarial loss: 0.399531\n",
      "epoch 142; iter: 0; batch classifier loss: 0.075357; batch adversarial loss: 0.515091\n",
      "epoch 143; iter: 0; batch classifier loss: 0.022693; batch adversarial loss: 0.517257\n",
      "epoch 144; iter: 0; batch classifier loss: 0.021658; batch adversarial loss: 0.483802\n",
      "epoch 145; iter: 0; batch classifier loss: 0.062479; batch adversarial loss: 0.419734\n",
      "epoch 146; iter: 0; batch classifier loss: 0.023781; batch adversarial loss: 0.398608\n",
      "epoch 147; iter: 0; batch classifier loss: 0.032033; batch adversarial loss: 0.461032\n",
      "epoch 148; iter: 0; batch classifier loss: 0.054956; batch adversarial loss: 0.453223\n",
      "epoch 149; iter: 0; batch classifier loss: 0.048305; batch adversarial loss: 0.324425\n",
      "epoch 150; iter: 0; batch classifier loss: 0.029144; batch adversarial loss: 0.390146\n",
      "epoch 151; iter: 0; batch classifier loss: 0.039051; batch adversarial loss: 0.543491\n",
      "epoch 152; iter: 0; batch classifier loss: 0.020362; batch adversarial loss: 0.448059\n",
      "epoch 153; iter: 0; batch classifier loss: 0.019364; batch adversarial loss: 0.378482\n",
      "epoch 154; iter: 0; batch classifier loss: 0.025726; batch adversarial loss: 0.374053\n",
      "epoch 155; iter: 0; batch classifier loss: 0.027450; batch adversarial loss: 0.445329\n",
      "epoch 156; iter: 0; batch classifier loss: 0.030529; batch adversarial loss: 0.461224\n",
      "epoch 157; iter: 0; batch classifier loss: 0.067695; batch adversarial loss: 0.547453\n",
      "epoch 158; iter: 0; batch classifier loss: 0.030077; batch adversarial loss: 0.419594\n",
      "epoch 159; iter: 0; batch classifier loss: 0.013392; batch adversarial loss: 0.426602\n",
      "epoch 160; iter: 0; batch classifier loss: 0.035495; batch adversarial loss: 0.383893\n",
      "epoch 161; iter: 0; batch classifier loss: 0.025031; batch adversarial loss: 0.578131\n",
      "epoch 162; iter: 0; batch classifier loss: 0.016473; batch adversarial loss: 0.452590\n",
      "epoch 163; iter: 0; batch classifier loss: 0.046373; batch adversarial loss: 0.425384\n",
      "epoch 164; iter: 0; batch classifier loss: 0.067775; batch adversarial loss: 0.456469\n",
      "epoch 165; iter: 0; batch classifier loss: 0.031457; batch adversarial loss: 0.476396\n",
      "epoch 166; iter: 0; batch classifier loss: 0.021929; batch adversarial loss: 0.423237\n",
      "epoch 167; iter: 0; batch classifier loss: 0.029902; batch adversarial loss: 0.419260\n",
      "epoch 168; iter: 0; batch classifier loss: 0.034077; batch adversarial loss: 0.459350\n",
      "epoch 169; iter: 0; batch classifier loss: 0.030482; batch adversarial loss: 0.429474\n",
      "epoch 170; iter: 0; batch classifier loss: 0.030808; batch adversarial loss: 0.417940\n",
      "epoch 171; iter: 0; batch classifier loss: 0.042441; batch adversarial loss: 0.479339\n",
      "epoch 172; iter: 0; batch classifier loss: 0.025860; batch adversarial loss: 0.464672\n",
      "epoch 173; iter: 0; batch classifier loss: 0.033331; batch adversarial loss: 0.367688\n",
      "epoch 174; iter: 0; batch classifier loss: 0.033898; batch adversarial loss: 0.477718\n",
      "epoch 175; iter: 0; batch classifier loss: 0.033492; batch adversarial loss: 0.373036\n",
      "epoch 176; iter: 0; batch classifier loss: 0.010804; batch adversarial loss: 0.511589\n",
      "epoch 177; iter: 0; batch classifier loss: 0.010168; batch adversarial loss: 0.467057\n",
      "epoch 178; iter: 0; batch classifier loss: 0.021220; batch adversarial loss: 0.465702\n",
      "epoch 179; iter: 0; batch classifier loss: 0.022006; batch adversarial loss: 0.448633\n",
      "epoch 180; iter: 0; batch classifier loss: 0.016471; batch adversarial loss: 0.499660\n",
      "epoch 181; iter: 0; batch classifier loss: 0.012072; batch adversarial loss: 0.509139\n",
      "epoch 182; iter: 0; batch classifier loss: 0.029690; batch adversarial loss: 0.464931\n",
      "epoch 183; iter: 0; batch classifier loss: 0.009262; batch adversarial loss: 0.415245\n",
      "epoch 184; iter: 0; batch classifier loss: 0.029474; batch adversarial loss: 0.427130\n",
      "epoch 185; iter: 0; batch classifier loss: 0.062629; batch adversarial loss: 0.424139\n",
      "epoch 186; iter: 0; batch classifier loss: 0.022513; batch adversarial loss: 0.575859\n",
      "epoch 187; iter: 0; batch classifier loss: 0.033415; batch adversarial loss: 0.408849\n",
      "epoch 188; iter: 0; batch classifier loss: 0.017216; batch adversarial loss: 0.478171\n",
      "epoch 189; iter: 0; batch classifier loss: 0.029508; batch adversarial loss: 0.431119\n",
      "epoch 190; iter: 0; batch classifier loss: 0.009648; batch adversarial loss: 0.395829\n",
      "epoch 191; iter: 0; batch classifier loss: 0.012630; batch adversarial loss: 0.450079\n",
      "epoch 192; iter: 0; batch classifier loss: 0.031220; batch adversarial loss: 0.415766\n",
      "epoch 193; iter: 0; batch classifier loss: 0.024578; batch adversarial loss: 0.447676\n",
      "epoch 194; iter: 0; batch classifier loss: 0.021676; batch adversarial loss: 0.420153\n",
      "epoch 195; iter: 0; batch classifier loss: 0.023586; batch adversarial loss: 0.475290\n",
      "epoch 196; iter: 0; batch classifier loss: 0.030486; batch adversarial loss: 0.452936\n",
      "epoch 197; iter: 0; batch classifier loss: 0.022328; batch adversarial loss: 0.428238\n",
      "epoch 198; iter: 0; batch classifier loss: 0.014671; batch adversarial loss: 0.512134\n",
      "epoch 199; iter: 0; batch classifier loss: 0.010841; batch adversarial loss: 0.451392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:51:15.186887: W tensorflow/c/c_api.cc:304] Operation '{name:'04a449be-ae24-11ee-be98-ef9b34f2853b/04a449be-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:5626 op device:{requested: '', assigned: ''} def:{{{node 04a449be-ae24-11ee-be98-ef9b34f2853b/04a449be-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a449be-ae24-11ee-be98-ef9b34f2853b/04a449be-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a449be-ae24-11ee-be98-ef9b34f2853b/04a449be-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.666891; batch adversarial loss: 0.824722\n",
      "epoch 1; iter: 0; batch classifier loss: 0.386234; batch adversarial loss: 0.850921\n",
      "epoch 2; iter: 0; batch classifier loss: 0.375162; batch adversarial loss: 0.776295\n",
      "epoch 3; iter: 0; batch classifier loss: 0.407265; batch adversarial loss: 0.729343\n",
      "epoch 4; iter: 0; batch classifier loss: 0.349325; batch adversarial loss: 0.690988\n",
      "epoch 5; iter: 0; batch classifier loss: 0.258330; batch adversarial loss: 0.635405\n",
      "epoch 6; iter: 0; batch classifier loss: 0.297013; batch adversarial loss: 0.596415\n",
      "epoch 7; iter: 0; batch classifier loss: 0.269489; batch adversarial loss: 0.567606\n",
      "epoch 8; iter: 0; batch classifier loss: 0.291968; batch adversarial loss: 0.562902\n",
      "epoch 9; iter: 0; batch classifier loss: 0.282816; batch adversarial loss: 0.555783\n",
      "epoch 10; iter: 0; batch classifier loss: 0.303971; batch adversarial loss: 0.540156\n",
      "epoch 11; iter: 0; batch classifier loss: 0.268710; batch adversarial loss: 0.547746\n",
      "epoch 12; iter: 0; batch classifier loss: 0.284602; batch adversarial loss: 0.519827\n",
      "epoch 13; iter: 0; batch classifier loss: 0.214303; batch adversarial loss: 0.492032\n",
      "epoch 14; iter: 0; batch classifier loss: 0.310333; batch adversarial loss: 0.511223\n",
      "epoch 15; iter: 0; batch classifier loss: 0.229911; batch adversarial loss: 0.538091\n",
      "epoch 16; iter: 0; batch classifier loss: 0.202435; batch adversarial loss: 0.444441\n",
      "epoch 17; iter: 0; batch classifier loss: 0.170136; batch adversarial loss: 0.447747\n",
      "epoch 18; iter: 0; batch classifier loss: 0.190617; batch adversarial loss: 0.452248\n",
      "epoch 19; iter: 0; batch classifier loss: 0.130626; batch adversarial loss: 0.489319\n",
      "epoch 20; iter: 0; batch classifier loss: 0.144575; batch adversarial loss: 0.454658\n",
      "epoch 21; iter: 0; batch classifier loss: 0.122073; batch adversarial loss: 0.465908\n",
      "epoch 22; iter: 0; batch classifier loss: 0.218120; batch adversarial loss: 0.417976\n",
      "epoch 23; iter: 0; batch classifier loss: 0.132650; batch adversarial loss: 0.519364\n",
      "epoch 24; iter: 0; batch classifier loss: 0.105769; batch adversarial loss: 0.429789\n",
      "epoch 25; iter: 0; batch classifier loss: 0.124961; batch adversarial loss: 0.406514\n",
      "epoch 26; iter: 0; batch classifier loss: 0.171830; batch adversarial loss: 0.413066\n",
      "epoch 27; iter: 0; batch classifier loss: 0.124669; batch adversarial loss: 0.386178\n",
      "epoch 28; iter: 0; batch classifier loss: 0.125337; batch adversarial loss: 0.456089\n",
      "epoch 29; iter: 0; batch classifier loss: 0.136895; batch adversarial loss: 0.474219\n",
      "epoch 30; iter: 0; batch classifier loss: 0.117035; batch adversarial loss: 0.396851\n",
      "epoch 31; iter: 0; batch classifier loss: 0.113249; batch adversarial loss: 0.430076\n",
      "epoch 32; iter: 0; batch classifier loss: 0.118472; batch adversarial loss: 0.530815\n",
      "epoch 33; iter: 0; batch classifier loss: 0.107088; batch adversarial loss: 0.440690\n",
      "epoch 34; iter: 0; batch classifier loss: 0.143324; batch adversarial loss: 0.507196\n",
      "epoch 35; iter: 0; batch classifier loss: 0.090436; batch adversarial loss: 0.422277\n",
      "epoch 36; iter: 0; batch classifier loss: 0.096733; batch adversarial loss: 0.475168\n",
      "epoch 37; iter: 0; batch classifier loss: 0.065276; batch adversarial loss: 0.369443\n",
      "epoch 38; iter: 0; batch classifier loss: 0.137461; batch adversarial loss: 0.477291\n",
      "epoch 39; iter: 0; batch classifier loss: 0.161153; batch adversarial loss: 0.431039\n",
      "epoch 40; iter: 0; batch classifier loss: 0.116486; batch adversarial loss: 0.382705\n",
      "epoch 41; iter: 0; batch classifier loss: 0.106226; batch adversarial loss: 0.363236\n",
      "epoch 42; iter: 0; batch classifier loss: 0.138063; batch adversarial loss: 0.444351\n",
      "epoch 43; iter: 0; batch classifier loss: 0.119357; batch adversarial loss: 0.506365\n",
      "epoch 44; iter: 0; batch classifier loss: 0.130895; batch adversarial loss: 0.528069\n",
      "epoch 45; iter: 0; batch classifier loss: 0.115615; batch adversarial loss: 0.404598\n",
      "epoch 46; iter: 0; batch classifier loss: 0.084167; batch adversarial loss: 0.368041\n",
      "epoch 47; iter: 0; batch classifier loss: 0.067650; batch adversarial loss: 0.409592\n",
      "epoch 48; iter: 0; batch classifier loss: 0.181055; batch adversarial loss: 0.377417\n",
      "epoch 49; iter: 0; batch classifier loss: 0.108534; batch adversarial loss: 0.429239\n",
      "epoch 50; iter: 0; batch classifier loss: 0.091869; batch adversarial loss: 0.384262\n",
      "epoch 51; iter: 0; batch classifier loss: 0.134004; batch adversarial loss: 0.431818\n",
      "epoch 52; iter: 0; batch classifier loss: 0.095615; batch adversarial loss: 0.422327\n",
      "epoch 53; iter: 0; batch classifier loss: 0.090123; batch adversarial loss: 0.547558\n",
      "epoch 54; iter: 0; batch classifier loss: 0.062852; batch adversarial loss: 0.430355\n",
      "epoch 55; iter: 0; batch classifier loss: 0.084603; batch adversarial loss: 0.400443\n",
      "epoch 56; iter: 0; batch classifier loss: 0.081680; batch adversarial loss: 0.428898\n",
      "epoch 57; iter: 0; batch classifier loss: 0.101696; batch adversarial loss: 0.451471\n",
      "epoch 58; iter: 0; batch classifier loss: 0.071853; batch adversarial loss: 0.534766\n",
      "epoch 59; iter: 0; batch classifier loss: 0.104650; batch adversarial loss: 0.406281\n",
      "epoch 60; iter: 0; batch classifier loss: 0.115947; batch adversarial loss: 0.466404\n",
      "epoch 61; iter: 0; batch classifier loss: 0.057668; batch adversarial loss: 0.426994\n",
      "epoch 62; iter: 0; batch classifier loss: 0.125717; batch adversarial loss: 0.439727\n",
      "epoch 63; iter: 0; batch classifier loss: 0.055717; batch adversarial loss: 0.402067\n",
      "epoch 64; iter: 0; batch classifier loss: 0.085146; batch adversarial loss: 0.464247\n",
      "epoch 65; iter: 0; batch classifier loss: 0.094990; batch adversarial loss: 0.412821\n",
      "epoch 66; iter: 0; batch classifier loss: 0.072008; batch adversarial loss: 0.492604\n",
      "epoch 67; iter: 0; batch classifier loss: 0.076729; batch adversarial loss: 0.471250\n",
      "epoch 68; iter: 0; batch classifier loss: 0.060644; batch adversarial loss: 0.516490\n",
      "epoch 69; iter: 0; batch classifier loss: 0.061220; batch adversarial loss: 0.393940\n",
      "epoch 70; iter: 0; batch classifier loss: 0.042333; batch adversarial loss: 0.441440\n",
      "epoch 71; iter: 0; batch classifier loss: 0.059735; batch adversarial loss: 0.411473\n",
      "epoch 72; iter: 0; batch classifier loss: 0.076553; batch adversarial loss: 0.469380\n",
      "epoch 73; iter: 0; batch classifier loss: 0.089220; batch adversarial loss: 0.407150\n",
      "epoch 74; iter: 0; batch classifier loss: 0.054039; batch adversarial loss: 0.453082\n",
      "epoch 75; iter: 0; batch classifier loss: 0.081508; batch adversarial loss: 0.463829\n",
      "epoch 76; iter: 0; batch classifier loss: 0.064556; batch adversarial loss: 0.389388\n",
      "epoch 77; iter: 0; batch classifier loss: 0.082258; batch adversarial loss: 0.433423\n",
      "epoch 78; iter: 0; batch classifier loss: 0.043318; batch adversarial loss: 0.456049\n",
      "epoch 79; iter: 0; batch classifier loss: 0.092656; batch adversarial loss: 0.459492\n",
      "epoch 80; iter: 0; batch classifier loss: 0.052174; batch adversarial loss: 0.467447\n",
      "epoch 81; iter: 0; batch classifier loss: 0.050595; batch adversarial loss: 0.390100\n",
      "epoch 82; iter: 0; batch classifier loss: 0.068597; batch adversarial loss: 0.488034\n",
      "epoch 83; iter: 0; batch classifier loss: 0.053984; batch adversarial loss: 0.299338\n",
      "epoch 84; iter: 0; batch classifier loss: 0.075297; batch adversarial loss: 0.423314\n",
      "epoch 85; iter: 0; batch classifier loss: 0.069841; batch adversarial loss: 0.437803\n",
      "epoch 86; iter: 0; batch classifier loss: 0.040973; batch adversarial loss: 0.337665\n",
      "epoch 87; iter: 0; batch classifier loss: 0.047334; batch adversarial loss: 0.498559\n",
      "epoch 88; iter: 0; batch classifier loss: 0.086863; batch adversarial loss: 0.379993\n",
      "epoch 89; iter: 0; batch classifier loss: 0.059601; batch adversarial loss: 0.423966\n",
      "epoch 90; iter: 0; batch classifier loss: 0.065753; batch adversarial loss: 0.436350\n",
      "epoch 91; iter: 0; batch classifier loss: 0.050423; batch adversarial loss: 0.413281\n",
      "epoch 92; iter: 0; batch classifier loss: 0.044605; batch adversarial loss: 0.452491\n",
      "epoch 93; iter: 0; batch classifier loss: 0.085659; batch adversarial loss: 0.455456\n",
      "epoch 94; iter: 0; batch classifier loss: 0.036699; batch adversarial loss: 0.418779\n",
      "epoch 95; iter: 0; batch classifier loss: 0.038515; batch adversarial loss: 0.306683\n",
      "epoch 96; iter: 0; batch classifier loss: 0.065382; batch adversarial loss: 0.463713\n",
      "epoch 97; iter: 0; batch classifier loss: 0.052676; batch adversarial loss: 0.427417\n",
      "epoch 98; iter: 0; batch classifier loss: 0.079171; batch adversarial loss: 0.442390\n",
      "epoch 99; iter: 0; batch classifier loss: 0.078597; batch adversarial loss: 0.471997\n",
      "epoch 100; iter: 0; batch classifier loss: 0.048852; batch adversarial loss: 0.377455\n",
      "epoch 101; iter: 0; batch classifier loss: 0.058528; batch adversarial loss: 0.428293\n",
      "epoch 102; iter: 0; batch classifier loss: 0.061109; batch adversarial loss: 0.453307\n",
      "epoch 103; iter: 0; batch classifier loss: 0.102335; batch adversarial loss: 0.428473\n",
      "epoch 104; iter: 0; batch classifier loss: 0.088654; batch adversarial loss: 0.404846\n",
      "epoch 105; iter: 0; batch classifier loss: 0.082711; batch adversarial loss: 0.465253\n",
      "epoch 106; iter: 0; batch classifier loss: 0.089386; batch adversarial loss: 0.458091\n",
      "epoch 107; iter: 0; batch classifier loss: 0.069182; batch adversarial loss: 0.414462\n",
      "epoch 108; iter: 0; batch classifier loss: 0.062280; batch adversarial loss: 0.435678\n",
      "epoch 109; iter: 0; batch classifier loss: 0.045033; batch adversarial loss: 0.325062\n",
      "epoch 110; iter: 0; batch classifier loss: 0.082001; batch adversarial loss: 0.451710\n",
      "epoch 111; iter: 0; batch classifier loss: 0.073990; batch adversarial loss: 0.423503\n",
      "epoch 112; iter: 0; batch classifier loss: 0.077987; batch adversarial loss: 0.402593\n",
      "epoch 113; iter: 0; batch classifier loss: 0.058943; batch adversarial loss: 0.405948\n",
      "epoch 114; iter: 0; batch classifier loss: 0.060615; batch adversarial loss: 0.351557\n",
      "epoch 115; iter: 0; batch classifier loss: 0.052122; batch adversarial loss: 0.419002\n",
      "epoch 116; iter: 0; batch classifier loss: 0.092606; batch adversarial loss: 0.431623\n",
      "epoch 117; iter: 0; batch classifier loss: 0.057966; batch adversarial loss: 0.428139\n",
      "epoch 118; iter: 0; batch classifier loss: 0.043020; batch adversarial loss: 0.349364\n",
      "epoch 119; iter: 0; batch classifier loss: 0.039114; batch adversarial loss: 0.445747\n",
      "epoch 120; iter: 0; batch classifier loss: 0.082346; batch adversarial loss: 0.365186\n",
      "epoch 121; iter: 0; batch classifier loss: 0.088178; batch adversarial loss: 0.413485\n",
      "epoch 122; iter: 0; batch classifier loss: 0.039628; batch adversarial loss: 0.398731\n",
      "epoch 123; iter: 0; batch classifier loss: 0.073529; batch adversarial loss: 0.503822\n",
      "epoch 124; iter: 0; batch classifier loss: 0.077902; batch adversarial loss: 0.465769\n",
      "epoch 125; iter: 0; batch classifier loss: 0.095959; batch adversarial loss: 0.430146\n",
      "epoch 126; iter: 0; batch classifier loss: 0.051752; batch adversarial loss: 0.418810\n",
      "epoch 127; iter: 0; batch classifier loss: 0.056154; batch adversarial loss: 0.506664\n",
      "epoch 128; iter: 0; batch classifier loss: 0.080556; batch adversarial loss: 0.460800\n",
      "epoch 129; iter: 0; batch classifier loss: 0.067734; batch adversarial loss: 0.401233\n",
      "epoch 130; iter: 0; batch classifier loss: 0.046544; batch adversarial loss: 0.505492\n",
      "epoch 131; iter: 0; batch classifier loss: 0.025639; batch adversarial loss: 0.454146\n",
      "epoch 132; iter: 0; batch classifier loss: 0.067384; batch adversarial loss: 0.444977\n",
      "epoch 133; iter: 0; batch classifier loss: 0.053020; batch adversarial loss: 0.443552\n",
      "epoch 134; iter: 0; batch classifier loss: 0.062288; batch adversarial loss: 0.389100\n",
      "epoch 135; iter: 0; batch classifier loss: 0.038183; batch adversarial loss: 0.409498\n",
      "epoch 136; iter: 0; batch classifier loss: 0.059105; batch adversarial loss: 0.450558\n",
      "epoch 137; iter: 0; batch classifier loss: 0.053278; batch adversarial loss: 0.498941\n",
      "epoch 138; iter: 0; batch classifier loss: 0.041008; batch adversarial loss: 0.442007\n",
      "epoch 139; iter: 0; batch classifier loss: 0.041973; batch adversarial loss: 0.383769\n",
      "epoch 140; iter: 0; batch classifier loss: 0.067171; batch adversarial loss: 0.431730\n",
      "epoch 141; iter: 0; batch classifier loss: 0.053692; batch adversarial loss: 0.400607\n",
      "epoch 142; iter: 0; batch classifier loss: 0.071567; batch adversarial loss: 0.412240\n",
      "epoch 143; iter: 0; batch classifier loss: 0.070646; batch adversarial loss: 0.518043\n",
      "epoch 144; iter: 0; batch classifier loss: 0.053036; batch adversarial loss: 0.370076\n",
      "epoch 145; iter: 0; batch classifier loss: 0.064275; batch adversarial loss: 0.474085\n",
      "epoch 146; iter: 0; batch classifier loss: 0.037896; batch adversarial loss: 0.369753\n",
      "epoch 147; iter: 0; batch classifier loss: 0.040991; batch adversarial loss: 0.455554\n",
      "epoch 148; iter: 0; batch classifier loss: 0.040436; batch adversarial loss: 0.394379\n",
      "epoch 149; iter: 0; batch classifier loss: 0.052383; batch adversarial loss: 0.447044\n",
      "epoch 150; iter: 0; batch classifier loss: 0.055120; batch adversarial loss: 0.469246\n",
      "epoch 151; iter: 0; batch classifier loss: 0.064081; batch adversarial loss: 0.439761\n",
      "epoch 152; iter: 0; batch classifier loss: 0.060346; batch adversarial loss: 0.500418\n",
      "epoch 153; iter: 0; batch classifier loss: 0.065947; batch adversarial loss: 0.385750\n",
      "epoch 154; iter: 0; batch classifier loss: 0.041860; batch adversarial loss: 0.457771\n",
      "epoch 155; iter: 0; batch classifier loss: 0.041780; batch adversarial loss: 0.399695\n",
      "epoch 156; iter: 0; batch classifier loss: 0.047704; batch adversarial loss: 0.457212\n",
      "epoch 157; iter: 0; batch classifier loss: 0.040207; batch adversarial loss: 0.443529\n",
      "epoch 158; iter: 0; batch classifier loss: 0.035801; batch adversarial loss: 0.343547\n",
      "epoch 159; iter: 0; batch classifier loss: 0.032633; batch adversarial loss: 0.394069\n",
      "epoch 160; iter: 0; batch classifier loss: 0.032733; batch adversarial loss: 0.435204\n",
      "epoch 161; iter: 0; batch classifier loss: 0.027901; batch adversarial loss: 0.396956\n",
      "epoch 162; iter: 0; batch classifier loss: 0.046676; batch adversarial loss: 0.418607\n",
      "epoch 163; iter: 0; batch classifier loss: 0.070829; batch adversarial loss: 0.380064\n",
      "epoch 164; iter: 0; batch classifier loss: 0.069448; batch adversarial loss: 0.352634\n",
      "epoch 165; iter: 0; batch classifier loss: 0.057716; batch adversarial loss: 0.407650\n",
      "epoch 166; iter: 0; batch classifier loss: 0.051436; batch adversarial loss: 0.523860\n",
      "epoch 167; iter: 0; batch classifier loss: 0.036489; batch adversarial loss: 0.430763\n",
      "epoch 168; iter: 0; batch classifier loss: 0.063229; batch adversarial loss: 0.494475\n",
      "epoch 169; iter: 0; batch classifier loss: 0.036109; batch adversarial loss: 0.382760\n",
      "epoch 170; iter: 0; batch classifier loss: 0.036372; batch adversarial loss: 0.526715\n",
      "epoch 171; iter: 0; batch classifier loss: 0.046662; batch adversarial loss: 0.412884\n",
      "epoch 172; iter: 0; batch classifier loss: 0.045668; batch adversarial loss: 0.433136\n",
      "epoch 173; iter: 0; batch classifier loss: 0.054473; batch adversarial loss: 0.406092\n",
      "epoch 174; iter: 0; batch classifier loss: 0.046066; batch adversarial loss: 0.302160\n",
      "epoch 175; iter: 0; batch classifier loss: 0.039446; batch adversarial loss: 0.464467\n",
      "epoch 176; iter: 0; batch classifier loss: 0.065575; batch adversarial loss: 0.394690\n",
      "epoch 177; iter: 0; batch classifier loss: 0.030344; batch adversarial loss: 0.378981\n",
      "epoch 178; iter: 0; batch classifier loss: 0.030379; batch adversarial loss: 0.363776\n",
      "epoch 179; iter: 0; batch classifier loss: 0.036306; batch adversarial loss: 0.415210\n",
      "epoch 180; iter: 0; batch classifier loss: 0.059729; batch adversarial loss: 0.431297\n",
      "epoch 181; iter: 0; batch classifier loss: 0.033832; batch adversarial loss: 0.409258\n",
      "epoch 182; iter: 0; batch classifier loss: 0.012158; batch adversarial loss: 0.452828\n",
      "epoch 183; iter: 0; batch classifier loss: 0.028873; batch adversarial loss: 0.442833\n",
      "epoch 184; iter: 0; batch classifier loss: 0.024075; batch adversarial loss: 0.513819\n",
      "epoch 185; iter: 0; batch classifier loss: 0.032804; batch adversarial loss: 0.492895\n",
      "epoch 186; iter: 0; batch classifier loss: 0.041055; batch adversarial loss: 0.451208\n",
      "epoch 187; iter: 0; batch classifier loss: 0.032801; batch adversarial loss: 0.430365\n",
      "epoch 188; iter: 0; batch classifier loss: 0.014628; batch adversarial loss: 0.541972\n",
      "epoch 189; iter: 0; batch classifier loss: 0.046677; batch adversarial loss: 0.538265\n",
      "epoch 190; iter: 0; batch classifier loss: 0.034663; batch adversarial loss: 0.420191\n",
      "epoch 191; iter: 0; batch classifier loss: 0.016803; batch adversarial loss: 0.376154\n",
      "epoch 192; iter: 0; batch classifier loss: 0.031497; batch adversarial loss: 0.368896\n",
      "epoch 193; iter: 0; batch classifier loss: 0.021440; batch adversarial loss: 0.434695\n",
      "epoch 194; iter: 0; batch classifier loss: 0.027047; batch adversarial loss: 0.482755\n",
      "epoch 195; iter: 0; batch classifier loss: 0.053629; batch adversarial loss: 0.453765\n",
      "epoch 196; iter: 0; batch classifier loss: 0.070503; batch adversarial loss: 0.420035\n",
      "epoch 197; iter: 0; batch classifier loss: 0.022780; batch adversarial loss: 0.410440\n",
      "epoch 198; iter: 0; batch classifier loss: 0.046932; batch adversarial loss: 0.365369\n",
      "epoch 199; iter: 0; batch classifier loss: 0.021898; batch adversarial loss: 0.408307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:51:52.097788: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44a40-ae24-11ee-be98-ef9b34f2853b/04a44a40-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:6433 op device:{requested: '', assigned: ''} def:{{{node 04a44a40-ae24-11ee-be98-ef9b34f2853b/04a44a40-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44a40-ae24-11ee-be98-ef9b34f2853b/04a44a40-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44a40-ae24-11ee-be98-ef9b34f2853b/04a44a40-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.715748; batch adversarial loss: 0.579090\n",
      "epoch 1; iter: 0; batch classifier loss: 0.494647; batch adversarial loss: 0.589756\n",
      "epoch 2; iter: 0; batch classifier loss: 0.384174; batch adversarial loss: 0.619756\n",
      "epoch 3; iter: 0; batch classifier loss: 0.419312; batch adversarial loss: 0.599301\n",
      "epoch 4; iter: 0; batch classifier loss: 0.407919; batch adversarial loss: 0.565708\n",
      "epoch 5; iter: 0; batch classifier loss: 0.321542; batch adversarial loss: 0.529589\n",
      "epoch 6; iter: 0; batch classifier loss: 0.399488; batch adversarial loss: 0.581595\n",
      "epoch 7; iter: 0; batch classifier loss: 0.325736; batch adversarial loss: 0.599724\n",
      "epoch 8; iter: 0; batch classifier loss: 0.403179; batch adversarial loss: 0.499842\n",
      "epoch 9; iter: 0; batch classifier loss: 0.329724; batch adversarial loss: 0.520299\n",
      "epoch 10; iter: 0; batch classifier loss: 0.288315; batch adversarial loss: 0.496034\n",
      "epoch 11; iter: 0; batch classifier loss: 0.299194; batch adversarial loss: 0.512712\n",
      "epoch 12; iter: 0; batch classifier loss: 0.269837; batch adversarial loss: 0.428558\n",
      "epoch 13; iter: 0; batch classifier loss: 0.337996; batch adversarial loss: 0.439708\n",
      "epoch 14; iter: 0; batch classifier loss: 0.278534; batch adversarial loss: 0.480072\n",
      "epoch 15; iter: 0; batch classifier loss: 0.217393; batch adversarial loss: 0.487099\n",
      "epoch 16; iter: 0; batch classifier loss: 0.221610; batch adversarial loss: 0.460807\n",
      "epoch 17; iter: 0; batch classifier loss: 0.207620; batch adversarial loss: 0.479320\n",
      "epoch 18; iter: 0; batch classifier loss: 0.242957; batch adversarial loss: 0.496599\n",
      "epoch 19; iter: 0; batch classifier loss: 0.221006; batch adversarial loss: 0.491837\n",
      "epoch 20; iter: 0; batch classifier loss: 0.221537; batch adversarial loss: 0.419478\n",
      "epoch 21; iter: 0; batch classifier loss: 0.187312; batch adversarial loss: 0.469251\n",
      "epoch 22; iter: 0; batch classifier loss: 0.214877; batch adversarial loss: 0.459743\n",
      "epoch 23; iter: 0; batch classifier loss: 0.195737; batch adversarial loss: 0.432824\n",
      "epoch 24; iter: 0; batch classifier loss: 0.214342; batch adversarial loss: 0.442860\n",
      "epoch 25; iter: 0; batch classifier loss: 0.192311; batch adversarial loss: 0.513919\n",
      "epoch 26; iter: 0; batch classifier loss: 0.238123; batch adversarial loss: 0.447687\n",
      "epoch 27; iter: 0; batch classifier loss: 0.217928; batch adversarial loss: 0.441165\n",
      "epoch 28; iter: 0; batch classifier loss: 0.267820; batch adversarial loss: 0.382088\n",
      "epoch 29; iter: 0; batch classifier loss: 0.185029; batch adversarial loss: 0.492736\n",
      "epoch 30; iter: 0; batch classifier loss: 0.254659; batch adversarial loss: 0.403727\n",
      "epoch 31; iter: 0; batch classifier loss: 0.216319; batch adversarial loss: 0.450472\n",
      "epoch 32; iter: 0; batch classifier loss: 0.236377; batch adversarial loss: 0.470189\n",
      "epoch 33; iter: 0; batch classifier loss: 0.180801; batch adversarial loss: 0.498055\n",
      "epoch 34; iter: 0; batch classifier loss: 0.155005; batch adversarial loss: 0.386420\n",
      "epoch 35; iter: 0; batch classifier loss: 0.247831; batch adversarial loss: 0.432798\n",
      "epoch 36; iter: 0; batch classifier loss: 0.194797; batch adversarial loss: 0.473697\n",
      "epoch 37; iter: 0; batch classifier loss: 0.197001; batch adversarial loss: 0.500540\n",
      "epoch 38; iter: 0; batch classifier loss: 0.147832; batch adversarial loss: 0.492197\n",
      "epoch 39; iter: 0; batch classifier loss: 0.262898; batch adversarial loss: 0.443135\n",
      "epoch 40; iter: 0; batch classifier loss: 0.237395; batch adversarial loss: 0.420340\n",
      "epoch 41; iter: 0; batch classifier loss: 0.172463; batch adversarial loss: 0.511586\n",
      "epoch 42; iter: 0; batch classifier loss: 0.152798; batch adversarial loss: 0.415734\n",
      "epoch 43; iter: 0; batch classifier loss: 0.247051; batch adversarial loss: 0.473576\n",
      "epoch 44; iter: 0; batch classifier loss: 0.217665; batch adversarial loss: 0.330930\n",
      "epoch 45; iter: 0; batch classifier loss: 0.240812; batch adversarial loss: 0.429021\n",
      "epoch 46; iter: 0; batch classifier loss: 0.251629; batch adversarial loss: 0.387498\n",
      "epoch 47; iter: 0; batch classifier loss: 0.203806; batch adversarial loss: 0.549492\n",
      "epoch 48; iter: 0; batch classifier loss: 0.196983; batch adversarial loss: 0.532358\n",
      "epoch 49; iter: 0; batch classifier loss: 0.206421; batch adversarial loss: 0.400020\n",
      "epoch 50; iter: 0; batch classifier loss: 0.291311; batch adversarial loss: 0.412937\n",
      "epoch 51; iter: 0; batch classifier loss: 0.234995; batch adversarial loss: 0.464114\n",
      "epoch 52; iter: 0; batch classifier loss: 0.203743; batch adversarial loss: 0.437295\n",
      "epoch 53; iter: 0; batch classifier loss: 0.171704; batch adversarial loss: 0.421880\n",
      "epoch 54; iter: 0; batch classifier loss: 0.217097; batch adversarial loss: 0.484463\n",
      "epoch 55; iter: 0; batch classifier loss: 0.287177; batch adversarial loss: 0.434993\n",
      "epoch 56; iter: 0; batch classifier loss: 0.287098; batch adversarial loss: 0.446195\n",
      "epoch 57; iter: 0; batch classifier loss: 0.292293; batch adversarial loss: 0.362645\n",
      "epoch 58; iter: 0; batch classifier loss: 0.255408; batch adversarial loss: 0.520686\n",
      "epoch 59; iter: 0; batch classifier loss: 0.219740; batch adversarial loss: 0.373641\n",
      "epoch 60; iter: 0; batch classifier loss: 0.261280; batch adversarial loss: 0.422387\n",
      "epoch 61; iter: 0; batch classifier loss: 0.251302; batch adversarial loss: 0.373711\n",
      "epoch 62; iter: 0; batch classifier loss: 0.160303; batch adversarial loss: 0.495333\n",
      "epoch 63; iter: 0; batch classifier loss: 0.204485; batch adversarial loss: 0.459487\n",
      "epoch 64; iter: 0; batch classifier loss: 0.196770; batch adversarial loss: 0.484571\n",
      "epoch 65; iter: 0; batch classifier loss: 0.241724; batch adversarial loss: 0.420726\n",
      "epoch 66; iter: 0; batch classifier loss: 0.196943; batch adversarial loss: 0.373531\n",
      "epoch 67; iter: 0; batch classifier loss: 0.166850; batch adversarial loss: 0.583684\n",
      "epoch 68; iter: 0; batch classifier loss: 0.184517; batch adversarial loss: 0.482468\n",
      "epoch 69; iter: 0; batch classifier loss: 0.238939; batch adversarial loss: 0.471513\n",
      "epoch 70; iter: 0; batch classifier loss: 0.223898; batch adversarial loss: 0.372281\n",
      "epoch 71; iter: 0; batch classifier loss: 0.226671; batch adversarial loss: 0.458516\n",
      "epoch 72; iter: 0; batch classifier loss: 0.138506; batch adversarial loss: 0.470724\n",
      "epoch 73; iter: 0; batch classifier loss: 0.096126; batch adversarial loss: 0.407561\n",
      "epoch 74; iter: 0; batch classifier loss: 0.151245; batch adversarial loss: 0.546343\n",
      "epoch 75; iter: 0; batch classifier loss: 0.191002; batch adversarial loss: 0.477796\n",
      "epoch 76; iter: 0; batch classifier loss: 0.219209; batch adversarial loss: 0.419718\n",
      "epoch 77; iter: 0; batch classifier loss: 0.181958; batch adversarial loss: 0.434307\n",
      "epoch 78; iter: 0; batch classifier loss: 0.183612; batch adversarial loss: 0.546336\n",
      "epoch 79; iter: 0; batch classifier loss: 0.177116; batch adversarial loss: 0.432719\n",
      "epoch 80; iter: 0; batch classifier loss: 0.216637; batch adversarial loss: 0.434167\n",
      "epoch 81; iter: 0; batch classifier loss: 0.196227; batch adversarial loss: 0.522734\n",
      "epoch 82; iter: 0; batch classifier loss: 0.195839; batch adversarial loss: 0.445456\n",
      "epoch 83; iter: 0; batch classifier loss: 0.247253; batch adversarial loss: 0.433714\n",
      "epoch 84; iter: 0; batch classifier loss: 0.213033; batch adversarial loss: 0.421581\n",
      "epoch 85; iter: 0; batch classifier loss: 0.223987; batch adversarial loss: 0.408326\n",
      "epoch 86; iter: 0; batch classifier loss: 0.188081; batch adversarial loss: 0.421727\n",
      "epoch 87; iter: 0; batch classifier loss: 0.259251; batch adversarial loss: 0.495833\n",
      "epoch 88; iter: 0; batch classifier loss: 0.189492; batch adversarial loss: 0.508839\n",
      "epoch 89; iter: 0; batch classifier loss: 0.206831; batch adversarial loss: 0.335091\n",
      "epoch 90; iter: 0; batch classifier loss: 0.254344; batch adversarial loss: 0.520712\n",
      "epoch 91; iter: 0; batch classifier loss: 0.127242; batch adversarial loss: 0.295425\n",
      "epoch 92; iter: 0; batch classifier loss: 0.118795; batch adversarial loss: 0.494383\n",
      "epoch 93; iter: 0; batch classifier loss: 0.116413; batch adversarial loss: 0.446245\n",
      "epoch 94; iter: 0; batch classifier loss: 0.221101; batch adversarial loss: 0.494081\n",
      "epoch 95; iter: 0; batch classifier loss: 0.224355; batch adversarial loss: 0.508037\n",
      "epoch 96; iter: 0; batch classifier loss: 0.149814; batch adversarial loss: 0.458267\n",
      "epoch 97; iter: 0; batch classifier loss: 0.283787; batch adversarial loss: 0.372313\n",
      "epoch 98; iter: 0; batch classifier loss: 0.236506; batch adversarial loss: 0.546600\n",
      "epoch 99; iter: 0; batch classifier loss: 0.188144; batch adversarial loss: 0.483421\n",
      "epoch 100; iter: 0; batch classifier loss: 0.222439; batch adversarial loss: 0.520137\n",
      "epoch 101; iter: 0; batch classifier loss: 0.177702; batch adversarial loss: 0.621828\n",
      "epoch 102; iter: 0; batch classifier loss: 0.238501; batch adversarial loss: 0.420696\n",
      "epoch 103; iter: 0; batch classifier loss: 0.162226; batch adversarial loss: 0.522869\n",
      "epoch 104; iter: 0; batch classifier loss: 0.153098; batch adversarial loss: 0.422807\n",
      "epoch 105; iter: 0; batch classifier loss: 0.156383; batch adversarial loss: 0.420321\n",
      "epoch 106; iter: 0; batch classifier loss: 0.162620; batch adversarial loss: 0.472602\n",
      "epoch 107; iter: 0; batch classifier loss: 0.137255; batch adversarial loss: 0.548836\n",
      "epoch 108; iter: 0; batch classifier loss: 0.157450; batch adversarial loss: 0.435569\n",
      "epoch 109; iter: 0; batch classifier loss: 0.184619; batch adversarial loss: 0.532978\n",
      "epoch 110; iter: 0; batch classifier loss: 0.148838; batch adversarial loss: 0.473635\n",
      "epoch 111; iter: 0; batch classifier loss: 0.107566; batch adversarial loss: 0.445492\n",
      "epoch 112; iter: 0; batch classifier loss: 0.165850; batch adversarial loss: 0.470711\n",
      "epoch 113; iter: 0; batch classifier loss: 0.128264; batch adversarial loss: 0.381929\n",
      "epoch 114; iter: 0; batch classifier loss: 0.100866; batch adversarial loss: 0.443520\n",
      "epoch 115; iter: 0; batch classifier loss: 0.072702; batch adversarial loss: 0.440258\n",
      "epoch 116; iter: 0; batch classifier loss: 0.067337; batch adversarial loss: 0.404619\n",
      "epoch 117; iter: 0; batch classifier loss: 0.046737; batch adversarial loss: 0.326161\n",
      "epoch 118; iter: 0; batch classifier loss: 0.049630; batch adversarial loss: 0.503857\n",
      "epoch 119; iter: 0; batch classifier loss: 0.073690; batch adversarial loss: 0.434894\n",
      "epoch 120; iter: 0; batch classifier loss: 0.042106; batch adversarial loss: 0.404217\n",
      "epoch 121; iter: 0; batch classifier loss: 0.039333; batch adversarial loss: 0.470904\n",
      "epoch 122; iter: 0; batch classifier loss: 0.047514; batch adversarial loss: 0.368961\n",
      "epoch 123; iter: 0; batch classifier loss: 0.059678; batch adversarial loss: 0.372505\n",
      "epoch 124; iter: 0; batch classifier loss: 0.042412; batch adversarial loss: 0.401693\n",
      "epoch 125; iter: 0; batch classifier loss: 0.048495; batch adversarial loss: 0.447831\n",
      "epoch 126; iter: 0; batch classifier loss: 0.037080; batch adversarial loss: 0.434010\n",
      "epoch 127; iter: 0; batch classifier loss: 0.046433; batch adversarial loss: 0.411668\n",
      "epoch 128; iter: 0; batch classifier loss: 0.027961; batch adversarial loss: 0.442815\n",
      "epoch 129; iter: 0; batch classifier loss: 0.063680; batch adversarial loss: 0.437824\n",
      "epoch 130; iter: 0; batch classifier loss: 0.051116; batch adversarial loss: 0.502811\n",
      "epoch 131; iter: 0; batch classifier loss: 0.017249; batch adversarial loss: 0.463921\n",
      "epoch 132; iter: 0; batch classifier loss: 0.025821; batch adversarial loss: 0.502786\n",
      "epoch 133; iter: 0; batch classifier loss: 0.032118; batch adversarial loss: 0.479978\n",
      "epoch 134; iter: 0; batch classifier loss: 0.020242; batch adversarial loss: 0.579753\n",
      "epoch 135; iter: 0; batch classifier loss: 0.027025; batch adversarial loss: 0.487015\n",
      "epoch 136; iter: 0; batch classifier loss: 0.018515; batch adversarial loss: 0.414811\n",
      "epoch 137; iter: 0; batch classifier loss: 0.019743; batch adversarial loss: 0.443406\n",
      "epoch 138; iter: 0; batch classifier loss: 0.014711; batch adversarial loss: 0.425846\n",
      "epoch 139; iter: 0; batch classifier loss: 0.061222; batch adversarial loss: 0.463644\n",
      "epoch 140; iter: 0; batch classifier loss: 0.030486; batch adversarial loss: 0.512053\n",
      "epoch 141; iter: 0; batch classifier loss: 0.008721; batch adversarial loss: 0.358234\n",
      "epoch 142; iter: 0; batch classifier loss: 0.029712; batch adversarial loss: 0.471983\n",
      "epoch 143; iter: 0; batch classifier loss: 0.049397; batch adversarial loss: 0.370775\n",
      "epoch 144; iter: 0; batch classifier loss: 0.039775; batch adversarial loss: 0.449155\n",
      "epoch 145; iter: 0; batch classifier loss: 0.014644; batch adversarial loss: 0.497042\n",
      "epoch 146; iter: 0; batch classifier loss: 0.016746; batch adversarial loss: 0.363120\n",
      "epoch 147; iter: 0; batch classifier loss: 0.049329; batch adversarial loss: 0.500336\n",
      "epoch 148; iter: 0; batch classifier loss: 0.016642; batch adversarial loss: 0.382825\n",
      "epoch 149; iter: 0; batch classifier loss: 0.012869; batch adversarial loss: 0.378045\n",
      "epoch 150; iter: 0; batch classifier loss: 0.013168; batch adversarial loss: 0.473749\n",
      "epoch 151; iter: 0; batch classifier loss: 0.022730; batch adversarial loss: 0.440604\n",
      "epoch 152; iter: 0; batch classifier loss: 0.033658; batch adversarial loss: 0.481751\n",
      "epoch 153; iter: 0; batch classifier loss: 0.012399; batch adversarial loss: 0.482579\n",
      "epoch 154; iter: 0; batch classifier loss: 0.016112; batch adversarial loss: 0.441439\n",
      "epoch 155; iter: 0; batch classifier loss: 0.015946; batch adversarial loss: 0.447373\n",
      "epoch 156; iter: 0; batch classifier loss: 0.033650; batch adversarial loss: 0.480240\n",
      "epoch 157; iter: 0; batch classifier loss: 0.011658; batch adversarial loss: 0.448299\n",
      "epoch 158; iter: 0; batch classifier loss: 0.018045; batch adversarial loss: 0.470528\n",
      "epoch 159; iter: 0; batch classifier loss: 0.004853; batch adversarial loss: 0.458181\n",
      "epoch 160; iter: 0; batch classifier loss: 0.025689; batch adversarial loss: 0.501225\n",
      "epoch 161; iter: 0; batch classifier loss: 0.010074; batch adversarial loss: 0.506985\n",
      "epoch 162; iter: 0; batch classifier loss: 0.011399; batch adversarial loss: 0.451227\n",
      "epoch 163; iter: 0; batch classifier loss: 0.042417; batch adversarial loss: 0.422524\n",
      "epoch 164; iter: 0; batch classifier loss: 0.015114; batch adversarial loss: 0.391352\n",
      "epoch 165; iter: 0; batch classifier loss: 0.029600; batch adversarial loss: 0.332093\n",
      "epoch 166; iter: 0; batch classifier loss: 0.018346; batch adversarial loss: 0.390093\n",
      "epoch 167; iter: 0; batch classifier loss: 0.023645; batch adversarial loss: 0.386988\n",
      "epoch 168; iter: 0; batch classifier loss: 0.017696; batch adversarial loss: 0.433516\n",
      "epoch 169; iter: 0; batch classifier loss: 0.009071; batch adversarial loss: 0.491562\n",
      "epoch 170; iter: 0; batch classifier loss: 0.032906; batch adversarial loss: 0.393674\n",
      "epoch 171; iter: 0; batch classifier loss: 0.005724; batch adversarial loss: 0.564659\n",
      "epoch 172; iter: 0; batch classifier loss: 0.035550; batch adversarial loss: 0.503365\n",
      "epoch 173; iter: 0; batch classifier loss: 0.022453; batch adversarial loss: 0.449883\n",
      "epoch 174; iter: 0; batch classifier loss: 0.010128; batch adversarial loss: 0.396177\n",
      "epoch 175; iter: 0; batch classifier loss: 0.018389; batch adversarial loss: 0.482131\n",
      "epoch 176; iter: 0; batch classifier loss: 0.006094; batch adversarial loss: 0.504420\n",
      "epoch 177; iter: 0; batch classifier loss: 0.031664; batch adversarial loss: 0.418467\n",
      "epoch 178; iter: 0; batch classifier loss: 0.006760; batch adversarial loss: 0.450015\n",
      "epoch 179; iter: 0; batch classifier loss: 0.010847; batch adversarial loss: 0.447454\n",
      "epoch 180; iter: 0; batch classifier loss: 0.025097; batch adversarial loss: 0.579088\n",
      "epoch 181; iter: 0; batch classifier loss: 0.012762; batch adversarial loss: 0.468544\n",
      "epoch 182; iter: 0; batch classifier loss: 0.008093; batch adversarial loss: 0.480263\n",
      "epoch 183; iter: 0; batch classifier loss: 0.004026; batch adversarial loss: 0.502261\n",
      "epoch 184; iter: 0; batch classifier loss: 0.017115; batch adversarial loss: 0.408404\n",
      "epoch 185; iter: 0; batch classifier loss: 0.014154; batch adversarial loss: 0.484823\n",
      "epoch 186; iter: 0; batch classifier loss: 0.015411; batch adversarial loss: 0.477639\n",
      "epoch 187; iter: 0; batch classifier loss: 0.004765; batch adversarial loss: 0.495320\n",
      "epoch 188; iter: 0; batch classifier loss: 0.023154; batch adversarial loss: 0.355280\n",
      "epoch 189; iter: 0; batch classifier loss: 0.008114; batch adversarial loss: 0.416485\n",
      "epoch 190; iter: 0; batch classifier loss: 0.024599; batch adversarial loss: 0.477785\n",
      "epoch 191; iter: 0; batch classifier loss: 0.016896; batch adversarial loss: 0.375823\n",
      "epoch 192; iter: 0; batch classifier loss: 0.002564; batch adversarial loss: 0.486337\n",
      "epoch 193; iter: 0; batch classifier loss: 0.011997; batch adversarial loss: 0.532713\n",
      "epoch 194; iter: 0; batch classifier loss: 0.008656; batch adversarial loss: 0.477772\n",
      "epoch 195; iter: 0; batch classifier loss: 0.018152; batch adversarial loss: 0.545017\n",
      "epoch 196; iter: 0; batch classifier loss: 0.026943; batch adversarial loss: 0.418610\n",
      "epoch 197; iter: 0; batch classifier loss: 0.020293; batch adversarial loss: 0.484025\n",
      "epoch 198; iter: 0; batch classifier loss: 0.008172; batch adversarial loss: 0.430710\n",
      "epoch 199; iter: 0; batch classifier loss: 0.008881; batch adversarial loss: 0.376113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:52:28.394513: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44acc-ae24-11ee-be98-ef9b34f2853b/04a44acc-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:7240 op device:{requested: '', assigned: ''} def:{{{node 04a44acc-ae24-11ee-be98-ef9b34f2853b/04a44acc-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44acc-ae24-11ee-be98-ef9b34f2853b/04a44acc-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44acc-ae24-11ee-be98-ef9b34f2853b/04a44acc-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.694012; batch adversarial loss: 0.558985\n",
      "epoch 1; iter: 0; batch classifier loss: 0.509830; batch adversarial loss: 0.629603\n",
      "epoch 2; iter: 0; batch classifier loss: 0.374906; batch adversarial loss: 0.600869\n",
      "epoch 3; iter: 0; batch classifier loss: 0.345048; batch adversarial loss: 0.649830\n",
      "epoch 4; iter: 0; batch classifier loss: 0.440369; batch adversarial loss: 0.608453\n",
      "epoch 5; iter: 0; batch classifier loss: 0.495135; batch adversarial loss: 0.601172\n",
      "epoch 6; iter: 0; batch classifier loss: 0.545936; batch adversarial loss: 0.569162\n",
      "epoch 7; iter: 0; batch classifier loss: 0.597017; batch adversarial loss: 0.556543\n",
      "epoch 8; iter: 0; batch classifier loss: 0.595120; batch adversarial loss: 0.531000\n",
      "epoch 9; iter: 0; batch classifier loss: 0.513992; batch adversarial loss: 0.593927\n",
      "epoch 10; iter: 0; batch classifier loss: 0.530040; batch adversarial loss: 0.512685\n",
      "epoch 11; iter: 0; batch classifier loss: 0.439402; batch adversarial loss: 0.545635\n",
      "epoch 12; iter: 0; batch classifier loss: 0.400823; batch adversarial loss: 0.521904\n",
      "epoch 13; iter: 0; batch classifier loss: 0.376011; batch adversarial loss: 0.473590\n",
      "epoch 14; iter: 0; batch classifier loss: 0.289097; batch adversarial loss: 0.527770\n",
      "epoch 15; iter: 0; batch classifier loss: 0.433297; batch adversarial loss: 0.478885\n",
      "epoch 16; iter: 0; batch classifier loss: 0.291646; batch adversarial loss: 0.510062\n",
      "epoch 17; iter: 0; batch classifier loss: 0.373085; batch adversarial loss: 0.497860\n",
      "epoch 18; iter: 0; batch classifier loss: 0.226731; batch adversarial loss: 0.519003\n",
      "epoch 19; iter: 0; batch classifier loss: 0.232753; batch adversarial loss: 0.466622\n",
      "epoch 20; iter: 0; batch classifier loss: 0.242975; batch adversarial loss: 0.455413\n",
      "epoch 21; iter: 0; batch classifier loss: 0.241387; batch adversarial loss: 0.537597\n",
      "epoch 22; iter: 0; batch classifier loss: 0.216325; batch adversarial loss: 0.460091\n",
      "epoch 23; iter: 0; batch classifier loss: 0.205107; batch adversarial loss: 0.449368\n",
      "epoch 24; iter: 0; batch classifier loss: 0.227253; batch adversarial loss: 0.440620\n",
      "epoch 25; iter: 0; batch classifier loss: 0.249562; batch adversarial loss: 0.440011\n",
      "epoch 26; iter: 0; batch classifier loss: 0.208607; batch adversarial loss: 0.506192\n",
      "epoch 27; iter: 0; batch classifier loss: 0.199186; batch adversarial loss: 0.463193\n",
      "epoch 28; iter: 0; batch classifier loss: 0.225429; batch adversarial loss: 0.556232\n",
      "epoch 29; iter: 0; batch classifier loss: 0.183651; batch adversarial loss: 0.460831\n",
      "epoch 30; iter: 0; batch classifier loss: 0.255874; batch adversarial loss: 0.445966\n",
      "epoch 31; iter: 0; batch classifier loss: 0.159818; batch adversarial loss: 0.457948\n",
      "epoch 32; iter: 0; batch classifier loss: 0.288142; batch adversarial loss: 0.423640\n",
      "epoch 33; iter: 0; batch classifier loss: 0.229654; batch adversarial loss: 0.499670\n",
      "epoch 34; iter: 0; batch classifier loss: 0.175036; batch adversarial loss: 0.512061\n",
      "epoch 35; iter: 0; batch classifier loss: 0.258912; batch adversarial loss: 0.414784\n",
      "epoch 36; iter: 0; batch classifier loss: 0.321653; batch adversarial loss: 0.441270\n",
      "epoch 37; iter: 0; batch classifier loss: 0.163459; batch adversarial loss: 0.531762\n",
      "epoch 38; iter: 0; batch classifier loss: 0.277997; batch adversarial loss: 0.443783\n",
      "epoch 39; iter: 0; batch classifier loss: 0.171257; batch adversarial loss: 0.517859\n",
      "epoch 40; iter: 0; batch classifier loss: 0.216274; batch adversarial loss: 0.475282\n",
      "epoch 41; iter: 0; batch classifier loss: 0.186999; batch adversarial loss: 0.437371\n",
      "epoch 42; iter: 0; batch classifier loss: 0.196882; batch adversarial loss: 0.452012\n",
      "epoch 43; iter: 0; batch classifier loss: 0.159985; batch adversarial loss: 0.530454\n",
      "epoch 44; iter: 0; batch classifier loss: 0.240687; batch adversarial loss: 0.521066\n",
      "epoch 45; iter: 0; batch classifier loss: 0.262007; batch adversarial loss: 0.379878\n",
      "epoch 46; iter: 0; batch classifier loss: 0.154285; batch adversarial loss: 0.450321\n",
      "epoch 47; iter: 0; batch classifier loss: 0.198545; batch adversarial loss: 0.409115\n",
      "epoch 48; iter: 0; batch classifier loss: 0.172537; batch adversarial loss: 0.532404\n",
      "epoch 49; iter: 0; batch classifier loss: 0.215047; batch adversarial loss: 0.406319\n",
      "epoch 50; iter: 0; batch classifier loss: 0.224461; batch adversarial loss: 0.483739\n",
      "epoch 51; iter: 0; batch classifier loss: 0.197157; batch adversarial loss: 0.474233\n",
      "epoch 52; iter: 0; batch classifier loss: 0.188499; batch adversarial loss: 0.614850\n",
      "epoch 53; iter: 0; batch classifier loss: 0.160968; batch adversarial loss: 0.414145\n",
      "epoch 54; iter: 0; batch classifier loss: 0.259341; batch adversarial loss: 0.399003\n",
      "epoch 55; iter: 0; batch classifier loss: 0.173206; batch adversarial loss: 0.415425\n",
      "epoch 56; iter: 0; batch classifier loss: 0.278232; batch adversarial loss: 0.436547\n",
      "epoch 57; iter: 0; batch classifier loss: 0.220172; batch adversarial loss: 0.422681\n",
      "epoch 58; iter: 0; batch classifier loss: 0.220107; batch adversarial loss: 0.508354\n",
      "epoch 59; iter: 0; batch classifier loss: 0.239483; batch adversarial loss: 0.409405\n",
      "epoch 60; iter: 0; batch classifier loss: 0.215702; batch adversarial loss: 0.459996\n",
      "epoch 61; iter: 0; batch classifier loss: 0.236224; batch adversarial loss: 0.361099\n",
      "epoch 62; iter: 0; batch classifier loss: 0.287291; batch adversarial loss: 0.519825\n",
      "epoch 63; iter: 0; batch classifier loss: 0.118711; batch adversarial loss: 0.372901\n",
      "epoch 64; iter: 0; batch classifier loss: 0.112272; batch adversarial loss: 0.494992\n",
      "epoch 65; iter: 0; batch classifier loss: 0.142180; batch adversarial loss: 0.409395\n",
      "epoch 66; iter: 0; batch classifier loss: 0.264623; batch adversarial loss: 0.547076\n",
      "epoch 67; iter: 0; batch classifier loss: 0.176135; batch adversarial loss: 0.373695\n",
      "epoch 68; iter: 0; batch classifier loss: 0.178729; batch adversarial loss: 0.484424\n",
      "epoch 69; iter: 0; batch classifier loss: 0.190636; batch adversarial loss: 0.496330\n",
      "epoch 70; iter: 0; batch classifier loss: 0.255418; batch adversarial loss: 0.421590\n",
      "epoch 71; iter: 0; batch classifier loss: 0.207541; batch adversarial loss: 0.508339\n",
      "epoch 72; iter: 0; batch classifier loss: 0.233745; batch adversarial loss: 0.472430\n",
      "epoch 73; iter: 0; batch classifier loss: 0.202653; batch adversarial loss: 0.397614\n",
      "epoch 74; iter: 0; batch classifier loss: 0.231304; batch adversarial loss: 0.507612\n",
      "epoch 75; iter: 0; batch classifier loss: 0.189778; batch adversarial loss: 0.532703\n",
      "epoch 76; iter: 0; batch classifier loss: 0.175817; batch adversarial loss: 0.458775\n",
      "epoch 77; iter: 0; batch classifier loss: 0.081731; batch adversarial loss: 0.405574\n",
      "epoch 78; iter: 0; batch classifier loss: 0.066777; batch adversarial loss: 0.505837\n",
      "epoch 79; iter: 0; batch classifier loss: 0.070844; batch adversarial loss: 0.496279\n",
      "epoch 80; iter: 0; batch classifier loss: 0.057965; batch adversarial loss: 0.517094\n",
      "epoch 81; iter: 0; batch classifier loss: 0.077897; batch adversarial loss: 0.397409\n",
      "epoch 82; iter: 0; batch classifier loss: 0.055659; batch adversarial loss: 0.449028\n",
      "epoch 83; iter: 0; batch classifier loss: 0.080421; batch adversarial loss: 0.467186\n",
      "epoch 84; iter: 0; batch classifier loss: 0.065403; batch adversarial loss: 0.474949\n",
      "epoch 85; iter: 0; batch classifier loss: 0.030242; batch adversarial loss: 0.461029\n",
      "epoch 86; iter: 0; batch classifier loss: 0.054994; batch adversarial loss: 0.471942\n",
      "epoch 87; iter: 0; batch classifier loss: 0.106390; batch adversarial loss: 0.471799\n",
      "epoch 88; iter: 0; batch classifier loss: 0.067081; batch adversarial loss: 0.520993\n",
      "epoch 89; iter: 0; batch classifier loss: 0.051916; batch adversarial loss: 0.472494\n",
      "epoch 90; iter: 0; batch classifier loss: 0.073595; batch adversarial loss: 0.453818\n",
      "epoch 91; iter: 0; batch classifier loss: 0.070477; batch adversarial loss: 0.510861\n",
      "epoch 92; iter: 0; batch classifier loss: 0.066008; batch adversarial loss: 0.477425\n",
      "epoch 93; iter: 0; batch classifier loss: 0.078630; batch adversarial loss: 0.504362\n",
      "epoch 94; iter: 0; batch classifier loss: 0.057076; batch adversarial loss: 0.475444\n",
      "epoch 95; iter: 0; batch classifier loss: 0.080270; batch adversarial loss: 0.394864\n",
      "epoch 96; iter: 0; batch classifier loss: 0.062137; batch adversarial loss: 0.446474\n",
      "epoch 97; iter: 0; batch classifier loss: 0.031343; batch adversarial loss: 0.471413\n",
      "epoch 98; iter: 0; batch classifier loss: 0.035075; batch adversarial loss: 0.433514\n",
      "epoch 99; iter: 0; batch classifier loss: 0.053434; batch adversarial loss: 0.435389\n",
      "epoch 100; iter: 0; batch classifier loss: 0.027878; batch adversarial loss: 0.571735\n",
      "epoch 101; iter: 0; batch classifier loss: 0.056909; batch adversarial loss: 0.445655\n",
      "epoch 102; iter: 0; batch classifier loss: 0.073704; batch adversarial loss: 0.440906\n",
      "epoch 103; iter: 0; batch classifier loss: 0.086433; batch adversarial loss: 0.500628\n",
      "epoch 104; iter: 0; batch classifier loss: 0.032984; batch adversarial loss: 0.480649\n",
      "epoch 105; iter: 0; batch classifier loss: 0.058752; batch adversarial loss: 0.367974\n",
      "epoch 106; iter: 0; batch classifier loss: 0.064137; batch adversarial loss: 0.456531\n",
      "epoch 107; iter: 0; batch classifier loss: 0.054625; batch adversarial loss: 0.499646\n",
      "epoch 108; iter: 0; batch classifier loss: 0.052328; batch adversarial loss: 0.399235\n",
      "epoch 109; iter: 0; batch classifier loss: 0.039547; batch adversarial loss: 0.421749\n",
      "epoch 110; iter: 0; batch classifier loss: 0.068182; batch adversarial loss: 0.451453\n",
      "epoch 111; iter: 0; batch classifier loss: 0.032974; batch adversarial loss: 0.438799\n",
      "epoch 112; iter: 0; batch classifier loss: 0.018510; batch adversarial loss: 0.404622\n",
      "epoch 113; iter: 0; batch classifier loss: 0.041488; batch adversarial loss: 0.415127\n",
      "epoch 114; iter: 0; batch classifier loss: 0.030094; batch adversarial loss: 0.555383\n",
      "epoch 115; iter: 0; batch classifier loss: 0.066269; batch adversarial loss: 0.485156\n",
      "epoch 116; iter: 0; batch classifier loss: 0.047832; batch adversarial loss: 0.418528\n",
      "epoch 117; iter: 0; batch classifier loss: 0.040029; batch adversarial loss: 0.521389\n",
      "epoch 118; iter: 0; batch classifier loss: 0.036189; batch adversarial loss: 0.425692\n",
      "epoch 119; iter: 0; batch classifier loss: 0.045099; batch adversarial loss: 0.461903\n",
      "epoch 120; iter: 0; batch classifier loss: 0.012857; batch adversarial loss: 0.419220\n",
      "epoch 121; iter: 0; batch classifier loss: 0.041366; batch adversarial loss: 0.393013\n",
      "epoch 122; iter: 0; batch classifier loss: 0.028763; batch adversarial loss: 0.476629\n",
      "epoch 123; iter: 0; batch classifier loss: 0.047351; batch adversarial loss: 0.428163\n",
      "epoch 124; iter: 0; batch classifier loss: 0.051472; batch adversarial loss: 0.403877\n",
      "epoch 125; iter: 0; batch classifier loss: 0.036934; batch adversarial loss: 0.373393\n",
      "epoch 126; iter: 0; batch classifier loss: 0.035516; batch adversarial loss: 0.545827\n",
      "epoch 127; iter: 0; batch classifier loss: 0.065639; batch adversarial loss: 0.396608\n",
      "epoch 128; iter: 0; batch classifier loss: 0.046038; batch adversarial loss: 0.502716\n",
      "epoch 129; iter: 0; batch classifier loss: 0.037888; batch adversarial loss: 0.423384\n",
      "epoch 130; iter: 0; batch classifier loss: 0.046264; batch adversarial loss: 0.455964\n",
      "epoch 131; iter: 0; batch classifier loss: 0.036332; batch adversarial loss: 0.431411\n",
      "epoch 132; iter: 0; batch classifier loss: 0.047923; batch adversarial loss: 0.471827\n",
      "epoch 133; iter: 0; batch classifier loss: 0.026227; batch adversarial loss: 0.452884\n",
      "epoch 134; iter: 0; batch classifier loss: 0.051348; batch adversarial loss: 0.490992\n",
      "epoch 135; iter: 0; batch classifier loss: 0.025745; batch adversarial loss: 0.519843\n",
      "epoch 136; iter: 0; batch classifier loss: 0.021785; batch adversarial loss: 0.494342\n",
      "epoch 137; iter: 0; batch classifier loss: 0.042789; batch adversarial loss: 0.426261\n",
      "epoch 138; iter: 0; batch classifier loss: 0.030594; batch adversarial loss: 0.453753\n",
      "epoch 139; iter: 0; batch classifier loss: 0.014517; batch adversarial loss: 0.411390\n",
      "epoch 140; iter: 0; batch classifier loss: 0.044955; batch adversarial loss: 0.485402\n",
      "epoch 141; iter: 0; batch classifier loss: 0.038268; batch adversarial loss: 0.483169\n",
      "epoch 142; iter: 0; batch classifier loss: 0.007985; batch adversarial loss: 0.538814\n",
      "epoch 143; iter: 0; batch classifier loss: 0.020118; batch adversarial loss: 0.422398\n",
      "epoch 144; iter: 0; batch classifier loss: 0.021248; batch adversarial loss: 0.431730\n",
      "epoch 145; iter: 0; batch classifier loss: 0.022349; batch adversarial loss: 0.403687\n",
      "epoch 146; iter: 0; batch classifier loss: 0.035632; batch adversarial loss: 0.406981\n",
      "epoch 147; iter: 0; batch classifier loss: 0.023480; batch adversarial loss: 0.482348\n",
      "epoch 148; iter: 0; batch classifier loss: 0.055539; batch adversarial loss: 0.446864\n",
      "epoch 149; iter: 0; batch classifier loss: 0.025524; batch adversarial loss: 0.452096\n",
      "epoch 150; iter: 0; batch classifier loss: 0.012976; batch adversarial loss: 0.436258\n",
      "epoch 151; iter: 0; batch classifier loss: 0.050904; batch adversarial loss: 0.488082\n",
      "epoch 152; iter: 0; batch classifier loss: 0.026929; batch adversarial loss: 0.520960\n",
      "epoch 153; iter: 0; batch classifier loss: 0.020956; batch adversarial loss: 0.388246\n",
      "epoch 154; iter: 0; batch classifier loss: 0.047648; batch adversarial loss: 0.465431\n",
      "epoch 155; iter: 0; batch classifier loss: 0.027428; batch adversarial loss: 0.387720\n",
      "epoch 156; iter: 0; batch classifier loss: 0.059809; batch adversarial loss: 0.343260\n",
      "epoch 157; iter: 0; batch classifier loss: 0.020215; batch adversarial loss: 0.456232\n",
      "epoch 158; iter: 0; batch classifier loss: 0.019970; batch adversarial loss: 0.436149\n",
      "epoch 159; iter: 0; batch classifier loss: 0.028203; batch adversarial loss: 0.406995\n",
      "epoch 160; iter: 0; batch classifier loss: 0.043083; batch adversarial loss: 0.386181\n",
      "epoch 161; iter: 0; batch classifier loss: 0.016228; batch adversarial loss: 0.417304\n",
      "epoch 162; iter: 0; batch classifier loss: 0.028370; batch adversarial loss: 0.392933\n",
      "epoch 163; iter: 0; batch classifier loss: 0.013595; batch adversarial loss: 0.516907\n",
      "epoch 164; iter: 0; batch classifier loss: 0.037975; batch adversarial loss: 0.434925\n",
      "epoch 165; iter: 0; batch classifier loss: 0.013795; batch adversarial loss: 0.497037\n",
      "epoch 166; iter: 0; batch classifier loss: 0.042607; batch adversarial loss: 0.409248\n",
      "epoch 167; iter: 0; batch classifier loss: 0.015639; batch adversarial loss: 0.400933\n",
      "epoch 168; iter: 0; batch classifier loss: 0.034757; batch adversarial loss: 0.469148\n",
      "epoch 169; iter: 0; batch classifier loss: 0.015057; batch adversarial loss: 0.393210\n",
      "epoch 170; iter: 0; batch classifier loss: 0.028503; batch adversarial loss: 0.393910\n",
      "epoch 171; iter: 0; batch classifier loss: 0.037093; batch adversarial loss: 0.541128\n",
      "epoch 172; iter: 0; batch classifier loss: 0.048828; batch adversarial loss: 0.504808\n",
      "epoch 173; iter: 0; batch classifier loss: 0.037083; batch adversarial loss: 0.517985\n",
      "epoch 174; iter: 0; batch classifier loss: 0.031466; batch adversarial loss: 0.394767\n",
      "epoch 175; iter: 0; batch classifier loss: 0.021963; batch adversarial loss: 0.448642\n",
      "epoch 176; iter: 0; batch classifier loss: 0.033476; batch adversarial loss: 0.463436\n",
      "epoch 177; iter: 0; batch classifier loss: 0.024381; batch adversarial loss: 0.414974\n",
      "epoch 178; iter: 0; batch classifier loss: 0.018678; batch adversarial loss: 0.431140\n",
      "epoch 179; iter: 0; batch classifier loss: 0.014473; batch adversarial loss: 0.416879\n",
      "epoch 180; iter: 0; batch classifier loss: 0.009088; batch adversarial loss: 0.437805\n",
      "epoch 181; iter: 0; batch classifier loss: 0.002947; batch adversarial loss: 0.601357\n",
      "epoch 182; iter: 0; batch classifier loss: 0.030258; batch adversarial loss: 0.428900\n",
      "epoch 183; iter: 0; batch classifier loss: 0.012385; batch adversarial loss: 0.474412\n",
      "epoch 184; iter: 0; batch classifier loss: 0.006580; batch adversarial loss: 0.498635\n",
      "epoch 185; iter: 0; batch classifier loss: 0.027856; batch adversarial loss: 0.464236\n",
      "epoch 186; iter: 0; batch classifier loss: 0.009014; batch adversarial loss: 0.346686\n",
      "epoch 187; iter: 0; batch classifier loss: 0.014830; batch adversarial loss: 0.442864\n",
      "epoch 188; iter: 0; batch classifier loss: 0.019228; batch adversarial loss: 0.533260\n",
      "epoch 189; iter: 0; batch classifier loss: 0.013824; batch adversarial loss: 0.399277\n",
      "epoch 190; iter: 0; batch classifier loss: 0.007882; batch adversarial loss: 0.496635\n",
      "epoch 191; iter: 0; batch classifier loss: 0.012674; batch adversarial loss: 0.386989\n",
      "epoch 192; iter: 0; batch classifier loss: 0.032288; batch adversarial loss: 0.482800\n",
      "epoch 193; iter: 0; batch classifier loss: 0.009258; batch adversarial loss: 0.494538\n",
      "epoch 194; iter: 0; batch classifier loss: 0.005099; batch adversarial loss: 0.506111\n",
      "epoch 195; iter: 0; batch classifier loss: 0.032720; batch adversarial loss: 0.439792\n",
      "epoch 196; iter: 0; batch classifier loss: 0.003580; batch adversarial loss: 0.543955\n",
      "epoch 197; iter: 0; batch classifier loss: 0.005270; batch adversarial loss: 0.348013\n",
      "epoch 198; iter: 0; batch classifier loss: 0.025699; batch adversarial loss: 0.492210\n",
      "epoch 199; iter: 0; batch classifier loss: 0.016129; batch adversarial loss: 0.417473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:53:09.795963: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44b4e-ae24-11ee-be98-ef9b34f2853b/04a44b4e-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:8047 op device:{requested: '', assigned: ''} def:{{{node 04a44b4e-ae24-11ee-be98-ef9b34f2853b/04a44b4e-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44b4e-ae24-11ee-be98-ef9b34f2853b/04a44b4e-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44b4e-ae24-11ee-be98-ef9b34f2853b/04a44b4e-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.717400; batch adversarial loss: 0.574204\n",
      "epoch 1; iter: 0; batch classifier loss: 0.513925; batch adversarial loss: 0.639956\n",
      "epoch 2; iter: 0; batch classifier loss: 0.408154; batch adversarial loss: 0.604649\n",
      "epoch 3; iter: 0; batch classifier loss: 0.444995; batch adversarial loss: 0.638216\n",
      "epoch 4; iter: 0; batch classifier loss: 0.398584; batch adversarial loss: 0.597230\n",
      "epoch 5; iter: 0; batch classifier loss: 0.503972; batch adversarial loss: 0.595833\n",
      "epoch 6; iter: 0; batch classifier loss: 0.545734; batch adversarial loss: 0.567781\n",
      "epoch 7; iter: 0; batch classifier loss: 0.505216; batch adversarial loss: 0.634671\n",
      "epoch 8; iter: 0; batch classifier loss: 0.472872; batch adversarial loss: 0.568430\n",
      "epoch 9; iter: 0; batch classifier loss: 0.508103; batch adversarial loss: 0.523399\n",
      "epoch 10; iter: 0; batch classifier loss: 0.438267; batch adversarial loss: 0.516781\n",
      "epoch 11; iter: 0; batch classifier loss: 0.367353; batch adversarial loss: 0.451831\n",
      "epoch 12; iter: 0; batch classifier loss: 0.330810; batch adversarial loss: 0.522648\n",
      "epoch 13; iter: 0; batch classifier loss: 0.273857; batch adversarial loss: 0.442934\n",
      "epoch 14; iter: 0; batch classifier loss: 0.182377; batch adversarial loss: 0.520535\n",
      "epoch 15; iter: 0; batch classifier loss: 0.253107; batch adversarial loss: 0.460038\n",
      "epoch 16; iter: 0; batch classifier loss: 0.419048; batch adversarial loss: 0.501122\n",
      "epoch 17; iter: 0; batch classifier loss: 0.211194; batch adversarial loss: 0.503103\n",
      "epoch 18; iter: 0; batch classifier loss: 0.238502; batch adversarial loss: 0.402449\n",
      "epoch 19; iter: 0; batch classifier loss: 0.315281; batch adversarial loss: 0.450137\n",
      "epoch 20; iter: 0; batch classifier loss: 0.236058; batch adversarial loss: 0.538562\n",
      "epoch 21; iter: 0; batch classifier loss: 0.235120; batch adversarial loss: 0.464644\n",
      "epoch 22; iter: 0; batch classifier loss: 0.228100; batch adversarial loss: 0.446663\n",
      "epoch 23; iter: 0; batch classifier loss: 0.121744; batch adversarial loss: 0.465928\n",
      "epoch 24; iter: 0; batch classifier loss: 0.267011; batch adversarial loss: 0.504611\n",
      "epoch 25; iter: 0; batch classifier loss: 0.143978; batch adversarial loss: 0.435995\n",
      "epoch 26; iter: 0; batch classifier loss: 0.176477; batch adversarial loss: 0.455102\n",
      "epoch 27; iter: 0; batch classifier loss: 0.182082; batch adversarial loss: 0.527307\n",
      "epoch 28; iter: 0; batch classifier loss: 0.168482; batch adversarial loss: 0.462720\n",
      "epoch 29; iter: 0; batch classifier loss: 0.259446; batch adversarial loss: 0.427106\n",
      "epoch 30; iter: 0; batch classifier loss: 0.167650; batch adversarial loss: 0.510969\n",
      "epoch 31; iter: 0; batch classifier loss: 0.136182; batch adversarial loss: 0.521803\n",
      "epoch 32; iter: 0; batch classifier loss: 0.133603; batch adversarial loss: 0.374338\n",
      "epoch 33; iter: 0; batch classifier loss: 0.125797; batch adversarial loss: 0.447543\n",
      "epoch 34; iter: 0; batch classifier loss: 0.177482; batch adversarial loss: 0.411739\n",
      "epoch 35; iter: 0; batch classifier loss: 0.140970; batch adversarial loss: 0.470526\n",
      "epoch 36; iter: 0; batch classifier loss: 0.134420; batch adversarial loss: 0.444807\n",
      "epoch 37; iter: 0; batch classifier loss: 0.112007; batch adversarial loss: 0.452476\n",
      "epoch 38; iter: 0; batch classifier loss: 0.144000; batch adversarial loss: 0.454149\n",
      "epoch 39; iter: 0; batch classifier loss: 0.127071; batch adversarial loss: 0.385469\n",
      "epoch 40; iter: 0; batch classifier loss: 0.195554; batch adversarial loss: 0.356207\n",
      "epoch 41; iter: 0; batch classifier loss: 0.168171; batch adversarial loss: 0.454021\n",
      "epoch 42; iter: 0; batch classifier loss: 0.162767; batch adversarial loss: 0.577005\n",
      "epoch 43; iter: 0; batch classifier loss: 0.176206; batch adversarial loss: 0.508327\n",
      "epoch 44; iter: 0; batch classifier loss: 0.121958; batch adversarial loss: 0.405655\n",
      "epoch 45; iter: 0; batch classifier loss: 0.202149; batch adversarial loss: 0.508358\n",
      "epoch 46; iter: 0; batch classifier loss: 0.110806; batch adversarial loss: 0.367496\n",
      "epoch 47; iter: 0; batch classifier loss: 0.168109; batch adversarial loss: 0.444048\n",
      "epoch 48; iter: 0; batch classifier loss: 0.110237; batch adversarial loss: 0.570594\n",
      "epoch 49; iter: 0; batch classifier loss: 0.142323; batch adversarial loss: 0.555859\n",
      "epoch 50; iter: 0; batch classifier loss: 0.114378; batch adversarial loss: 0.445823\n",
      "epoch 51; iter: 0; batch classifier loss: 0.083506; batch adversarial loss: 0.427901\n",
      "epoch 52; iter: 0; batch classifier loss: 0.100984; batch adversarial loss: 0.460126\n",
      "epoch 53; iter: 0; batch classifier loss: 0.107977; batch adversarial loss: 0.450661\n",
      "epoch 54; iter: 0; batch classifier loss: 0.123911; batch adversarial loss: 0.483487\n",
      "epoch 55; iter: 0; batch classifier loss: 0.133519; batch adversarial loss: 0.554805\n",
      "epoch 56; iter: 0; batch classifier loss: 0.104734; batch adversarial loss: 0.427146\n",
      "epoch 57; iter: 0; batch classifier loss: 0.205184; batch adversarial loss: 0.368118\n",
      "epoch 58; iter: 0; batch classifier loss: 0.165106; batch adversarial loss: 0.407255\n",
      "epoch 59; iter: 0; batch classifier loss: 0.099801; batch adversarial loss: 0.478413\n",
      "epoch 60; iter: 0; batch classifier loss: 0.136993; batch adversarial loss: 0.464506\n",
      "epoch 61; iter: 0; batch classifier loss: 0.139727; batch adversarial loss: 0.573254\n",
      "epoch 62; iter: 0; batch classifier loss: 0.191586; batch adversarial loss: 0.368625\n",
      "epoch 63; iter: 0; batch classifier loss: 0.104358; batch adversarial loss: 0.433779\n",
      "epoch 64; iter: 0; batch classifier loss: 0.132338; batch adversarial loss: 0.491611\n",
      "epoch 65; iter: 0; batch classifier loss: 0.143644; batch adversarial loss: 0.575528\n",
      "epoch 66; iter: 0; batch classifier loss: 0.091088; batch adversarial loss: 0.392280\n",
      "epoch 67; iter: 0; batch classifier loss: 0.115763; batch adversarial loss: 0.508436\n",
      "epoch 68; iter: 0; batch classifier loss: 0.166996; batch adversarial loss: 0.391436\n",
      "epoch 69; iter: 0; batch classifier loss: 0.129914; batch adversarial loss: 0.431809\n",
      "epoch 70; iter: 0; batch classifier loss: 0.110672; batch adversarial loss: 0.520370\n",
      "epoch 71; iter: 0; batch classifier loss: 0.111539; batch adversarial loss: 0.400416\n",
      "epoch 72; iter: 0; batch classifier loss: 0.103522; batch adversarial loss: 0.467072\n",
      "epoch 73; iter: 0; batch classifier loss: 0.098266; batch adversarial loss: 0.462310\n",
      "epoch 74; iter: 0; batch classifier loss: 0.120000; batch adversarial loss: 0.491513\n",
      "epoch 75; iter: 0; batch classifier loss: 0.116802; batch adversarial loss: 0.548497\n",
      "epoch 76; iter: 0; batch classifier loss: 0.114005; batch adversarial loss: 0.484385\n",
      "epoch 77; iter: 0; batch classifier loss: 0.130081; batch adversarial loss: 0.418573\n",
      "epoch 78; iter: 0; batch classifier loss: 0.155914; batch adversarial loss: 0.405639\n",
      "epoch 79; iter: 0; batch classifier loss: 0.159407; batch adversarial loss: 0.443184\n",
      "epoch 80; iter: 0; batch classifier loss: 0.123908; batch adversarial loss: 0.394593\n",
      "epoch 81; iter: 0; batch classifier loss: 0.148014; batch adversarial loss: 0.521213\n",
      "epoch 82; iter: 0; batch classifier loss: 0.086637; batch adversarial loss: 0.439099\n",
      "epoch 83; iter: 0; batch classifier loss: 0.123908; batch adversarial loss: 0.452195\n",
      "epoch 84; iter: 0; batch classifier loss: 0.102649; batch adversarial loss: 0.504450\n",
      "epoch 85; iter: 0; batch classifier loss: 0.127852; batch adversarial loss: 0.405270\n",
      "epoch 86; iter: 0; batch classifier loss: 0.146526; batch adversarial loss: 0.431269\n",
      "epoch 87; iter: 0; batch classifier loss: 0.101114; batch adversarial loss: 0.426574\n",
      "epoch 88; iter: 0; batch classifier loss: 0.091884; batch adversarial loss: 0.542754\n",
      "epoch 89; iter: 0; batch classifier loss: 0.066685; batch adversarial loss: 0.508276\n",
      "epoch 90; iter: 0; batch classifier loss: 0.090460; batch adversarial loss: 0.445994\n",
      "epoch 91; iter: 0; batch classifier loss: 0.053242; batch adversarial loss: 0.415740\n",
      "epoch 92; iter: 0; batch classifier loss: 0.124224; batch adversarial loss: 0.557653\n",
      "epoch 93; iter: 0; batch classifier loss: 0.104871; batch adversarial loss: 0.402853\n",
      "epoch 94; iter: 0; batch classifier loss: 0.112521; batch adversarial loss: 0.474090\n",
      "epoch 95; iter: 0; batch classifier loss: 0.080695; batch adversarial loss: 0.464466\n",
      "epoch 96; iter: 0; batch classifier loss: 0.083011; batch adversarial loss: 0.341153\n",
      "epoch 97; iter: 0; batch classifier loss: 0.091414; batch adversarial loss: 0.524689\n",
      "epoch 98; iter: 0; batch classifier loss: 0.060429; batch adversarial loss: 0.388775\n",
      "epoch 99; iter: 0; batch classifier loss: 0.128115; batch adversarial loss: 0.404462\n",
      "epoch 100; iter: 0; batch classifier loss: 0.045668; batch adversarial loss: 0.461220\n",
      "epoch 101; iter: 0; batch classifier loss: 0.075066; batch adversarial loss: 0.393494\n",
      "epoch 102; iter: 0; batch classifier loss: 0.054458; batch adversarial loss: 0.342120\n",
      "epoch 103; iter: 0; batch classifier loss: 0.098681; batch adversarial loss: 0.453038\n",
      "epoch 104; iter: 0; batch classifier loss: 0.033932; batch adversarial loss: 0.408106\n",
      "epoch 105; iter: 0; batch classifier loss: 0.085406; batch adversarial loss: 0.444660\n",
      "epoch 106; iter: 0; batch classifier loss: 0.077877; batch adversarial loss: 0.426850\n",
      "epoch 107; iter: 0; batch classifier loss: 0.045085; batch adversarial loss: 0.476344\n",
      "epoch 108; iter: 0; batch classifier loss: 0.076483; batch adversarial loss: 0.464910\n",
      "epoch 109; iter: 0; batch classifier loss: 0.046186; batch adversarial loss: 0.473949\n",
      "epoch 110; iter: 0; batch classifier loss: 0.069983; batch adversarial loss: 0.350408\n",
      "epoch 111; iter: 0; batch classifier loss: 0.072425; batch adversarial loss: 0.417050\n",
      "epoch 112; iter: 0; batch classifier loss: 0.035874; batch adversarial loss: 0.458792\n",
      "epoch 113; iter: 0; batch classifier loss: 0.038199; batch adversarial loss: 0.448669\n",
      "epoch 114; iter: 0; batch classifier loss: 0.022588; batch adversarial loss: 0.488221\n",
      "epoch 115; iter: 0; batch classifier loss: 0.058229; batch adversarial loss: 0.437277\n",
      "epoch 116; iter: 0; batch classifier loss: 0.021005; batch adversarial loss: 0.518840\n",
      "epoch 117; iter: 0; batch classifier loss: 0.035517; batch adversarial loss: 0.607610\n",
      "epoch 118; iter: 0; batch classifier loss: 0.030386; batch adversarial loss: 0.421011\n",
      "epoch 119; iter: 0; batch classifier loss: 0.018755; batch adversarial loss: 0.427886\n",
      "epoch 120; iter: 0; batch classifier loss: 0.021058; batch adversarial loss: 0.438298\n",
      "epoch 121; iter: 0; batch classifier loss: 0.053253; batch adversarial loss: 0.465143\n",
      "epoch 122; iter: 0; batch classifier loss: 0.072365; batch adversarial loss: 0.550298\n",
      "epoch 123; iter: 0; batch classifier loss: 0.018631; batch adversarial loss: 0.433924\n",
      "epoch 124; iter: 0; batch classifier loss: 0.039823; batch adversarial loss: 0.451249\n",
      "epoch 125; iter: 0; batch classifier loss: 0.011473; batch adversarial loss: 0.510260\n",
      "epoch 126; iter: 0; batch classifier loss: 0.016740; batch adversarial loss: 0.633558\n",
      "epoch 127; iter: 0; batch classifier loss: 0.038840; batch adversarial loss: 0.495630\n",
      "epoch 128; iter: 0; batch classifier loss: 0.021312; batch adversarial loss: 0.477084\n",
      "epoch 129; iter: 0; batch classifier loss: 0.031306; batch adversarial loss: 0.513171\n",
      "epoch 130; iter: 0; batch classifier loss: 0.048722; batch adversarial loss: 0.436726\n",
      "epoch 131; iter: 0; batch classifier loss: 0.068165; batch adversarial loss: 0.360012\n",
      "epoch 132; iter: 0; batch classifier loss: 0.016616; batch adversarial loss: 0.479100\n",
      "epoch 133; iter: 0; batch classifier loss: 0.034077; batch adversarial loss: 0.545149\n",
      "epoch 134; iter: 0; batch classifier loss: 0.018824; batch adversarial loss: 0.472322\n",
      "epoch 135; iter: 0; batch classifier loss: 0.015097; batch adversarial loss: 0.429287\n",
      "epoch 136; iter: 0; batch classifier loss: 0.019034; batch adversarial loss: 0.419519\n",
      "epoch 137; iter: 0; batch classifier loss: 0.041624; batch adversarial loss: 0.390103\n",
      "epoch 138; iter: 0; batch classifier loss: 0.023111; batch adversarial loss: 0.430119\n",
      "epoch 139; iter: 0; batch classifier loss: 0.020284; batch adversarial loss: 0.342669\n",
      "epoch 140; iter: 0; batch classifier loss: 0.035561; batch adversarial loss: 0.538397\n",
      "epoch 141; iter: 0; batch classifier loss: 0.025233; batch adversarial loss: 0.551674\n",
      "epoch 142; iter: 0; batch classifier loss: 0.021175; batch adversarial loss: 0.513725\n",
      "epoch 143; iter: 0; batch classifier loss: 0.015299; batch adversarial loss: 0.479766\n",
      "epoch 144; iter: 0; batch classifier loss: 0.020482; batch adversarial loss: 0.458611\n",
      "epoch 145; iter: 0; batch classifier loss: 0.078488; batch adversarial loss: 0.399020\n",
      "epoch 146; iter: 0; batch classifier loss: 0.007686; batch adversarial loss: 0.444644\n",
      "epoch 147; iter: 0; batch classifier loss: 0.018996; batch adversarial loss: 0.472114\n",
      "epoch 148; iter: 0; batch classifier loss: 0.024611; batch adversarial loss: 0.485187\n",
      "epoch 149; iter: 0; batch classifier loss: 0.037878; batch adversarial loss: 0.487363\n",
      "epoch 150; iter: 0; batch classifier loss: 0.025060; batch adversarial loss: 0.473277\n",
      "epoch 151; iter: 0; batch classifier loss: 0.041281; batch adversarial loss: 0.457905\n",
      "epoch 152; iter: 0; batch classifier loss: 0.013479; batch adversarial loss: 0.501610\n",
      "epoch 153; iter: 0; batch classifier loss: 0.028854; batch adversarial loss: 0.488946\n",
      "epoch 154; iter: 0; batch classifier loss: 0.044891; batch adversarial loss: 0.451551\n",
      "epoch 155; iter: 0; batch classifier loss: 0.017691; batch adversarial loss: 0.461792\n",
      "epoch 156; iter: 0; batch classifier loss: 0.024764; batch adversarial loss: 0.440089\n",
      "epoch 157; iter: 0; batch classifier loss: 0.013085; batch adversarial loss: 0.408105\n",
      "epoch 158; iter: 0; batch classifier loss: 0.043904; batch adversarial loss: 0.464140\n",
      "epoch 159; iter: 0; batch classifier loss: 0.039879; batch adversarial loss: 0.423497\n",
      "epoch 160; iter: 0; batch classifier loss: 0.022290; batch adversarial loss: 0.426538\n",
      "epoch 161; iter: 0; batch classifier loss: 0.027830; batch adversarial loss: 0.396986\n",
      "epoch 162; iter: 0; batch classifier loss: 0.013008; batch adversarial loss: 0.470901\n",
      "epoch 163; iter: 0; batch classifier loss: 0.008657; batch adversarial loss: 0.404670\n",
      "epoch 164; iter: 0; batch classifier loss: 0.019534; batch adversarial loss: 0.444628\n",
      "epoch 165; iter: 0; batch classifier loss: 0.036387; batch adversarial loss: 0.516780\n",
      "epoch 166; iter: 0; batch classifier loss: 0.024676; batch adversarial loss: 0.424909\n",
      "epoch 167; iter: 0; batch classifier loss: 0.008627; batch adversarial loss: 0.405266\n",
      "epoch 168; iter: 0; batch classifier loss: 0.019662; batch adversarial loss: 0.449022\n",
      "epoch 169; iter: 0; batch classifier loss: 0.031366; batch adversarial loss: 0.505850\n",
      "epoch 170; iter: 0; batch classifier loss: 0.013947; batch adversarial loss: 0.476491\n",
      "epoch 171; iter: 0; batch classifier loss: 0.004694; batch adversarial loss: 0.458005\n",
      "epoch 172; iter: 0; batch classifier loss: 0.035585; batch adversarial loss: 0.503636\n",
      "epoch 173; iter: 0; batch classifier loss: 0.024527; batch adversarial loss: 0.536133\n",
      "epoch 174; iter: 0; batch classifier loss: 0.019639; batch adversarial loss: 0.516976\n",
      "epoch 175; iter: 0; batch classifier loss: 0.014001; batch adversarial loss: 0.461102\n",
      "epoch 176; iter: 0; batch classifier loss: 0.015071; batch adversarial loss: 0.411804\n",
      "epoch 177; iter: 0; batch classifier loss: 0.058697; batch adversarial loss: 0.428729\n",
      "epoch 178; iter: 0; batch classifier loss: 0.033247; batch adversarial loss: 0.436345\n",
      "epoch 179; iter: 0; batch classifier loss: 0.006299; batch adversarial loss: 0.461711\n",
      "epoch 180; iter: 0; batch classifier loss: 0.004940; batch adversarial loss: 0.438481\n",
      "epoch 181; iter: 0; batch classifier loss: 0.022206; batch adversarial loss: 0.436962\n",
      "epoch 182; iter: 0; batch classifier loss: 0.021535; batch adversarial loss: 0.428681\n",
      "epoch 183; iter: 0; batch classifier loss: 0.030089; batch adversarial loss: 0.442688\n",
      "epoch 184; iter: 0; batch classifier loss: 0.012840; batch adversarial loss: 0.487105\n",
      "epoch 185; iter: 0; batch classifier loss: 0.026761; batch adversarial loss: 0.436671\n",
      "epoch 186; iter: 0; batch classifier loss: 0.016815; batch adversarial loss: 0.408216\n",
      "epoch 187; iter: 0; batch classifier loss: 0.010477; batch adversarial loss: 0.393187\n",
      "epoch 188; iter: 0; batch classifier loss: 0.005979; batch adversarial loss: 0.574045\n",
      "epoch 189; iter: 0; batch classifier loss: 0.019919; batch adversarial loss: 0.503234\n",
      "epoch 190; iter: 0; batch classifier loss: 0.026746; batch adversarial loss: 0.443980\n",
      "epoch 191; iter: 0; batch classifier loss: 0.011589; batch adversarial loss: 0.432873\n",
      "epoch 192; iter: 0; batch classifier loss: 0.020223; batch adversarial loss: 0.375319\n",
      "epoch 193; iter: 0; batch classifier loss: 0.017699; batch adversarial loss: 0.444835\n",
      "epoch 194; iter: 0; batch classifier loss: 0.005758; batch adversarial loss: 0.458976\n",
      "epoch 195; iter: 0; batch classifier loss: 0.050809; batch adversarial loss: 0.433629\n",
      "epoch 196; iter: 0; batch classifier loss: 0.026747; batch adversarial loss: 0.448372\n",
      "epoch 197; iter: 0; batch classifier loss: 0.008582; batch adversarial loss: 0.409427\n",
      "epoch 198; iter: 0; batch classifier loss: 0.028301; batch adversarial loss: 0.421145\n",
      "epoch 199; iter: 0; batch classifier loss: 0.010759; batch adversarial loss: 0.429698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:53:54.351819: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44bda-ae24-11ee-be98-ef9b34f2853b/04a44bda-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:8854 op device:{requested: '', assigned: ''} def:{{{node 04a44bda-ae24-11ee-be98-ef9b34f2853b/04a44bda-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44bda-ae24-11ee-be98-ef9b34f2853b/04a44bda-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44bda-ae24-11ee-be98-ef9b34f2853b/04a44bda-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.679535; batch adversarial loss: 0.584821\n",
      "epoch 1; iter: 0; batch classifier loss: 0.465717; batch adversarial loss: 0.581241\n",
      "epoch 2; iter: 0; batch classifier loss: 0.388760; batch adversarial loss: 0.586548\n",
      "epoch 3; iter: 0; batch classifier loss: 0.375985; batch adversarial loss: 0.571179\n",
      "epoch 4; iter: 0; batch classifier loss: 0.282448; batch adversarial loss: 0.570565\n",
      "epoch 5; iter: 0; batch classifier loss: 0.380238; batch adversarial loss: 0.571597\n",
      "epoch 6; iter: 0; batch classifier loss: 0.350073; batch adversarial loss: 0.589570\n",
      "epoch 7; iter: 0; batch classifier loss: 0.262419; batch adversarial loss: 0.533590\n",
      "epoch 8; iter: 0; batch classifier loss: 0.298742; batch adversarial loss: 0.606175\n",
      "epoch 9; iter: 0; batch classifier loss: 0.347328; batch adversarial loss: 0.549068\n",
      "epoch 10; iter: 0; batch classifier loss: 0.230923; batch adversarial loss: 0.622033\n",
      "epoch 11; iter: 0; batch classifier loss: 0.265777; batch adversarial loss: 0.574754\n",
      "epoch 12; iter: 0; batch classifier loss: 0.330586; batch adversarial loss: 0.538386\n",
      "epoch 13; iter: 0; batch classifier loss: 0.286923; batch adversarial loss: 0.516927\n",
      "epoch 14; iter: 0; batch classifier loss: 0.417872; batch adversarial loss: 0.652987\n",
      "epoch 15; iter: 0; batch classifier loss: 0.308015; batch adversarial loss: 0.509610\n",
      "epoch 16; iter: 0; batch classifier loss: 0.329177; batch adversarial loss: 0.483816\n",
      "epoch 17; iter: 0; batch classifier loss: 0.521802; batch adversarial loss: 0.466479\n",
      "epoch 18; iter: 0; batch classifier loss: 0.430248; batch adversarial loss: 0.482123\n",
      "epoch 19; iter: 0; batch classifier loss: 0.505449; batch adversarial loss: 0.443442\n",
      "epoch 20; iter: 0; batch classifier loss: 0.247009; batch adversarial loss: 0.417987\n",
      "epoch 21; iter: 0; batch classifier loss: 0.204741; batch adversarial loss: 0.456604\n",
      "epoch 22; iter: 0; batch classifier loss: 0.216818; batch adversarial loss: 0.535955\n",
      "epoch 23; iter: 0; batch classifier loss: 0.212069; batch adversarial loss: 0.549524\n",
      "epoch 24; iter: 0; batch classifier loss: 0.217507; batch adversarial loss: 0.420464\n",
      "epoch 25; iter: 0; batch classifier loss: 0.172737; batch adversarial loss: 0.471871\n",
      "epoch 26; iter: 0; batch classifier loss: 0.202687; batch adversarial loss: 0.445726\n",
      "epoch 27; iter: 0; batch classifier loss: 0.193506; batch adversarial loss: 0.511117\n",
      "epoch 28; iter: 0; batch classifier loss: 0.136667; batch adversarial loss: 0.480544\n",
      "epoch 29; iter: 0; batch classifier loss: 0.144544; batch adversarial loss: 0.508069\n",
      "epoch 30; iter: 0; batch classifier loss: 0.138689; batch adversarial loss: 0.436832\n",
      "epoch 31; iter: 0; batch classifier loss: 0.113951; batch adversarial loss: 0.410293\n",
      "epoch 32; iter: 0; batch classifier loss: 0.187141; batch adversarial loss: 0.483146\n",
      "epoch 33; iter: 0; batch classifier loss: 0.093474; batch adversarial loss: 0.492227\n",
      "epoch 34; iter: 0; batch classifier loss: 0.134697; batch adversarial loss: 0.520785\n",
      "epoch 35; iter: 0; batch classifier loss: 0.114113; batch adversarial loss: 0.417871\n",
      "epoch 36; iter: 0; batch classifier loss: 0.111733; batch adversarial loss: 0.441463\n",
      "epoch 37; iter: 0; batch classifier loss: 0.136193; batch adversarial loss: 0.464014\n",
      "epoch 38; iter: 0; batch classifier loss: 0.082109; batch adversarial loss: 0.543944\n",
      "epoch 39; iter: 0; batch classifier loss: 0.089364; batch adversarial loss: 0.483150\n",
      "epoch 40; iter: 0; batch classifier loss: 0.102005; batch adversarial loss: 0.454894\n",
      "epoch 41; iter: 0; batch classifier loss: 0.177673; batch adversarial loss: 0.420737\n",
      "epoch 42; iter: 0; batch classifier loss: 0.120934; batch adversarial loss: 0.431405\n",
      "epoch 43; iter: 0; batch classifier loss: 0.158235; batch adversarial loss: 0.545582\n",
      "epoch 44; iter: 0; batch classifier loss: 0.148415; batch adversarial loss: 0.497401\n",
      "epoch 45; iter: 0; batch classifier loss: 0.110685; batch adversarial loss: 0.470701\n",
      "epoch 46; iter: 0; batch classifier loss: 0.127034; batch adversarial loss: 0.358630\n",
      "epoch 47; iter: 0; batch classifier loss: 0.123874; batch adversarial loss: 0.550605\n",
      "epoch 48; iter: 0; batch classifier loss: 0.106341; batch adversarial loss: 0.450733\n",
      "epoch 49; iter: 0; batch classifier loss: 0.106492; batch adversarial loss: 0.436910\n",
      "epoch 50; iter: 0; batch classifier loss: 0.108128; batch adversarial loss: 0.464279\n",
      "epoch 51; iter: 0; batch classifier loss: 0.128973; batch adversarial loss: 0.543767\n",
      "epoch 52; iter: 0; batch classifier loss: 0.118026; batch adversarial loss: 0.517559\n",
      "epoch 53; iter: 0; batch classifier loss: 0.114258; batch adversarial loss: 0.536567\n",
      "epoch 54; iter: 0; batch classifier loss: 0.115402; batch adversarial loss: 0.492350\n",
      "epoch 55; iter: 0; batch classifier loss: 0.104334; batch adversarial loss: 0.405796\n",
      "epoch 56; iter: 0; batch classifier loss: 0.128317; batch adversarial loss: 0.475840\n",
      "epoch 57; iter: 0; batch classifier loss: 0.108677; batch adversarial loss: 0.515003\n",
      "epoch 58; iter: 0; batch classifier loss: 0.159572; batch adversarial loss: 0.364673\n",
      "epoch 59; iter: 0; batch classifier loss: 0.117869; batch adversarial loss: 0.433051\n",
      "epoch 60; iter: 0; batch classifier loss: 0.147688; batch adversarial loss: 0.469854\n",
      "epoch 61; iter: 0; batch classifier loss: 0.086747; batch adversarial loss: 0.539705\n",
      "epoch 62; iter: 0; batch classifier loss: 0.153112; batch adversarial loss: 0.469561\n",
      "epoch 63; iter: 0; batch classifier loss: 0.118385; batch adversarial loss: 0.441648\n",
      "epoch 64; iter: 0; batch classifier loss: 0.120374; batch adversarial loss: 0.465031\n",
      "epoch 65; iter: 0; batch classifier loss: 0.126606; batch adversarial loss: 0.451157\n",
      "epoch 66; iter: 0; batch classifier loss: 0.129446; batch adversarial loss: 0.537425\n",
      "epoch 67; iter: 0; batch classifier loss: 0.147263; batch adversarial loss: 0.360763\n",
      "epoch 68; iter: 0; batch classifier loss: 0.126757; batch adversarial loss: 0.482558\n",
      "epoch 69; iter: 0; batch classifier loss: 0.143882; batch adversarial loss: 0.428813\n",
      "epoch 70; iter: 0; batch classifier loss: 0.139965; batch adversarial loss: 0.364154\n",
      "epoch 71; iter: 0; batch classifier loss: 0.173931; batch adversarial loss: 0.483440\n",
      "epoch 72; iter: 0; batch classifier loss: 0.104551; batch adversarial loss: 0.420627\n",
      "epoch 73; iter: 0; batch classifier loss: 0.143712; batch adversarial loss: 0.362673\n",
      "epoch 74; iter: 0; batch classifier loss: 0.141157; batch adversarial loss: 0.545294\n",
      "epoch 75; iter: 0; batch classifier loss: 0.176088; batch adversarial loss: 0.443131\n",
      "epoch 76; iter: 0; batch classifier loss: 0.111117; batch adversarial loss: 0.529781\n",
      "epoch 77; iter: 0; batch classifier loss: 0.183250; batch adversarial loss: 0.449923\n",
      "epoch 78; iter: 0; batch classifier loss: 0.115755; batch adversarial loss: 0.389953\n",
      "epoch 79; iter: 0; batch classifier loss: 0.081490; batch adversarial loss: 0.436524\n",
      "epoch 80; iter: 0; batch classifier loss: 0.083983; batch adversarial loss: 0.543918\n",
      "epoch 81; iter: 0; batch classifier loss: 0.137381; batch adversarial loss: 0.511080\n",
      "epoch 82; iter: 0; batch classifier loss: 0.059537; batch adversarial loss: 0.485011\n",
      "epoch 83; iter: 0; batch classifier loss: 0.101514; batch adversarial loss: 0.469037\n",
      "epoch 84; iter: 0; batch classifier loss: 0.102181; batch adversarial loss: 0.551090\n",
      "epoch 85; iter: 0; batch classifier loss: 0.070990; batch adversarial loss: 0.445660\n",
      "epoch 86; iter: 0; batch classifier loss: 0.075281; batch adversarial loss: 0.522876\n",
      "epoch 87; iter: 0; batch classifier loss: 0.079833; batch adversarial loss: 0.410670\n",
      "epoch 88; iter: 0; batch classifier loss: 0.101447; batch adversarial loss: 0.445483\n",
      "epoch 89; iter: 0; batch classifier loss: 0.089807; batch adversarial loss: 0.356680\n",
      "epoch 90; iter: 0; batch classifier loss: 0.115786; batch adversarial loss: 0.441706\n",
      "epoch 91; iter: 0; batch classifier loss: 0.065847; batch adversarial loss: 0.524996\n",
      "epoch 92; iter: 0; batch classifier loss: 0.104784; batch adversarial loss: 0.470563\n",
      "epoch 93; iter: 0; batch classifier loss: 0.144650; batch adversarial loss: 0.502807\n",
      "epoch 94; iter: 0; batch classifier loss: 0.093625; batch adversarial loss: 0.451478\n",
      "epoch 95; iter: 0; batch classifier loss: 0.080438; batch adversarial loss: 0.448218\n",
      "epoch 96; iter: 0; batch classifier loss: 0.138930; batch adversarial loss: 0.481387\n",
      "epoch 97; iter: 0; batch classifier loss: 0.093637; batch adversarial loss: 0.441399\n",
      "epoch 98; iter: 0; batch classifier loss: 0.120956; batch adversarial loss: 0.411591\n",
      "epoch 99; iter: 0; batch classifier loss: 0.113504; batch adversarial loss: 0.502126\n",
      "epoch 100; iter: 0; batch classifier loss: 0.079159; batch adversarial loss: 0.508528\n",
      "epoch 101; iter: 0; batch classifier loss: 0.070119; batch adversarial loss: 0.499065\n",
      "epoch 102; iter: 0; batch classifier loss: 0.089295; batch adversarial loss: 0.450113\n",
      "epoch 103; iter: 0; batch classifier loss: 0.033581; batch adversarial loss: 0.449846\n",
      "epoch 104; iter: 0; batch classifier loss: 0.080683; batch adversarial loss: 0.398519\n",
      "epoch 105; iter: 0; batch classifier loss: 0.066895; batch adversarial loss: 0.582357\n",
      "epoch 106; iter: 0; batch classifier loss: 0.099608; batch adversarial loss: 0.349298\n",
      "epoch 107; iter: 0; batch classifier loss: 0.062471; batch adversarial loss: 0.440848\n",
      "epoch 108; iter: 0; batch classifier loss: 0.072609; batch adversarial loss: 0.513332\n",
      "epoch 109; iter: 0; batch classifier loss: 0.082990; batch adversarial loss: 0.460879\n",
      "epoch 110; iter: 0; batch classifier loss: 0.049558; batch adversarial loss: 0.387267\n",
      "epoch 111; iter: 0; batch classifier loss: 0.059844; batch adversarial loss: 0.482038\n",
      "epoch 112; iter: 0; batch classifier loss: 0.079004; batch adversarial loss: 0.418549\n",
      "epoch 113; iter: 0; batch classifier loss: 0.080856; batch adversarial loss: 0.346043\n",
      "epoch 114; iter: 0; batch classifier loss: 0.052362; batch adversarial loss: 0.404611\n",
      "epoch 115; iter: 0; batch classifier loss: 0.066913; batch adversarial loss: 0.440122\n",
      "epoch 116; iter: 0; batch classifier loss: 0.077036; batch adversarial loss: 0.442665\n",
      "epoch 117; iter: 0; batch classifier loss: 0.121575; batch adversarial loss: 0.474932\n",
      "epoch 118; iter: 0; batch classifier loss: 0.044459; batch adversarial loss: 0.399735\n",
      "epoch 119; iter: 0; batch classifier loss: 0.026305; batch adversarial loss: 0.364238\n",
      "epoch 120; iter: 0; batch classifier loss: 0.101076; batch adversarial loss: 0.488269\n",
      "epoch 121; iter: 0; batch classifier loss: 0.060515; batch adversarial loss: 0.450790\n",
      "epoch 122; iter: 0; batch classifier loss: 0.054761; batch adversarial loss: 0.427711\n",
      "epoch 123; iter: 0; batch classifier loss: 0.073530; batch adversarial loss: 0.455848\n",
      "epoch 124; iter: 0; batch classifier loss: 0.028281; batch adversarial loss: 0.456030\n",
      "epoch 125; iter: 0; batch classifier loss: 0.036493; batch adversarial loss: 0.406457\n",
      "epoch 126; iter: 0; batch classifier loss: 0.054293; batch adversarial loss: 0.495978\n",
      "epoch 127; iter: 0; batch classifier loss: 0.039429; batch adversarial loss: 0.473323\n",
      "epoch 128; iter: 0; batch classifier loss: 0.039468; batch adversarial loss: 0.432937\n",
      "epoch 129; iter: 0; batch classifier loss: 0.024886; batch adversarial loss: 0.451244\n",
      "epoch 130; iter: 0; batch classifier loss: 0.019978; batch adversarial loss: 0.480034\n",
      "epoch 131; iter: 0; batch classifier loss: 0.037951; batch adversarial loss: 0.388287\n",
      "epoch 132; iter: 0; batch classifier loss: 0.066105; batch adversarial loss: 0.483203\n",
      "epoch 133; iter: 0; batch classifier loss: 0.036842; batch adversarial loss: 0.335711\n",
      "epoch 134; iter: 0; batch classifier loss: 0.022607; batch adversarial loss: 0.486859\n",
      "epoch 135; iter: 0; batch classifier loss: 0.027475; batch adversarial loss: 0.461278\n",
      "epoch 136; iter: 0; batch classifier loss: 0.027910; batch adversarial loss: 0.334747\n",
      "epoch 137; iter: 0; batch classifier loss: 0.045433; batch adversarial loss: 0.457469\n",
      "epoch 138; iter: 0; batch classifier loss: 0.065296; batch adversarial loss: 0.424626\n",
      "epoch 139; iter: 0; batch classifier loss: 0.036901; batch adversarial loss: 0.490626\n",
      "epoch 140; iter: 0; batch classifier loss: 0.051220; batch adversarial loss: 0.448227\n",
      "epoch 141; iter: 0; batch classifier loss: 0.022773; batch adversarial loss: 0.433997\n",
      "epoch 142; iter: 0; batch classifier loss: 0.038549; batch adversarial loss: 0.446320\n",
      "epoch 143; iter: 0; batch classifier loss: 0.020266; batch adversarial loss: 0.456949\n",
      "epoch 144; iter: 0; batch classifier loss: 0.035189; batch adversarial loss: 0.344524\n",
      "epoch 145; iter: 0; batch classifier loss: 0.048464; batch adversarial loss: 0.578404\n",
      "epoch 146; iter: 0; batch classifier loss: 0.027775; batch adversarial loss: 0.445859\n",
      "epoch 147; iter: 0; batch classifier loss: 0.030014; batch adversarial loss: 0.587379\n",
      "epoch 148; iter: 0; batch classifier loss: 0.069814; batch adversarial loss: 0.433978\n",
      "epoch 149; iter: 0; batch classifier loss: 0.024878; batch adversarial loss: 0.384782\n",
      "epoch 150; iter: 0; batch classifier loss: 0.029134; batch adversarial loss: 0.455989\n",
      "epoch 151; iter: 0; batch classifier loss: 0.019165; batch adversarial loss: 0.340293\n",
      "epoch 152; iter: 0; batch classifier loss: 0.020860; batch adversarial loss: 0.502885\n",
      "epoch 153; iter: 0; batch classifier loss: 0.015491; batch adversarial loss: 0.388083\n",
      "epoch 154; iter: 0; batch classifier loss: 0.031398; batch adversarial loss: 0.460039\n",
      "epoch 155; iter: 0; batch classifier loss: 0.036260; batch adversarial loss: 0.427612\n",
      "epoch 156; iter: 0; batch classifier loss: 0.026931; batch adversarial loss: 0.445274\n",
      "epoch 157; iter: 0; batch classifier loss: 0.016359; batch adversarial loss: 0.575218\n",
      "epoch 158; iter: 0; batch classifier loss: 0.027639; batch adversarial loss: 0.421642\n",
      "epoch 159; iter: 0; batch classifier loss: 0.013721; batch adversarial loss: 0.422245\n",
      "epoch 160; iter: 0; batch classifier loss: 0.028583; batch adversarial loss: 0.492847\n",
      "epoch 161; iter: 0; batch classifier loss: 0.017512; batch adversarial loss: 0.467281\n",
      "epoch 162; iter: 0; batch classifier loss: 0.057892; batch adversarial loss: 0.529325\n",
      "epoch 163; iter: 0; batch classifier loss: 0.011437; batch adversarial loss: 0.359645\n",
      "epoch 164; iter: 0; batch classifier loss: 0.054052; batch adversarial loss: 0.389724\n",
      "epoch 165; iter: 0; batch classifier loss: 0.020406; batch adversarial loss: 0.413420\n",
      "epoch 166; iter: 0; batch classifier loss: 0.010748; batch adversarial loss: 0.476473\n",
      "epoch 167; iter: 0; batch classifier loss: 0.023901; batch adversarial loss: 0.466930\n",
      "epoch 168; iter: 0; batch classifier loss: 0.019994; batch adversarial loss: 0.461608\n",
      "epoch 169; iter: 0; batch classifier loss: 0.034975; batch adversarial loss: 0.490778\n",
      "epoch 170; iter: 0; batch classifier loss: 0.025185; batch adversarial loss: 0.488516\n",
      "epoch 171; iter: 0; batch classifier loss: 0.037289; batch adversarial loss: 0.364459\n",
      "epoch 172; iter: 0; batch classifier loss: 0.018228; batch adversarial loss: 0.499730\n",
      "epoch 173; iter: 0; batch classifier loss: 0.029421; batch adversarial loss: 0.353629\n",
      "epoch 174; iter: 0; batch classifier loss: 0.015898; batch adversarial loss: 0.415837\n",
      "epoch 175; iter: 0; batch classifier loss: 0.022618; batch adversarial loss: 0.470178\n",
      "epoch 176; iter: 0; batch classifier loss: 0.020778; batch adversarial loss: 0.513651\n",
      "epoch 177; iter: 0; batch classifier loss: 0.020028; batch adversarial loss: 0.406852\n",
      "epoch 178; iter: 0; batch classifier loss: 0.042225; batch adversarial loss: 0.463796\n",
      "epoch 179; iter: 0; batch classifier loss: 0.019504; batch adversarial loss: 0.479012\n",
      "epoch 180; iter: 0; batch classifier loss: 0.024037; batch adversarial loss: 0.539097\n",
      "epoch 181; iter: 0; batch classifier loss: 0.018522; batch adversarial loss: 0.434568\n",
      "epoch 182; iter: 0; batch classifier loss: 0.006002; batch adversarial loss: 0.425297\n",
      "epoch 183; iter: 0; batch classifier loss: 0.019834; batch adversarial loss: 0.465186\n",
      "epoch 184; iter: 0; batch classifier loss: 0.010005; batch adversarial loss: 0.426643\n",
      "epoch 185; iter: 0; batch classifier loss: 0.039775; batch adversarial loss: 0.353008\n",
      "epoch 186; iter: 0; batch classifier loss: 0.026425; batch adversarial loss: 0.384144\n",
      "epoch 187; iter: 0; batch classifier loss: 0.062151; batch adversarial loss: 0.512327\n",
      "epoch 188; iter: 0; batch classifier loss: 0.015508; batch adversarial loss: 0.515212\n",
      "epoch 189; iter: 0; batch classifier loss: 0.036785; batch adversarial loss: 0.473187\n",
      "epoch 190; iter: 0; batch classifier loss: 0.023544; batch adversarial loss: 0.550500\n",
      "epoch 191; iter: 0; batch classifier loss: 0.025334; batch adversarial loss: 0.397719\n",
      "epoch 192; iter: 0; batch classifier loss: 0.019546; batch adversarial loss: 0.446255\n",
      "epoch 193; iter: 0; batch classifier loss: 0.018792; batch adversarial loss: 0.383906\n",
      "epoch 194; iter: 0; batch classifier loss: 0.012980; batch adversarial loss: 0.505552\n",
      "epoch 195; iter: 0; batch classifier loss: 0.030369; batch adversarial loss: 0.424852\n",
      "epoch 196; iter: 0; batch classifier loss: 0.016714; batch adversarial loss: 0.390577\n",
      "epoch 197; iter: 0; batch classifier loss: 0.024266; batch adversarial loss: 0.369010\n",
      "epoch 198; iter: 0; batch classifier loss: 0.041458; batch adversarial loss: 0.377837\n",
      "epoch 199; iter: 0; batch classifier loss: 0.008859; batch adversarial loss: 0.450062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:54:33.596673: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44c52-ae24-11ee-be98-ef9b34f2853b/04a44c52-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:9661 op device:{requested: '', assigned: ''} def:{{{node 04a44c52-ae24-11ee-be98-ef9b34f2853b/04a44c52-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44c52-ae24-11ee-be98-ef9b34f2853b/04a44c52-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44c52-ae24-11ee-be98-ef9b34f2853b/04a44c52-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.696574; batch adversarial loss: 0.918161\n",
      "epoch 1; iter: 0; batch classifier loss: 0.653131; batch adversarial loss: 0.972577\n",
      "epoch 2; iter: 0; batch classifier loss: 0.826752; batch adversarial loss: 0.983022\n",
      "epoch 3; iter: 0; batch classifier loss: 1.100579; batch adversarial loss: 0.969707\n",
      "epoch 4; iter: 0; batch classifier loss: 0.902944; batch adversarial loss: 0.842189\n",
      "epoch 5; iter: 0; batch classifier loss: 0.875490; batch adversarial loss: 0.754196\n",
      "epoch 6; iter: 0; batch classifier loss: 0.911201; batch adversarial loss: 0.756410\n",
      "epoch 7; iter: 0; batch classifier loss: 0.720031; batch adversarial loss: 0.648950\n",
      "epoch 8; iter: 0; batch classifier loss: 0.645935; batch adversarial loss: 0.585255\n",
      "epoch 9; iter: 0; batch classifier loss: 0.530660; batch adversarial loss: 0.572684\n",
      "epoch 10; iter: 0; batch classifier loss: 0.372061; batch adversarial loss: 0.523040\n",
      "epoch 11; iter: 0; batch classifier loss: 0.302008; batch adversarial loss: 0.489431\n",
      "epoch 12; iter: 0; batch classifier loss: 0.293139; batch adversarial loss: 0.453266\n",
      "epoch 13; iter: 0; batch classifier loss: 0.265007; batch adversarial loss: 0.489363\n",
      "epoch 14; iter: 0; batch classifier loss: 0.247522; batch adversarial loss: 0.459315\n",
      "epoch 15; iter: 0; batch classifier loss: 0.357649; batch adversarial loss: 0.509795\n",
      "epoch 16; iter: 0; batch classifier loss: 0.251201; batch adversarial loss: 0.523606\n",
      "epoch 17; iter: 0; batch classifier loss: 0.253206; batch adversarial loss: 0.452497\n",
      "epoch 18; iter: 0; batch classifier loss: 0.213948; batch adversarial loss: 0.447841\n",
      "epoch 19; iter: 0; batch classifier loss: 0.210127; batch adversarial loss: 0.522370\n",
      "epoch 20; iter: 0; batch classifier loss: 0.233020; batch adversarial loss: 0.402344\n",
      "epoch 21; iter: 0; batch classifier loss: 0.219489; batch adversarial loss: 0.510854\n",
      "epoch 22; iter: 0; batch classifier loss: 0.194768; batch adversarial loss: 0.433053\n",
      "epoch 23; iter: 0; batch classifier loss: 0.245774; batch adversarial loss: 0.463537\n",
      "epoch 24; iter: 0; batch classifier loss: 0.250370; batch adversarial loss: 0.498462\n",
      "epoch 25; iter: 0; batch classifier loss: 0.199607; batch adversarial loss: 0.464685\n",
      "epoch 26; iter: 0; batch classifier loss: 0.232559; batch adversarial loss: 0.485992\n",
      "epoch 27; iter: 0; batch classifier loss: 0.162869; batch adversarial loss: 0.400782\n",
      "epoch 28; iter: 0; batch classifier loss: 0.186708; batch adversarial loss: 0.465631\n",
      "epoch 29; iter: 0; batch classifier loss: 0.199012; batch adversarial loss: 0.369518\n",
      "epoch 30; iter: 0; batch classifier loss: 0.155707; batch adversarial loss: 0.496180\n",
      "epoch 31; iter: 0; batch classifier loss: 0.227132; batch adversarial loss: 0.506072\n",
      "epoch 32; iter: 0; batch classifier loss: 0.182880; batch adversarial loss: 0.463302\n",
      "epoch 33; iter: 0; batch classifier loss: 0.164262; batch adversarial loss: 0.484981\n",
      "epoch 34; iter: 0; batch classifier loss: 0.167446; batch adversarial loss: 0.485264\n",
      "epoch 35; iter: 0; batch classifier loss: 0.206662; batch adversarial loss: 0.475892\n",
      "epoch 36; iter: 0; batch classifier loss: 0.158576; batch adversarial loss: 0.456321\n",
      "epoch 37; iter: 0; batch classifier loss: 0.214378; batch adversarial loss: 0.424613\n",
      "epoch 38; iter: 0; batch classifier loss: 0.226545; batch adversarial loss: 0.415788\n",
      "epoch 39; iter: 0; batch classifier loss: 0.119977; batch adversarial loss: 0.460195\n",
      "epoch 40; iter: 0; batch classifier loss: 0.153009; batch adversarial loss: 0.507694\n",
      "epoch 41; iter: 0; batch classifier loss: 0.125731; batch adversarial loss: 0.472301\n",
      "epoch 42; iter: 0; batch classifier loss: 0.121872; batch adversarial loss: 0.416893\n",
      "epoch 43; iter: 0; batch classifier loss: 0.165406; batch adversarial loss: 0.392585\n",
      "epoch 44; iter: 0; batch classifier loss: 0.148951; batch adversarial loss: 0.455799\n",
      "epoch 45; iter: 0; batch classifier loss: 0.135269; batch adversarial loss: 0.459848\n",
      "epoch 46; iter: 0; batch classifier loss: 0.078242; batch adversarial loss: 0.503658\n",
      "epoch 47; iter: 0; batch classifier loss: 0.090659; batch adversarial loss: 0.432104\n",
      "epoch 48; iter: 0; batch classifier loss: 0.097935; batch adversarial loss: 0.451251\n",
      "epoch 49; iter: 0; batch classifier loss: 0.113533; batch adversarial loss: 0.366985\n",
      "epoch 50; iter: 0; batch classifier loss: 0.102227; batch adversarial loss: 0.471575\n",
      "epoch 51; iter: 0; batch classifier loss: 0.118681; batch adversarial loss: 0.412803\n",
      "epoch 52; iter: 0; batch classifier loss: 0.101295; batch adversarial loss: 0.469613\n",
      "epoch 53; iter: 0; batch classifier loss: 0.077811; batch adversarial loss: 0.506543\n",
      "epoch 54; iter: 0; batch classifier loss: 0.117188; batch adversarial loss: 0.476949\n",
      "epoch 55; iter: 0; batch classifier loss: 0.109084; batch adversarial loss: 0.460172\n",
      "epoch 56; iter: 0; batch classifier loss: 0.106329; batch adversarial loss: 0.360718\n",
      "epoch 57; iter: 0; batch classifier loss: 0.080528; batch adversarial loss: 0.422541\n",
      "epoch 58; iter: 0; batch classifier loss: 0.080411; batch adversarial loss: 0.383185\n",
      "epoch 59; iter: 0; batch classifier loss: 0.116842; batch adversarial loss: 0.390417\n",
      "epoch 60; iter: 0; batch classifier loss: 0.113815; batch adversarial loss: 0.486879\n",
      "epoch 61; iter: 0; batch classifier loss: 0.082780; batch adversarial loss: 0.406146\n",
      "epoch 62; iter: 0; batch classifier loss: 0.119402; batch adversarial loss: 0.349968\n",
      "epoch 63; iter: 0; batch classifier loss: 0.066786; batch adversarial loss: 0.571477\n",
      "epoch 64; iter: 0; batch classifier loss: 0.050166; batch adversarial loss: 0.471051\n",
      "epoch 65; iter: 0; batch classifier loss: 0.101493; batch adversarial loss: 0.431610\n",
      "epoch 66; iter: 0; batch classifier loss: 0.102828; batch adversarial loss: 0.423340\n",
      "epoch 67; iter: 0; batch classifier loss: 0.088759; batch adversarial loss: 0.456991\n",
      "epoch 68; iter: 0; batch classifier loss: 0.072708; batch adversarial loss: 0.463031\n",
      "epoch 69; iter: 0; batch classifier loss: 0.086807; batch adversarial loss: 0.405307\n",
      "epoch 70; iter: 0; batch classifier loss: 0.049776; batch adversarial loss: 0.387412\n",
      "epoch 71; iter: 0; batch classifier loss: 0.108509; batch adversarial loss: 0.383210\n",
      "epoch 72; iter: 0; batch classifier loss: 0.072287; batch adversarial loss: 0.445334\n",
      "epoch 73; iter: 0; batch classifier loss: 0.099000; batch adversarial loss: 0.513059\n",
      "epoch 74; iter: 0; batch classifier loss: 0.080863; batch adversarial loss: 0.490568\n",
      "epoch 75; iter: 0; batch classifier loss: 0.040164; batch adversarial loss: 0.489557\n",
      "epoch 76; iter: 0; batch classifier loss: 0.063749; batch adversarial loss: 0.497319\n",
      "epoch 77; iter: 0; batch classifier loss: 0.042700; batch adversarial loss: 0.420255\n",
      "epoch 78; iter: 0; batch classifier loss: 0.063334; batch adversarial loss: 0.410785\n",
      "epoch 79; iter: 0; batch classifier loss: 0.078316; batch adversarial loss: 0.427428\n",
      "epoch 80; iter: 0; batch classifier loss: 0.067276; batch adversarial loss: 0.499810\n",
      "epoch 81; iter: 0; batch classifier loss: 0.086941; batch adversarial loss: 0.467774\n",
      "epoch 82; iter: 0; batch classifier loss: 0.068160; batch adversarial loss: 0.429686\n",
      "epoch 83; iter: 0; batch classifier loss: 0.053558; batch adversarial loss: 0.426551\n",
      "epoch 84; iter: 0; batch classifier loss: 0.062101; batch adversarial loss: 0.518559\n",
      "epoch 85; iter: 0; batch classifier loss: 0.064650; batch adversarial loss: 0.440156\n",
      "epoch 86; iter: 0; batch classifier loss: 0.048567; batch adversarial loss: 0.409341\n",
      "epoch 87; iter: 0; batch classifier loss: 0.030544; batch adversarial loss: 0.555340\n",
      "epoch 88; iter: 0; batch classifier loss: 0.075119; batch adversarial loss: 0.474729\n",
      "epoch 89; iter: 0; batch classifier loss: 0.042337; batch adversarial loss: 0.424836\n",
      "epoch 90; iter: 0; batch classifier loss: 0.107627; batch adversarial loss: 0.413092\n",
      "epoch 91; iter: 0; batch classifier loss: 0.094555; batch adversarial loss: 0.430200\n",
      "epoch 92; iter: 0; batch classifier loss: 0.036167; batch adversarial loss: 0.415922\n",
      "epoch 93; iter: 0; batch classifier loss: 0.070330; batch adversarial loss: 0.482096\n",
      "epoch 94; iter: 0; batch classifier loss: 0.028834; batch adversarial loss: 0.388637\n",
      "epoch 95; iter: 0; batch classifier loss: 0.057866; batch adversarial loss: 0.451905\n",
      "epoch 96; iter: 0; batch classifier loss: 0.049143; batch adversarial loss: 0.429276\n",
      "epoch 97; iter: 0; batch classifier loss: 0.054189; batch adversarial loss: 0.479891\n",
      "epoch 98; iter: 0; batch classifier loss: 0.033957; batch adversarial loss: 0.441946\n",
      "epoch 99; iter: 0; batch classifier loss: 0.028978; batch adversarial loss: 0.453937\n",
      "epoch 100; iter: 0; batch classifier loss: 0.056732; batch adversarial loss: 0.546221\n",
      "epoch 101; iter: 0; batch classifier loss: 0.032553; batch adversarial loss: 0.588605\n",
      "epoch 102; iter: 0; batch classifier loss: 0.036551; batch adversarial loss: 0.499135\n",
      "epoch 103; iter: 0; batch classifier loss: 0.054372; batch adversarial loss: 0.449512\n",
      "epoch 104; iter: 0; batch classifier loss: 0.063325; batch adversarial loss: 0.418913\n",
      "epoch 105; iter: 0; batch classifier loss: 0.066965; batch adversarial loss: 0.504647\n",
      "epoch 106; iter: 0; batch classifier loss: 0.036945; batch adversarial loss: 0.480376\n",
      "epoch 107; iter: 0; batch classifier loss: 0.043665; batch adversarial loss: 0.483318\n",
      "epoch 108; iter: 0; batch classifier loss: 0.032184; batch adversarial loss: 0.461614\n",
      "epoch 109; iter: 0; batch classifier loss: 0.056721; batch adversarial loss: 0.403176\n",
      "epoch 110; iter: 0; batch classifier loss: 0.025753; batch adversarial loss: 0.473057\n",
      "epoch 111; iter: 0; batch classifier loss: 0.056864; batch adversarial loss: 0.531654\n",
      "epoch 112; iter: 0; batch classifier loss: 0.027886; batch adversarial loss: 0.544619\n",
      "epoch 113; iter: 0; batch classifier loss: 0.040134; batch adversarial loss: 0.588377\n",
      "epoch 114; iter: 0; batch classifier loss: 0.050168; batch adversarial loss: 0.408762\n",
      "epoch 115; iter: 0; batch classifier loss: 0.050786; batch adversarial loss: 0.405453\n",
      "epoch 116; iter: 0; batch classifier loss: 0.043826; batch adversarial loss: 0.426101\n",
      "epoch 117; iter: 0; batch classifier loss: 0.041545; batch adversarial loss: 0.544062\n",
      "epoch 118; iter: 0; batch classifier loss: 0.033160; batch adversarial loss: 0.413984\n",
      "epoch 119; iter: 0; batch classifier loss: 0.013983; batch adversarial loss: 0.521754\n",
      "epoch 120; iter: 0; batch classifier loss: 0.030720; batch adversarial loss: 0.417749\n",
      "epoch 121; iter: 0; batch classifier loss: 0.051654; batch adversarial loss: 0.418596\n",
      "epoch 122; iter: 0; batch classifier loss: 0.033123; batch adversarial loss: 0.355218\n",
      "epoch 123; iter: 0; batch classifier loss: 0.032682; batch adversarial loss: 0.446256\n",
      "epoch 124; iter: 0; batch classifier loss: 0.042643; batch adversarial loss: 0.446988\n",
      "epoch 125; iter: 0; batch classifier loss: 0.023433; batch adversarial loss: 0.475732\n",
      "epoch 126; iter: 0; batch classifier loss: 0.031051; batch adversarial loss: 0.449769\n",
      "epoch 127; iter: 0; batch classifier loss: 0.035672; batch adversarial loss: 0.376895\n",
      "epoch 128; iter: 0; batch classifier loss: 0.027586; batch adversarial loss: 0.438926\n",
      "epoch 129; iter: 0; batch classifier loss: 0.032121; batch adversarial loss: 0.450761\n",
      "epoch 130; iter: 0; batch classifier loss: 0.028844; batch adversarial loss: 0.480882\n",
      "epoch 131; iter: 0; batch classifier loss: 0.030960; batch adversarial loss: 0.442218\n",
      "epoch 132; iter: 0; batch classifier loss: 0.030898; batch adversarial loss: 0.540785\n",
      "epoch 133; iter: 0; batch classifier loss: 0.020435; batch adversarial loss: 0.371234\n",
      "epoch 134; iter: 0; batch classifier loss: 0.053148; batch adversarial loss: 0.402392\n",
      "epoch 135; iter: 0; batch classifier loss: 0.023878; batch adversarial loss: 0.434294\n",
      "epoch 136; iter: 0; batch classifier loss: 0.023104; batch adversarial loss: 0.408869\n",
      "epoch 137; iter: 0; batch classifier loss: 0.046836; batch adversarial loss: 0.392018\n",
      "epoch 138; iter: 0; batch classifier loss: 0.044581; batch adversarial loss: 0.517423\n",
      "epoch 139; iter: 0; batch classifier loss: 0.032461; batch adversarial loss: 0.487806\n",
      "epoch 140; iter: 0; batch classifier loss: 0.037531; batch adversarial loss: 0.400423\n",
      "epoch 141; iter: 0; batch classifier loss: 0.025603; batch adversarial loss: 0.328366\n",
      "epoch 142; iter: 0; batch classifier loss: 0.033373; batch adversarial loss: 0.421290\n",
      "epoch 143; iter: 0; batch classifier loss: 0.054578; batch adversarial loss: 0.375302\n",
      "epoch 144; iter: 0; batch classifier loss: 0.016981; batch adversarial loss: 0.490765\n",
      "epoch 145; iter: 0; batch classifier loss: 0.017230; batch adversarial loss: 0.390909\n",
      "epoch 146; iter: 0; batch classifier loss: 0.041595; batch adversarial loss: 0.440813\n",
      "epoch 147; iter: 0; batch classifier loss: 0.025771; batch adversarial loss: 0.495538\n",
      "epoch 148; iter: 0; batch classifier loss: 0.030284; batch adversarial loss: 0.412572\n",
      "epoch 149; iter: 0; batch classifier loss: 0.041382; batch adversarial loss: 0.472492\n",
      "epoch 150; iter: 0; batch classifier loss: 0.014914; batch adversarial loss: 0.422300\n",
      "epoch 151; iter: 0; batch classifier loss: 0.010955; batch adversarial loss: 0.550492\n",
      "epoch 152; iter: 0; batch classifier loss: 0.039405; batch adversarial loss: 0.363547\n",
      "epoch 153; iter: 0; batch classifier loss: 0.027252; batch adversarial loss: 0.431859\n",
      "epoch 154; iter: 0; batch classifier loss: 0.023752; batch adversarial loss: 0.438749\n",
      "epoch 155; iter: 0; batch classifier loss: 0.027286; batch adversarial loss: 0.522433\n",
      "epoch 156; iter: 0; batch classifier loss: 0.011443; batch adversarial loss: 0.423043\n",
      "epoch 157; iter: 0; batch classifier loss: 0.005170; batch adversarial loss: 0.400585\n",
      "epoch 158; iter: 0; batch classifier loss: 0.038432; batch adversarial loss: 0.400573\n",
      "epoch 159; iter: 0; batch classifier loss: 0.011300; batch adversarial loss: 0.433084\n",
      "epoch 160; iter: 0; batch classifier loss: 0.007250; batch adversarial loss: 0.432007\n",
      "epoch 161; iter: 0; batch classifier loss: 0.033779; batch adversarial loss: 0.416448\n",
      "epoch 162; iter: 0; batch classifier loss: 0.024153; batch adversarial loss: 0.508147\n",
      "epoch 163; iter: 0; batch classifier loss: 0.007889; batch adversarial loss: 0.469868\n",
      "epoch 164; iter: 0; batch classifier loss: 0.006432; batch adversarial loss: 0.572385\n",
      "epoch 165; iter: 0; batch classifier loss: 0.014551; batch adversarial loss: 0.411961\n",
      "epoch 166; iter: 0; batch classifier loss: 0.012144; batch adversarial loss: 0.468441\n",
      "epoch 167; iter: 0; batch classifier loss: 0.027107; batch adversarial loss: 0.360425\n",
      "epoch 168; iter: 0; batch classifier loss: 0.026529; batch adversarial loss: 0.416093\n",
      "epoch 169; iter: 0; batch classifier loss: 0.029171; batch adversarial loss: 0.475468\n",
      "epoch 170; iter: 0; batch classifier loss: 0.019732; batch adversarial loss: 0.499422\n",
      "epoch 171; iter: 0; batch classifier loss: 0.011107; batch adversarial loss: 0.499612\n",
      "epoch 172; iter: 0; batch classifier loss: 0.007455; batch adversarial loss: 0.496049\n",
      "epoch 173; iter: 0; batch classifier loss: 0.008112; batch adversarial loss: 0.470125\n",
      "epoch 174; iter: 0; batch classifier loss: 0.007304; batch adversarial loss: 0.439334\n",
      "epoch 175; iter: 0; batch classifier loss: 0.015879; batch adversarial loss: 0.481130\n",
      "epoch 176; iter: 0; batch classifier loss: 0.024156; batch adversarial loss: 0.399351\n",
      "epoch 177; iter: 0; batch classifier loss: 0.024736; batch adversarial loss: 0.410532\n",
      "epoch 178; iter: 0; batch classifier loss: 0.007146; batch adversarial loss: 0.458773\n",
      "epoch 179; iter: 0; batch classifier loss: 0.010081; batch adversarial loss: 0.454371\n",
      "epoch 180; iter: 0; batch classifier loss: 0.010984; batch adversarial loss: 0.446305\n",
      "epoch 181; iter: 0; batch classifier loss: 0.022143; batch adversarial loss: 0.517967\n",
      "epoch 182; iter: 0; batch classifier loss: 0.028770; batch adversarial loss: 0.362340\n",
      "epoch 183; iter: 0; batch classifier loss: 0.015541; batch adversarial loss: 0.370017\n",
      "epoch 184; iter: 0; batch classifier loss: 0.007202; batch adversarial loss: 0.498931\n",
      "epoch 185; iter: 0; batch classifier loss: 0.003745; batch adversarial loss: 0.457355\n",
      "epoch 186; iter: 0; batch classifier loss: 0.010366; batch adversarial loss: 0.365862\n",
      "epoch 187; iter: 0; batch classifier loss: 0.025355; batch adversarial loss: 0.394531\n",
      "epoch 188; iter: 0; batch classifier loss: 0.017221; batch adversarial loss: 0.495004\n",
      "epoch 189; iter: 0; batch classifier loss: 0.010183; batch adversarial loss: 0.391453\n",
      "epoch 190; iter: 0; batch classifier loss: 0.004264; batch adversarial loss: 0.475738\n",
      "epoch 191; iter: 0; batch classifier loss: 0.021832; batch adversarial loss: 0.491655\n",
      "epoch 192; iter: 0; batch classifier loss: 0.038524; batch adversarial loss: 0.526798\n",
      "epoch 193; iter: 0; batch classifier loss: 0.026529; batch adversarial loss: 0.365327\n",
      "epoch 194; iter: 0; batch classifier loss: 0.020019; batch adversarial loss: 0.458650\n",
      "epoch 195; iter: 0; batch classifier loss: 0.010319; batch adversarial loss: 0.425357\n",
      "epoch 196; iter: 0; batch classifier loss: 0.005694; batch adversarial loss: 0.425716\n",
      "epoch 197; iter: 0; batch classifier loss: 0.018047; batch adversarial loss: 0.501881\n",
      "epoch 198; iter: 0; batch classifier loss: 0.011961; batch adversarial loss: 0.468673\n",
      "epoch 199; iter: 0; batch classifier loss: 0.004687; batch adversarial loss: 0.471598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:55:11.526087: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44cca-ae24-11ee-be98-ef9b34f2853b/04a44cca-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:10468 op device:{requested: '', assigned: ''} def:{{{node 04a44cca-ae24-11ee-be98-ef9b34f2853b/04a44cca-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44cca-ae24-11ee-be98-ef9b34f2853b/04a44cca-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44cca-ae24-11ee-be98-ef9b34f2853b/04a44cca-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.706639; batch adversarial loss: 1.174444\n",
      "epoch 1; iter: 0; batch classifier loss: 0.597160; batch adversarial loss: 1.199304\n",
      "epoch 2; iter: 0; batch classifier loss: 0.766984; batch adversarial loss: 1.301707\n",
      "epoch 3; iter: 0; batch classifier loss: 0.855626; batch adversarial loss: 1.268135\n",
      "epoch 4; iter: 0; batch classifier loss: 0.790631; batch adversarial loss: 1.216039\n",
      "epoch 5; iter: 0; batch classifier loss: 0.830810; batch adversarial loss: 1.056415\n",
      "epoch 6; iter: 0; batch classifier loss: 0.767373; batch adversarial loss: 0.947991\n",
      "epoch 7; iter: 0; batch classifier loss: 0.694552; batch adversarial loss: 0.851086\n",
      "epoch 8; iter: 0; batch classifier loss: 0.449256; batch adversarial loss: 0.762045\n",
      "epoch 9; iter: 0; batch classifier loss: 0.320607; batch adversarial loss: 0.682473\n",
      "epoch 10; iter: 0; batch classifier loss: 0.277630; batch adversarial loss: 0.682347\n",
      "epoch 11; iter: 0; batch classifier loss: 0.181067; batch adversarial loss: 0.632815\n",
      "epoch 12; iter: 0; batch classifier loss: 0.254075; batch adversarial loss: 0.565694\n",
      "epoch 13; iter: 0; batch classifier loss: 0.228479; batch adversarial loss: 0.588717\n",
      "epoch 14; iter: 0; batch classifier loss: 0.281953; batch adversarial loss: 0.583207\n",
      "epoch 15; iter: 0; batch classifier loss: 0.236594; batch adversarial loss: 0.563754\n",
      "epoch 16; iter: 0; batch classifier loss: 0.216703; batch adversarial loss: 0.537607\n",
      "epoch 17; iter: 0; batch classifier loss: 0.227646; batch adversarial loss: 0.558964\n",
      "epoch 18; iter: 0; batch classifier loss: 0.219827; batch adversarial loss: 0.544061\n",
      "epoch 19; iter: 0; batch classifier loss: 0.206921; batch adversarial loss: 0.486433\n",
      "epoch 20; iter: 0; batch classifier loss: 0.243075; batch adversarial loss: 0.498867\n",
      "epoch 21; iter: 0; batch classifier loss: 0.203892; batch adversarial loss: 0.503009\n",
      "epoch 22; iter: 0; batch classifier loss: 0.226135; batch adversarial loss: 0.478972\n",
      "epoch 23; iter: 0; batch classifier loss: 0.133844; batch adversarial loss: 0.549158\n",
      "epoch 24; iter: 0; batch classifier loss: 0.189987; batch adversarial loss: 0.491393\n",
      "epoch 25; iter: 0; batch classifier loss: 0.167686; batch adversarial loss: 0.430811\n",
      "epoch 26; iter: 0; batch classifier loss: 0.234561; batch adversarial loss: 0.426182\n",
      "epoch 27; iter: 0; batch classifier loss: 0.206034; batch adversarial loss: 0.416489\n",
      "epoch 28; iter: 0; batch classifier loss: 0.151295; batch adversarial loss: 0.495112\n",
      "epoch 29; iter: 0; batch classifier loss: 0.126914; batch adversarial loss: 0.455113\n",
      "epoch 30; iter: 0; batch classifier loss: 0.129797; batch adversarial loss: 0.415874\n",
      "epoch 31; iter: 0; batch classifier loss: 0.138497; batch adversarial loss: 0.437929\n",
      "epoch 32; iter: 0; batch classifier loss: 0.107809; batch adversarial loss: 0.467833\n",
      "epoch 33; iter: 0; batch classifier loss: 0.190824; batch adversarial loss: 0.432450\n",
      "epoch 34; iter: 0; batch classifier loss: 0.091993; batch adversarial loss: 0.451625\n",
      "epoch 35; iter: 0; batch classifier loss: 0.227210; batch adversarial loss: 0.339759\n",
      "epoch 36; iter: 0; batch classifier loss: 0.180993; batch adversarial loss: 0.496426\n",
      "epoch 37; iter: 0; batch classifier loss: 0.164487; batch adversarial loss: 0.430671\n",
      "epoch 38; iter: 0; batch classifier loss: 0.248272; batch adversarial loss: 0.490836\n",
      "epoch 39; iter: 0; batch classifier loss: 0.139881; batch adversarial loss: 0.430672\n",
      "epoch 40; iter: 0; batch classifier loss: 0.113677; batch adversarial loss: 0.460850\n",
      "epoch 41; iter: 0; batch classifier loss: 0.148536; batch adversarial loss: 0.536717\n",
      "epoch 42; iter: 0; batch classifier loss: 0.153376; batch adversarial loss: 0.472602\n",
      "epoch 43; iter: 0; batch classifier loss: 0.130568; batch adversarial loss: 0.472673\n",
      "epoch 44; iter: 0; batch classifier loss: 0.174484; batch adversarial loss: 0.580486\n",
      "epoch 45; iter: 0; batch classifier loss: 0.122880; batch adversarial loss: 0.408071\n",
      "epoch 46; iter: 0; batch classifier loss: 0.102494; batch adversarial loss: 0.423951\n",
      "epoch 47; iter: 0; batch classifier loss: 0.097179; batch adversarial loss: 0.522436\n",
      "epoch 48; iter: 0; batch classifier loss: 0.109871; batch adversarial loss: 0.430242\n",
      "epoch 49; iter: 0; batch classifier loss: 0.104861; batch adversarial loss: 0.390509\n",
      "epoch 50; iter: 0; batch classifier loss: 0.078314; batch adversarial loss: 0.401234\n",
      "epoch 51; iter: 0; batch classifier loss: 0.092506; batch adversarial loss: 0.514535\n",
      "epoch 52; iter: 0; batch classifier loss: 0.094462; batch adversarial loss: 0.366663\n",
      "epoch 53; iter: 0; batch classifier loss: 0.135330; batch adversarial loss: 0.498152\n",
      "epoch 54; iter: 0; batch classifier loss: 0.109400; batch adversarial loss: 0.421478\n",
      "epoch 55; iter: 0; batch classifier loss: 0.063100; batch adversarial loss: 0.455641\n",
      "epoch 56; iter: 0; batch classifier loss: 0.095944; batch adversarial loss: 0.451652\n",
      "epoch 57; iter: 0; batch classifier loss: 0.107661; batch adversarial loss: 0.450838\n",
      "epoch 58; iter: 0; batch classifier loss: 0.101712; batch adversarial loss: 0.436128\n",
      "epoch 59; iter: 0; batch classifier loss: 0.109454; batch adversarial loss: 0.476916\n",
      "epoch 60; iter: 0; batch classifier loss: 0.055044; batch adversarial loss: 0.475391\n",
      "epoch 61; iter: 0; batch classifier loss: 0.082561; batch adversarial loss: 0.346211\n",
      "epoch 62; iter: 0; batch classifier loss: 0.086360; batch adversarial loss: 0.478405\n",
      "epoch 63; iter: 0; batch classifier loss: 0.059140; batch adversarial loss: 0.439088\n",
      "epoch 64; iter: 0; batch classifier loss: 0.083220; batch adversarial loss: 0.496009\n",
      "epoch 65; iter: 0; batch classifier loss: 0.088949; batch adversarial loss: 0.353655\n",
      "epoch 66; iter: 0; batch classifier loss: 0.058313; batch adversarial loss: 0.363032\n",
      "epoch 67; iter: 0; batch classifier loss: 0.040041; batch adversarial loss: 0.411106\n",
      "epoch 68; iter: 0; batch classifier loss: 0.076230; batch adversarial loss: 0.460921\n",
      "epoch 69; iter: 0; batch classifier loss: 0.064940; batch adversarial loss: 0.475367\n",
      "epoch 70; iter: 0; batch classifier loss: 0.078045; batch adversarial loss: 0.394004\n",
      "epoch 71; iter: 0; batch classifier loss: 0.063390; batch adversarial loss: 0.357592\n",
      "epoch 72; iter: 0; batch classifier loss: 0.052980; batch adversarial loss: 0.425605\n",
      "epoch 73; iter: 0; batch classifier loss: 0.043812; batch adversarial loss: 0.478212\n",
      "epoch 74; iter: 0; batch classifier loss: 0.058304; batch adversarial loss: 0.417469\n",
      "epoch 75; iter: 0; batch classifier loss: 0.045213; batch adversarial loss: 0.349147\n",
      "epoch 76; iter: 0; batch classifier loss: 0.055764; batch adversarial loss: 0.507468\n",
      "epoch 77; iter: 0; batch classifier loss: 0.039830; batch adversarial loss: 0.429098\n",
      "epoch 78; iter: 0; batch classifier loss: 0.058331; batch adversarial loss: 0.438073\n",
      "epoch 79; iter: 0; batch classifier loss: 0.069190; batch adversarial loss: 0.440570\n",
      "epoch 80; iter: 0; batch classifier loss: 0.079900; batch adversarial loss: 0.359457\n",
      "epoch 81; iter: 0; batch classifier loss: 0.105554; batch adversarial loss: 0.507122\n",
      "epoch 82; iter: 0; batch classifier loss: 0.044980; batch adversarial loss: 0.407303\n",
      "epoch 83; iter: 0; batch classifier loss: 0.046961; batch adversarial loss: 0.477444\n",
      "epoch 84; iter: 0; batch classifier loss: 0.046502; batch adversarial loss: 0.343645\n",
      "epoch 85; iter: 0; batch classifier loss: 0.050027; batch adversarial loss: 0.448104\n",
      "epoch 86; iter: 0; batch classifier loss: 0.050090; batch adversarial loss: 0.374354\n",
      "epoch 87; iter: 0; batch classifier loss: 0.059312; batch adversarial loss: 0.429228\n",
      "epoch 88; iter: 0; batch classifier loss: 0.056393; batch adversarial loss: 0.455878\n",
      "epoch 89; iter: 0; batch classifier loss: 0.068356; batch adversarial loss: 0.436031\n",
      "epoch 90; iter: 0; batch classifier loss: 0.089534; batch adversarial loss: 0.475296\n",
      "epoch 91; iter: 0; batch classifier loss: 0.031767; batch adversarial loss: 0.345614\n",
      "epoch 92; iter: 0; batch classifier loss: 0.045348; batch adversarial loss: 0.459418\n",
      "epoch 93; iter: 0; batch classifier loss: 0.037640; batch adversarial loss: 0.450912\n",
      "epoch 94; iter: 0; batch classifier loss: 0.051797; batch adversarial loss: 0.427319\n",
      "epoch 95; iter: 0; batch classifier loss: 0.048943; batch adversarial loss: 0.427882\n",
      "epoch 96; iter: 0; batch classifier loss: 0.066515; batch adversarial loss: 0.439216\n",
      "epoch 97; iter: 0; batch classifier loss: 0.051049; batch adversarial loss: 0.452007\n",
      "epoch 98; iter: 0; batch classifier loss: 0.047987; batch adversarial loss: 0.459134\n",
      "epoch 99; iter: 0; batch classifier loss: 0.063036; batch adversarial loss: 0.588398\n",
      "epoch 100; iter: 0; batch classifier loss: 0.063428; batch adversarial loss: 0.405061\n",
      "epoch 101; iter: 0; batch classifier loss: 0.018428; batch adversarial loss: 0.495317\n",
      "epoch 102; iter: 0; batch classifier loss: 0.024814; batch adversarial loss: 0.374257\n",
      "epoch 103; iter: 0; batch classifier loss: 0.033694; batch adversarial loss: 0.446290\n",
      "epoch 104; iter: 0; batch classifier loss: 0.100948; batch adversarial loss: 0.577646\n",
      "epoch 105; iter: 0; batch classifier loss: 0.033305; batch adversarial loss: 0.391544\n",
      "epoch 106; iter: 0; batch classifier loss: 0.046551; batch adversarial loss: 0.385412\n",
      "epoch 107; iter: 0; batch classifier loss: 0.025032; batch adversarial loss: 0.431341\n",
      "epoch 108; iter: 0; batch classifier loss: 0.054427; batch adversarial loss: 0.378899\n",
      "epoch 109; iter: 0; batch classifier loss: 0.045210; batch adversarial loss: 0.499517\n",
      "epoch 110; iter: 0; batch classifier loss: 0.013607; batch adversarial loss: 0.360541\n",
      "epoch 111; iter: 0; batch classifier loss: 0.034523; batch adversarial loss: 0.414029\n",
      "epoch 112; iter: 0; batch classifier loss: 0.049755; batch adversarial loss: 0.419707\n",
      "epoch 113; iter: 0; batch classifier loss: 0.050485; batch adversarial loss: 0.388182\n",
      "epoch 114; iter: 0; batch classifier loss: 0.032824; batch adversarial loss: 0.392155\n",
      "epoch 115; iter: 0; batch classifier loss: 0.065520; batch adversarial loss: 0.408846\n",
      "epoch 116; iter: 0; batch classifier loss: 0.072758; batch adversarial loss: 0.462432\n",
      "epoch 117; iter: 0; batch classifier loss: 0.098920; batch adversarial loss: 0.396708\n",
      "epoch 118; iter: 0; batch classifier loss: 0.034443; batch adversarial loss: 0.431215\n",
      "epoch 119; iter: 0; batch classifier loss: 0.055761; batch adversarial loss: 0.375634\n",
      "epoch 120; iter: 0; batch classifier loss: 0.053456; batch adversarial loss: 0.341768\n",
      "epoch 121; iter: 0; batch classifier loss: 0.023806; batch adversarial loss: 0.391067\n",
      "epoch 122; iter: 0; batch classifier loss: 0.059735; batch adversarial loss: 0.519651\n",
      "epoch 123; iter: 0; batch classifier loss: 0.033363; batch adversarial loss: 0.426835\n",
      "epoch 124; iter: 0; batch classifier loss: 0.026238; batch adversarial loss: 0.411314\n",
      "epoch 125; iter: 0; batch classifier loss: 0.063848; batch adversarial loss: 0.386931\n",
      "epoch 126; iter: 0; batch classifier loss: 0.060558; batch adversarial loss: 0.432794\n",
      "epoch 127; iter: 0; batch classifier loss: 0.045794; batch adversarial loss: 0.388011\n",
      "epoch 128; iter: 0; batch classifier loss: 0.044201; batch adversarial loss: 0.372483\n",
      "epoch 129; iter: 0; batch classifier loss: 0.131986; batch adversarial loss: 0.461208\n",
      "epoch 130; iter: 0; batch classifier loss: 0.039459; batch adversarial loss: 0.452718\n",
      "epoch 131; iter: 0; batch classifier loss: 0.092969; batch adversarial loss: 0.450002\n",
      "epoch 132; iter: 0; batch classifier loss: 0.062004; batch adversarial loss: 0.458231\n",
      "epoch 133; iter: 0; batch classifier loss: 0.043386; batch adversarial loss: 0.417249\n",
      "epoch 134; iter: 0; batch classifier loss: 0.046121; batch adversarial loss: 0.475283\n",
      "epoch 135; iter: 0; batch classifier loss: 0.034104; batch adversarial loss: 0.355935\n",
      "epoch 136; iter: 0; batch classifier loss: 0.032606; batch adversarial loss: 0.405841\n",
      "epoch 137; iter: 0; batch classifier loss: 0.032436; batch adversarial loss: 0.479631\n",
      "epoch 138; iter: 0; batch classifier loss: 0.081538; batch adversarial loss: 0.594062\n",
      "epoch 139; iter: 0; batch classifier loss: 0.069126; batch adversarial loss: 0.385373\n",
      "epoch 140; iter: 0; batch classifier loss: 0.050920; batch adversarial loss: 0.475941\n",
      "epoch 141; iter: 0; batch classifier loss: 0.050906; batch adversarial loss: 0.412849\n",
      "epoch 142; iter: 0; batch classifier loss: 0.060159; batch adversarial loss: 0.362063\n",
      "epoch 143; iter: 0; batch classifier loss: 0.050089; batch adversarial loss: 0.448032\n",
      "epoch 144; iter: 0; batch classifier loss: 0.060657; batch adversarial loss: 0.456720\n",
      "epoch 145; iter: 0; batch classifier loss: 0.043356; batch adversarial loss: 0.439357\n",
      "epoch 146; iter: 0; batch classifier loss: 0.069049; batch adversarial loss: 0.380480\n",
      "epoch 147; iter: 0; batch classifier loss: 0.036167; batch adversarial loss: 0.464583\n",
      "epoch 148; iter: 0; batch classifier loss: 0.029222; batch adversarial loss: 0.394160\n",
      "epoch 149; iter: 0; batch classifier loss: 0.032885; batch adversarial loss: 0.359921\n",
      "epoch 150; iter: 0; batch classifier loss: 0.086287; batch adversarial loss: 0.411034\n",
      "epoch 151; iter: 0; batch classifier loss: 0.035740; batch adversarial loss: 0.342329\n",
      "epoch 152; iter: 0; batch classifier loss: 0.022412; batch adversarial loss: 0.333692\n",
      "epoch 153; iter: 0; batch classifier loss: 0.050799; batch adversarial loss: 0.492809\n",
      "epoch 154; iter: 0; batch classifier loss: 0.027055; batch adversarial loss: 0.401236\n",
      "epoch 155; iter: 0; batch classifier loss: 0.045856; batch adversarial loss: 0.406329\n",
      "epoch 156; iter: 0; batch classifier loss: 0.055544; batch adversarial loss: 0.333467\n",
      "epoch 157; iter: 0; batch classifier loss: 0.049537; batch adversarial loss: 0.506099\n",
      "epoch 158; iter: 0; batch classifier loss: 0.041119; batch adversarial loss: 0.377248\n",
      "epoch 159; iter: 0; batch classifier loss: 0.047343; batch adversarial loss: 0.433920\n",
      "epoch 160; iter: 0; batch classifier loss: 0.067456; batch adversarial loss: 0.447562\n",
      "epoch 161; iter: 0; batch classifier loss: 0.062780; batch adversarial loss: 0.456851\n",
      "epoch 162; iter: 0; batch classifier loss: 0.023240; batch adversarial loss: 0.427010\n",
      "epoch 163; iter: 0; batch classifier loss: 0.048679; batch adversarial loss: 0.440324\n",
      "epoch 164; iter: 0; batch classifier loss: 0.041612; batch adversarial loss: 0.451145\n",
      "epoch 165; iter: 0; batch classifier loss: 0.017970; batch adversarial loss: 0.422893\n",
      "epoch 166; iter: 0; batch classifier loss: 0.049324; batch adversarial loss: 0.408991\n",
      "epoch 167; iter: 0; batch classifier loss: 0.083708; batch adversarial loss: 0.444949\n",
      "epoch 168; iter: 0; batch classifier loss: 0.037149; batch adversarial loss: 0.384430\n",
      "epoch 169; iter: 0; batch classifier loss: 0.043305; batch adversarial loss: 0.474407\n",
      "epoch 170; iter: 0; batch classifier loss: 0.034872; batch adversarial loss: 0.418610\n",
      "epoch 171; iter: 0; batch classifier loss: 0.040991; batch adversarial loss: 0.470224\n",
      "epoch 172; iter: 0; batch classifier loss: 0.035151; batch adversarial loss: 0.409784\n",
      "epoch 173; iter: 0; batch classifier loss: 0.059373; batch adversarial loss: 0.463103\n",
      "epoch 174; iter: 0; batch classifier loss: 0.064592; batch adversarial loss: 0.396219\n",
      "epoch 175; iter: 0; batch classifier loss: 0.027270; batch adversarial loss: 0.447031\n",
      "epoch 176; iter: 0; batch classifier loss: 0.034036; batch adversarial loss: 0.376433\n",
      "epoch 177; iter: 0; batch classifier loss: 0.053720; batch adversarial loss: 0.335070\n",
      "epoch 178; iter: 0; batch classifier loss: 0.030395; batch adversarial loss: 0.411109\n",
      "epoch 179; iter: 0; batch classifier loss: 0.036429; batch adversarial loss: 0.433524\n",
      "epoch 180; iter: 0; batch classifier loss: 0.038346; batch adversarial loss: 0.434988\n",
      "epoch 181; iter: 0; batch classifier loss: 0.078375; batch adversarial loss: 0.377665\n",
      "epoch 182; iter: 0; batch classifier loss: 0.034244; batch adversarial loss: 0.472991\n",
      "epoch 183; iter: 0; batch classifier loss: 0.015628; batch adversarial loss: 0.431179\n",
      "epoch 184; iter: 0; batch classifier loss: 0.035975; batch adversarial loss: 0.468455\n",
      "epoch 185; iter: 0; batch classifier loss: 0.031383; batch adversarial loss: 0.443148\n",
      "epoch 186; iter: 0; batch classifier loss: 0.055856; batch adversarial loss: 0.348622\n",
      "epoch 187; iter: 0; batch classifier loss: 0.037161; batch adversarial loss: 0.473564\n",
      "epoch 188; iter: 0; batch classifier loss: 0.040315; batch adversarial loss: 0.437746\n",
      "epoch 189; iter: 0; batch classifier loss: 0.012708; batch adversarial loss: 0.367728\n",
      "epoch 190; iter: 0; batch classifier loss: 0.019073; batch adversarial loss: 0.454979\n",
      "epoch 191; iter: 0; batch classifier loss: 0.041554; batch adversarial loss: 0.402241\n",
      "epoch 192; iter: 0; batch classifier loss: 0.071528; batch adversarial loss: 0.343377\n",
      "epoch 193; iter: 0; batch classifier loss: 0.023096; batch adversarial loss: 0.377989\n",
      "epoch 194; iter: 0; batch classifier loss: 0.040201; batch adversarial loss: 0.364490\n",
      "epoch 195; iter: 0; batch classifier loss: 0.045939; batch adversarial loss: 0.371595\n",
      "epoch 196; iter: 0; batch classifier loss: 0.039028; batch adversarial loss: 0.426696\n",
      "epoch 197; iter: 0; batch classifier loss: 0.034531; batch adversarial loss: 0.279768\n",
      "epoch 198; iter: 0; batch classifier loss: 0.063004; batch adversarial loss: 0.477385\n",
      "epoch 199; iter: 0; batch classifier loss: 0.018905; batch adversarial loss: 0.336722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:55:49.303152: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44d42-ae24-11ee-be98-ef9b34f2853b/04a44d42-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:11275 op device:{requested: '', assigned: ''} def:{{{node 04a44d42-ae24-11ee-be98-ef9b34f2853b/04a44d42-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44d42-ae24-11ee-be98-ef9b34f2853b/04a44d42-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44d42-ae24-11ee-be98-ef9b34f2853b/04a44d42-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.701849; batch adversarial loss: 0.664982\n",
      "epoch 1; iter: 0; batch classifier loss: 0.523903; batch adversarial loss: 0.611022\n",
      "epoch 2; iter: 0; batch classifier loss: 0.484170; batch adversarial loss: 0.604089\n",
      "epoch 3; iter: 0; batch classifier loss: 0.347245; batch adversarial loss: 0.592437\n",
      "epoch 4; iter: 0; batch classifier loss: 0.327393; batch adversarial loss: 0.538444\n",
      "epoch 5; iter: 0; batch classifier loss: 0.426526; batch adversarial loss: 0.547943\n",
      "epoch 6; iter: 0; batch classifier loss: 0.358629; batch adversarial loss: 0.575391\n",
      "epoch 7; iter: 0; batch classifier loss: 0.306544; batch adversarial loss: 0.511176\n",
      "epoch 8; iter: 0; batch classifier loss: 0.316909; batch adversarial loss: 0.495703\n",
      "epoch 9; iter: 0; batch classifier loss: 0.292700; batch adversarial loss: 0.511967\n",
      "epoch 10; iter: 0; batch classifier loss: 0.263540; batch adversarial loss: 0.455691\n",
      "epoch 11; iter: 0; batch classifier loss: 0.347998; batch adversarial loss: 0.543389\n",
      "epoch 12; iter: 0; batch classifier loss: 0.331614; batch adversarial loss: 0.473780\n",
      "epoch 13; iter: 0; batch classifier loss: 0.310850; batch adversarial loss: 0.427806\n",
      "epoch 14; iter: 0; batch classifier loss: 0.316401; batch adversarial loss: 0.532531\n",
      "epoch 15; iter: 0; batch classifier loss: 0.285871; batch adversarial loss: 0.459004\n",
      "epoch 16; iter: 0; batch classifier loss: 0.185087; batch adversarial loss: 0.494619\n",
      "epoch 17; iter: 0; batch classifier loss: 0.186598; batch adversarial loss: 0.522948\n",
      "epoch 18; iter: 0; batch classifier loss: 0.195457; batch adversarial loss: 0.484891\n",
      "epoch 19; iter: 0; batch classifier loss: 0.209028; batch adversarial loss: 0.502523\n",
      "epoch 20; iter: 0; batch classifier loss: 0.163837; batch adversarial loss: 0.536455\n",
      "epoch 21; iter: 0; batch classifier loss: 0.244315; batch adversarial loss: 0.526866\n",
      "epoch 22; iter: 0; batch classifier loss: 0.221578; batch adversarial loss: 0.492736\n",
      "epoch 23; iter: 0; batch classifier loss: 0.159052; batch adversarial loss: 0.556648\n",
      "epoch 24; iter: 0; batch classifier loss: 0.175734; batch adversarial loss: 0.449205\n",
      "epoch 25; iter: 0; batch classifier loss: 0.183787; batch adversarial loss: 0.452170\n",
      "epoch 26; iter: 0; batch classifier loss: 0.147866; batch adversarial loss: 0.438863\n",
      "epoch 27; iter: 0; batch classifier loss: 0.178539; batch adversarial loss: 0.455940\n",
      "epoch 28; iter: 0; batch classifier loss: 0.170410; batch adversarial loss: 0.458329\n",
      "epoch 29; iter: 0; batch classifier loss: 0.187765; batch adversarial loss: 0.486422\n",
      "epoch 30; iter: 0; batch classifier loss: 0.156357; batch adversarial loss: 0.493761\n",
      "epoch 31; iter: 0; batch classifier loss: 0.246134; batch adversarial loss: 0.436560\n",
      "epoch 32; iter: 0; batch classifier loss: 0.211550; batch adversarial loss: 0.472744\n",
      "epoch 33; iter: 0; batch classifier loss: 0.141418; batch adversarial loss: 0.528089\n",
      "epoch 34; iter: 0; batch classifier loss: 0.151828; batch adversarial loss: 0.437303\n",
      "epoch 35; iter: 0; batch classifier loss: 0.176860; batch adversarial loss: 0.421721\n",
      "epoch 36; iter: 0; batch classifier loss: 0.214254; batch adversarial loss: 0.423617\n",
      "epoch 37; iter: 0; batch classifier loss: 0.136194; batch adversarial loss: 0.528104\n",
      "epoch 38; iter: 0; batch classifier loss: 0.147379; batch adversarial loss: 0.499674\n",
      "epoch 39; iter: 0; batch classifier loss: 0.169399; batch adversarial loss: 0.465510\n",
      "epoch 40; iter: 0; batch classifier loss: 0.138863; batch adversarial loss: 0.463496\n",
      "epoch 41; iter: 0; batch classifier loss: 0.215370; batch adversarial loss: 0.499530\n",
      "epoch 42; iter: 0; batch classifier loss: 0.192381; batch adversarial loss: 0.467590\n",
      "epoch 43; iter: 0; batch classifier loss: 0.190942; batch adversarial loss: 0.435473\n",
      "epoch 44; iter: 0; batch classifier loss: 0.252182; batch adversarial loss: 0.456016\n",
      "epoch 45; iter: 0; batch classifier loss: 0.221513; batch adversarial loss: 0.503972\n",
      "epoch 46; iter: 0; batch classifier loss: 0.237170; batch adversarial loss: 0.429869\n",
      "epoch 47; iter: 0; batch classifier loss: 0.233406; batch adversarial loss: 0.473404\n",
      "epoch 48; iter: 0; batch classifier loss: 0.323745; batch adversarial loss: 0.469408\n",
      "epoch 49; iter: 0; batch classifier loss: 0.202434; batch adversarial loss: 0.406387\n",
      "epoch 50; iter: 0; batch classifier loss: 0.215270; batch adversarial loss: 0.520960\n",
      "epoch 51; iter: 0; batch classifier loss: 0.174698; batch adversarial loss: 0.458705\n",
      "epoch 52; iter: 0; batch classifier loss: 0.233714; batch adversarial loss: 0.436435\n",
      "epoch 53; iter: 0; batch classifier loss: 0.251774; batch adversarial loss: 0.418595\n",
      "epoch 54; iter: 0; batch classifier loss: 0.220993; batch adversarial loss: 0.409507\n",
      "epoch 55; iter: 0; batch classifier loss: 0.203820; batch adversarial loss: 0.482182\n",
      "epoch 56; iter: 0; batch classifier loss: 0.276883; batch adversarial loss: 0.362303\n",
      "epoch 57; iter: 0; batch classifier loss: 0.206488; batch adversarial loss: 0.458899\n",
      "epoch 58; iter: 0; batch classifier loss: 0.282423; batch adversarial loss: 0.470803\n",
      "epoch 59; iter: 0; batch classifier loss: 0.103446; batch adversarial loss: 0.372377\n",
      "epoch 60; iter: 0; batch classifier loss: 0.100201; batch adversarial loss: 0.446728\n",
      "epoch 61; iter: 0; batch classifier loss: 0.074795; batch adversarial loss: 0.392792\n",
      "epoch 62; iter: 0; batch classifier loss: 0.072051; batch adversarial loss: 0.413233\n",
      "epoch 63; iter: 0; batch classifier loss: 0.098675; batch adversarial loss: 0.325475\n",
      "epoch 64; iter: 0; batch classifier loss: 0.054323; batch adversarial loss: 0.472780\n",
      "epoch 65; iter: 0; batch classifier loss: 0.056469; batch adversarial loss: 0.443857\n",
      "epoch 66; iter: 0; batch classifier loss: 0.063593; batch adversarial loss: 0.525252\n",
      "epoch 67; iter: 0; batch classifier loss: 0.090701; batch adversarial loss: 0.382189\n",
      "epoch 68; iter: 0; batch classifier loss: 0.043490; batch adversarial loss: 0.417658\n",
      "epoch 69; iter: 0; batch classifier loss: 0.059639; batch adversarial loss: 0.600807\n",
      "epoch 70; iter: 0; batch classifier loss: 0.085783; batch adversarial loss: 0.322125\n",
      "epoch 71; iter: 0; batch classifier loss: 0.060645; batch adversarial loss: 0.409242\n",
      "epoch 72; iter: 0; batch classifier loss: 0.048569; batch adversarial loss: 0.508757\n",
      "epoch 73; iter: 0; batch classifier loss: 0.092977; batch adversarial loss: 0.459916\n",
      "epoch 74; iter: 0; batch classifier loss: 0.087396; batch adversarial loss: 0.425738\n",
      "epoch 75; iter: 0; batch classifier loss: 0.087968; batch adversarial loss: 0.373353\n",
      "epoch 76; iter: 0; batch classifier loss: 0.067540; batch adversarial loss: 0.323181\n",
      "epoch 77; iter: 0; batch classifier loss: 0.121382; batch adversarial loss: 0.411574\n",
      "epoch 78; iter: 0; batch classifier loss: 0.077478; batch adversarial loss: 0.439519\n",
      "epoch 79; iter: 0; batch classifier loss: 0.066058; batch adversarial loss: 0.325666\n",
      "epoch 80; iter: 0; batch classifier loss: 0.076462; batch adversarial loss: 0.341735\n",
      "epoch 81; iter: 0; batch classifier loss: 0.067848; batch adversarial loss: 0.456588\n",
      "epoch 82; iter: 0; batch classifier loss: 0.050705; batch adversarial loss: 0.502818\n",
      "epoch 83; iter: 0; batch classifier loss: 0.096485; batch adversarial loss: 0.380900\n",
      "epoch 84; iter: 0; batch classifier loss: 0.076981; batch adversarial loss: 0.411423\n",
      "epoch 85; iter: 0; batch classifier loss: 0.057978; batch adversarial loss: 0.446627\n",
      "epoch 86; iter: 0; batch classifier loss: 0.063241; batch adversarial loss: 0.413175\n",
      "epoch 87; iter: 0; batch classifier loss: 0.059759; batch adversarial loss: 0.550317\n",
      "epoch 88; iter: 0; batch classifier loss: 0.081352; batch adversarial loss: 0.370878\n",
      "epoch 89; iter: 0; batch classifier loss: 0.053414; batch adversarial loss: 0.452041\n",
      "epoch 90; iter: 0; batch classifier loss: 0.053447; batch adversarial loss: 0.343451\n",
      "epoch 91; iter: 0; batch classifier loss: 0.063124; batch adversarial loss: 0.403876\n",
      "epoch 92; iter: 0; batch classifier loss: 0.057708; batch adversarial loss: 0.420563\n",
      "epoch 93; iter: 0; batch classifier loss: 0.055352; batch adversarial loss: 0.389825\n",
      "epoch 94; iter: 0; batch classifier loss: 0.044574; batch adversarial loss: 0.486806\n",
      "epoch 95; iter: 0; batch classifier loss: 0.053448; batch adversarial loss: 0.456252\n",
      "epoch 96; iter: 0; batch classifier loss: 0.055338; batch adversarial loss: 0.508710\n",
      "epoch 97; iter: 0; batch classifier loss: 0.094459; batch adversarial loss: 0.384831\n",
      "epoch 98; iter: 0; batch classifier loss: 0.070394; batch adversarial loss: 0.385292\n",
      "epoch 99; iter: 0; batch classifier loss: 0.054991; batch adversarial loss: 0.420336\n",
      "epoch 100; iter: 0; batch classifier loss: 0.068708; batch adversarial loss: 0.463810\n",
      "epoch 101; iter: 0; batch classifier loss: 0.087969; batch adversarial loss: 0.385004\n",
      "epoch 102; iter: 0; batch classifier loss: 0.093158; batch adversarial loss: 0.487712\n",
      "epoch 103; iter: 0; batch classifier loss: 0.058168; batch adversarial loss: 0.387303\n",
      "epoch 104; iter: 0; batch classifier loss: 0.075789; batch adversarial loss: 0.449849\n",
      "epoch 105; iter: 0; batch classifier loss: 0.076018; batch adversarial loss: 0.473229\n",
      "epoch 106; iter: 0; batch classifier loss: 0.033757; batch adversarial loss: 0.380730\n",
      "epoch 107; iter: 0; batch classifier loss: 0.049309; batch adversarial loss: 0.392121\n",
      "epoch 108; iter: 0; batch classifier loss: 0.071832; batch adversarial loss: 0.338173\n",
      "epoch 109; iter: 0; batch classifier loss: 0.061604; batch adversarial loss: 0.437941\n",
      "epoch 110; iter: 0; batch classifier loss: 0.066832; batch adversarial loss: 0.427852\n",
      "epoch 111; iter: 0; batch classifier loss: 0.063603; batch adversarial loss: 0.339599\n",
      "epoch 112; iter: 0; batch classifier loss: 0.055663; batch adversarial loss: 0.430620\n",
      "epoch 113; iter: 0; batch classifier loss: 0.060175; batch adversarial loss: 0.428363\n",
      "epoch 114; iter: 0; batch classifier loss: 0.060793; batch adversarial loss: 0.416586\n",
      "epoch 115; iter: 0; batch classifier loss: 0.048358; batch adversarial loss: 0.429879\n",
      "epoch 116; iter: 0; batch classifier loss: 0.060692; batch adversarial loss: 0.422315\n",
      "epoch 117; iter: 0; batch classifier loss: 0.066085; batch adversarial loss: 0.451415\n",
      "epoch 118; iter: 0; batch classifier loss: 0.043981; batch adversarial loss: 0.467771\n",
      "epoch 119; iter: 0; batch classifier loss: 0.033865; batch adversarial loss: 0.394111\n",
      "epoch 120; iter: 0; batch classifier loss: 0.072491; batch adversarial loss: 0.518069\n",
      "epoch 121; iter: 0; batch classifier loss: 0.055063; batch adversarial loss: 0.380305\n",
      "epoch 122; iter: 0; batch classifier loss: 0.054388; batch adversarial loss: 0.384857\n",
      "epoch 123; iter: 0; batch classifier loss: 0.061733; batch adversarial loss: 0.396925\n",
      "epoch 124; iter: 0; batch classifier loss: 0.076511; batch adversarial loss: 0.472341\n",
      "epoch 125; iter: 0; batch classifier loss: 0.067633; batch adversarial loss: 0.340054\n",
      "epoch 126; iter: 0; batch classifier loss: 0.063553; batch adversarial loss: 0.299924\n",
      "epoch 127; iter: 0; batch classifier loss: 0.033910; batch adversarial loss: 0.515830\n",
      "epoch 128; iter: 0; batch classifier loss: 0.052978; batch adversarial loss: 0.399786\n",
      "epoch 129; iter: 0; batch classifier loss: 0.066408; batch adversarial loss: 0.347995\n",
      "epoch 130; iter: 0; batch classifier loss: 0.054819; batch adversarial loss: 0.461586\n",
      "epoch 131; iter: 0; batch classifier loss: 0.068120; batch adversarial loss: 0.391644\n",
      "epoch 132; iter: 0; batch classifier loss: 0.048004; batch adversarial loss: 0.496832\n",
      "epoch 133; iter: 0; batch classifier loss: 0.062150; batch adversarial loss: 0.491444\n",
      "epoch 134; iter: 0; batch classifier loss: 0.043906; batch adversarial loss: 0.398324\n",
      "epoch 135; iter: 0; batch classifier loss: 0.054724; batch adversarial loss: 0.462143\n",
      "epoch 136; iter: 0; batch classifier loss: 0.073182; batch adversarial loss: 0.423342\n",
      "epoch 137; iter: 0; batch classifier loss: 0.044263; batch adversarial loss: 0.439194\n",
      "epoch 138; iter: 0; batch classifier loss: 0.059936; batch adversarial loss: 0.372928\n",
      "epoch 139; iter: 0; batch classifier loss: 0.051871; batch adversarial loss: 0.427235\n",
      "epoch 140; iter: 0; batch classifier loss: 0.046443; batch adversarial loss: 0.436408\n",
      "epoch 141; iter: 0; batch classifier loss: 0.051276; batch adversarial loss: 0.335668\n",
      "epoch 142; iter: 0; batch classifier loss: 0.044264; batch adversarial loss: 0.486246\n",
      "epoch 143; iter: 0; batch classifier loss: 0.035818; batch adversarial loss: 0.441963\n",
      "epoch 144; iter: 0; batch classifier loss: 0.046768; batch adversarial loss: 0.490623\n",
      "epoch 145; iter: 0; batch classifier loss: 0.067444; batch adversarial loss: 0.493089\n",
      "epoch 146; iter: 0; batch classifier loss: 0.053725; batch adversarial loss: 0.460104\n",
      "epoch 147; iter: 0; batch classifier loss: 0.061674; batch adversarial loss: 0.428956\n",
      "epoch 148; iter: 0; batch classifier loss: 0.044699; batch adversarial loss: 0.362906\n",
      "epoch 149; iter: 0; batch classifier loss: 0.049029; batch adversarial loss: 0.378322\n",
      "epoch 150; iter: 0; batch classifier loss: 0.050946; batch adversarial loss: 0.499518\n",
      "epoch 151; iter: 0; batch classifier loss: 0.057887; batch adversarial loss: 0.347514\n",
      "epoch 152; iter: 0; batch classifier loss: 0.052909; batch adversarial loss: 0.339504\n",
      "epoch 153; iter: 0; batch classifier loss: 0.027490; batch adversarial loss: 0.380487\n",
      "epoch 154; iter: 0; batch classifier loss: 0.046495; batch adversarial loss: 0.524522\n",
      "epoch 155; iter: 0; batch classifier loss: 0.055862; batch adversarial loss: 0.486559\n",
      "epoch 156; iter: 0; batch classifier loss: 0.029944; batch adversarial loss: 0.381874\n",
      "epoch 157; iter: 0; batch classifier loss: 0.031270; batch adversarial loss: 0.334441\n",
      "epoch 158; iter: 0; batch classifier loss: 0.044938; batch adversarial loss: 0.360598\n",
      "epoch 159; iter: 0; batch classifier loss: 0.026191; batch adversarial loss: 0.379079\n",
      "epoch 160; iter: 0; batch classifier loss: 0.036181; batch adversarial loss: 0.386073\n",
      "epoch 161; iter: 0; batch classifier loss: 0.031856; batch adversarial loss: 0.456659\n",
      "epoch 162; iter: 0; batch classifier loss: 0.042208; batch adversarial loss: 0.367444\n",
      "epoch 163; iter: 0; batch classifier loss: 0.017275; batch adversarial loss: 0.459421\n",
      "epoch 164; iter: 0; batch classifier loss: 0.014228; batch adversarial loss: 0.429353\n",
      "epoch 165; iter: 0; batch classifier loss: 0.030881; batch adversarial loss: 0.437153\n",
      "epoch 166; iter: 0; batch classifier loss: 0.019984; batch adversarial loss: 0.314131\n",
      "epoch 167; iter: 0; batch classifier loss: 0.033981; batch adversarial loss: 0.446535\n",
      "epoch 168; iter: 0; batch classifier loss: 0.014724; batch adversarial loss: 0.472323\n",
      "epoch 169; iter: 0; batch classifier loss: 0.026663; batch adversarial loss: 0.419173\n",
      "epoch 170; iter: 0; batch classifier loss: 0.028213; batch adversarial loss: 0.459027\n",
      "epoch 171; iter: 0; batch classifier loss: 0.016315; batch adversarial loss: 0.585825\n",
      "epoch 172; iter: 0; batch classifier loss: 0.012164; batch adversarial loss: 0.381909\n",
      "epoch 173; iter: 0; batch classifier loss: 0.029465; batch adversarial loss: 0.403490\n",
      "epoch 174; iter: 0; batch classifier loss: 0.033316; batch adversarial loss: 0.454936\n",
      "epoch 175; iter: 0; batch classifier loss: 0.032204; batch adversarial loss: 0.354676\n",
      "epoch 176; iter: 0; batch classifier loss: 0.015168; batch adversarial loss: 0.315848\n",
      "epoch 177; iter: 0; batch classifier loss: 0.026135; batch adversarial loss: 0.356428\n",
      "epoch 178; iter: 0; batch classifier loss: 0.020143; batch adversarial loss: 0.430414\n",
      "epoch 179; iter: 0; batch classifier loss: 0.015638; batch adversarial loss: 0.460614\n",
      "epoch 180; iter: 0; batch classifier loss: 0.031036; batch adversarial loss: 0.420324\n",
      "epoch 181; iter: 0; batch classifier loss: 0.021814; batch adversarial loss: 0.510702\n",
      "epoch 182; iter: 0; batch classifier loss: 0.030285; batch adversarial loss: 0.424993\n",
      "epoch 183; iter: 0; batch classifier loss: 0.013640; batch adversarial loss: 0.449881\n",
      "epoch 184; iter: 0; batch classifier loss: 0.030289; batch adversarial loss: 0.511931\n",
      "epoch 185; iter: 0; batch classifier loss: 0.031510; batch adversarial loss: 0.350718\n",
      "epoch 186; iter: 0; batch classifier loss: 0.026708; batch adversarial loss: 0.362294\n",
      "epoch 187; iter: 0; batch classifier loss: 0.030780; batch adversarial loss: 0.406788\n",
      "epoch 188; iter: 0; batch classifier loss: 0.010177; batch adversarial loss: 0.355876\n",
      "epoch 189; iter: 0; batch classifier loss: 0.028933; batch adversarial loss: 0.375490\n",
      "epoch 190; iter: 0; batch classifier loss: 0.043804; batch adversarial loss: 0.382073\n",
      "epoch 191; iter: 0; batch classifier loss: 0.014378; batch adversarial loss: 0.425310\n",
      "epoch 192; iter: 0; batch classifier loss: 0.008747; batch adversarial loss: 0.461982\n",
      "epoch 193; iter: 0; batch classifier loss: 0.021885; batch adversarial loss: 0.468137\n",
      "epoch 194; iter: 0; batch classifier loss: 0.022056; batch adversarial loss: 0.467398\n",
      "epoch 195; iter: 0; batch classifier loss: 0.010530; batch adversarial loss: 0.489523\n",
      "epoch 196; iter: 0; batch classifier loss: 0.014399; batch adversarial loss: 0.441948\n",
      "epoch 197; iter: 0; batch classifier loss: 0.015011; batch adversarial loss: 0.466959\n",
      "epoch 198; iter: 0; batch classifier loss: 0.029940; batch adversarial loss: 0.474356\n",
      "epoch 199; iter: 0; batch classifier loss: 0.028664; batch adversarial loss: 0.472153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:56:24.870553: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44dba-ae24-11ee-be98-ef9b34f2853b/04a44dba-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:12082 op device:{requested: '', assigned: ''} def:{{{node 04a44dba-ae24-11ee-be98-ef9b34f2853b/04a44dba-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44dba-ae24-11ee-be98-ef9b34f2853b/04a44dba-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44dba-ae24-11ee-be98-ef9b34f2853b/04a44dba-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.691798; batch adversarial loss: 0.922252\n",
      "epoch 1; iter: 0; batch classifier loss: 0.670368; batch adversarial loss: 0.975829\n",
      "epoch 2; iter: 0; batch classifier loss: 0.932354; batch adversarial loss: 1.005665\n",
      "epoch 3; iter: 0; batch classifier loss: 0.995136; batch adversarial loss: 0.899219\n",
      "epoch 4; iter: 0; batch classifier loss: 1.063127; batch adversarial loss: 0.837535\n",
      "epoch 5; iter: 0; batch classifier loss: 0.875866; batch adversarial loss: 0.730490\n",
      "epoch 6; iter: 0; batch classifier loss: 0.775888; batch adversarial loss: 0.652571\n",
      "epoch 7; iter: 0; batch classifier loss: 0.652290; batch adversarial loss: 0.630379\n",
      "epoch 8; iter: 0; batch classifier loss: 0.517922; batch adversarial loss: 0.534116\n",
      "epoch 9; iter: 0; batch classifier loss: 0.393408; batch adversarial loss: 0.550994\n",
      "epoch 10; iter: 0; batch classifier loss: 0.327496; batch adversarial loss: 0.538105\n",
      "epoch 11; iter: 0; batch classifier loss: 0.269574; batch adversarial loss: 0.550115\n",
      "epoch 12; iter: 0; batch classifier loss: 0.281649; batch adversarial loss: 0.545857\n",
      "epoch 13; iter: 0; batch classifier loss: 0.257935; batch adversarial loss: 0.542528\n",
      "epoch 14; iter: 0; batch classifier loss: 0.328239; batch adversarial loss: 0.524874\n",
      "epoch 15; iter: 0; batch classifier loss: 0.263198; batch adversarial loss: 0.531336\n",
      "epoch 16; iter: 0; batch classifier loss: 0.261644; batch adversarial loss: 0.503169\n",
      "epoch 17; iter: 0; batch classifier loss: 0.269565; batch adversarial loss: 0.532659\n",
      "epoch 18; iter: 0; batch classifier loss: 0.321007; batch adversarial loss: 0.527828\n",
      "epoch 19; iter: 0; batch classifier loss: 0.255065; batch adversarial loss: 0.494351\n",
      "epoch 20; iter: 0; batch classifier loss: 0.254649; batch adversarial loss: 0.429623\n",
      "epoch 21; iter: 0; batch classifier loss: 0.261853; batch adversarial loss: 0.490712\n",
      "epoch 22; iter: 0; batch classifier loss: 0.213755; batch adversarial loss: 0.415272\n",
      "epoch 23; iter: 0; batch classifier loss: 0.207056; batch adversarial loss: 0.515513\n",
      "epoch 24; iter: 0; batch classifier loss: 0.172669; batch adversarial loss: 0.451321\n",
      "epoch 25; iter: 0; batch classifier loss: 0.347311; batch adversarial loss: 0.461330\n",
      "epoch 26; iter: 0; batch classifier loss: 0.156011; batch adversarial loss: 0.563620\n",
      "epoch 27; iter: 0; batch classifier loss: 0.191806; batch adversarial loss: 0.548212\n",
      "epoch 28; iter: 0; batch classifier loss: 0.213610; batch adversarial loss: 0.406235\n",
      "epoch 29; iter: 0; batch classifier loss: 0.164955; batch adversarial loss: 0.497447\n",
      "epoch 30; iter: 0; batch classifier loss: 0.240369; batch adversarial loss: 0.432520\n",
      "epoch 31; iter: 0; batch classifier loss: 0.217921; batch adversarial loss: 0.430588\n",
      "epoch 32; iter: 0; batch classifier loss: 0.254701; batch adversarial loss: 0.415801\n",
      "epoch 33; iter: 0; batch classifier loss: 0.175130; batch adversarial loss: 0.428482\n",
      "epoch 34; iter: 0; batch classifier loss: 0.176385; batch adversarial loss: 0.450621\n",
      "epoch 35; iter: 0; batch classifier loss: 0.215349; batch adversarial loss: 0.506532\n",
      "epoch 36; iter: 0; batch classifier loss: 0.161388; batch adversarial loss: 0.491907\n",
      "epoch 37; iter: 0; batch classifier loss: 0.181611; batch adversarial loss: 0.464269\n",
      "epoch 38; iter: 0; batch classifier loss: 0.171276; batch adversarial loss: 0.522529\n",
      "epoch 39; iter: 0; batch classifier loss: 0.176064; batch adversarial loss: 0.413711\n",
      "epoch 40; iter: 0; batch classifier loss: 0.160192; batch adversarial loss: 0.463796\n",
      "epoch 41; iter: 0; batch classifier loss: 0.128185; batch adversarial loss: 0.481254\n",
      "epoch 42; iter: 0; batch classifier loss: 0.200017; batch adversarial loss: 0.503235\n",
      "epoch 43; iter: 0; batch classifier loss: 0.198514; batch adversarial loss: 0.477505\n",
      "epoch 44; iter: 0; batch classifier loss: 0.185112; batch adversarial loss: 0.481996\n",
      "epoch 45; iter: 0; batch classifier loss: 0.210762; batch adversarial loss: 0.503799\n",
      "epoch 46; iter: 0; batch classifier loss: 0.238747; batch adversarial loss: 0.525780\n",
      "epoch 47; iter: 0; batch classifier loss: 0.133356; batch adversarial loss: 0.435277\n",
      "epoch 48; iter: 0; batch classifier loss: 0.168109; batch adversarial loss: 0.463880\n",
      "epoch 49; iter: 0; batch classifier loss: 0.120164; batch adversarial loss: 0.407718\n",
      "epoch 50; iter: 0; batch classifier loss: 0.144122; batch adversarial loss: 0.397841\n",
      "epoch 51; iter: 0; batch classifier loss: 0.141319; batch adversarial loss: 0.367010\n",
      "epoch 52; iter: 0; batch classifier loss: 0.126296; batch adversarial loss: 0.428373\n",
      "epoch 53; iter: 0; batch classifier loss: 0.139502; batch adversarial loss: 0.452014\n",
      "epoch 54; iter: 0; batch classifier loss: 0.136651; batch adversarial loss: 0.435871\n",
      "epoch 55; iter: 0; batch classifier loss: 0.127082; batch adversarial loss: 0.508636\n",
      "epoch 56; iter: 0; batch classifier loss: 0.119103; batch adversarial loss: 0.392965\n",
      "epoch 57; iter: 0; batch classifier loss: 0.139181; batch adversarial loss: 0.471632\n",
      "epoch 58; iter: 0; batch classifier loss: 0.159095; batch adversarial loss: 0.446866\n",
      "epoch 59; iter: 0; batch classifier loss: 0.165663; batch adversarial loss: 0.480759\n",
      "epoch 60; iter: 0; batch classifier loss: 0.180554; batch adversarial loss: 0.345602\n",
      "epoch 61; iter: 0; batch classifier loss: 0.151476; batch adversarial loss: 0.528475\n",
      "epoch 62; iter: 0; batch classifier loss: 0.116959; batch adversarial loss: 0.508741\n",
      "epoch 63; iter: 0; batch classifier loss: 0.161562; batch adversarial loss: 0.435419\n",
      "epoch 64; iter: 0; batch classifier loss: 0.164404; batch adversarial loss: 0.367669\n",
      "epoch 65; iter: 0; batch classifier loss: 0.116039; batch adversarial loss: 0.525819\n",
      "epoch 66; iter: 0; batch classifier loss: 0.154912; batch adversarial loss: 0.451451\n",
      "epoch 67; iter: 0; batch classifier loss: 0.130657; batch adversarial loss: 0.415608\n",
      "epoch 68; iter: 0; batch classifier loss: 0.137948; batch adversarial loss: 0.399102\n",
      "epoch 69; iter: 0; batch classifier loss: 0.137067; batch adversarial loss: 0.440499\n",
      "epoch 70; iter: 0; batch classifier loss: 0.147067; batch adversarial loss: 0.456555\n",
      "epoch 71; iter: 0; batch classifier loss: 0.125347; batch adversarial loss: 0.512063\n",
      "epoch 72; iter: 0; batch classifier loss: 0.099773; batch adversarial loss: 0.503229\n",
      "epoch 73; iter: 0; batch classifier loss: 0.127172; batch adversarial loss: 0.526740\n",
      "epoch 74; iter: 0; batch classifier loss: 0.081685; batch adversarial loss: 0.492333\n",
      "epoch 75; iter: 0; batch classifier loss: 0.132541; batch adversarial loss: 0.466555\n",
      "epoch 76; iter: 0; batch classifier loss: 0.171733; batch adversarial loss: 0.380263\n",
      "epoch 77; iter: 0; batch classifier loss: 0.103633; batch adversarial loss: 0.526200\n",
      "epoch 78; iter: 0; batch classifier loss: 0.100809; batch adversarial loss: 0.421861\n",
      "epoch 79; iter: 0; batch classifier loss: 0.062331; batch adversarial loss: 0.464757\n",
      "epoch 80; iter: 0; batch classifier loss: 0.151196; batch adversarial loss: 0.451088\n",
      "epoch 81; iter: 0; batch classifier loss: 0.108878; batch adversarial loss: 0.420976\n",
      "epoch 82; iter: 0; batch classifier loss: 0.109824; batch adversarial loss: 0.460367\n",
      "epoch 83; iter: 0; batch classifier loss: 0.161195; batch adversarial loss: 0.360066\n",
      "epoch 84; iter: 0; batch classifier loss: 0.092720; batch adversarial loss: 0.406794\n",
      "epoch 85; iter: 0; batch classifier loss: 0.079732; batch adversarial loss: 0.497559\n",
      "epoch 86; iter: 0; batch classifier loss: 0.120006; batch adversarial loss: 0.505808\n",
      "epoch 87; iter: 0; batch classifier loss: 0.109578; batch adversarial loss: 0.467954\n",
      "epoch 88; iter: 0; batch classifier loss: 0.094972; batch adversarial loss: 0.416776\n",
      "epoch 89; iter: 0; batch classifier loss: 0.086409; batch adversarial loss: 0.481599\n",
      "epoch 90; iter: 0; batch classifier loss: 0.112865; batch adversarial loss: 0.506287\n",
      "epoch 91; iter: 0; batch classifier loss: 0.105421; batch adversarial loss: 0.445852\n",
      "epoch 92; iter: 0; batch classifier loss: 0.092598; batch adversarial loss: 0.512352\n",
      "epoch 93; iter: 0; batch classifier loss: 0.037604; batch adversarial loss: 0.458678\n",
      "epoch 94; iter: 0; batch classifier loss: 0.128426; batch adversarial loss: 0.461210\n",
      "epoch 95; iter: 0; batch classifier loss: 0.082986; batch adversarial loss: 0.483927\n",
      "epoch 96; iter: 0; batch classifier loss: 0.081699; batch adversarial loss: 0.499725\n",
      "epoch 97; iter: 0; batch classifier loss: 0.125748; batch adversarial loss: 0.542530\n",
      "epoch 98; iter: 0; batch classifier loss: 0.103540; batch adversarial loss: 0.495488\n",
      "epoch 99; iter: 0; batch classifier loss: 0.125773; batch adversarial loss: 0.455372\n",
      "epoch 100; iter: 0; batch classifier loss: 0.062087; batch adversarial loss: 0.467962\n",
      "epoch 101; iter: 0; batch classifier loss: 0.045122; batch adversarial loss: 0.430633\n",
      "epoch 102; iter: 0; batch classifier loss: 0.084463; batch adversarial loss: 0.439047\n",
      "epoch 103; iter: 0; batch classifier loss: 0.062964; batch adversarial loss: 0.448760\n",
      "epoch 104; iter: 0; batch classifier loss: 0.078698; batch adversarial loss: 0.472325\n",
      "epoch 105; iter: 0; batch classifier loss: 0.059821; batch adversarial loss: 0.450662\n",
      "epoch 106; iter: 0; batch classifier loss: 0.056868; batch adversarial loss: 0.549873\n",
      "epoch 107; iter: 0; batch classifier loss: 0.064088; batch adversarial loss: 0.445579\n",
      "epoch 108; iter: 0; batch classifier loss: 0.061880; batch adversarial loss: 0.538416\n",
      "epoch 109; iter: 0; batch classifier loss: 0.043395; batch adversarial loss: 0.436376\n",
      "epoch 110; iter: 0; batch classifier loss: 0.051576; batch adversarial loss: 0.417789\n",
      "epoch 111; iter: 0; batch classifier loss: 0.050034; batch adversarial loss: 0.459838\n",
      "epoch 112; iter: 0; batch classifier loss: 0.070116; batch adversarial loss: 0.553040\n",
      "epoch 113; iter: 0; batch classifier loss: 0.025100; batch adversarial loss: 0.474007\n",
      "epoch 114; iter: 0; batch classifier loss: 0.094490; batch adversarial loss: 0.428451\n",
      "epoch 115; iter: 0; batch classifier loss: 0.045221; batch adversarial loss: 0.482209\n",
      "epoch 116; iter: 0; batch classifier loss: 0.022390; batch adversarial loss: 0.456698\n",
      "epoch 117; iter: 0; batch classifier loss: 0.060766; batch adversarial loss: 0.448771\n",
      "epoch 118; iter: 0; batch classifier loss: 0.024907; batch adversarial loss: 0.503756\n",
      "epoch 119; iter: 0; batch classifier loss: 0.058428; batch adversarial loss: 0.430575\n",
      "epoch 120; iter: 0; batch classifier loss: 0.037997; batch adversarial loss: 0.439637\n",
      "epoch 121; iter: 0; batch classifier loss: 0.035780; batch adversarial loss: 0.356492\n",
      "epoch 122; iter: 0; batch classifier loss: 0.059612; batch adversarial loss: 0.454661\n",
      "epoch 123; iter: 0; batch classifier loss: 0.043171; batch adversarial loss: 0.466677\n",
      "epoch 124; iter: 0; batch classifier loss: 0.026113; batch adversarial loss: 0.485766\n",
      "epoch 125; iter: 0; batch classifier loss: 0.057778; batch adversarial loss: 0.431370\n",
      "epoch 126; iter: 0; batch classifier loss: 0.046354; batch adversarial loss: 0.479915\n",
      "epoch 127; iter: 0; batch classifier loss: 0.052881; batch adversarial loss: 0.467398\n",
      "epoch 128; iter: 0; batch classifier loss: 0.014553; batch adversarial loss: 0.484394\n",
      "epoch 129; iter: 0; batch classifier loss: 0.026745; batch adversarial loss: 0.437275\n",
      "epoch 130; iter: 0; batch classifier loss: 0.036753; batch adversarial loss: 0.353732\n",
      "epoch 131; iter: 0; batch classifier loss: 0.045443; batch adversarial loss: 0.427757\n",
      "epoch 132; iter: 0; batch classifier loss: 0.024167; batch adversarial loss: 0.464082\n",
      "epoch 133; iter: 0; batch classifier loss: 0.028759; batch adversarial loss: 0.471063\n",
      "epoch 134; iter: 0; batch classifier loss: 0.025715; batch adversarial loss: 0.464163\n",
      "epoch 135; iter: 0; batch classifier loss: 0.027765; batch adversarial loss: 0.582146\n",
      "epoch 136; iter: 0; batch classifier loss: 0.035279; batch adversarial loss: 0.518245\n",
      "epoch 137; iter: 0; batch classifier loss: 0.025477; batch adversarial loss: 0.420102\n",
      "epoch 138; iter: 0; batch classifier loss: 0.034970; batch adversarial loss: 0.430672\n",
      "epoch 139; iter: 0; batch classifier loss: 0.020500; batch adversarial loss: 0.328586\n",
      "epoch 140; iter: 0; batch classifier loss: 0.022696; batch adversarial loss: 0.427264\n",
      "epoch 141; iter: 0; batch classifier loss: 0.013945; batch adversarial loss: 0.392956\n",
      "epoch 142; iter: 0; batch classifier loss: 0.005668; batch adversarial loss: 0.543670\n",
      "epoch 143; iter: 0; batch classifier loss: 0.014700; batch adversarial loss: 0.382436\n",
      "epoch 144; iter: 0; batch classifier loss: 0.013040; batch adversarial loss: 0.456712\n",
      "epoch 145; iter: 0; batch classifier loss: 0.016794; batch adversarial loss: 0.623643\n",
      "epoch 146; iter: 0; batch classifier loss: 0.020994; batch adversarial loss: 0.381921\n",
      "epoch 147; iter: 0; batch classifier loss: 0.033273; batch adversarial loss: 0.431318\n",
      "epoch 148; iter: 0; batch classifier loss: 0.030143; batch adversarial loss: 0.433886\n",
      "epoch 149; iter: 0; batch classifier loss: 0.016018; batch adversarial loss: 0.365106\n",
      "epoch 150; iter: 0; batch classifier loss: 0.016218; batch adversarial loss: 0.409365\n",
      "epoch 151; iter: 0; batch classifier loss: 0.022037; batch adversarial loss: 0.333745\n",
      "epoch 152; iter: 0; batch classifier loss: 0.021415; batch adversarial loss: 0.386016\n",
      "epoch 153; iter: 0; batch classifier loss: 0.009992; batch adversarial loss: 0.513607\n",
      "epoch 154; iter: 0; batch classifier loss: 0.021182; batch adversarial loss: 0.411289\n",
      "epoch 155; iter: 0; batch classifier loss: 0.028462; batch adversarial loss: 0.496820\n",
      "epoch 156; iter: 0; batch classifier loss: 0.003134; batch adversarial loss: 0.550007\n",
      "epoch 157; iter: 0; batch classifier loss: 0.017803; batch adversarial loss: 0.483262\n",
      "epoch 158; iter: 0; batch classifier loss: 0.003309; batch adversarial loss: 0.432772\n",
      "epoch 159; iter: 0; batch classifier loss: 0.017633; batch adversarial loss: 0.327870\n",
      "epoch 160; iter: 0; batch classifier loss: 0.034745; batch adversarial loss: 0.385430\n",
      "epoch 161; iter: 0; batch classifier loss: 0.013353; batch adversarial loss: 0.434665\n",
      "epoch 162; iter: 0; batch classifier loss: 0.010313; batch adversarial loss: 0.511951\n",
      "epoch 163; iter: 0; batch classifier loss: 0.006446; batch adversarial loss: 0.483959\n",
      "epoch 164; iter: 0; batch classifier loss: 0.023146; batch adversarial loss: 0.562227\n",
      "epoch 165; iter: 0; batch classifier loss: 0.013256; batch adversarial loss: 0.475309\n",
      "epoch 166; iter: 0; batch classifier loss: 0.006712; batch adversarial loss: 0.425766\n",
      "epoch 167; iter: 0; batch classifier loss: 0.024027; batch adversarial loss: 0.599266\n",
      "epoch 168; iter: 0; batch classifier loss: 0.033714; batch adversarial loss: 0.505307\n",
      "epoch 169; iter: 0; batch classifier loss: 0.004132; batch adversarial loss: 0.510636\n",
      "epoch 170; iter: 0; batch classifier loss: 0.023619; batch adversarial loss: 0.433336\n",
      "epoch 171; iter: 0; batch classifier loss: 0.010020; batch adversarial loss: 0.488712\n",
      "epoch 172; iter: 0; batch classifier loss: 0.007906; batch adversarial loss: 0.512350\n",
      "epoch 173; iter: 0; batch classifier loss: 0.008608; batch adversarial loss: 0.417615\n",
      "epoch 174; iter: 0; batch classifier loss: 0.022265; batch adversarial loss: 0.439469\n",
      "epoch 175; iter: 0; batch classifier loss: 0.006867; batch adversarial loss: 0.489528\n",
      "epoch 176; iter: 0; batch classifier loss: 0.009472; batch adversarial loss: 0.475872\n",
      "epoch 177; iter: 0; batch classifier loss: 0.015481; batch adversarial loss: 0.460237\n",
      "epoch 178; iter: 0; batch classifier loss: 0.011741; batch adversarial loss: 0.562833\n",
      "epoch 179; iter: 0; batch classifier loss: 0.017235; batch adversarial loss: 0.479560\n",
      "epoch 180; iter: 0; batch classifier loss: 0.026420; batch adversarial loss: 0.434705\n",
      "epoch 181; iter: 0; batch classifier loss: 0.018426; batch adversarial loss: 0.471895\n",
      "epoch 182; iter: 0; batch classifier loss: 0.020538; batch adversarial loss: 0.529637\n",
      "epoch 183; iter: 0; batch classifier loss: 0.027222; batch adversarial loss: 0.421572\n",
      "epoch 184; iter: 0; batch classifier loss: 0.006982; batch adversarial loss: 0.558652\n",
      "epoch 185; iter: 0; batch classifier loss: 0.004962; batch adversarial loss: 0.449380\n",
      "epoch 186; iter: 0; batch classifier loss: 0.009105; batch adversarial loss: 0.488787\n",
      "epoch 187; iter: 0; batch classifier loss: 0.003109; batch adversarial loss: 0.400244\n",
      "epoch 188; iter: 0; batch classifier loss: 0.007168; batch adversarial loss: 0.593224\n",
      "epoch 189; iter: 0; batch classifier loss: 0.008567; batch adversarial loss: 0.373686\n",
      "epoch 190; iter: 0; batch classifier loss: 0.008015; batch adversarial loss: 0.403163\n",
      "epoch 191; iter: 0; batch classifier loss: 0.014900; batch adversarial loss: 0.461618\n",
      "epoch 192; iter: 0; batch classifier loss: 0.008002; batch adversarial loss: 0.408734\n",
      "epoch 193; iter: 0; batch classifier loss: 0.018896; batch adversarial loss: 0.488519\n",
      "epoch 194; iter: 0; batch classifier loss: 0.006533; batch adversarial loss: 0.484263\n",
      "epoch 195; iter: 0; batch classifier loss: 0.015032; batch adversarial loss: 0.464223\n",
      "epoch 196; iter: 0; batch classifier loss: 0.017296; batch adversarial loss: 0.462529\n",
      "epoch 197; iter: 0; batch classifier loss: 0.024138; batch adversarial loss: 0.494385\n",
      "epoch 198; iter: 0; batch classifier loss: 0.003035; batch adversarial loss: 0.430043\n",
      "epoch 199; iter: 0; batch classifier loss: 0.023916; batch adversarial loss: 0.410182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:57:00.337253: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44e28-ae24-11ee-be98-ef9b34f2853b/04a44e28-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:12889 op device:{requested: '', assigned: ''} def:{{{node 04a44e28-ae24-11ee-be98-ef9b34f2853b/04a44e28-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44e28-ae24-11ee-be98-ef9b34f2853b/04a44e28-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44e28-ae24-11ee-be98-ef9b34f2853b/04a44e28-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.683173; batch adversarial loss: 1.049921\n",
      "epoch 1; iter: 0; batch classifier loss: 0.659012; batch adversarial loss: 1.294042\n",
      "epoch 2; iter: 0; batch classifier loss: 0.852268; batch adversarial loss: 1.389852\n",
      "epoch 3; iter: 0; batch classifier loss: 0.887420; batch adversarial loss: 1.356849\n",
      "epoch 4; iter: 0; batch classifier loss: 1.185821; batch adversarial loss: 1.152256\n",
      "epoch 5; iter: 0; batch classifier loss: 0.980375; batch adversarial loss: 1.103120\n",
      "epoch 6; iter: 0; batch classifier loss: 0.870693; batch adversarial loss: 1.036124\n",
      "epoch 7; iter: 0; batch classifier loss: 0.995026; batch adversarial loss: 0.901514\n",
      "epoch 8; iter: 0; batch classifier loss: 0.785636; batch adversarial loss: 0.829812\n",
      "epoch 9; iter: 0; batch classifier loss: 0.580449; batch adversarial loss: 0.758735\n",
      "epoch 10; iter: 0; batch classifier loss: 0.404401; batch adversarial loss: 0.693150\n",
      "epoch 11; iter: 0; batch classifier loss: 0.295553; batch adversarial loss: 0.644169\n",
      "epoch 12; iter: 0; batch classifier loss: 0.194128; batch adversarial loss: 0.606602\n",
      "epoch 13; iter: 0; batch classifier loss: 0.265217; batch adversarial loss: 0.604152\n",
      "epoch 14; iter: 0; batch classifier loss: 0.201921; batch adversarial loss: 0.549006\n",
      "epoch 15; iter: 0; batch classifier loss: 0.233600; batch adversarial loss: 0.569455\n",
      "epoch 16; iter: 0; batch classifier loss: 0.196668; batch adversarial loss: 0.515597\n",
      "epoch 17; iter: 0; batch classifier loss: 0.219437; batch adversarial loss: 0.556199\n",
      "epoch 18; iter: 0; batch classifier loss: 0.248595; batch adversarial loss: 0.483811\n",
      "epoch 19; iter: 0; batch classifier loss: 0.229672; batch adversarial loss: 0.502449\n",
      "epoch 20; iter: 0; batch classifier loss: 0.285478; batch adversarial loss: 0.494282\n",
      "epoch 21; iter: 0; batch classifier loss: 0.291613; batch adversarial loss: 0.473587\n",
      "epoch 22; iter: 0; batch classifier loss: 0.221297; batch adversarial loss: 0.543699\n",
      "epoch 23; iter: 0; batch classifier loss: 0.233642; batch adversarial loss: 0.517674\n",
      "epoch 24; iter: 0; batch classifier loss: 0.211319; batch adversarial loss: 0.484413\n",
      "epoch 25; iter: 0; batch classifier loss: 0.243330; batch adversarial loss: 0.492299\n",
      "epoch 26; iter: 0; batch classifier loss: 0.283794; batch adversarial loss: 0.471670\n",
      "epoch 27; iter: 0; batch classifier loss: 0.222106; batch adversarial loss: 0.497997\n",
      "epoch 28; iter: 0; batch classifier loss: 0.238535; batch adversarial loss: 0.412311\n",
      "epoch 29; iter: 0; batch classifier loss: 0.271536; batch adversarial loss: 0.411311\n",
      "epoch 30; iter: 0; batch classifier loss: 0.270900; batch adversarial loss: 0.409586\n",
      "epoch 31; iter: 0; batch classifier loss: 0.242439; batch adversarial loss: 0.462911\n",
      "epoch 32; iter: 0; batch classifier loss: 0.259111; batch adversarial loss: 0.436224\n",
      "epoch 33; iter: 0; batch classifier loss: 0.329957; batch adversarial loss: 0.390133\n",
      "epoch 34; iter: 0; batch classifier loss: 0.275911; batch adversarial loss: 0.462250\n",
      "epoch 35; iter: 0; batch classifier loss: 0.318678; batch adversarial loss: 0.548899\n",
      "epoch 36; iter: 0; batch classifier loss: 0.261455; batch adversarial loss: 0.495476\n",
      "epoch 37; iter: 0; batch classifier loss: 0.199135; batch adversarial loss: 0.499213\n",
      "epoch 38; iter: 0; batch classifier loss: 0.180128; batch adversarial loss: 0.420547\n",
      "epoch 39; iter: 0; batch classifier loss: 0.128853; batch adversarial loss: 0.389010\n",
      "epoch 40; iter: 0; batch classifier loss: 0.118167; batch adversarial loss: 0.441202\n",
      "epoch 41; iter: 0; batch classifier loss: 0.121514; batch adversarial loss: 0.451271\n",
      "epoch 42; iter: 0; batch classifier loss: 0.133198; batch adversarial loss: 0.518759\n",
      "epoch 43; iter: 0; batch classifier loss: 0.080151; batch adversarial loss: 0.450527\n",
      "epoch 44; iter: 0; batch classifier loss: 0.074663; batch adversarial loss: 0.586850\n",
      "epoch 45; iter: 0; batch classifier loss: 0.081619; batch adversarial loss: 0.481126\n",
      "epoch 46; iter: 0; batch classifier loss: 0.120688; batch adversarial loss: 0.424653\n",
      "epoch 47; iter: 0; batch classifier loss: 0.057852; batch adversarial loss: 0.445928\n",
      "epoch 48; iter: 0; batch classifier loss: 0.095598; batch adversarial loss: 0.397113\n",
      "epoch 49; iter: 0; batch classifier loss: 0.052999; batch adversarial loss: 0.441923\n",
      "epoch 50; iter: 0; batch classifier loss: 0.097911; batch adversarial loss: 0.368389\n",
      "epoch 51; iter: 0; batch classifier loss: 0.092212; batch adversarial loss: 0.394690\n",
      "epoch 52; iter: 0; batch classifier loss: 0.057943; batch adversarial loss: 0.453987\n",
      "epoch 53; iter: 0; batch classifier loss: 0.051236; batch adversarial loss: 0.476328\n",
      "epoch 54; iter: 0; batch classifier loss: 0.072114; batch adversarial loss: 0.472815\n",
      "epoch 55; iter: 0; batch classifier loss: 0.076795; batch adversarial loss: 0.414301\n",
      "epoch 56; iter: 0; batch classifier loss: 0.074049; batch adversarial loss: 0.457203\n",
      "epoch 57; iter: 0; batch classifier loss: 0.068611; batch adversarial loss: 0.338148\n",
      "epoch 58; iter: 0; batch classifier loss: 0.032670; batch adversarial loss: 0.452260\n",
      "epoch 59; iter: 0; batch classifier loss: 0.082137; batch adversarial loss: 0.472312\n",
      "epoch 60; iter: 0; batch classifier loss: 0.095668; batch adversarial loss: 0.462129\n",
      "epoch 61; iter: 0; batch classifier loss: 0.071830; batch adversarial loss: 0.509778\n",
      "epoch 62; iter: 0; batch classifier loss: 0.049495; batch adversarial loss: 0.491524\n",
      "epoch 63; iter: 0; batch classifier loss: 0.077188; batch adversarial loss: 0.534518\n",
      "epoch 64; iter: 0; batch classifier loss: 0.057619; batch adversarial loss: 0.390009\n",
      "epoch 65; iter: 0; batch classifier loss: 0.051342; batch adversarial loss: 0.480506\n",
      "epoch 66; iter: 0; batch classifier loss: 0.076442; batch adversarial loss: 0.393973\n",
      "epoch 67; iter: 0; batch classifier loss: 0.071741; batch adversarial loss: 0.528924\n",
      "epoch 68; iter: 0; batch classifier loss: 0.105712; batch adversarial loss: 0.433323\n",
      "epoch 69; iter: 0; batch classifier loss: 0.096135; batch adversarial loss: 0.367246\n",
      "epoch 70; iter: 0; batch classifier loss: 0.033517; batch adversarial loss: 0.455427\n",
      "epoch 71; iter: 0; batch classifier loss: 0.044060; batch adversarial loss: 0.523440\n",
      "epoch 72; iter: 0; batch classifier loss: 0.041334; batch adversarial loss: 0.411515\n",
      "epoch 73; iter: 0; batch classifier loss: 0.071680; batch adversarial loss: 0.401118\n",
      "epoch 74; iter: 0; batch classifier loss: 0.036113; batch adversarial loss: 0.470095\n",
      "epoch 75; iter: 0; batch classifier loss: 0.071176; batch adversarial loss: 0.449760\n",
      "epoch 76; iter: 0; batch classifier loss: 0.072685; batch adversarial loss: 0.424968\n",
      "epoch 77; iter: 0; batch classifier loss: 0.048961; batch adversarial loss: 0.536268\n",
      "epoch 78; iter: 0; batch classifier loss: 0.073866; batch adversarial loss: 0.558017\n",
      "epoch 79; iter: 0; batch classifier loss: 0.063921; batch adversarial loss: 0.390896\n",
      "epoch 80; iter: 0; batch classifier loss: 0.080938; batch adversarial loss: 0.471910\n",
      "epoch 81; iter: 0; batch classifier loss: 0.037194; batch adversarial loss: 0.523395\n",
      "epoch 82; iter: 0; batch classifier loss: 0.068936; batch adversarial loss: 0.482594\n",
      "epoch 83; iter: 0; batch classifier loss: 0.071657; batch adversarial loss: 0.454446\n",
      "epoch 84; iter: 0; batch classifier loss: 0.064186; batch adversarial loss: 0.481328\n",
      "epoch 85; iter: 0; batch classifier loss: 0.068625; batch adversarial loss: 0.447050\n",
      "epoch 86; iter: 0; batch classifier loss: 0.068853; batch adversarial loss: 0.489011\n",
      "epoch 87; iter: 0; batch classifier loss: 0.051716; batch adversarial loss: 0.407238\n",
      "epoch 88; iter: 0; batch classifier loss: 0.027333; batch adversarial loss: 0.454455\n",
      "epoch 89; iter: 0; batch classifier loss: 0.059669; batch adversarial loss: 0.507223\n",
      "epoch 90; iter: 0; batch classifier loss: 0.085284; batch adversarial loss: 0.418706\n",
      "epoch 91; iter: 0; batch classifier loss: 0.080299; batch adversarial loss: 0.441198\n",
      "epoch 92; iter: 0; batch classifier loss: 0.061665; batch adversarial loss: 0.406142\n",
      "epoch 93; iter: 0; batch classifier loss: 0.059954; batch adversarial loss: 0.465156\n",
      "epoch 94; iter: 0; batch classifier loss: 0.026381; batch adversarial loss: 0.468915\n",
      "epoch 95; iter: 0; batch classifier loss: 0.061561; batch adversarial loss: 0.485627\n",
      "epoch 96; iter: 0; batch classifier loss: 0.042667; batch adversarial loss: 0.511770\n",
      "epoch 97; iter: 0; batch classifier loss: 0.047568; batch adversarial loss: 0.445790\n",
      "epoch 98; iter: 0; batch classifier loss: 0.045836; batch adversarial loss: 0.461310\n",
      "epoch 99; iter: 0; batch classifier loss: 0.031729; batch adversarial loss: 0.495365\n",
      "epoch 100; iter: 0; batch classifier loss: 0.051576; batch adversarial loss: 0.324551\n",
      "epoch 101; iter: 0; batch classifier loss: 0.041439; batch adversarial loss: 0.492555\n",
      "epoch 102; iter: 0; batch classifier loss: 0.031178; batch adversarial loss: 0.472125\n",
      "epoch 103; iter: 0; batch classifier loss: 0.065298; batch adversarial loss: 0.404785\n",
      "epoch 104; iter: 0; batch classifier loss: 0.035882; batch adversarial loss: 0.405267\n",
      "epoch 105; iter: 0; batch classifier loss: 0.021336; batch adversarial loss: 0.418003\n",
      "epoch 106; iter: 0; batch classifier loss: 0.052479; batch adversarial loss: 0.482770\n",
      "epoch 107; iter: 0; batch classifier loss: 0.063304; batch adversarial loss: 0.452910\n",
      "epoch 108; iter: 0; batch classifier loss: 0.050220; batch adversarial loss: 0.439795\n",
      "epoch 109; iter: 0; batch classifier loss: 0.033977; batch adversarial loss: 0.408693\n",
      "epoch 110; iter: 0; batch classifier loss: 0.047902; batch adversarial loss: 0.479562\n",
      "epoch 111; iter: 0; batch classifier loss: 0.020360; batch adversarial loss: 0.478760\n",
      "epoch 112; iter: 0; batch classifier loss: 0.019321; batch adversarial loss: 0.453122\n",
      "epoch 113; iter: 0; batch classifier loss: 0.035143; batch adversarial loss: 0.482743\n",
      "epoch 114; iter: 0; batch classifier loss: 0.042118; batch adversarial loss: 0.435363\n",
      "epoch 115; iter: 0; batch classifier loss: 0.077624; batch adversarial loss: 0.426674\n",
      "epoch 116; iter: 0; batch classifier loss: 0.082355; batch adversarial loss: 0.491711\n",
      "epoch 117; iter: 0; batch classifier loss: 0.054365; batch adversarial loss: 0.530976\n",
      "epoch 118; iter: 0; batch classifier loss: 0.031306; batch adversarial loss: 0.400468\n",
      "epoch 119; iter: 0; batch classifier loss: 0.035871; batch adversarial loss: 0.403199\n",
      "epoch 120; iter: 0; batch classifier loss: 0.029964; batch adversarial loss: 0.528943\n",
      "epoch 121; iter: 0; batch classifier loss: 0.056882; batch adversarial loss: 0.454558\n",
      "epoch 122; iter: 0; batch classifier loss: 0.030440; batch adversarial loss: 0.461174\n",
      "epoch 123; iter: 0; batch classifier loss: 0.022704; batch adversarial loss: 0.454115\n",
      "epoch 124; iter: 0; batch classifier loss: 0.036738; batch adversarial loss: 0.386195\n",
      "epoch 125; iter: 0; batch classifier loss: 0.037575; batch adversarial loss: 0.446171\n",
      "epoch 126; iter: 0; batch classifier loss: 0.044618; batch adversarial loss: 0.492846\n",
      "epoch 127; iter: 0; batch classifier loss: 0.035752; batch adversarial loss: 0.388905\n",
      "epoch 128; iter: 0; batch classifier loss: 0.055110; batch adversarial loss: 0.447241\n",
      "epoch 129; iter: 0; batch classifier loss: 0.040441; batch adversarial loss: 0.391739\n",
      "epoch 130; iter: 0; batch classifier loss: 0.069561; batch adversarial loss: 0.404411\n",
      "epoch 131; iter: 0; batch classifier loss: 0.018621; batch adversarial loss: 0.463230\n",
      "epoch 132; iter: 0; batch classifier loss: 0.025612; batch adversarial loss: 0.465678\n",
      "epoch 133; iter: 0; batch classifier loss: 0.042573; batch adversarial loss: 0.517844\n",
      "epoch 134; iter: 0; batch classifier loss: 0.013700; batch adversarial loss: 0.393158\n",
      "epoch 135; iter: 0; batch classifier loss: 0.024097; batch adversarial loss: 0.452598\n",
      "epoch 136; iter: 0; batch classifier loss: 0.019685; batch adversarial loss: 0.522631\n",
      "epoch 137; iter: 0; batch classifier loss: 0.094325; batch adversarial loss: 0.448331\n",
      "epoch 138; iter: 0; batch classifier loss: 0.036813; batch adversarial loss: 0.432833\n",
      "epoch 139; iter: 0; batch classifier loss: 0.049778; batch adversarial loss: 0.501385\n",
      "epoch 140; iter: 0; batch classifier loss: 0.026342; batch adversarial loss: 0.420633\n",
      "epoch 141; iter: 0; batch classifier loss: 0.017557; batch adversarial loss: 0.537791\n",
      "epoch 142; iter: 0; batch classifier loss: 0.047349; batch adversarial loss: 0.566666\n",
      "epoch 143; iter: 0; batch classifier loss: 0.079512; batch adversarial loss: 0.396320\n",
      "epoch 144; iter: 0; batch classifier loss: 0.013171; batch adversarial loss: 0.550891\n",
      "epoch 145; iter: 0; batch classifier loss: 0.051790; batch adversarial loss: 0.470275\n",
      "epoch 146; iter: 0; batch classifier loss: 0.025705; batch adversarial loss: 0.394592\n",
      "epoch 147; iter: 0; batch classifier loss: 0.041579; batch adversarial loss: 0.476563\n",
      "epoch 148; iter: 0; batch classifier loss: 0.038159; batch adversarial loss: 0.435251\n",
      "epoch 149; iter: 0; batch classifier loss: 0.057664; batch adversarial loss: 0.452813\n",
      "epoch 150; iter: 0; batch classifier loss: 0.034810; batch adversarial loss: 0.492145\n",
      "epoch 151; iter: 0; batch classifier loss: 0.040383; batch adversarial loss: 0.511133\n",
      "epoch 152; iter: 0; batch classifier loss: 0.028024; batch adversarial loss: 0.489585\n",
      "epoch 153; iter: 0; batch classifier loss: 0.042013; batch adversarial loss: 0.508535\n",
      "epoch 154; iter: 0; batch classifier loss: 0.020684; batch adversarial loss: 0.411368\n",
      "epoch 155; iter: 0; batch classifier loss: 0.023965; batch adversarial loss: 0.473587\n",
      "epoch 156; iter: 0; batch classifier loss: 0.031640; batch adversarial loss: 0.481378\n",
      "epoch 157; iter: 0; batch classifier loss: 0.037403; batch adversarial loss: 0.484177\n",
      "epoch 158; iter: 0; batch classifier loss: 0.019616; batch adversarial loss: 0.506008\n",
      "epoch 159; iter: 0; batch classifier loss: 0.044864; batch adversarial loss: 0.457323\n",
      "epoch 160; iter: 0; batch classifier loss: 0.020470; batch adversarial loss: 0.478883\n",
      "epoch 161; iter: 0; batch classifier loss: 0.057801; batch adversarial loss: 0.496567\n",
      "epoch 162; iter: 0; batch classifier loss: 0.043740; batch adversarial loss: 0.431437\n",
      "epoch 163; iter: 0; batch classifier loss: 0.009518; batch adversarial loss: 0.486663\n",
      "epoch 164; iter: 0; batch classifier loss: 0.057604; batch adversarial loss: 0.384463\n",
      "epoch 165; iter: 0; batch classifier loss: 0.029733; batch adversarial loss: 0.434266\n",
      "epoch 166; iter: 0; batch classifier loss: 0.018717; batch adversarial loss: 0.472595\n",
      "epoch 167; iter: 0; batch classifier loss: 0.065762; batch adversarial loss: 0.366371\n",
      "epoch 168; iter: 0; batch classifier loss: 0.029802; batch adversarial loss: 0.487968\n",
      "epoch 169; iter: 0; batch classifier loss: 0.017770; batch adversarial loss: 0.550183\n",
      "epoch 170; iter: 0; batch classifier loss: 0.022210; batch adversarial loss: 0.432241\n",
      "epoch 171; iter: 0; batch classifier loss: 0.047741; batch adversarial loss: 0.482394\n",
      "epoch 172; iter: 0; batch classifier loss: 0.043100; batch adversarial loss: 0.404883\n",
      "epoch 173; iter: 0; batch classifier loss: 0.011162; batch adversarial loss: 0.436226\n",
      "epoch 174; iter: 0; batch classifier loss: 0.032753; batch adversarial loss: 0.405601\n",
      "epoch 175; iter: 0; batch classifier loss: 0.050602; batch adversarial loss: 0.421925\n",
      "epoch 176; iter: 0; batch classifier loss: 0.028809; batch adversarial loss: 0.423159\n",
      "epoch 177; iter: 0; batch classifier loss: 0.034621; batch adversarial loss: 0.425521\n",
      "epoch 178; iter: 0; batch classifier loss: 0.025224; batch adversarial loss: 0.484839\n",
      "epoch 179; iter: 0; batch classifier loss: 0.039964; batch adversarial loss: 0.417425\n",
      "epoch 180; iter: 0; batch classifier loss: 0.023521; batch adversarial loss: 0.454790\n",
      "epoch 181; iter: 0; batch classifier loss: 0.042776; batch adversarial loss: 0.454774\n",
      "epoch 182; iter: 0; batch classifier loss: 0.016652; batch adversarial loss: 0.424518\n",
      "epoch 183; iter: 0; batch classifier loss: 0.049633; batch adversarial loss: 0.523680\n",
      "epoch 184; iter: 0; batch classifier loss: 0.025688; batch adversarial loss: 0.479730\n",
      "epoch 185; iter: 0; batch classifier loss: 0.023716; batch adversarial loss: 0.459128\n",
      "epoch 186; iter: 0; batch classifier loss: 0.021421; batch adversarial loss: 0.472278\n",
      "epoch 187; iter: 0; batch classifier loss: 0.014622; batch adversarial loss: 0.457845\n",
      "epoch 188; iter: 0; batch classifier loss: 0.023020; batch adversarial loss: 0.428760\n",
      "epoch 189; iter: 0; batch classifier loss: 0.009389; batch adversarial loss: 0.538628\n",
      "epoch 190; iter: 0; batch classifier loss: 0.039420; batch adversarial loss: 0.393357\n",
      "epoch 191; iter: 0; batch classifier loss: 0.032698; batch adversarial loss: 0.472641\n",
      "epoch 192; iter: 0; batch classifier loss: 0.021129; batch adversarial loss: 0.488576\n",
      "epoch 193; iter: 0; batch classifier loss: 0.019709; batch adversarial loss: 0.502563\n",
      "epoch 194; iter: 0; batch classifier loss: 0.029401; batch adversarial loss: 0.385747\n",
      "epoch 195; iter: 0; batch classifier loss: 0.007338; batch adversarial loss: 0.428407\n",
      "epoch 196; iter: 0; batch classifier loss: 0.025532; batch adversarial loss: 0.466889\n",
      "epoch 197; iter: 0; batch classifier loss: 0.024658; batch adversarial loss: 0.375204\n",
      "epoch 198; iter: 0; batch classifier loss: 0.012878; batch adversarial loss: 0.408993\n",
      "epoch 199; iter: 0; batch classifier loss: 0.036193; batch adversarial loss: 0.442245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:57:35.982588: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44ea0-ae24-11ee-be98-ef9b34f2853b/04a44ea0-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:13696 op device:{requested: '', assigned: ''} def:{{{node 04a44ea0-ae24-11ee-be98-ef9b34f2853b/04a44ea0-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44ea0-ae24-11ee-be98-ef9b34f2853b/04a44ea0-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44ea0-ae24-11ee-be98-ef9b34f2853b/04a44ea0-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.690958; batch adversarial loss: 0.601918\n",
      "epoch 1; iter: 0; batch classifier loss: 0.486647; batch adversarial loss: 0.621875\n",
      "epoch 2; iter: 0; batch classifier loss: 0.476848; batch adversarial loss: 0.612967\n",
      "epoch 3; iter: 0; batch classifier loss: 0.401120; batch adversarial loss: 0.561927\n",
      "epoch 4; iter: 0; batch classifier loss: 0.402268; batch adversarial loss: 0.604077\n",
      "epoch 5; iter: 0; batch classifier loss: 0.353751; batch adversarial loss: 0.587483\n",
      "epoch 6; iter: 0; batch classifier loss: 0.426546; batch adversarial loss: 0.579646\n",
      "epoch 7; iter: 0; batch classifier loss: 0.532518; batch adversarial loss: 0.544902\n",
      "epoch 8; iter: 0; batch classifier loss: 0.501794; batch adversarial loss: 0.596470\n",
      "epoch 9; iter: 0; batch classifier loss: 0.765488; batch adversarial loss: 0.572311\n",
      "epoch 10; iter: 0; batch classifier loss: 0.553021; batch adversarial loss: 0.556403\n",
      "epoch 11; iter: 0; batch classifier loss: 0.452127; batch adversarial loss: 0.478319\n",
      "epoch 12; iter: 0; batch classifier loss: 0.347745; batch adversarial loss: 0.493399\n",
      "epoch 13; iter: 0; batch classifier loss: 0.311416; batch adversarial loss: 0.533496\n",
      "epoch 14; iter: 0; batch classifier loss: 0.291989; batch adversarial loss: 0.547972\n",
      "epoch 15; iter: 0; batch classifier loss: 0.267045; batch adversarial loss: 0.497792\n",
      "epoch 16; iter: 0; batch classifier loss: 0.241204; batch adversarial loss: 0.485432\n",
      "epoch 17; iter: 0; batch classifier loss: 0.197541; batch adversarial loss: 0.628074\n",
      "epoch 18; iter: 0; batch classifier loss: 0.302399; batch adversarial loss: 0.404349\n",
      "epoch 19; iter: 0; batch classifier loss: 0.269602; batch adversarial loss: 0.474115\n",
      "epoch 20; iter: 0; batch classifier loss: 0.226502; batch adversarial loss: 0.418749\n",
      "epoch 21; iter: 0; batch classifier loss: 0.252804; batch adversarial loss: 0.464666\n",
      "epoch 22; iter: 0; batch classifier loss: 0.261711; batch adversarial loss: 0.476222\n",
      "epoch 23; iter: 0; batch classifier loss: 0.160289; batch adversarial loss: 0.470506\n",
      "epoch 24; iter: 0; batch classifier loss: 0.206993; batch adversarial loss: 0.472132\n",
      "epoch 25; iter: 0; batch classifier loss: 0.192690; batch adversarial loss: 0.479169\n",
      "epoch 26; iter: 0; batch classifier loss: 0.250725; batch adversarial loss: 0.492169\n",
      "epoch 27; iter: 0; batch classifier loss: 0.155477; batch adversarial loss: 0.511060\n",
      "epoch 28; iter: 0; batch classifier loss: 0.266026; batch adversarial loss: 0.357988\n",
      "epoch 29; iter: 0; batch classifier loss: 0.160103; batch adversarial loss: 0.476116\n",
      "epoch 30; iter: 0; batch classifier loss: 0.125954; batch adversarial loss: 0.454256\n",
      "epoch 31; iter: 0; batch classifier loss: 0.172183; batch adversarial loss: 0.489585\n",
      "epoch 32; iter: 0; batch classifier loss: 0.122674; batch adversarial loss: 0.528415\n",
      "epoch 33; iter: 0; batch classifier loss: 0.185356; batch adversarial loss: 0.445626\n",
      "epoch 34; iter: 0; batch classifier loss: 0.226013; batch adversarial loss: 0.497474\n",
      "epoch 35; iter: 0; batch classifier loss: 0.158546; batch adversarial loss: 0.425138\n",
      "epoch 36; iter: 0; batch classifier loss: 0.152943; batch adversarial loss: 0.433779\n",
      "epoch 37; iter: 0; batch classifier loss: 0.153530; batch adversarial loss: 0.492053\n",
      "epoch 38; iter: 0; batch classifier loss: 0.237614; batch adversarial loss: 0.529103\n",
      "epoch 39; iter: 0; batch classifier loss: 0.197273; batch adversarial loss: 0.481795\n",
      "epoch 40; iter: 0; batch classifier loss: 0.186078; batch adversarial loss: 0.484841\n",
      "epoch 41; iter: 0; batch classifier loss: 0.180783; batch adversarial loss: 0.531214\n",
      "epoch 42; iter: 0; batch classifier loss: 0.211848; batch adversarial loss: 0.500895\n",
      "epoch 43; iter: 0; batch classifier loss: 0.194819; batch adversarial loss: 0.495103\n",
      "epoch 44; iter: 0; batch classifier loss: 0.176012; batch adversarial loss: 0.458817\n",
      "epoch 45; iter: 0; batch classifier loss: 0.177204; batch adversarial loss: 0.430742\n",
      "epoch 46; iter: 0; batch classifier loss: 0.152844; batch adversarial loss: 0.508374\n",
      "epoch 47; iter: 0; batch classifier loss: 0.132161; batch adversarial loss: 0.475982\n",
      "epoch 48; iter: 0; batch classifier loss: 0.154818; batch adversarial loss: 0.447126\n",
      "epoch 49; iter: 0; batch classifier loss: 0.157911; batch adversarial loss: 0.564791\n",
      "epoch 50; iter: 0; batch classifier loss: 0.139742; batch adversarial loss: 0.429811\n",
      "epoch 51; iter: 0; batch classifier loss: 0.158932; batch adversarial loss: 0.451659\n",
      "epoch 52; iter: 0; batch classifier loss: 0.149083; batch adversarial loss: 0.461392\n",
      "epoch 53; iter: 0; batch classifier loss: 0.162515; batch adversarial loss: 0.501639\n",
      "epoch 54; iter: 0; batch classifier loss: 0.148350; batch adversarial loss: 0.449854\n",
      "epoch 55; iter: 0; batch classifier loss: 0.095869; batch adversarial loss: 0.536999\n",
      "epoch 56; iter: 0; batch classifier loss: 0.169330; batch adversarial loss: 0.420821\n",
      "epoch 57; iter: 0; batch classifier loss: 0.252791; batch adversarial loss: 0.457168\n",
      "epoch 58; iter: 0; batch classifier loss: 0.191727; batch adversarial loss: 0.417355\n",
      "epoch 59; iter: 0; batch classifier loss: 0.117529; batch adversarial loss: 0.439211\n",
      "epoch 60; iter: 0; batch classifier loss: 0.210631; batch adversarial loss: 0.456759\n",
      "epoch 61; iter: 0; batch classifier loss: 0.152520; batch adversarial loss: 0.440451\n",
      "epoch 62; iter: 0; batch classifier loss: 0.145353; batch adversarial loss: 0.410677\n",
      "epoch 63; iter: 0; batch classifier loss: 0.221622; batch adversarial loss: 0.368387\n",
      "epoch 64; iter: 0; batch classifier loss: 0.183739; batch adversarial loss: 0.465635\n",
      "epoch 65; iter: 0; batch classifier loss: 0.115137; batch adversarial loss: 0.407948\n",
      "epoch 66; iter: 0; batch classifier loss: 0.166844; batch adversarial loss: 0.507281\n",
      "epoch 67; iter: 0; batch classifier loss: 0.183045; batch adversarial loss: 0.411313\n",
      "epoch 68; iter: 0; batch classifier loss: 0.184984; batch adversarial loss: 0.417615\n",
      "epoch 69; iter: 0; batch classifier loss: 0.174381; batch adversarial loss: 0.472224\n",
      "epoch 70; iter: 0; batch classifier loss: 0.181243; batch adversarial loss: 0.445247\n",
      "epoch 71; iter: 0; batch classifier loss: 0.118180; batch adversarial loss: 0.470825\n",
      "epoch 72; iter: 0; batch classifier loss: 0.189826; batch adversarial loss: 0.531423\n",
      "epoch 73; iter: 0; batch classifier loss: 0.148722; batch adversarial loss: 0.440071\n",
      "epoch 74; iter: 0; batch classifier loss: 0.120309; batch adversarial loss: 0.382529\n",
      "epoch 75; iter: 0; batch classifier loss: 0.166891; batch adversarial loss: 0.373042\n",
      "epoch 76; iter: 0; batch classifier loss: 0.089878; batch adversarial loss: 0.410342\n",
      "epoch 77; iter: 0; batch classifier loss: 0.148882; batch adversarial loss: 0.431366\n",
      "epoch 78; iter: 0; batch classifier loss: 0.129646; batch adversarial loss: 0.489831\n",
      "epoch 79; iter: 0; batch classifier loss: 0.132202; batch adversarial loss: 0.419258\n",
      "epoch 80; iter: 0; batch classifier loss: 0.144469; batch adversarial loss: 0.428317\n",
      "epoch 81; iter: 0; batch classifier loss: 0.132267; batch adversarial loss: 0.434588\n",
      "epoch 82; iter: 0; batch classifier loss: 0.146916; batch adversarial loss: 0.414999\n",
      "epoch 83; iter: 0; batch classifier loss: 0.177686; batch adversarial loss: 0.483999\n",
      "epoch 84; iter: 0; batch classifier loss: 0.146684; batch adversarial loss: 0.468944\n",
      "epoch 85; iter: 0; batch classifier loss: 0.212568; batch adversarial loss: 0.428257\n",
      "epoch 86; iter: 0; batch classifier loss: 0.114422; batch adversarial loss: 0.519872\n",
      "epoch 87; iter: 0; batch classifier loss: 0.147512; batch adversarial loss: 0.477834\n",
      "epoch 88; iter: 0; batch classifier loss: 0.151489; batch adversarial loss: 0.488543\n",
      "epoch 89; iter: 0; batch classifier loss: 0.097893; batch adversarial loss: 0.460134\n",
      "epoch 90; iter: 0; batch classifier loss: 0.092039; batch adversarial loss: 0.507105\n",
      "epoch 91; iter: 0; batch classifier loss: 0.133929; batch adversarial loss: 0.416810\n",
      "epoch 92; iter: 0; batch classifier loss: 0.118540; batch adversarial loss: 0.405622\n",
      "epoch 93; iter: 0; batch classifier loss: 0.110958; batch adversarial loss: 0.526316\n",
      "epoch 94; iter: 0; batch classifier loss: 0.162931; batch adversarial loss: 0.380493\n",
      "epoch 95; iter: 0; batch classifier loss: 0.122797; batch adversarial loss: 0.492907\n",
      "epoch 96; iter: 0; batch classifier loss: 0.082247; batch adversarial loss: 0.411900\n",
      "epoch 97; iter: 0; batch classifier loss: 0.097348; batch adversarial loss: 0.468513\n",
      "epoch 98; iter: 0; batch classifier loss: 0.111377; batch adversarial loss: 0.499021\n",
      "epoch 99; iter: 0; batch classifier loss: 0.098009; batch adversarial loss: 0.411739\n",
      "epoch 100; iter: 0; batch classifier loss: 0.068869; batch adversarial loss: 0.398517\n",
      "epoch 101; iter: 0; batch classifier loss: 0.073340; batch adversarial loss: 0.406299\n",
      "epoch 102; iter: 0; batch classifier loss: 0.092604; batch adversarial loss: 0.455273\n",
      "epoch 103; iter: 0; batch classifier loss: 0.134875; batch adversarial loss: 0.382052\n",
      "epoch 104; iter: 0; batch classifier loss: 0.071028; batch adversarial loss: 0.444440\n",
      "epoch 105; iter: 0; batch classifier loss: 0.075348; batch adversarial loss: 0.432808\n",
      "epoch 106; iter: 0; batch classifier loss: 0.059842; batch adversarial loss: 0.481770\n",
      "epoch 107; iter: 0; batch classifier loss: 0.056991; batch adversarial loss: 0.495370\n",
      "epoch 108; iter: 0; batch classifier loss: 0.048190; batch adversarial loss: 0.460752\n",
      "epoch 109; iter: 0; batch classifier loss: 0.066201; batch adversarial loss: 0.453234\n",
      "epoch 110; iter: 0; batch classifier loss: 0.051635; batch adversarial loss: 0.516590\n",
      "epoch 111; iter: 0; batch classifier loss: 0.052682; batch adversarial loss: 0.340089\n",
      "epoch 112; iter: 0; batch classifier loss: 0.064047; batch adversarial loss: 0.435751\n",
      "epoch 113; iter: 0; batch classifier loss: 0.042333; batch adversarial loss: 0.455830\n",
      "epoch 114; iter: 0; batch classifier loss: 0.083712; batch adversarial loss: 0.476727\n",
      "epoch 115; iter: 0; batch classifier loss: 0.024520; batch adversarial loss: 0.572106\n",
      "epoch 116; iter: 0; batch classifier loss: 0.031662; batch adversarial loss: 0.395095\n",
      "epoch 117; iter: 0; batch classifier loss: 0.063928; batch adversarial loss: 0.399389\n",
      "epoch 118; iter: 0; batch classifier loss: 0.049294; batch adversarial loss: 0.410749\n",
      "epoch 119; iter: 0; batch classifier loss: 0.068482; batch adversarial loss: 0.591574\n",
      "epoch 120; iter: 0; batch classifier loss: 0.028644; batch adversarial loss: 0.499372\n",
      "epoch 121; iter: 0; batch classifier loss: 0.056157; batch adversarial loss: 0.371393\n",
      "epoch 122; iter: 0; batch classifier loss: 0.045832; batch adversarial loss: 0.453900\n",
      "epoch 123; iter: 0; batch classifier loss: 0.068967; batch adversarial loss: 0.391014\n",
      "epoch 124; iter: 0; batch classifier loss: 0.016169; batch adversarial loss: 0.402797\n",
      "epoch 125; iter: 0; batch classifier loss: 0.055157; batch adversarial loss: 0.407060\n",
      "epoch 126; iter: 0; batch classifier loss: 0.029426; batch adversarial loss: 0.591057\n",
      "epoch 127; iter: 0; batch classifier loss: 0.042754; batch adversarial loss: 0.460107\n",
      "epoch 128; iter: 0; batch classifier loss: 0.018633; batch adversarial loss: 0.480898\n",
      "epoch 129; iter: 0; batch classifier loss: 0.039046; batch adversarial loss: 0.486923\n",
      "epoch 130; iter: 0; batch classifier loss: 0.022976; batch adversarial loss: 0.443359\n",
      "epoch 131; iter: 0; batch classifier loss: 0.012947; batch adversarial loss: 0.458760\n",
      "epoch 132; iter: 0; batch classifier loss: 0.021176; batch adversarial loss: 0.563774\n",
      "epoch 133; iter: 0; batch classifier loss: 0.019550; batch adversarial loss: 0.516491\n",
      "epoch 134; iter: 0; batch classifier loss: 0.012808; batch adversarial loss: 0.432651\n",
      "epoch 135; iter: 0; batch classifier loss: 0.035071; batch adversarial loss: 0.445973\n",
      "epoch 136; iter: 0; batch classifier loss: 0.050107; batch adversarial loss: 0.494927\n",
      "epoch 137; iter: 0; batch classifier loss: 0.009768; batch adversarial loss: 0.609624\n",
      "epoch 138; iter: 0; batch classifier loss: 0.020962; batch adversarial loss: 0.487453\n",
      "epoch 139; iter: 0; batch classifier loss: 0.041505; batch adversarial loss: 0.421227\n",
      "epoch 140; iter: 0; batch classifier loss: 0.020644; batch adversarial loss: 0.356929\n",
      "epoch 141; iter: 0; batch classifier loss: 0.016869; batch adversarial loss: 0.482082\n",
      "epoch 142; iter: 0; batch classifier loss: 0.011133; batch adversarial loss: 0.316279\n",
      "epoch 143; iter: 0; batch classifier loss: 0.016673; batch adversarial loss: 0.403915\n",
      "epoch 144; iter: 0; batch classifier loss: 0.019181; batch adversarial loss: 0.375538\n",
      "epoch 145; iter: 0; batch classifier loss: 0.004204; batch adversarial loss: 0.520021\n",
      "epoch 146; iter: 0; batch classifier loss: 0.026691; batch adversarial loss: 0.447454\n",
      "epoch 147; iter: 0; batch classifier loss: 0.045329; batch adversarial loss: 0.507728\n",
      "epoch 148; iter: 0; batch classifier loss: 0.016981; batch adversarial loss: 0.431038\n",
      "epoch 149; iter: 0; batch classifier loss: 0.023257; batch adversarial loss: 0.449333\n",
      "epoch 150; iter: 0; batch classifier loss: 0.028934; batch adversarial loss: 0.446353\n",
      "epoch 151; iter: 0; batch classifier loss: 0.012843; batch adversarial loss: 0.419821\n",
      "epoch 152; iter: 0; batch classifier loss: 0.032494; batch adversarial loss: 0.461637\n",
      "epoch 153; iter: 0; batch classifier loss: 0.026108; batch adversarial loss: 0.508135\n",
      "epoch 154; iter: 0; batch classifier loss: 0.029386; batch adversarial loss: 0.495117\n",
      "epoch 155; iter: 0; batch classifier loss: 0.006015; batch adversarial loss: 0.459160\n",
      "epoch 156; iter: 0; batch classifier loss: 0.026389; batch adversarial loss: 0.481618\n",
      "epoch 157; iter: 0; batch classifier loss: 0.032742; batch adversarial loss: 0.490892\n",
      "epoch 158; iter: 0; batch classifier loss: 0.026512; batch adversarial loss: 0.419190\n",
      "epoch 159; iter: 0; batch classifier loss: 0.030455; batch adversarial loss: 0.492428\n",
      "epoch 160; iter: 0; batch classifier loss: 0.033925; batch adversarial loss: 0.411468\n",
      "epoch 161; iter: 0; batch classifier loss: 0.017125; batch adversarial loss: 0.460999\n",
      "epoch 162; iter: 0; batch classifier loss: 0.030776; batch adversarial loss: 0.440007\n",
      "epoch 163; iter: 0; batch classifier loss: 0.035770; batch adversarial loss: 0.392796\n",
      "epoch 164; iter: 0; batch classifier loss: 0.007011; batch adversarial loss: 0.456057\n",
      "epoch 165; iter: 0; batch classifier loss: 0.027340; batch adversarial loss: 0.472723\n",
      "epoch 166; iter: 0; batch classifier loss: 0.017088; batch adversarial loss: 0.508912\n",
      "epoch 167; iter: 0; batch classifier loss: 0.020563; batch adversarial loss: 0.417939\n",
      "epoch 168; iter: 0; batch classifier loss: 0.059203; batch adversarial loss: 0.427153\n",
      "epoch 169; iter: 0; batch classifier loss: 0.019443; batch adversarial loss: 0.450960\n",
      "epoch 170; iter: 0; batch classifier loss: 0.055285; batch adversarial loss: 0.437186\n",
      "epoch 171; iter: 0; batch classifier loss: 0.018672; batch adversarial loss: 0.500860\n",
      "epoch 172; iter: 0; batch classifier loss: 0.018131; batch adversarial loss: 0.479317\n",
      "epoch 173; iter: 0; batch classifier loss: 0.015967; batch adversarial loss: 0.573900\n",
      "epoch 174; iter: 0; batch classifier loss: 0.010289; batch adversarial loss: 0.486798\n",
      "epoch 175; iter: 0; batch classifier loss: 0.040882; batch adversarial loss: 0.396112\n",
      "epoch 176; iter: 0; batch classifier loss: 0.005294; batch adversarial loss: 0.401219\n",
      "epoch 177; iter: 0; batch classifier loss: 0.013484; batch adversarial loss: 0.404824\n",
      "epoch 178; iter: 0; batch classifier loss: 0.022538; batch adversarial loss: 0.435906\n",
      "epoch 179; iter: 0; batch classifier loss: 0.019638; batch adversarial loss: 0.434669\n",
      "epoch 180; iter: 0; batch classifier loss: 0.013275; batch adversarial loss: 0.461069\n",
      "epoch 181; iter: 0; batch classifier loss: 0.007006; batch adversarial loss: 0.450947\n",
      "epoch 182; iter: 0; batch classifier loss: 0.004234; batch adversarial loss: 0.369680\n",
      "epoch 183; iter: 0; batch classifier loss: 0.017637; batch adversarial loss: 0.385077\n",
      "epoch 184; iter: 0; batch classifier loss: 0.035489; batch adversarial loss: 0.568143\n",
      "epoch 185; iter: 0; batch classifier loss: 0.023577; batch adversarial loss: 0.396488\n",
      "epoch 186; iter: 0; batch classifier loss: 0.011228; batch adversarial loss: 0.352878\n",
      "epoch 187; iter: 0; batch classifier loss: 0.038386; batch adversarial loss: 0.441038\n",
      "epoch 188; iter: 0; batch classifier loss: 0.013308; batch adversarial loss: 0.459898\n",
      "epoch 189; iter: 0; batch classifier loss: 0.010482; batch adversarial loss: 0.472150\n",
      "epoch 190; iter: 0; batch classifier loss: 0.010571; batch adversarial loss: 0.574614\n",
      "epoch 191; iter: 0; batch classifier loss: 0.011248; batch adversarial loss: 0.460090\n",
      "epoch 192; iter: 0; batch classifier loss: 0.013093; batch adversarial loss: 0.486093\n",
      "epoch 193; iter: 0; batch classifier loss: 0.023117; batch adversarial loss: 0.364825\n",
      "epoch 194; iter: 0; batch classifier loss: 0.007172; batch adversarial loss: 0.457797\n",
      "epoch 195; iter: 0; batch classifier loss: 0.021359; batch adversarial loss: 0.557130\n",
      "epoch 196; iter: 0; batch classifier loss: 0.013529; batch adversarial loss: 0.333743\n",
      "epoch 197; iter: 0; batch classifier loss: 0.050756; batch adversarial loss: 0.443671\n",
      "epoch 198; iter: 0; batch classifier loss: 0.013808; batch adversarial loss: 0.457380\n",
      "epoch 199; iter: 0; batch classifier loss: 0.003819; batch adversarial loss: 0.432872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:58:13.762653: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44f18-ae24-11ee-be98-ef9b34f2853b/04a44f18-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:14503 op device:{requested: '', assigned: ''} def:{{{node 04a44f18-ae24-11ee-be98-ef9b34f2853b/04a44f18-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44f18-ae24-11ee-be98-ef9b34f2853b/04a44f18-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44f18-ae24-11ee-be98-ef9b34f2853b/04a44f18-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.666435; batch adversarial loss: 0.800770\n",
      "epoch 1; iter: 0; batch classifier loss: 0.554344; batch adversarial loss: 0.748459\n",
      "epoch 2; iter: 0; batch classifier loss: 0.604689; batch adversarial loss: 0.722318\n",
      "epoch 3; iter: 0; batch classifier loss: 0.639290; batch adversarial loss: 0.663460\n",
      "epoch 4; iter: 0; batch classifier loss: 0.501358; batch adversarial loss: 0.626447\n",
      "epoch 5; iter: 0; batch classifier loss: 0.357676; batch adversarial loss: 0.607015\n",
      "epoch 6; iter: 0; batch classifier loss: 0.351710; batch adversarial loss: 0.577602\n",
      "epoch 7; iter: 0; batch classifier loss: 0.313884; batch adversarial loss: 0.566381\n",
      "epoch 8; iter: 0; batch classifier loss: 0.353490; batch adversarial loss: 0.552686\n",
      "epoch 9; iter: 0; batch classifier loss: 0.318089; batch adversarial loss: 0.656434\n",
      "epoch 10; iter: 0; batch classifier loss: 0.295380; batch adversarial loss: 0.543356\n",
      "epoch 11; iter: 0; batch classifier loss: 0.344444; batch adversarial loss: 0.582973\n",
      "epoch 12; iter: 0; batch classifier loss: 0.346967; batch adversarial loss: 0.498679\n",
      "epoch 13; iter: 0; batch classifier loss: 0.311962; batch adversarial loss: 0.496808\n",
      "epoch 14; iter: 0; batch classifier loss: 0.313049; batch adversarial loss: 0.536778\n",
      "epoch 15; iter: 0; batch classifier loss: 0.321739; batch adversarial loss: 0.498860\n",
      "epoch 16; iter: 0; batch classifier loss: 0.308494; batch adversarial loss: 0.496162\n",
      "epoch 17; iter: 0; batch classifier loss: 0.268881; batch adversarial loss: 0.532256\n",
      "epoch 18; iter: 0; batch classifier loss: 0.249281; batch adversarial loss: 0.519489\n",
      "epoch 19; iter: 0; batch classifier loss: 0.295092; batch adversarial loss: 0.462116\n",
      "epoch 20; iter: 0; batch classifier loss: 0.225821; batch adversarial loss: 0.531746\n",
      "epoch 21; iter: 0; batch classifier loss: 0.258138; batch adversarial loss: 0.468528\n",
      "epoch 22; iter: 0; batch classifier loss: 0.317907; batch adversarial loss: 0.452340\n",
      "epoch 23; iter: 0; batch classifier loss: 0.286210; batch adversarial loss: 0.407493\n",
      "epoch 24; iter: 0; batch classifier loss: 0.248595; batch adversarial loss: 0.467794\n",
      "epoch 25; iter: 0; batch classifier loss: 0.251553; batch adversarial loss: 0.475062\n",
      "epoch 26; iter: 0; batch classifier loss: 0.249557; batch adversarial loss: 0.446019\n",
      "epoch 27; iter: 0; batch classifier loss: 0.231562; batch adversarial loss: 0.445893\n",
      "epoch 28; iter: 0; batch classifier loss: 0.178045; batch adversarial loss: 0.455702\n",
      "epoch 29; iter: 0; batch classifier loss: 0.196641; batch adversarial loss: 0.474996\n",
      "epoch 30; iter: 0; batch classifier loss: 0.282369; batch adversarial loss: 0.444550\n",
      "epoch 31; iter: 0; batch classifier loss: 0.206681; batch adversarial loss: 0.467786\n",
      "epoch 32; iter: 0; batch classifier loss: 0.238674; batch adversarial loss: 0.545328\n",
      "epoch 33; iter: 0; batch classifier loss: 0.298108; batch adversarial loss: 0.401553\n",
      "epoch 34; iter: 0; batch classifier loss: 0.247076; batch adversarial loss: 0.437279\n",
      "epoch 35; iter: 0; batch classifier loss: 0.288646; batch adversarial loss: 0.371556\n",
      "epoch 36; iter: 0; batch classifier loss: 0.201237; batch adversarial loss: 0.523612\n",
      "epoch 37; iter: 0; batch classifier loss: 0.243274; batch adversarial loss: 0.453225\n",
      "epoch 38; iter: 0; batch classifier loss: 0.242003; batch adversarial loss: 0.508180\n",
      "epoch 39; iter: 0; batch classifier loss: 0.216089; batch adversarial loss: 0.433985\n",
      "epoch 40; iter: 0; batch classifier loss: 0.217870; batch adversarial loss: 0.477859\n",
      "epoch 41; iter: 0; batch classifier loss: 0.233469; batch adversarial loss: 0.435197\n",
      "epoch 42; iter: 0; batch classifier loss: 0.233933; batch adversarial loss: 0.448009\n",
      "epoch 43; iter: 0; batch classifier loss: 0.232735; batch adversarial loss: 0.398820\n",
      "epoch 44; iter: 0; batch classifier loss: 0.220824; batch adversarial loss: 0.412212\n",
      "epoch 45; iter: 0; batch classifier loss: 0.166736; batch adversarial loss: 0.475106\n",
      "epoch 46; iter: 0; batch classifier loss: 0.184730; batch adversarial loss: 0.422260\n",
      "epoch 47; iter: 0; batch classifier loss: 0.172949; batch adversarial loss: 0.413521\n",
      "epoch 48; iter: 0; batch classifier loss: 0.183225; batch adversarial loss: 0.461948\n",
      "epoch 49; iter: 0; batch classifier loss: 0.161020; batch adversarial loss: 0.527749\n",
      "epoch 50; iter: 0; batch classifier loss: 0.216188; batch adversarial loss: 0.423290\n",
      "epoch 51; iter: 0; batch classifier loss: 0.259620; batch adversarial loss: 0.389778\n",
      "epoch 52; iter: 0; batch classifier loss: 0.221340; batch adversarial loss: 0.471802\n",
      "epoch 53; iter: 0; batch classifier loss: 0.215885; batch adversarial loss: 0.535134\n",
      "epoch 54; iter: 0; batch classifier loss: 0.243374; batch adversarial loss: 0.509137\n",
      "epoch 55; iter: 0; batch classifier loss: 0.181789; batch adversarial loss: 0.449024\n",
      "epoch 56; iter: 0; batch classifier loss: 0.220317; batch adversarial loss: 0.499564\n",
      "epoch 57; iter: 0; batch classifier loss: 0.209837; batch adversarial loss: 0.398364\n",
      "epoch 58; iter: 0; batch classifier loss: 0.277739; batch adversarial loss: 0.469112\n",
      "epoch 59; iter: 0; batch classifier loss: 0.221262; batch adversarial loss: 0.410085\n",
      "epoch 60; iter: 0; batch classifier loss: 0.177708; batch adversarial loss: 0.435707\n",
      "epoch 61; iter: 0; batch classifier loss: 0.214998; batch adversarial loss: 0.316081\n",
      "epoch 62; iter: 0; batch classifier loss: 0.176440; batch adversarial loss: 0.531038\n",
      "epoch 63; iter: 0; batch classifier loss: 0.185242; batch adversarial loss: 0.506146\n",
      "epoch 64; iter: 0; batch classifier loss: 0.168400; batch adversarial loss: 0.494824\n",
      "epoch 65; iter: 0; batch classifier loss: 0.147501; batch adversarial loss: 0.519157\n",
      "epoch 66; iter: 0; batch classifier loss: 0.206820; batch adversarial loss: 0.458880\n",
      "epoch 67; iter: 0; batch classifier loss: 0.218804; batch adversarial loss: 0.470551\n",
      "epoch 68; iter: 0; batch classifier loss: 0.204554; batch adversarial loss: 0.410668\n",
      "epoch 69; iter: 0; batch classifier loss: 0.192023; batch adversarial loss: 0.386680\n",
      "epoch 70; iter: 0; batch classifier loss: 0.083026; batch adversarial loss: 0.410396\n",
      "epoch 71; iter: 0; batch classifier loss: 0.150973; batch adversarial loss: 0.421888\n",
      "epoch 72; iter: 0; batch classifier loss: 0.096181; batch adversarial loss: 0.456538\n",
      "epoch 73; iter: 0; batch classifier loss: 0.061488; batch adversarial loss: 0.588727\n",
      "epoch 74; iter: 0; batch classifier loss: 0.087431; batch adversarial loss: 0.440915\n",
      "epoch 75; iter: 0; batch classifier loss: 0.110412; batch adversarial loss: 0.498607\n",
      "epoch 76; iter: 0; batch classifier loss: 0.128617; batch adversarial loss: 0.420367\n",
      "epoch 77; iter: 0; batch classifier loss: 0.175658; batch adversarial loss: 0.486612\n",
      "epoch 78; iter: 0; batch classifier loss: 0.098749; batch adversarial loss: 0.551308\n",
      "epoch 79; iter: 0; batch classifier loss: 0.128258; batch adversarial loss: 0.435202\n",
      "epoch 80; iter: 0; batch classifier loss: 0.078203; batch adversarial loss: 0.505378\n",
      "epoch 81; iter: 0; batch classifier loss: 0.115293; batch adversarial loss: 0.516014\n",
      "epoch 82; iter: 0; batch classifier loss: 0.101177; batch adversarial loss: 0.560916\n",
      "epoch 83; iter: 0; batch classifier loss: 0.098469; batch adversarial loss: 0.437638\n",
      "epoch 84; iter: 0; batch classifier loss: 0.091651; batch adversarial loss: 0.489183\n",
      "epoch 85; iter: 0; batch classifier loss: 0.046992; batch adversarial loss: 0.589325\n",
      "epoch 86; iter: 0; batch classifier loss: 0.124482; batch adversarial loss: 0.457224\n",
      "epoch 87; iter: 0; batch classifier loss: 0.060123; batch adversarial loss: 0.407405\n",
      "epoch 88; iter: 0; batch classifier loss: 0.063126; batch adversarial loss: 0.380355\n",
      "epoch 89; iter: 0; batch classifier loss: 0.079176; batch adversarial loss: 0.461366\n",
      "epoch 90; iter: 0; batch classifier loss: 0.060723; batch adversarial loss: 0.526339\n",
      "epoch 91; iter: 0; batch classifier loss: 0.046690; batch adversarial loss: 0.514393\n",
      "epoch 92; iter: 0; batch classifier loss: 0.066610; batch adversarial loss: 0.541992\n",
      "epoch 93; iter: 0; batch classifier loss: 0.046880; batch adversarial loss: 0.472290\n",
      "epoch 94; iter: 0; batch classifier loss: 0.050740; batch adversarial loss: 0.486867\n",
      "epoch 95; iter: 0; batch classifier loss: 0.051684; batch adversarial loss: 0.468599\n",
      "epoch 96; iter: 0; batch classifier loss: 0.040422; batch adversarial loss: 0.470264\n",
      "epoch 97; iter: 0; batch classifier loss: 0.060029; batch adversarial loss: 0.520934\n",
      "epoch 98; iter: 0; batch classifier loss: 0.042494; batch adversarial loss: 0.464194\n",
      "epoch 99; iter: 0; batch classifier loss: 0.030219; batch adversarial loss: 0.471912\n",
      "epoch 100; iter: 0; batch classifier loss: 0.040265; batch adversarial loss: 0.439166\n",
      "epoch 101; iter: 0; batch classifier loss: 0.027200; batch adversarial loss: 0.543241\n",
      "epoch 102; iter: 0; batch classifier loss: 0.038853; batch adversarial loss: 0.401062\n",
      "epoch 103; iter: 0; batch classifier loss: 0.024802; batch adversarial loss: 0.462691\n",
      "epoch 104; iter: 0; batch classifier loss: 0.047882; batch adversarial loss: 0.465586\n",
      "epoch 105; iter: 0; batch classifier loss: 0.042438; batch adversarial loss: 0.401984\n",
      "epoch 106; iter: 0; batch classifier loss: 0.035767; batch adversarial loss: 0.461186\n",
      "epoch 107; iter: 0; batch classifier loss: 0.061861; batch adversarial loss: 0.436138\n",
      "epoch 108; iter: 0; batch classifier loss: 0.027639; batch adversarial loss: 0.531478\n",
      "epoch 109; iter: 0; batch classifier loss: 0.033076; batch adversarial loss: 0.457740\n",
      "epoch 110; iter: 0; batch classifier loss: 0.026740; batch adversarial loss: 0.466196\n",
      "epoch 111; iter: 0; batch classifier loss: 0.036434; batch adversarial loss: 0.374388\n",
      "epoch 112; iter: 0; batch classifier loss: 0.037315; batch adversarial loss: 0.491953\n",
      "epoch 113; iter: 0; batch classifier loss: 0.068087; batch adversarial loss: 0.509288\n",
      "epoch 114; iter: 0; batch classifier loss: 0.030779; batch adversarial loss: 0.455263\n",
      "epoch 115; iter: 0; batch classifier loss: 0.021976; batch adversarial loss: 0.408581\n",
      "epoch 116; iter: 0; batch classifier loss: 0.011669; batch adversarial loss: 0.437887\n",
      "epoch 117; iter: 0; batch classifier loss: 0.040111; batch adversarial loss: 0.429578\n",
      "epoch 118; iter: 0; batch classifier loss: 0.024623; batch adversarial loss: 0.462579\n",
      "epoch 119; iter: 0; batch classifier loss: 0.033242; batch adversarial loss: 0.366598\n",
      "epoch 120; iter: 0; batch classifier loss: 0.014885; batch adversarial loss: 0.533644\n",
      "epoch 121; iter: 0; batch classifier loss: 0.037568; batch adversarial loss: 0.429318\n",
      "epoch 122; iter: 0; batch classifier loss: 0.030153; batch adversarial loss: 0.397430\n",
      "epoch 123; iter: 0; batch classifier loss: 0.031314; batch adversarial loss: 0.462365\n",
      "epoch 124; iter: 0; batch classifier loss: 0.017651; batch adversarial loss: 0.410545\n",
      "epoch 125; iter: 0; batch classifier loss: 0.030622; batch adversarial loss: 0.417617\n",
      "epoch 126; iter: 0; batch classifier loss: 0.039395; batch adversarial loss: 0.434583\n",
      "epoch 127; iter: 0; batch classifier loss: 0.013147; batch adversarial loss: 0.543910\n",
      "epoch 128; iter: 0; batch classifier loss: 0.015506; batch adversarial loss: 0.455881\n",
      "epoch 129; iter: 0; batch classifier loss: 0.012347; batch adversarial loss: 0.351686\n",
      "epoch 130; iter: 0; batch classifier loss: 0.019475; batch adversarial loss: 0.414266\n",
      "epoch 131; iter: 0; batch classifier loss: 0.012207; batch adversarial loss: 0.505297\n",
      "epoch 132; iter: 0; batch classifier loss: 0.017834; batch adversarial loss: 0.480245\n",
      "epoch 133; iter: 0; batch classifier loss: 0.005077; batch adversarial loss: 0.460678\n",
      "epoch 134; iter: 0; batch classifier loss: 0.020076; batch adversarial loss: 0.391699\n",
      "epoch 135; iter: 0; batch classifier loss: 0.013223; batch adversarial loss: 0.489066\n",
      "epoch 136; iter: 0; batch classifier loss: 0.019810; batch adversarial loss: 0.370795\n",
      "epoch 137; iter: 0; batch classifier loss: 0.010134; batch adversarial loss: 0.525376\n",
      "epoch 138; iter: 0; batch classifier loss: 0.034777; batch adversarial loss: 0.423143\n",
      "epoch 139; iter: 0; batch classifier loss: 0.025395; batch adversarial loss: 0.422063\n",
      "epoch 140; iter: 0; batch classifier loss: 0.018412; batch adversarial loss: 0.436081\n",
      "epoch 141; iter: 0; batch classifier loss: 0.025493; batch adversarial loss: 0.427146\n",
      "epoch 142; iter: 0; batch classifier loss: 0.039616; batch adversarial loss: 0.456567\n",
      "epoch 143; iter: 0; batch classifier loss: 0.034039; batch adversarial loss: 0.464295\n",
      "epoch 144; iter: 0; batch classifier loss: 0.017834; batch adversarial loss: 0.469825\n",
      "epoch 145; iter: 0; batch classifier loss: 0.008502; batch adversarial loss: 0.384502\n",
      "epoch 146; iter: 0; batch classifier loss: 0.016679; batch adversarial loss: 0.421209\n",
      "epoch 147; iter: 0; batch classifier loss: 0.019452; batch adversarial loss: 0.518859\n",
      "epoch 148; iter: 0; batch classifier loss: 0.013502; batch adversarial loss: 0.363228\n",
      "epoch 149; iter: 0; batch classifier loss: 0.027162; batch adversarial loss: 0.439358\n",
      "epoch 150; iter: 0; batch classifier loss: 0.013714; batch adversarial loss: 0.466917\n",
      "epoch 151; iter: 0; batch classifier loss: 0.016739; batch adversarial loss: 0.410231\n",
      "epoch 152; iter: 0; batch classifier loss: 0.013209; batch adversarial loss: 0.428505\n",
      "epoch 153; iter: 0; batch classifier loss: 0.015567; batch adversarial loss: 0.528347\n",
      "epoch 154; iter: 0; batch classifier loss: 0.005625; batch adversarial loss: 0.430436\n",
      "epoch 155; iter: 0; batch classifier loss: 0.012260; batch adversarial loss: 0.455857\n",
      "epoch 156; iter: 0; batch classifier loss: 0.017387; batch adversarial loss: 0.487984\n",
      "epoch 157; iter: 0; batch classifier loss: 0.015126; batch adversarial loss: 0.401330\n",
      "epoch 158; iter: 0; batch classifier loss: 0.013743; batch adversarial loss: 0.543365\n",
      "epoch 159; iter: 0; batch classifier loss: 0.005452; batch adversarial loss: 0.527703\n",
      "epoch 160; iter: 0; batch classifier loss: 0.010718; batch adversarial loss: 0.375390\n",
      "epoch 161; iter: 0; batch classifier loss: 0.011713; batch adversarial loss: 0.502742\n",
      "epoch 162; iter: 0; batch classifier loss: 0.005180; batch adversarial loss: 0.414910\n",
      "epoch 163; iter: 0; batch classifier loss: 0.020991; batch adversarial loss: 0.437333\n",
      "epoch 164; iter: 0; batch classifier loss: 0.040111; batch adversarial loss: 0.498429\n",
      "epoch 165; iter: 0; batch classifier loss: 0.030897; batch adversarial loss: 0.475122\n",
      "epoch 166; iter: 0; batch classifier loss: 0.007260; batch adversarial loss: 0.455778\n",
      "epoch 167; iter: 0; batch classifier loss: 0.022739; batch adversarial loss: 0.573084\n",
      "epoch 168; iter: 0; batch classifier loss: 0.019665; batch adversarial loss: 0.519952\n",
      "epoch 169; iter: 0; batch classifier loss: 0.018170; batch adversarial loss: 0.514984\n",
      "epoch 170; iter: 0; batch classifier loss: 0.011567; batch adversarial loss: 0.446094\n",
      "epoch 171; iter: 0; batch classifier loss: 0.005762; batch adversarial loss: 0.622437\n",
      "epoch 172; iter: 0; batch classifier loss: 0.006031; batch adversarial loss: 0.389501\n",
      "epoch 173; iter: 0; batch classifier loss: 0.015537; batch adversarial loss: 0.498618\n",
      "epoch 174; iter: 0; batch classifier loss: 0.003983; batch adversarial loss: 0.526495\n",
      "epoch 175; iter: 0; batch classifier loss: 0.040394; batch adversarial loss: 0.476284\n",
      "epoch 176; iter: 0; batch classifier loss: 0.007446; batch adversarial loss: 0.505303\n",
      "epoch 177; iter: 0; batch classifier loss: 0.017261; batch adversarial loss: 0.507891\n",
      "epoch 178; iter: 0; batch classifier loss: 0.023714; batch adversarial loss: 0.530832\n",
      "epoch 179; iter: 0; batch classifier loss: 0.010718; batch adversarial loss: 0.458773\n",
      "epoch 180; iter: 0; batch classifier loss: 0.012704; batch adversarial loss: 0.427235\n",
      "epoch 181; iter: 0; batch classifier loss: 0.006145; batch adversarial loss: 0.453387\n",
      "epoch 182; iter: 0; batch classifier loss: 0.009862; batch adversarial loss: 0.488229\n",
      "epoch 183; iter: 0; batch classifier loss: 0.013131; batch adversarial loss: 0.417198\n",
      "epoch 184; iter: 0; batch classifier loss: 0.011065; batch adversarial loss: 0.509545\n",
      "epoch 185; iter: 0; batch classifier loss: 0.013384; batch adversarial loss: 0.449989\n",
      "epoch 186; iter: 0; batch classifier loss: 0.010889; batch adversarial loss: 0.617415\n",
      "epoch 187; iter: 0; batch classifier loss: 0.011007; batch adversarial loss: 0.404594\n",
      "epoch 188; iter: 0; batch classifier loss: 0.027001; batch adversarial loss: 0.415795\n",
      "epoch 189; iter: 0; batch classifier loss: 0.016274; batch adversarial loss: 0.376426\n",
      "epoch 190; iter: 0; batch classifier loss: 0.014416; batch adversarial loss: 0.409345\n",
      "epoch 191; iter: 0; batch classifier loss: 0.041559; batch adversarial loss: 0.385549\n",
      "epoch 192; iter: 0; batch classifier loss: 0.017877; batch adversarial loss: 0.424396\n",
      "epoch 193; iter: 0; batch classifier loss: 0.004359; batch adversarial loss: 0.432911\n",
      "epoch 194; iter: 0; batch classifier loss: 0.021161; batch adversarial loss: 0.440195\n",
      "epoch 195; iter: 0; batch classifier loss: 0.007982; batch adversarial loss: 0.559790\n",
      "epoch 196; iter: 0; batch classifier loss: 0.018393; batch adversarial loss: 0.420889\n",
      "epoch 197; iter: 0; batch classifier loss: 0.044169; batch adversarial loss: 0.554846\n",
      "epoch 198; iter: 0; batch classifier loss: 0.029469; batch adversarial loss: 0.343689\n",
      "epoch 199; iter: 0; batch classifier loss: 0.003920; batch adversarial loss: 0.479960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 14:58:50.202666: W tensorflow/c/c_api.cc:304] Operation '{name:'04a44f90-ae24-11ee-be98-ef9b34f2853b/04a44f90-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign' id:15310 op device:{requested: '', assigned: ''} def:{{{node 04a44f90-ae24-11ee-be98-ef9b34f2853b/04a44f90-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](04a44f90-ae24-11ee-be98-ef9b34f2853b/04a44f90-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1, 04a44f90-ae24-11ee-be98-ef9b34f2853b/04a44f90-ae24-11ee-be98-ef9b34f2853b/adversary_model/b2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.676853; batch adversarial loss: 0.481390\n",
      "epoch 1; iter: 0; batch classifier loss: 0.437419; batch adversarial loss: 0.552155\n",
      "epoch 2; iter: 0; batch classifier loss: 0.440226; batch adversarial loss: 0.583142\n",
      "epoch 3; iter: 0; batch classifier loss: 0.351797; batch adversarial loss: 0.574292\n",
      "epoch 4; iter: 0; batch classifier loss: 0.338018; batch adversarial loss: 0.542819\n",
      "epoch 5; iter: 0; batch classifier loss: 0.300229; batch adversarial loss: 0.553174\n",
      "epoch 6; iter: 0; batch classifier loss: 0.335455; batch adversarial loss: 0.639205\n",
      "epoch 7; iter: 0; batch classifier loss: 0.278252; batch adversarial loss: 0.551955\n",
      "epoch 8; iter: 0; batch classifier loss: 0.352989; batch adversarial loss: 0.570928\n",
      "epoch 9; iter: 0; batch classifier loss: 0.285665; batch adversarial loss: 0.527702\n",
      "epoch 10; iter: 0; batch classifier loss: 0.428697; batch adversarial loss: 0.575132\n",
      "epoch 11; iter: 0; batch classifier loss: 0.292515; batch adversarial loss: 0.517718\n",
      "epoch 12; iter: 0; batch classifier loss: 0.308193; batch adversarial loss: 0.528431\n",
      "epoch 13; iter: 0; batch classifier loss: 0.347390; batch adversarial loss: 0.615143\n",
      "epoch 14; iter: 0; batch classifier loss: 0.352985; batch adversarial loss: 0.549480\n",
      "epoch 15; iter: 0; batch classifier loss: 0.438081; batch adversarial loss: 0.562634\n",
      "epoch 16; iter: 0; batch classifier loss: 0.474666; batch adversarial loss: 0.484092\n",
      "epoch 17; iter: 0; batch classifier loss: 0.537933; batch adversarial loss: 0.462633\n",
      "epoch 18; iter: 0; batch classifier loss: 0.431101; batch adversarial loss: 0.457069\n",
      "epoch 19; iter: 0; batch classifier loss: 0.251998; batch adversarial loss: 0.517472\n",
      "epoch 20; iter: 0; batch classifier loss: 0.224597; batch adversarial loss: 0.467308\n",
      "epoch 21; iter: 0; batch classifier loss: 0.276607; batch adversarial loss: 0.398939\n",
      "epoch 22; iter: 0; batch classifier loss: 0.181572; batch adversarial loss: 0.400296\n",
      "epoch 23; iter: 0; batch classifier loss: 0.177965; batch adversarial loss: 0.429645\n",
      "epoch 24; iter: 0; batch classifier loss: 0.241091; batch adversarial loss: 0.425885\n",
      "epoch 25; iter: 0; batch classifier loss: 0.174425; batch adversarial loss: 0.420373\n",
      "epoch 26; iter: 0; batch classifier loss: 0.155972; batch adversarial loss: 0.484405\n",
      "epoch 27; iter: 0; batch classifier loss: 0.166552; batch adversarial loss: 0.473448\n",
      "epoch 28; iter: 0; batch classifier loss: 0.147148; batch adversarial loss: 0.505852\n",
      "epoch 29; iter: 0; batch classifier loss: 0.128623; batch adversarial loss: 0.487755\n",
      "epoch 30; iter: 0; batch classifier loss: 0.128199; batch adversarial loss: 0.397756\n",
      "epoch 31; iter: 0; batch classifier loss: 0.135050; batch adversarial loss: 0.461068\n",
      "epoch 32; iter: 0; batch classifier loss: 0.168073; batch adversarial loss: 0.532609\n",
      "epoch 33; iter: 0; batch classifier loss: 0.140002; batch adversarial loss: 0.460714\n",
      "epoch 34; iter: 0; batch classifier loss: 0.116493; batch adversarial loss: 0.478705\n",
      "epoch 35; iter: 0; batch classifier loss: 0.128863; batch adversarial loss: 0.455109\n",
      "epoch 36; iter: 0; batch classifier loss: 0.158762; batch adversarial loss: 0.469932\n",
      "epoch 37; iter: 0; batch classifier loss: 0.174318; batch adversarial loss: 0.421824\n",
      "epoch 38; iter: 0; batch classifier loss: 0.156649; batch adversarial loss: 0.462608\n",
      "epoch 39; iter: 0; batch classifier loss: 0.071805; batch adversarial loss: 0.463100\n",
      "epoch 40; iter: 0; batch classifier loss: 0.117126; batch adversarial loss: 0.377382\n",
      "epoch 41; iter: 0; batch classifier loss: 0.134884; batch adversarial loss: 0.433760\n",
      "epoch 42; iter: 0; batch classifier loss: 0.138672; batch adversarial loss: 0.416238\n",
      "epoch 43; iter: 0; batch classifier loss: 0.130794; batch adversarial loss: 0.418580\n",
      "epoch 44; iter: 0; batch classifier loss: 0.131218; batch adversarial loss: 0.371258\n",
      "epoch 45; iter: 0; batch classifier loss: 0.178845; batch adversarial loss: 0.408608\n",
      "epoch 46; iter: 0; batch classifier loss: 0.134694; batch adversarial loss: 0.421502\n",
      "epoch 47; iter: 0; batch classifier loss: 0.118609; batch adversarial loss: 0.384739\n",
      "epoch 48; iter: 0; batch classifier loss: 0.119672; batch adversarial loss: 0.326731\n",
      "epoch 49; iter: 0; batch classifier loss: 0.109274; batch adversarial loss: 0.427638\n",
      "epoch 50; iter: 0; batch classifier loss: 0.104195; batch adversarial loss: 0.476334\n",
      "epoch 51; iter: 0; batch classifier loss: 0.107918; batch adversarial loss: 0.443087\n",
      "epoch 52; iter: 0; batch classifier loss: 0.175923; batch adversarial loss: 0.391850\n",
      "epoch 53; iter: 0; batch classifier loss: 0.186730; batch adversarial loss: 0.467400\n",
      "epoch 54; iter: 0; batch classifier loss: 0.089621; batch adversarial loss: 0.453003\n",
      "epoch 55; iter: 0; batch classifier loss: 0.138659; batch adversarial loss: 0.376105\n",
      "epoch 56; iter: 0; batch classifier loss: 0.117478; batch adversarial loss: 0.447369\n",
      "epoch 57; iter: 0; batch classifier loss: 0.149595; batch adversarial loss: 0.556537\n",
      "epoch 58; iter: 0; batch classifier loss: 0.133162; batch adversarial loss: 0.373994\n",
      "epoch 59; iter: 0; batch classifier loss: 0.095284; batch adversarial loss: 0.399273\n",
      "epoch 60; iter: 0; batch classifier loss: 0.189702; batch adversarial loss: 0.486745\n",
      "epoch 61; iter: 0; batch classifier loss: 0.236525; batch adversarial loss: 0.339823\n",
      "epoch 62; iter: 0; batch classifier loss: 0.134629; batch adversarial loss: 0.517877\n",
      "epoch 63; iter: 0; batch classifier loss: 0.139199; batch adversarial loss: 0.442403\n",
      "epoch 64; iter: 0; batch classifier loss: 0.181728; batch adversarial loss: 0.426999\n",
      "epoch 65; iter: 0; batch classifier loss: 0.142184; batch adversarial loss: 0.545779\n",
      "epoch 66; iter: 0; batch classifier loss: 0.191631; batch adversarial loss: 0.406939\n",
      "epoch 67; iter: 0; batch classifier loss: 0.063987; batch adversarial loss: 0.455702\n",
      "epoch 68; iter: 0; batch classifier loss: 0.116220; batch adversarial loss: 0.481868\n",
      "epoch 69; iter: 0; batch classifier loss: 0.125946; batch adversarial loss: 0.531857\n",
      "epoch 70; iter: 0; batch classifier loss: 0.147037; batch adversarial loss: 0.390363\n",
      "epoch 71; iter: 0; batch classifier loss: 0.103622; batch adversarial loss: 0.424851\n",
      "epoch 72; iter: 0; batch classifier loss: 0.148261; batch adversarial loss: 0.402337\n",
      "epoch 73; iter: 0; batch classifier loss: 0.134391; batch adversarial loss: 0.442036\n",
      "epoch 74; iter: 0; batch classifier loss: 0.133925; batch adversarial loss: 0.521973\n",
      "epoch 75; iter: 0; batch classifier loss: 0.135973; batch adversarial loss: 0.398517\n",
      "epoch 76; iter: 0; batch classifier loss: 0.109718; batch adversarial loss: 0.481613\n",
      "epoch 77; iter: 0; batch classifier loss: 0.119464; batch adversarial loss: 0.474885\n",
      "epoch 78; iter: 0; batch classifier loss: 0.094658; batch adversarial loss: 0.439886\n",
      "epoch 79; iter: 0; batch classifier loss: 0.104213; batch adversarial loss: 0.487369\n",
      "epoch 80; iter: 0; batch classifier loss: 0.106088; batch adversarial loss: 0.485202\n",
      "epoch 81; iter: 0; batch classifier loss: 0.123654; batch adversarial loss: 0.409275\n",
      "epoch 82; iter: 0; batch classifier loss: 0.122631; batch adversarial loss: 0.517049\n",
      "epoch 83; iter: 0; batch classifier loss: 0.075787; batch adversarial loss: 0.482589\n",
      "epoch 84; iter: 0; batch classifier loss: 0.152383; batch adversarial loss: 0.478143\n",
      "epoch 85; iter: 0; batch classifier loss: 0.110850; batch adversarial loss: 0.523421\n",
      "epoch 86; iter: 0; batch classifier loss: 0.117871; batch adversarial loss: 0.425409\n",
      "epoch 87; iter: 0; batch classifier loss: 0.087887; batch adversarial loss: 0.502164\n",
      "epoch 88; iter: 0; batch classifier loss: 0.125275; batch adversarial loss: 0.480645\n",
      "epoch 89; iter: 0; batch classifier loss: 0.153359; batch adversarial loss: 0.481203\n",
      "epoch 90; iter: 0; batch classifier loss: 0.071890; batch adversarial loss: 0.477529\n",
      "epoch 91; iter: 0; batch classifier loss: 0.098165; batch adversarial loss: 0.454899\n",
      "epoch 92; iter: 0; batch classifier loss: 0.088133; batch adversarial loss: 0.389929\n",
      "epoch 93; iter: 0; batch classifier loss: 0.092738; batch adversarial loss: 0.448154\n",
      "epoch 94; iter: 0; batch classifier loss: 0.133431; batch adversarial loss: 0.489638\n",
      "epoch 95; iter: 0; batch classifier loss: 0.137739; batch adversarial loss: 0.499588\n",
      "epoch 96; iter: 0; batch classifier loss: 0.087353; batch adversarial loss: 0.420943\n",
      "epoch 97; iter: 0; batch classifier loss: 0.117786; batch adversarial loss: 0.497951\n",
      "epoch 98; iter: 0; batch classifier loss: 0.065249; batch adversarial loss: 0.428515\n",
      "epoch 99; iter: 0; batch classifier loss: 0.090230; batch adversarial loss: 0.493973\n",
      "epoch 100; iter: 0; batch classifier loss: 0.112380; batch adversarial loss: 0.465067\n",
      "epoch 101; iter: 0; batch classifier loss: 0.090965; batch adversarial loss: 0.395062\n",
      "epoch 102; iter: 0; batch classifier loss: 0.094135; batch adversarial loss: 0.400996\n",
      "epoch 103; iter: 0; batch classifier loss: 0.089004; batch adversarial loss: 0.465836\n",
      "epoch 104; iter: 0; batch classifier loss: 0.059487; batch adversarial loss: 0.451737\n",
      "epoch 105; iter: 0; batch classifier loss: 0.100756; batch adversarial loss: 0.467687\n",
      "epoch 106; iter: 0; batch classifier loss: 0.081118; batch adversarial loss: 0.465168\n",
      "epoch 107; iter: 0; batch classifier loss: 0.095309; batch adversarial loss: 0.463123\n",
      "epoch 108; iter: 0; batch classifier loss: 0.092906; batch adversarial loss: 0.409132\n",
      "epoch 109; iter: 0; batch classifier loss: 0.076942; batch adversarial loss: 0.405416\n",
      "epoch 110; iter: 0; batch classifier loss: 0.093007; batch adversarial loss: 0.436791\n",
      "epoch 111; iter: 0; batch classifier loss: 0.043238; batch adversarial loss: 0.504762\n",
      "epoch 112; iter: 0; batch classifier loss: 0.079584; batch adversarial loss: 0.350126\n",
      "epoch 113; iter: 0; batch classifier loss: 0.037386; batch adversarial loss: 0.393828\n",
      "epoch 114; iter: 0; batch classifier loss: 0.075760; batch adversarial loss: 0.465935\n",
      "epoch 115; iter: 0; batch classifier loss: 0.057886; batch adversarial loss: 0.434064\n",
      "epoch 116; iter: 0; batch classifier loss: 0.091209; batch adversarial loss: 0.368064\n",
      "epoch 117; iter: 0; batch classifier loss: 0.050625; batch adversarial loss: 0.381148\n",
      "epoch 118; iter: 0; batch classifier loss: 0.078911; batch adversarial loss: 0.411944\n",
      "epoch 119; iter: 0; batch classifier loss: 0.050333; batch adversarial loss: 0.475839\n",
      "epoch 120; iter: 0; batch classifier loss: 0.068269; batch adversarial loss: 0.443330\n",
      "epoch 121; iter: 0; batch classifier loss: 0.057436; batch adversarial loss: 0.551034\n",
      "epoch 122; iter: 0; batch classifier loss: 0.083078; batch adversarial loss: 0.454475\n",
      "epoch 123; iter: 0; batch classifier loss: 0.049261; batch adversarial loss: 0.488999\n",
      "epoch 124; iter: 0; batch classifier loss: 0.028920; batch adversarial loss: 0.537698\n",
      "epoch 125; iter: 0; batch classifier loss: 0.030366; batch adversarial loss: 0.488534\n",
      "epoch 126; iter: 0; batch classifier loss: 0.046178; batch adversarial loss: 0.547241\n",
      "epoch 127; iter: 0; batch classifier loss: 0.062631; batch adversarial loss: 0.416532\n",
      "epoch 128; iter: 0; batch classifier loss: 0.053062; batch adversarial loss: 0.465343\n",
      "epoch 129; iter: 0; batch classifier loss: 0.072464; batch adversarial loss: 0.489214\n",
      "epoch 130; iter: 0; batch classifier loss: 0.062374; batch adversarial loss: 0.364591\n",
      "epoch 131; iter: 0; batch classifier loss: 0.027356; batch adversarial loss: 0.449469\n",
      "epoch 132; iter: 0; batch classifier loss: 0.041628; batch adversarial loss: 0.498924\n",
      "epoch 133; iter: 0; batch classifier loss: 0.044129; batch adversarial loss: 0.515805\n",
      "epoch 134; iter: 0; batch classifier loss: 0.038179; batch adversarial loss: 0.456757\n",
      "epoch 135; iter: 0; batch classifier loss: 0.035924; batch adversarial loss: 0.469502\n",
      "epoch 136; iter: 0; batch classifier loss: 0.046598; batch adversarial loss: 0.343064\n",
      "epoch 137; iter: 0; batch classifier loss: 0.028527; batch adversarial loss: 0.415441\n",
      "epoch 138; iter: 0; batch classifier loss: 0.076236; batch adversarial loss: 0.463869\n",
      "epoch 139; iter: 0; batch classifier loss: 0.036949; batch adversarial loss: 0.480957\n",
      "epoch 140; iter: 0; batch classifier loss: 0.042520; batch adversarial loss: 0.401348\n",
      "epoch 141; iter: 0; batch classifier loss: 0.036564; batch adversarial loss: 0.504474\n",
      "epoch 142; iter: 0; batch classifier loss: 0.028633; batch adversarial loss: 0.453227\n",
      "epoch 143; iter: 0; batch classifier loss: 0.037946; batch adversarial loss: 0.419074\n",
      "epoch 144; iter: 0; batch classifier loss: 0.031549; batch adversarial loss: 0.475747\n",
      "epoch 145; iter: 0; batch classifier loss: 0.025553; batch adversarial loss: 0.471519\n",
      "epoch 146; iter: 0; batch classifier loss: 0.010562; batch adversarial loss: 0.498739\n",
      "epoch 147; iter: 0; batch classifier loss: 0.028155; batch adversarial loss: 0.451639\n",
      "epoch 148; iter: 0; batch classifier loss: 0.046120; batch adversarial loss: 0.376928\n",
      "epoch 149; iter: 0; batch classifier loss: 0.021840; batch adversarial loss: 0.432979\n",
      "epoch 150; iter: 0; batch classifier loss: 0.034294; batch adversarial loss: 0.342120\n",
      "epoch 151; iter: 0; batch classifier loss: 0.054529; batch adversarial loss: 0.326593\n",
      "epoch 152; iter: 0; batch classifier loss: 0.077630; batch adversarial loss: 0.448940\n",
      "epoch 153; iter: 0; batch classifier loss: 0.040042; batch adversarial loss: 0.545753\n",
      "epoch 154; iter: 0; batch classifier loss: 0.016531; batch adversarial loss: 0.480704\n",
      "epoch 155; iter: 0; batch classifier loss: 0.076214; batch adversarial loss: 0.515809\n",
      "epoch 156; iter: 0; batch classifier loss: 0.045667; batch adversarial loss: 0.368160\n",
      "epoch 157; iter: 0; batch classifier loss: 0.012038; batch adversarial loss: 0.452551\n",
      "epoch 158; iter: 0; batch classifier loss: 0.011967; batch adversarial loss: 0.466432\n",
      "epoch 159; iter: 0; batch classifier loss: 0.026693; batch adversarial loss: 0.368178\n",
      "epoch 160; iter: 0; batch classifier loss: 0.043749; batch adversarial loss: 0.388480\n",
      "epoch 161; iter: 0; batch classifier loss: 0.044206; batch adversarial loss: 0.394147\n",
      "epoch 162; iter: 0; batch classifier loss: 0.012782; batch adversarial loss: 0.381311\n",
      "epoch 163; iter: 0; batch classifier loss: 0.021634; batch adversarial loss: 0.458700\n",
      "epoch 164; iter: 0; batch classifier loss: 0.033911; batch adversarial loss: 0.485215\n",
      "epoch 165; iter: 0; batch classifier loss: 0.017904; batch adversarial loss: 0.434055\n",
      "epoch 166; iter: 0; batch classifier loss: 0.036839; batch adversarial loss: 0.392474\n",
      "epoch 167; iter: 0; batch classifier loss: 0.051227; batch adversarial loss: 0.472954\n",
      "epoch 168; iter: 0; batch classifier loss: 0.033330; batch adversarial loss: 0.512624\n",
      "epoch 169; iter: 0; batch classifier loss: 0.024520; batch adversarial loss: 0.470773\n",
      "epoch 170; iter: 0; batch classifier loss: 0.036503; batch adversarial loss: 0.498794\n",
      "epoch 171; iter: 0; batch classifier loss: 0.015565; batch adversarial loss: 0.436696\n",
      "epoch 172; iter: 0; batch classifier loss: 0.034949; batch adversarial loss: 0.382899\n",
      "epoch 173; iter: 0; batch classifier loss: 0.028163; batch adversarial loss: 0.412533\n",
      "epoch 174; iter: 0; batch classifier loss: 0.029861; batch adversarial loss: 0.490693\n",
      "epoch 175; iter: 0; batch classifier loss: 0.034575; batch adversarial loss: 0.398280\n",
      "epoch 176; iter: 0; batch classifier loss: 0.029430; batch adversarial loss: 0.351811\n",
      "epoch 177; iter: 0; batch classifier loss: 0.030654; batch adversarial loss: 0.419205\n",
      "epoch 178; iter: 0; batch classifier loss: 0.028839; batch adversarial loss: 0.452305\n",
      "epoch 179; iter: 0; batch classifier loss: 0.009186; batch adversarial loss: 0.447255\n",
      "epoch 180; iter: 0; batch classifier loss: 0.020922; batch adversarial loss: 0.480464\n",
      "epoch 181; iter: 0; batch classifier loss: 0.057891; batch adversarial loss: 0.391066\n",
      "epoch 182; iter: 0; batch classifier loss: 0.041594; batch adversarial loss: 0.424874\n",
      "epoch 183; iter: 0; batch classifier loss: 0.011007; batch adversarial loss: 0.526353\n",
      "epoch 184; iter: 0; batch classifier loss: 0.016264; batch adversarial loss: 0.471092\n",
      "epoch 185; iter: 0; batch classifier loss: 0.050446; batch adversarial loss: 0.374883\n",
      "epoch 186; iter: 0; batch classifier loss: 0.017392; batch adversarial loss: 0.468230\n",
      "epoch 187; iter: 0; batch classifier loss: 0.026993; batch adversarial loss: 0.399024\n",
      "epoch 188; iter: 0; batch classifier loss: 0.044627; batch adversarial loss: 0.422825\n",
      "epoch 189; iter: 0; batch classifier loss: 0.006847; batch adversarial loss: 0.463184\n",
      "epoch 190; iter: 0; batch classifier loss: 0.013429; batch adversarial loss: 0.546738\n",
      "epoch 191; iter: 0; batch classifier loss: 0.014486; batch adversarial loss: 0.468869\n",
      "epoch 192; iter: 0; batch classifier loss: 0.028755; batch adversarial loss: 0.426485\n",
      "epoch 193; iter: 0; batch classifier loss: 0.018699; batch adversarial loss: 0.512493\n",
      "epoch 194; iter: 0; batch classifier loss: 0.034065; batch adversarial loss: 0.396689\n",
      "epoch 195; iter: 0; batch classifier loss: 0.010914; batch adversarial loss: 0.533134\n",
      "epoch 196; iter: 0; batch classifier loss: 0.041668; batch adversarial loss: 0.497749\n",
      "epoch 197; iter: 0; batch classifier loss: 0.018364; batch adversarial loss: 0.379092\n",
      "epoch 198; iter: 0; batch classifier loss: 0.017675; batch adversarial loss: 0.470979\n",
      "epoch 199; iter: 0; batch classifier loss: 0.004900; batch adversarial loss: 0.464659\n"
     ]
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='ACSIncomeDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d8270",
   "metadata": {},
   "source": [
    "### Experiment iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc40e12",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EXPERIMENT_SEEDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Configs for an experiment iteration\u001B[39;00m\n\u001B[1;32m      2\u001B[0m exp_iter_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m----> 3\u001B[0m experiment_seed \u001B[38;5;241m=\u001B[39m \u001B[43mEXPERIMENT_SEEDS\u001B[49m[exp_iter_num \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m      4\u001B[0m tuned_params_filenames \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      5\u001B[0m ]\n\u001B[1;32m      6\u001B[0m tuned_params_df_paths \u001B[38;5;241m=\u001B[39m [os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(ROOT_DIR, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiff_fairness_interventions_exp\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      7\u001B[0m                                       FAIRNESS_INTERVENTION_NAME, EXPERIMENT_NAME, tuned_params_filename)\n\u001B[1;32m      8\u001B[0m                          \u001B[38;5;28;01mfor\u001B[39;00m tuned_params_filename \u001B[38;5;129;01min\u001B[39;00m tuned_params_filenames]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'EXPERIMENT_SEEDS' is not defined"
     ]
    }
   ],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 2\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be9a5b79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:56.249510Z",
     "start_time": "2024-01-04T20:53:56.233525Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 17:15:39 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 200,\n",
      " 'experiment_iteration': 'Exp_iter_2',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'intervention_param': 'True',\n",
      " 'model_init_seed': 200,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5645c6009d3b48d29619a238bc476502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 17:15:39 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-04 17:15:39 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([322, 293, 576, 300, 391, 343, 294, 558, 560, 439, 355, 440, 277,\n",
      "            492, 644, 639, 589, 259, 313, 129],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([322, 293, 576, 300, 391, 343, 294, 558, 560, 439, 355, 440, 277,\n",
      "            492, 644, 639, 589, 259, 313, 129],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 200, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c0277603884950a0c35cc9dcc3f539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7137564e8e0d4fcbbbfaaf03d73e170a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa30ab3dbbfe4acb8557a8c1e2d6556b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a36e32d08b44c1b01ae496881c6b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='ACSIncomeDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834a569",
   "metadata": {},
   "source": [
    "### Experiment iteration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d130fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 3\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab940edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:46.750905Z",
     "start_time": "2024-01-04T20:53:46.744795Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 19:29:01 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 300,\n",
      " 'experiment_iteration': 'Exp_iter_3',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'model_init_seed': 300,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48b0aa26f024ad1b1089190e134193d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 19:29:01 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-04 19:29:01 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([250, 438, 479, 326,  46, 565, 534, 382, 377, 457,  97, 388, 123,\n",
      "            156, 430, 466,  38, 474, 167, 524],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([250, 438, 479, 326,  46, 565, 534, 382, 377, 457,  97, 388, 123,\n",
      "            156, 430, 466,  38, 474, 167, 524],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 300, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eddcae8f6fc4651885a3144088ae9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43ba8d965cc436a91f10cc39b3d5e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94495e7bec64c9ebf4c05cdd52e605d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f7167dd20740a68f8f90158ec19560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='ACSIncomeDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9f5f7",
   "metadata": {},
   "source": [
    "### Experiment iteration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b363156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 4\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "048d1899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:37.675129Z",
     "start_time": "2024-01-04T20:53:37.670178Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 20:15:17 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 400,\n",
      " 'experiment_iteration': 'Exp_iter_4',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'intervention_param': 'True',\n",
      " 'model_init_seed': 400,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5b534568b14897b8686d22a9c79d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 20:15:17 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-04 20:15:17 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([331, 157, 559, 553, 580, 169, 561, 452, 180, 257, 160, 289, 197,\n",
      "             39, 290,  68,  56, 638,  54, 120],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([331, 157, 559, 553, 580, 169, 561, 452, 180, 257, 160, 289, 197,\n",
      "             39, 290,  68,  56, 638,  54, 120],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 400, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42164aebf0d74a5992f0ca8075639200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a128a0a62f40fb9bd0b88e41d55103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6300cf6db9cc4bcf95389b1f6e9b8108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e75feb8b5c4e00a1fc551544462dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='ACSIncomeDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6ac04",
   "metadata": {},
   "source": [
    "### Experiment iteration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13c30aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 5\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4077068c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:27.080554Z",
     "start_time": "2024-01-04T20:53:27.072313Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 03:55:49 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 500,\n",
      " 'experiment_iteration': 'Exp_iter_5',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'model_init_seed': 500,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8257cfd4f743e8887bbc74613694af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 03:55:49 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-05 03:55:49 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([ 92, 640, 589, 519, 377, 478, 298, 336, 149, 278, 343, 573, 365,\n",
      "            174, 171, 219, 469, 162, 567, 203],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([ 92, 640, 589, 519, 377, 478, 298, 336, 149, 278, 343, 573, 365,\n",
      "            174, 171, 219, 469, 162, 567, 203],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 500, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2d40e5f54c4d67bf9f1e66289ae23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63535df3c576469e9e855b70194bc572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15688b8597f74cdaadb60cc93599720c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3165f4884fae4f29829de3acc475dac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='ACSIncomeDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f1dfe",
   "metadata": {},
   "source": [
    "### Experiment iteration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e80e4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for an experiment iteration\n",
    "exp_iter_num = 6\n",
    "experiment_seed = EXPERIMENT_SEEDS[exp_iter_num - 1]\n",
    "custom_table_fields_dct['experiment_iteration'] = f'Exp_iter_{exp_iter_num}'\n",
    "exp_iter_data_loader = copy.deepcopy(data_loader)  # Add deepcopy to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad31a7be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T20:53:16.632770Z",
     "start_time": "2024-01-04T20:53:16.629083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 04:49:43 experiment_interface.py INFO    : Start an experiment iteration for the following custom params:\n",
      "INFO:root:Start an experiment iteration for the following custom params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_split_seed': 600,\n",
      " 'experiment_iteration': 'Exp_iter_6',\n",
      " 'fair_intervention_params_lst': '[True]',\n",
      " 'intervention_param': 'True',\n",
      " 'model_init_seed': 600,\n",
      " 'session_uuid': '0626d80f-e288-4f16-b8ab-260eb34d62d3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2670abffe30e4a388a7a09306feb6673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multiple alphas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 04:49:43 experiment_interface.py INFO    : The dataset is preprocessed\n",
      "INFO:root:The dataset is preprocessed\n",
      "2024-01-05 04:49:43 experiment_interface.py INFO    : Models config is loaded from the input file\n",
      "INFO:root:Models config is loaded from the input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention_option:  True\n",
      "Using ROC postprocessor\n",
      "cur_base_flow_dataset.X_train_val.columns:  Index(['cat__school_GP', 'cat__school_MS', 'cat__address_R', 'cat__address_U',\n",
      "       'cat__famsize_GT3', 'cat__famsize_LE3', 'cat__Pstatus_A',\n",
      "       'cat__Pstatus_T', 'cat__Mjob_at_home', 'cat__Mjob_health',\n",
      "       'cat__Mjob_other', 'cat__Mjob_services', 'cat__Mjob_teacher',\n",
      "       'cat__Fjob_at_home', 'cat__Fjob_health', 'cat__Fjob_other',\n",
      "       'cat__Fjob_services', 'cat__Fjob_teacher', 'cat__reason_course',\n",
      "       'cat__reason_home', 'cat__reason_other', 'cat__reason_reputation',\n",
      "       'cat__guardian_father', 'cat__guardian_mother', 'cat__guardian_other',\n",
      "       'cat__schoolsup_no', 'cat__schoolsup_yes', 'cat__famsup_no',\n",
      "       'cat__famsup_yes', 'cat__paid_no', 'cat__paid_yes',\n",
      "       'cat__activities_no', 'cat__activities_yes', 'cat__nursery_no',\n",
      "       'cat__nursery_yes', 'cat__higher_no', 'cat__higher_yes',\n",
      "       'cat__internet_no', 'cat__internet_yes', 'cat__romantic_no',\n",
      "       'cat__romantic_yes', 'num__age', 'num__Medu', 'num__Fedu',\n",
      "       'num__traveltime', 'num__studytime', 'num__failures', 'num__famrel',\n",
      "       'num__freetime', 'num__goout', 'num__Dalc', 'num__Walc', 'num__health',\n",
      "       'num__absences', 'num__G1', 'num__G2', 'sex_binary'],\n",
      "      dtype='object')\n",
      "Top indexes of an X_test in the current base flow dataset:  Int64Index([390, 397, 375, 507, 625, 369, 245, 428, 593, 102,  49, 621,  92,\n",
      "            158, 505, 612,  29, 452,  56, 145],\n",
      "           dtype='int64')\n",
      "Top indexes of an y_test in the current base flow dataset:  Int64Index([390, 397, 375, 507, 625, 369, 245, 428, 593, 102,  49, 621,  92,\n",
      "            158, 505, 612,  29, 452,  56, 145],\n",
      "           dtype='int64')\n",
      "Path for tuned params:  /home/dh3553/projects/fairness-variance/results/diff_fairness_interventions_exp/ROC/ROC_student_performance/tuning_results_Student_Performance_Por_20240104__212802.csv\n",
      "LGBMClassifier:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 600, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'min_data_in_leaf': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f28080438649c0aa4a0f71f4b322c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyze multiple models:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511b20dacfe5420981d3cf173f60fc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40a989d937f483784b5138783479302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled a postprocessing mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c5fe28a14a47118da3a7d485a77e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifiers testing by bootstrap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_exp_iter_with_inprocessor(data_loader=exp_iter_data_loader,\n",
    "                              experiment_seed=experiment_seed,\n",
    "                              test_set_fraction=TEST_SET_FRACTION,\n",
    "                              db_writer_func=db_writer_func,\n",
    "                              fair_intervention_params_lst=FAIR_INTERVENTION_PARAMS_LST,\n",
    "                              metrics_computation_config=metrics_computation_config,\n",
    "                              custom_table_fields_dct=custom_table_fields_dct,\n",
    "                              dataset_name='ACSIncomeDataset',\n",
    "                              inprocessor_name='AdversarialDebiasing',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb640e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
