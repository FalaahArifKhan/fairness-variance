{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Raiser","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"api/overview/","title":"Overview","text":""},{"location":"api/overview/#analyzers","title":"analyzers","text":"<p>Subgroups Fairness and Stability Analyzers.</p> <p>This module contains fairness and stability analysing methods for defined subgroups. The purpose of an analyzer is to analyse defined metrics for defined subgroups.</p> <ul> <li>AbstractOverallVarianceAnalyzer</li> <li>AbstractSubgroupsAnalyzer</li> <li>BatchOverallVarianceAnalyzer</li> <li>SubgroupsStatisticalBiasAnalyzer</li> <li>SubgroupsVarianceAnalyzer</li> <li>SubgroupsVarianceCalculator</li> </ul>"},{"location":"api/overview/#custom_classes","title":"custom_classes","text":""},{"location":"api/overview/#metrics","title":"metrics","text":""},{"location":"api/overview/#preprocessing","title":"preprocessing","text":""},{"location":"api/overview/#user_interfaces","title":"user_interfaces","text":"<p>User interfaces.</p> <p>This module contains user interfaces for metrics computation.</p> <ul> <li>run_metrics_computation_with_config</li> </ul>"},{"location":"api/overview/#utils","title":"utils","text":""},{"location":"api/analyzers/AbstractOverallVarianceAnalyzer/","title":"AbstractOverallVarianceAnalyzer","text":"<p>AbstractOverallVarianceAnalyzer description.</p>"},{"location":"api/analyzers/AbstractOverallVarianceAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>base_model</p> <p>Base model for stability measuring</p> </li> <li> <p>base_model_name (str)</p> <p>Model name like 'HoeffdingTreeClassifier' or 'LogisticRegression'</p> </li> <li> <p>bootstrap_fraction (float)</p> <p>[0-1], fraction from train_pd_dataset for fitting an ensemble of base models</p> </li> <li> <p>X_train (pandas.core.frame.DataFrame)</p> <p>Processed features train set</p> </li> <li> <p>y_train (pandas.core.frame.DataFrame)</p> <p>Targets train set</p> </li> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>dataset_name (str)</p> <p>Name of dataset, used for correct results naming</p> </li> <li> <p>n_estimators (int)</p> <p>Number of estimators in ensemble to measure base_model stability</p> </li> </ul>"},{"location":"api/analyzers/AbstractOverallVarianceAnalyzer/#methods","title":"Methods","text":"UQ_by_boostrap <p>Quantifying uncertainty of the base model by constructing an ensemble from bootstrapped samples.</p> <p>Parameters</p> <ul> <li>boostrap_size     (int)    </li> <li>with_replacement     (bool)    </li> </ul> <p>Returns</p> <p>Dictionary where keys are models indexes,</p> compute_metrics <p>Measure metrics for the base model. Display plots for analysis if needed. Save results to a .pkl file</p> <p>Parameters</p> <ul> <li>make_plots     (bool)     \u2013 defaults to <code>False</code> </li> <li>save_results     (bool)     \u2013 defaults to <code>True</code> </li> </ul> get_metrics_dict print_metrics save_metrics_to_file"},{"location":"api/analyzers/AbstractSubgroupsAnalyzer/","title":"AbstractSubgroupsAnalyzer","text":"<p>AbstractSubgroupsAnalyzer description.</p>"},{"location":"api/analyzers/AbstractSubgroupsAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attributes names (including attributes intersections),  and values are privilege values for these subgroups</p> </li> <li> <p>test_groups (dict)</p> <p>A dictionary where keys are sensitive attributes, and values input dataset rows  that are correspondent to these sensitive attributes</p> </li> </ul>"},{"location":"api/analyzers/AbstractSubgroupsAnalyzer/#methods","title":"Methods","text":"compute_subgroups_metrics save_metrics_to_file"},{"location":"api/analyzers/BatchOverallVarianceAnalyzer/","title":"BatchOverallVarianceAnalyzer","text":"<p>BatchOverallVarianceAnalyzer description.</p>"},{"location":"api/analyzers/BatchOverallVarianceAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>base_model</p> <p>Base model for stability measuring</p> </li> <li> <p>base_model_name (str)</p> <p>Model name like 'HoeffdingTreeClassifier' or 'LogisticRegression'</p> </li> <li> <p>bootstrap_fraction (float)</p> <p>[0-1], fraction from train_pd_dataset for fitting an ensemble of base models</p> </li> <li> <p>X_train</p> <p>Processed features train set</p> </li> <li> <p>y_train</p> <p>Targets train set</p> </li> <li> <p>X_test</p> <p>Processed features test set</p> </li> <li> <p>y_test</p> <p>Targets test set</p> </li> <li> <p>target_column (str)</p> <p>Name of the target column</p> </li> <li> <p>dataset_name (str)</p> <p>Name of dataset, used for correct results naming</p> </li> <li> <p>n_estimators (int)</p> <p>Number of estimators in ensemble to measure base_model stability</p> </li> </ul>"},{"location":"api/analyzers/BatchOverallVarianceAnalyzer/#methods","title":"Methods","text":"UQ_by_boostrap <p>Quantifying uncertainty of the base model by constructing an ensemble from bootstrapped samples.</p> <p>Parameters</p> <ul> <li>boostrap_size     (int)    </li> <li>with_replacement     (bool)    </li> </ul> <p>Returns</p> <p>Dictionary where keys are models indexes,</p> compute_metrics <p>Measure metrics for the base model. Display plots for analysis if needed. Save results to a .pkl file</p> <p>Parameters</p> <ul> <li>make_plots     (bool)     \u2013 defaults to <code>False</code> </li> <li>save_results     (bool)     \u2013 defaults to <code>True</code> </li> </ul> get_metrics_dict print_metrics save_metrics_to_file"},{"location":"api/analyzers/SubgroupsStatisticalBiasAnalyzer/","title":"SubgroupsStatisticalBiasAnalyzer","text":"<p>SubgroupsStatisticalBiasAnalyzer description.</p>"},{"location":"api/analyzers/SubgroupsStatisticalBiasAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>X_test</p> <p>Processed features test set</p> </li> <li> <p>y_test</p> <p>Targets test set</p> </li> <li> <p>sensitive_attributes_dct</p> <p>A dictionary where keys are sensitive attributes names (including attributes intersections),  and values are privilege values for these subgroups</p> </li> <li> <p>test_groups \u2013 defaults to <code>None</code></p> <p>A dictionary where keys are sensitive attributes, and values input dataset rows  that are correspondent to these sensitive attributes</p> </li> </ul>"},{"location":"api/analyzers/SubgroupsStatisticalBiasAnalyzer/#methods","title":"Methods","text":"compute_subgroups_metrics save_metrics_to_file"},{"location":"api/analyzers/SubgroupsVarianceAnalyzer/","title":"SubgroupsVarianceAnalyzer","text":"<p>SubgroupsVarianceAnalyzer description.</p>"},{"location":"api/analyzers/SubgroupsVarianceAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>model_setting</p> <p>Constant from configs.constants.ModelSetting</p> </li> <li> <p>n_estimators (int)</p> <p>Number of estimators for bootstrap</p> </li> <li> <p>base_model</p> <p>Initialized base model to analyze</p> </li> <li> <p>base_model_name (str)</p> <p>Model name</p> </li> <li> <p>bootstrap_fraction (float)</p> <p>[0-1], fraction from train_pd_dataset for fitting an ensemble of base models</p> </li> <li> <p>base_pipeline (source.custom_classes.generic_pipeline.GenericPipeline)</p> <p>Initialized object of GenericPipeline class</p> </li> <li> <p>dataset_name (str)</p> <p>Name of dataset, used for correct results naming</p> </li> </ul>"},{"location":"api/analyzers/SubgroupsVarianceAnalyzer/#methods","title":"Methods","text":"compute_metrics <p>Measure variance metrics for subgroups for the base model. Display stability plots for analysis if needed.  Save results to a .csv file if needed.</p> <p>:param save_results: bool if we need to save metrics in a file :param make_plots: bool, if display plots for analysis</p> <p>Parameters</p> <ul> <li>save_results </li> <li>result_filename </li> <li>save_dir_path </li> <li>make_plots     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/analyzers/SubgroupsVarianceCalculator/","title":"SubgroupsVarianceCalculator","text":"<p>SubgroupsVarianceCalculator description.</p>"},{"location":"api/analyzers/SubgroupsVarianceCalculator/#parameters","title":"Parameters","text":"<ul> <li> <p>X_test</p> <p>Processed features test set</p> </li> <li> <p>y_test</p> <p>Targets test set</p> </li> <li> <p>sensitive_attributes_dct</p> <p>A dictionary where keys are sensitive attributes names (including attributes intersections),  and values are privilege values for these subgroups</p> </li> <li> <p>test_groups \u2013 defaults to <code>None</code></p> <p>A dictionary where keys are sensitive attributes, and values input dataset rows  that are correspondent to these sensitive attributes</p> </li> </ul>"},{"location":"api/analyzers/SubgroupsVarianceCalculator/#methods","title":"Methods","text":"compute_subgroups_metrics <p>Compute variance metrics for subgroups</p> <p>:param models_predictions: dict of lists, where key is a model index and value is model predictions based on X_test :return: dict of dicts, where key is 'overall' or a subgroup name, and value is a dict of metrics for this subgroup</p> <p>Parameters</p> <ul> <li>models_predictions </li> <li>save_results     (bool)    </li> <li>result_filename     (str)    </li> <li>save_dir_path     (str)    </li> </ul> save_metrics_to_file set_overall_stability_metrics"},{"location":"api/user-interfaces/run-metrics-computation-with-config/","title":"run_metrics_computation_with_config","text":"<p>Find variance and statistical bias metrics for each model in models_config. Save results in <code>save_results_dir_path</code> folder.</p> <p>Returns a dictionary where keys are model names, and values are metrics for sensitive attributes defined in config.</p>"},{"location":"api/user-interfaces/run-metrics-computation-with-config/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset (source.custom_classes.base_dataset.BaseDataset)</p> <p>Dataset object that contains all needed attributes like target, features, numerical_columns etc</p> </li> <li> <p>config</p> <p>Object that contains test_set_fraction, bootstrap_fraction, dataset_name,  n_estimators, sensitive_attributes_dct attributes</p> </li> <li> <p>models_config (dict)</p> <p>Dictionary where keys are model names, and values are initialized models</p> </li> <li> <p>save_results_dir_path (str)</p> <p>Location where to save result files with metrics</p> </li> <li> <p>run_seed (int) \u2013 defaults to <code>None</code></p> <p>Base seed for this run</p> </li> <li> <p>debug_mode (bool) \u2013 defaults to <code>False</code></p> <p>Enable or disable extra logs</p> </li> </ul>"},{"location":"examples/Interfaces_With_Config_Usage_Example/","title":"Interfaces With Config Usage Example","text":"<pre><code>%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n</code></pre> <pre><code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code></pre> <pre><code>import os\nimport warnings\nwarnings.filterwarnings('ignore')\nos.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n</code></pre> <pre><code>cur_folder_name = os.getcwd().split('/')[-1]\nif cur_folder_name != \"fairness-variance\":\n    os.chdir(\"..\")\n\nprint('Current location: ', os.getcwd())\n</code></pre> <pre><code>Current location:  /home/denys_herasymuk/UCU/4course_2term/Bachelor_Thesis/Code/fairness-variance\n</code></pre>"},{"location":"examples/Interfaces_With_Config_Usage_Example/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nimport pandas as pd\nfrom datetime import datetime, timezone\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom source.custom_initializers import create_config_obj\nfrom source.custom_classes.data_loaders import CompasWithoutSensitiveAttrsDataset\nfrom source.metrics_computation_interfaces import run_metrics_computation_with_config, compute_model_metrics_with_config\n</code></pre>"},{"location":"examples/Interfaces_With_Config_Usage_Example/#configs","title":"Configs","text":"<pre><code>config = create_config_obj(config_yaml_path=os.path.join('configs', 'experiment1_compas_config.yaml'))\nSAVE_RESULTS_DIR_PATH = os.path.join('results', 'hypothesis_space',\n                                     f'{config.dataset_name}_Metrics_{datetime.now(timezone.utc).strftime(\"%Y%m%d__%H%M%S\")}')\n</code></pre> <pre><code>models_config = {\n    'DecisionTreeClassifier': DecisionTreeClassifier(criterion='gini',\n                                                     max_depth=20,\n                                                     max_features=0.6,\n                                                     min_samples_split=0.1),\n    'LogisticRegression': LogisticRegression(C=1,\n                                             max_iter=50,\n                                             penalty='l2',\n                                             solver='newton-cg'),\n}\n</code></pre>"},{"location":"examples/Interfaces_With_Config_Usage_Example/#load-dataset","title":"Load dataset","text":"<pre><code>dataset = CompasWithoutSensitiveAttrsDataset(dataset_path='data/COMPAS.csv')\ndataset.X_data.head()\n</code></pre> juv_fel_count juv_misd_count juv_other_count priors_count age_cat_25 - 45 age_cat_Greater than 45 age_cat_Less than 25 c_charge_degree_F c_charge_degree_M 0 0.0 -2.340451 1.0 -15.010999 1 0 0 0 1 1 0.0 0.000000 0.0 0.000000 1 0 0 1 0 2 0.0 0.000000 0.0 0.000000 0 0 1 1 0 3 0.0 0.000000 0.0 6.000000 1 0 0 0 1 4 0.0 0.000000 0.0 7.513697 1 0 0 1 0"},{"location":"examples/Interfaces_With_Config_Usage_Example/#get-metrics-for-a-base-model-with-a-compute_model_metrics_with_config-interface-and-input-arguments-as-a-config","title":"Get metrics for a base model with a compute_model_metrics_with_config interface and input arguments as a config","text":"<pre><code>model_name = 'DecisionTreeClassifier'\nmetrics_df = compute_model_metrics_with_config(models_config[model_name], model_name, dataset,\n                                               config, SAVE_RESULTS_DIR_PATH,\n                                               save_results=True,\n                                               debug_mode=True)\nprint('Subgroups statistical bias and variance metrics: ')\nmetrics_df\n</code></pre> <pre><code>Model random_state:  623\nBaseline X_train shape:  (4222, 9)\nBaseline X_test shape:  (1056, 9)\n\nProtected groups splits:\nsex_priv (214, 11)\nsex_dis (842, 11)\nrace_priv (420, 11)\nrace_dis (636, 11)\nsex&amp;race_priv (93, 11)\nsex&amp;race_dis (515, 11)\n\n\nTop rows of processed X train + validation set:\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  juv_fel_count age_cat_25 - 45_1 juv_misd_count juv_other_count age_cat_Greater than 45_1 age_cat_Greater than 45_0 age_cat_Less than 25_0 c_charge_degree_F_0 c_charge_degree_F_1 age_cat_25 - 45_0 c_charge_degree_M_1 c_charge_degree_M_0 priors_count age_cat_Less than 25_1 3600 -0.102581 1 -0.13003 -0.149275 0 1 1 0 1 0 0 1 -0.274707 0 3043 -0.102581 0 -0.13003 -0.149275 0 1 0 1 0 1 1 0 -0.660459 1 418 -0.102581 1 -0.13003 -0.149275 0 1 1 1 0 0 1 0 -0.467583 0 3874 -0.102581 1 -0.13003 -0.149275 0 1 1 0 1 0 0 1 -0.274707 0 442 -0.102581 0 -0.13003 -0.149275 0 1 0 1 0 1 1 0 -0.660459 1 4812 -0.102581 1 -0.13003 -0.149275 0 1 1 1 0 0 1 0 -0.660459 0 4487 -0.102581 1 -1.57448 -0.149275 0 1 1 1 0 0 1 0 -0.467583 0 4968 -0.102581 0 -0.13003 -0.149275 1 0 1 0 1 1 0 1 -0.660459 0 4394 -0.102581 0 -0.13003 -0.149275 1 0 1 0 1 1 0 1 -0.081831 0 3122 -0.102581 1 -0.13003 -0.149275 0 1 1 1 0 0 1 0 -0.660459 0 <pre><code>2023-01-27 00:30:40 abstract_overall_variance_analyzer.py INFO    : Start classifiers testing by bootstrap\nClassifiers testing by bootstrap: 100%|\u001b[34m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 100/100 [00:00&lt;00:00, 172.04it/s]\n\n\n\n\n\n\n2023-01-27 00:30:41 abstract_overall_variance_analyzer.py INFO    : Successfully tested classifiers by bootstrap\n2023-01-27 00:30:45 abstract_overall_variance_analyzer.py INFO    : Successfully computed predict proba metrics\n\n\nSubgroups statistical bias and variance metrics:\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  Metric overall sex_priv sex_dis race_priv race_dis sex&amp;race_priv sex&amp;race_dis Model_Seed 0 General_Ensemble_Accuracy 0.667614 0.696262 0.660333 0.647619 0.680818 0.655914 0.669903 623 1 Mean 0.524041 0.558159 0.515370 0.577132 0.488981 0.594027 0.479205 623 2 Std 0.074339 0.075245 0.074109 0.069531 0.077515 0.076792 0.078327 623 3 IQR 0.087800 0.082206 0.089221 0.079059 0.093572 0.078600 0.095591 623 4 Entropy 0.192341 0.180935 0.195240 0.161033 0.213016 0.171565 0.218861 623 5 Jitter 0.120997 0.111341 0.123452 0.101976 0.133559 0.105502 0.137724 623 6 Per_Sample_Accuracy 0.656477 0.695280 0.646615 0.645976 0.663412 0.653763 0.648427 623 7 Label_Stability 0.837727 0.851682 0.834181 0.861095 0.822296 0.860645 0.817010 623 8 TPR 0.631356 0.618421 0.633838 0.472050 0.713826 0.472222 0.708487 623 9 TNR 0.696918 0.739130 0.683857 0.756757 0.649231 0.771930 0.627049 623 10 PPV 0.627368 0.566265 0.640306 0.546763 0.660714 0.566667 0.678445 623 11 FNR 0.368644 0.381579 0.366162 0.527950 0.286174 0.527778 0.291513 623 12 FPR 0.303082 0.260870 0.316143 0.243243 0.350769 0.228070 0.372951 623 13 Accuracy 0.667614 0.696262 0.660333 0.647619 0.680818 0.655914 0.669903 623 14 F1 0.629356 0.591195 0.637056 0.506667 0.686244 0.515152 0.693141 623 15 Selection-Rate 0.449811 0.387850 0.465558 0.330952 0.528302 0.322581 0.549515 623 16 Positive-Rate 1.006356 1.092105 0.989899 0.863354 1.080386 0.833333 1.044280 623"},{"location":"examples/Interfaces_With_Config_Usage_Example/#get-metrics-for-a-list-of-models-with-a-run_metrics_computation_with_config-interface-and-input-arguments-as-a-config","title":"Get metrics for a list of models with a run_metrics_computation_with_config interface and input arguments as a config","text":"<pre><code>models_metrics_dct = run_metrics_computation_with_config(dataset, config, models_config, SAVE_RESULTS_DIR_PATH, debug_mode=True)\n</code></pre> <pre><code>Analyze models in one run:   0%|\u001b[31m          \u001b[0m| 0/2 [00:00&lt;?, ?it/s]\n\n##############################  [Model 1 / 2] Analyze DecisionTreeClassifier  ##############################\nModel random_state:  491\nBaseline X_train shape:  (4222, 9)\nBaseline X_test shape:  (1056, 9)\n\nProtected groups splits:\nsex_priv (194, 11)\nsex_dis (862, 11)\nrace_priv (394, 11)\nrace_dis (662, 11)\nsex&amp;race_priv (98, 11)\nsex&amp;race_dis (566, 11)\n\n\nTop rows of processed X train + validation set:\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  juv_fel_count age_cat_25 - 45_1 juv_misd_count juv_other_count age_cat_Greater than 45_1 age_cat_Greater than 45_0 age_cat_Less than 25_0 c_charge_degree_F_0 c_charge_degree_F_1 age_cat_25 - 45_0 c_charge_degree_M_1 c_charge_degree_M_0 priors_count age_cat_Less than 25_1 2244 -0.093219 1 -0.130521 -0.154060 0 1 1 0 1 0 0 1 -0.083401 0 2266 -0.093219 1 -0.130521 -0.154060 0 1 1 0 1 0 0 1 0.862647 0 78 -0.093219 1 -0.130521 -0.154060 0 1 1 0 1 0 0 1 -0.651029 0 3323 -0.093219 0 -0.130521 -0.154060 1 0 1 0 1 1 0 1 -0.651029 0 1127 -4.406383 1 -0.130521 3.785338 0 1 1 0 1 0 0 1 -4.495524 0 4793 -0.093219 1 -0.130521 -0.154060 0 1 1 0 1 0 0 1 0.862647 0 3494 -0.093219 1 -0.130521 -0.154060 0 1 1 1 0 0 1 0 -0.083401 0 1289 -0.250909 1 1.104331 3.785338 0 1 1 0 1 0 0 1 2.085418 0 1530 -0.093219 1 -0.130521 -0.154060 0 1 1 0 1 0 0 1 0.295018 0 1636 -0.093219 0 -0.130521 -0.154060 1 0 1 0 1 1 0 1 0.314656 0 <pre><code>2023-01-27 00:30:55 abstract_overall_variance_analyzer.py INFO    : Start classifiers testing by bootstrap\n\nClassifiers testing by bootstrap: 100%|\u001b[34m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 100/100 [00:00&lt;00:00, 159.14it/s]\n\n\n\n\n\n\n2023-01-27 00:30:55 abstract_overall_variance_analyzer.py INFO    : Successfully tested classifiers by bootstrap\n2023-01-27 00:31:00 abstract_overall_variance_analyzer.py INFO    : Successfully computed predict proba metrics\n\n\n\n[DecisionTreeClassifier] Metrics matrix:\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  Metric overall sex_priv sex_dis race_priv race_dis sex&amp;race_priv sex&amp;race_dis Model_Seed Model_Name 0 General_Ensemble_Accuracy 0.703598 0.670103 0.711137 0.677665 0.719033 0.642857 0.722615 491 DecisionTreeClassifier 1 Mean 0.521465 0.567385 0.511131 0.582361 0.485222 0.593591 0.475824 491 DecisionTreeClassifier 2 Std 0.073755 0.080002 0.072348 0.068526 0.076867 0.081239 0.076549 491 DecisionTreeClassifier 3 IQR 0.080800 0.088440 0.079081 0.076076 0.083612 0.083395 0.081920 491 DecisionTreeClassifier 4 Entropy 0.219793 0.233656 0.216673 0.189168 0.238021 0.206719 0.234097 491 DecisionTreeClassifier 5 Jitter 0.139630 0.148898 0.137544 0.120191 0.151199 0.129994 0.148316 491 DecisionTreeClassifier 6 Per_Sample_Accuracy 0.681420 0.680670 0.681589 0.677589 0.683701 0.690612 0.685936 491 DecisionTreeClassifier 7 Label_Stability 0.807689 0.795155 0.810510 0.832640 0.792840 0.826939 0.797951 491 DecisionTreeClassifier 8 TPR 0.679359 0.523077 0.702765 0.523490 0.745714 0.433333 0.761905 491 DecisionTreeClassifier 9 TNR 0.725314 0.744186 0.719626 0.771429 0.689103 0.735294 0.673307 491 DecisionTreeClassifier 10 PPV 0.689024 0.507463 0.717647 0.582090 0.729050 0.419355 0.745342 491 DecisionTreeClassifier 11 FNR 0.320641 0.476923 0.297235 0.476510 0.254286 0.566667 0.238095 491 DecisionTreeClassifier 12 FPR 0.274686 0.255814 0.280374 0.228571 0.310897 0.264706 0.326693 491 DecisionTreeClassifier 13 Accuracy 0.703598 0.670103 0.711137 0.677665 0.719033 0.642857 0.722615 491 DecisionTreeClassifier 14 F1 0.684157 0.515152 0.710128 0.551237 0.737288 0.426230 0.753532 491 DecisionTreeClassifier 15 Selection-Rate 0.465909 0.345361 0.493039 0.340102 0.540785 0.316327 0.568905 491 DecisionTreeClassifier 16 Positive-Rate 0.985972 1.030769 0.979263 0.899329 1.022857 1.033333 1.022222 491 DecisionTreeClassifier <pre><code>Analyze models in one run:  50%|\u001b[31m\u2588\u2588\u2588\u2588\u2588     \u001b[0m| 1/2 [00:15&lt;00:15, 15.99s/it]\n\n\n\n\n\n##############################  [Model 2 / 2] Analyze LogisticRegression  ##############################\nModel random_state:  492\nBaseline X_train shape:  (4222, 9)\nBaseline X_test shape:  (1056, 9)\n\nProtected groups splits:\nsex_priv (223, 11)\nsex_dis (833, 11)\nrace_priv (402, 11)\nrace_dis (654, 11)\nsex&amp;race_priv (107, 11)\nsex&amp;race_dis (538, 11)\n\n\nTop rows of processed X train + validation set:\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  juv_fel_count age_cat_25 - 45_1 juv_misd_count juv_other_count age_cat_Greater than 45_1 age_cat_Greater than 45_0 age_cat_Less than 25_0 c_charge_degree_F_0 c_charge_degree_F_1 age_cat_25 - 45_0 c_charge_degree_M_1 c_charge_degree_M_0 priors_count age_cat_Less than 25_1 658 -0.089367 1 -0.125836 -0.146782 0 1 1 0 1 0 0 1 -0.648738 0 4149 -0.089367 0 -0.125836 -0.146782 0 1 0 0 1 1 0 1 -0.270758 1 2851 -0.089367 1 -0.125836 -0.146782 0 1 1 0 1 0 0 1 -0.459748 0 1331 -0.089367 1 -0.125836 -0.146782 0 1 1 0 1 0 0 1 -0.648738 0 3069 -0.089367 1 -0.125836 -0.146782 0 1 1 0 1 0 0 1 -0.270758 0 4670 -0.089367 1 -0.125836 -0.146782 0 1 1 0 1 0 0 1 -0.270758 0 616 -0.089367 0 -0.125836 -0.146782 0 1 0 0 1 1 0 1 -0.270758 1 4413 -0.089367 0 -0.125836 -0.146782 0 1 0 0 1 1 0 1 -0.270758 1 1244 -0.089367 0 -0.125836 -0.146782 1 0 1 0 1 1 0 1 -0.606756 0 947 -0.089367 0 -0.125836 -0.146782 0 1 0 0 1 1 0 1 0.863181 1 <pre><code>2023-01-27 00:31:11 abstract_overall_variance_analyzer.py INFO    : Start classifiers testing by bootstrap\n\nClassifiers testing by bootstrap: 100%|\u001b[34m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 100/100 [00:05&lt;00:00, 19.17it/s]\n\n\n\n\n\n\n2023-01-27 00:31:16 abstract_overall_variance_analyzer.py INFO    : Successfully tested classifiers by bootstrap\n2023-01-27 00:31:19 abstract_overall_variance_analyzer.py INFO    : Successfully computed predict proba metrics\n\n\n\n[LogisticRegression] Metrics matrix:\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  Metric overall sex_priv sex_dis race_priv race_dis sex&amp;race_priv sex&amp;race_dis Model_Seed Model_Name 0 General_Ensemble_Accuracy 0.679924 0.659193 0.685474 0.671642 0.685015 0.672897 0.693309 492 LogisticRegression 1 Mean 0.520122 0.565524 0.507968 0.581083 0.482651 0.606199 0.472871 492 LogisticRegression 2 Std 0.021520 0.019211 0.022138 0.019990 0.022461 0.019146 0.023148 492 LogisticRegression 3 IQR 0.027995 0.024942 0.028812 0.026028 0.029203 0.024786 0.030091 492 LogisticRegression 4 Entropy 0.079387 0.000000 0.080575 0.077811 0.000000 0.000000 0.000000 492 LogisticRegression 5 Jitter 0.049364 0.045588 0.050375 0.049021 0.049576 0.040521 0.049428 492 LogisticRegression 6 Per_Sample_Accuracy 0.674536 0.652108 0.680540 0.664229 0.680872 0.663364 0.689312 492 LogisticRegression 7 Label_Stability 0.935625 0.942422 0.933806 0.934527 0.936300 0.946168 0.935725 492 LogisticRegression 8 TPR 0.632860 0.455696 0.666667 0.440000 0.717201 0.323529 0.741611 492 LogisticRegression 9 TNR 0.721137 0.770833 0.704057 0.809524 0.649518 0.835616 0.633333 492 LogisticRegression 10 PPV 0.665245 0.521739 0.690000 0.578947 0.692958 0.478261 0.715210 492 LogisticRegression 11 FNR 0.367140 0.544304 0.333333 0.560000 0.282799 0.676471 0.258389 492 LogisticRegression 12 FPR 0.278863 0.229167 0.295943 0.190476 0.350482 0.164384 0.366667 492 LogisticRegression 13 Accuracy 0.679924 0.659193 0.685474 0.671642 0.685015 0.672897 0.693309 492 LogisticRegression 14 F1 0.648649 0.486486 0.678133 0.500000 0.704871 0.385965 0.728171 492 LogisticRegression 15 Selection-Rate 0.444129 0.309417 0.480192 0.283582 0.542813 0.214953 0.574349 492 LogisticRegression 16 Positive-Rate 0.951318 0.873418 0.966184 0.760000 1.034985 0.676471 1.036913 492 LogisticRegression <pre><code>Analyze models in one run: 100%|\u001b[31m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 2/2 [00:34&lt;00:00, 17.23s/it]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"examples/Interfaces_With_Function_Args_Usage_Example/","title":"Interfaces With Function Args Usage Example","text":"<pre><code>%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n</code></pre> <pre><code>import os\nimport warnings\nwarnings.filterwarnings('ignore')\nos.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n</code></pre> <pre><code>cur_folder_name = os.getcwd().split('/')[-1]\nif cur_folder_name != \"fairness-variance\":\n    os.chdir(\"..\")\n\nprint('Current location: ', os.getcwd())\n</code></pre> <pre><code>Current location:  /home/denys_herasymuk/UCU/4course_2term/Bachelor_Thesis/Code/fairness-variance\n</code></pre>"},{"location":"examples/Interfaces_With_Function_Args_Usage_Example/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nimport pandas as pd\nfrom datetime import datetime, timezone\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom source.custom_initializers import create_config_obj\nfrom source.custom_classes.data_loaders import CompasWithoutSensitiveAttrsDataset\nfrom source.metrics_computation_interfaces import run_metrics_computation, compute_model_metrics\n</code></pre>"},{"location":"examples/Interfaces_With_Function_Args_Usage_Example/#configs","title":"Configs","text":"<pre><code>config = create_config_obj(config_yaml_path=os.path.join('configs', 'experiment1_compas_config.yaml'))\nSAVE_RESULTS_DIR_PATH = os.path.join('results', 'hypothesis_space',\n                                     f'{config.dataset_name}_Metrics_{datetime.now(timezone.utc).strftime(\"%Y%m%d__%H%M%S\")}')\n</code></pre> <pre><code>models_config = {\n    'DecisionTreeClassifier': DecisionTreeClassifier(criterion='gini',\n                                                     max_depth=20,\n                                                     max_features=0.6,\n                                                     min_samples_split=0.1),\n    'LogisticRegression': LogisticRegression(C=1,\n                                             max_iter=50,\n                                             penalty='l2',\n                                             solver='newton-cg'),\n}\n</code></pre>"},{"location":"examples/Interfaces_With_Function_Args_Usage_Example/#load-dataset","title":"Load dataset","text":"<pre><code>dataset = CompasWithoutSensitiveAttrsDataset(dataset_path='data/COMPAS.csv')\ndataset.X_data.head()\n</code></pre> juv_fel_count juv_misd_count juv_other_count priors_count age_cat_25 - 45 age_cat_Greater than 45 age_cat_Less than 25 c_charge_degree_F c_charge_degree_M 0 0.0 -2.340451 1.0 -15.010999 1 0 0 0 1 1 0.0 0.000000 0.0 0.000000 1 0 0 1 0 2 0.0 0.000000 0.0 0.000000 0 0 1 1 0 3 0.0 0.000000 0.0 6.000000 1 0 0 0 1 4 0.0 0.000000 0.0 7.513697 1 0 0 1 0"},{"location":"examples/Interfaces_With_Function_Args_Usage_Example/#get-metrics-for-a-base-model-with-a-compute_model_metrics-function-and-input-arguments","title":"Get metrics for a base model with a compute_model_metrics function and input arguments","text":"<pre><code>model_name = 'DecisionTreeClassifier'\nmetrics_df = compute_model_metrics(models_config[model_name], config.n_estimators,\n                                   dataset, config.test_set_fraction,\n                                   config.bootstrap_fraction, config.sensitive_attributes_dct,\n                                   model_seed=101,\n                                   dataset_name=config.dataset_name,\n                                   base_model_name=model_name,\n                                   save_results=True,\n                                   save_results_dir_path=SAVE_RESULTS_DIR_PATH,\n                                   debug_mode=False)\nprint('Subgroups statistical bias and variance metrics: ')\nmetrics_df\n</code></pre> <pre><code>Model random_state:  101\nBaseline X_train shape:  (4222, 9)\nBaseline X_test shape:  (1056, 9)\n\n\n\n\n2023-01-27 00:50:32 abstract_overall_variance_analyzer.py INFO    : Start classifiers testing by bootstrap\nClassifiers testing by bootstrap: 100%|\u001b[34m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 100/100 [00:00&lt;00:00, 176.21it/s]\n\n\n\n\n\n\n2023-01-27 00:50:32 abstract_overall_variance_analyzer.py INFO    : Successfully tested classifiers by bootstrap\n2023-01-27 00:50:36 abstract_overall_variance_analyzer.py INFO    : Successfully computed predict proba metrics\n\n\nSubgroups statistical bias and variance metrics:\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  Metric overall sex_priv sex_dis race_priv race_dis sex&amp;race_priv sex&amp;race_dis Model_Seed 0 General_Ensemble_Accuracy 0.679924 0.693467 0.676779 0.669789 0.686804 0.659091 0.679537 101 1 Mean 0.525578 0.561690 0.517193 0.590746 0.481339 0.589092 0.468776 101 2 Std 0.071635 0.078384 0.070068 0.069772 0.072900 0.088442 0.073433 101 3 IQR 0.089278 0.096598 0.087578 0.090645 0.088350 0.113437 0.089443 101 4 Entropy 0.000000 0.216088 0.000000 0.000000 0.207275 0.219363 0.205943 101 5 Jitter 0.122677 0.139908 0.118676 0.109246 0.131795 0.141933 0.130401 101 6 Per_Sample_Accuracy 0.662689 0.684724 0.657573 0.659180 0.665072 0.649773 0.654923 101 7 Label_Stability 0.830152 0.798191 0.837573 0.843794 0.820890 0.792727 0.824826 101 8 TPR 0.622177 0.557143 0.633094 0.459119 0.701220 0.440000 0.713781 101 9 TNR 0.729350 0.767442 0.718182 0.794776 0.671096 0.746032 0.638298 101 10 PPV 0.663020 0.565217 0.680412 0.570312 0.699088 0.407407 0.703833 101 11 FNR 0.377823 0.442857 0.366906 0.540881 0.298780 0.560000 0.286219 101 12 FPR 0.270650 0.232558 0.281818 0.205224 0.328904 0.253968 0.361702 101 13 Accuracy 0.679924 0.693467 0.676779 0.669789 0.686804 0.659091 0.679537 101 14 F1 0.641949 0.561151 0.655901 0.508711 0.700152 0.423077 0.708772 101 15 Selection-Rate 0.432765 0.346734 0.452742 0.299766 0.523052 0.306818 0.554054 101 16 Positive-Rate 0.938398 0.985714 0.930456 0.805031 1.003049 1.080000 1.014134 101"},{"location":"examples/Interfaces_With_Function_Args_Usage_Example/#get-metrics-for-a-list-of-models-with-a-run_metrics_computation-function-and-input-arguments","title":"Get metrics for a list of models with a run_metrics_computation function and input arguments","text":"<pre><code>models_metrics_dct = run_metrics_computation(dataset, config.test_set_fraction, config.bootstrap_fraction,\n                                             config.dataset_name, models_config, config.n_estimators,\n                                             config.sensitive_attributes_dct,\n                                             model_seed=200,\n                                             save_results_dir_path=SAVE_RESULTS_DIR_PATH,\n                                             save_results=True,\n                                             debug_mode=False)\n</code></pre> <pre><code>Analyze models in one run:   0%|\u001b[31m          \u001b[0m| 0/2 [00:00&lt;?, ?it/s]\n\n##############################  [Model 1 / 2] Analyze DecisionTreeClassifier  ##############################\nModel random_state:  201\nBaseline X_train shape:  (4222, 9)\nBaseline X_test shape:  (1056, 9)\n\n\n\n\n2023-01-27 00:50:44 abstract_overall_variance_analyzer.py INFO    : Start classifiers testing by bootstrap\n\nClassifiers testing by bootstrap: 100%|\u001b[34m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 100/100 [00:00&lt;00:00, 205.15it/s]\n\n\n\n\n\n\n2023-01-27 00:50:44 abstract_overall_variance_analyzer.py INFO    : Successfully tested classifiers by bootstrap\n2023-01-27 00:50:47 abstract_overall_variance_analyzer.py INFO    : Successfully computed predict proba metrics\nAnalyze models in one run:  50%|\u001b[31m\u2588\u2588\u2588\u2588\u2588     \u001b[0m| 1/2 [00:11&lt;00:11, 11.13s/it]\n\n\n\n\n\n##############################  [Model 2 / 2] Analyze LogisticRegression  ##############################\nModel random_state:  202\nBaseline X_train shape:  (4222, 9)\nBaseline X_test shape:  (1056, 9)\n\n\n\n\n2023-01-27 00:50:55 abstract_overall_variance_analyzer.py INFO    : Start classifiers testing by bootstrap\n\nClassifiers testing by bootstrap: 100%|\u001b[34m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 100/100 [00:05&lt;00:00, 19.05it/s]\n\n\n\n\n\n\n2023-01-27 00:51:00 abstract_overall_variance_analyzer.py INFO    : Successfully tested classifiers by bootstrap\n2023-01-27 00:51:03 abstract_overall_variance_analyzer.py INFO    : Successfully computed predict proba metrics\nAnalyze models in one run: 100%|\u001b[31m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 2/2 [00:27&lt;00:00, 13.55s/it]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"examples/cloning-and-mutating/","title":"Cloning and mutating","text":"<p>Sometimes you might want to reset a model, or edit (what we call mutate) its attributes. This can be useful in an online environment. Indeed, if you detect a drift, then you might want to mutate a model's attributes. Or if you see that a performance's model is plummeting, then you might to reset it to its \"factory settings\".</p> <p>Anyway, this is not to convince you, but rather to say that a model's attributes don't have be to set in stone throughout its lifetime. In particular, if you're developping your own model, then you might want to have good tools to do this. This is what this recipe is about.</p>"},{"location":"examples/cloning-and-mutating/#cloning","title":"Cloning","text":"<p>The first thing you can do is clone a model. This creates a deep copy of the model. The resulting model is entirely independent of the original model. The clone is fresh, in the sense that it is as if it hasn't seen any data.</p> <p>For instance, say you have a linear regression model which you have trained on some data.</p> <pre><code>from river import datasets, linear_model, optim, preprocessing\n\nmodel = (\n    preprocessing.StandardScaler() |\n    linear_model.LinearRegression(\n        optimizer=optim.SGD(3e-2)\n    )\n)\n\nfor x, y in datasets.TrumpApproval():\n    model.predict_one(x)\n    model.learn_one(x, y)\n\nmodel[-1].weights\n</code></pre> <pre>\n{\n    'ordinal_date': 20.59955380229643,\n    'gallup': 0.39114944304212645,\n    'ipsos': 0.4101918314868111,\n    'morning_consult': 0.12042970179504908,\n    'rasmussen': 0.18951231512561392,\n    'you_gov': 0.04991712783831687\n}\n</pre> <p>For whatever reason, we may want to clone this model. This can be done with the <code>clone</code> method.</p> <pre><code>clone = model.clone()\nclone[-1].weights\n</code></pre> <pre>{}\n</pre> <p>As we can see, there are no weights because the clone is fresh copy that has not seen any data. However, the learning rate we specified is preserved.</p> <pre><code>clone[-1].optimizer.learning_rate\n</code></pre> <pre>0.03\n</pre> <p>You may also specify parameters you want changed. For instance, let's say we want to clone the model, but we want to change the optimizer:</p> <pre><code>clone = model.clone({\"LinearRegression\": {\"optimizer\": optim.Adam()}})\nclone[-1].optimizer\n</code></pre> <pre>Adam({'lr': Constant({'learning_rate': 0.1}), 'n_iterations': 0, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'm': None, 'v': None})\n</pre> <p>The first key indicates that we want to specify a different parameter for the <code>LinearRegression</code> part of the pipeline. Then the second key accesses the linear regression's <code>optimizer</code> parameter.</p> <p>Finally, note that the <code>clone</code> method isn't reserved to models. Indeed, every object in River has it. That's because they all inherit from the <code>Base</code> class in the <code>base</code> module.</p>"},{"location":"examples/cloning-and-mutating/#mutating-attributes","title":"Mutating attributes","text":"<p>Cloning a model can be useful, but the fact that it essentially resets the model may not be desired. Instead, you might want to change a attribute while preserving the model's state. For example, let's change the <code>l2</code> attribute, and the optimizer's <code>lr</code> attribute.</p> <pre><code>model.mutate({\n    \"LinearRegression\": {\n        \"l2\": 0.1,\n        \"optimizer\": {\n            \"lr\": optim.schedulers.Constant(25e-3)\n        }\n    }\n})\n\nprint(repr(model))\n</code></pre> <pre><code>Pipeline (\n  StandardScaler (\n    with_std=True\n  ),\n  LinearRegression (\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.025\n      )\n    )\n    loss=Squared ()\n    l2=0.1\n    l1=0.\n    intercept_init=0.\n    intercept_lr=Constant (\n      learning_rate=0.01\n    )\n    clip_gradient=1e+12\n    initializer=Zeros ()\n  )\n)\n</code></pre> <p>We can see the attributes we specified have changed. However, the model's state is preserved:</p> <pre><code>model[-1].weights\n</code></pre> <pre>\n{\n    'ordinal_date': 20.59955380229643,\n    'gallup': 0.39114944304212645,\n    'ipsos': 0.4101918314868111,\n    'morning_consult': 0.12042970179504908,\n    'rasmussen': 0.18951231512561392,\n    'you_gov': 0.04991712783831687\n}\n</pre> <p>In other words, the <code>mutate</code> method does not create a deep copy of the model. It just sets attributes. At this point you may ask:</p> <p>Why can't I just change the attribute directly, without calling <code>mutate</code>?</p> <p>The answer is that you're free to do proceed as such, but it's not the way we recommend. The <code>mutate</code> method is safer, in that it prevents you from mutating attributes you shouldn't be touching. We call these immutable attributes. For instance, there's no reason you should be modifying the weights.</p> <pre><code>try:\n    model.mutate({\n        \"LinearRegression\": {\n            \"weights\": \"this makes no sense\"\n        }\n    })\nexcept ValueError as e:\n    print(e)\n</code></pre> <pre><code>'weights' is not a mutable attribute of LinearRegression\n</code></pre> <p>All attributes are immutable by default. Under the hood, each model can specify a set of mutable attributes via the <code>_mutable_attributes</code> property. In theory this can be overriden. But the general idea is that we will progressively add more and more mutable attributes with time.</p> <p>And that concludes this recipe. Arguably, this recipe caters to advanced users, and in particular users who are developping their own models. And yet, one could also argue that modifying parameters of a model on-the-fly is a great tool to have at your disposal when you're doing online machine learning.</p>"}]}